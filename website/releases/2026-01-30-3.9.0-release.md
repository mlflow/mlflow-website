---
title: "MLflow 3.9.0 Highlights: AI Assistant, Dashboards, and Judge Optimization"
sidebar_label: MLflow 3.9.0
slug: 3.9.0
authors: [mlflow-maintainers]
---

MLflow 3.9.0 is a major release focused on AI Observability and Evaluation capabilities, bringing powerful new features for building, monitoring, and optimizing AI agents. This release introduces an AI-powered assistant, comprehensive dashboards for agent performance, a new judge optimization algorithm, judge builder UI, continuous monitoring with LLM judges, and distributed tracing.

## 1. MLflow Assistant Powered by Claude Code

<video controls autoplay muted loop playsinline width="100%">
  <source src={require("/img/releases/3.9.0/mlflow_assistant.mp4").default} type="video/mp4" />
</video>

MLflow Assistant transforms coding agents like Claude Code into experienced AI engineers by your side. Unlike typical chatbots, the assistant is **aware of your codebase and context**—it's not just a Q&A tool, but a full-fledged AI engineer that can find root causes for issues, set up quality tests, and apply LLMOps best practices to your project.

Key capabilities include:

- **No additional costs**: Use your existing Claude Code subscription. MLflow provides the knowledge and integration at no cost.
- **Context-rich assistance**: Understands your local codebase, project structure, and provides tailored recommendations—not generic advice.
- **Complete dev-loop**: Goes beyond Q&A to fetch MLflow data, read your code, and add tracing, evaluation, and versioning to your project.
- **Fully customizable**: Add custom skills, sub-agents, and permissions. Everything runs on your machine with full transparency.

Open the MLflow UI, navigate to the Assistant panel in any experiment page, and follow the setup wizard to get started.

[Learn more about MLflow Assistant](https://mlflow.org/docs/latest/genai/getting-started/try-assistant/)

## 2. Dashboards for Agent Performance Metrics

<video controls autoplay muted loop playsinline width="100%">
  <source src="/blog/mlflow-agent-dashboard/overview_demo.mp4" type="video/mp4" />
</video>

A new "Overview" tab in GenAI experiments provides pre-built charts and visualizations for monitoring agent performance at a glance. Monitor key metrics like latency, request counts, and quality scores without manual configuration. Identify performance trends and anomalies across your agent deployments, and get tool call summaries to understand how your agents are utilizing available tools.

Navigate to any GenAI experiment and click the "Overview" tab to access the dashboard. Charts are automatically populated based on your trace data. Have a specific visualization need? Request additional charts via [GitHub Issues](https://github.com/mlflow/mlflow/issues).

[Learn more about GenAI Dashboards](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces/ui/)

## 3. MemAlign: A New Judge Optimizer Algorithm

MemAlign is a new optimization algorithm that learns evaluation guidelines from past feedback and dynamically retrieves relevant examples at runtime. Improve judge accuracy by learning from human feedback patterns, reduce prompt engineering effort with automatic guideline extraction, and adapt judge behavior dynamically based on the input being evaluated.

Use the `MemAlignOptimizer` to optimize your judges with historical feedback:

```python
from mlflow.genai.judges.optimize import MemAlignOptimizer

# Initialize optimizer with your judge
optimizer = MemAlignOptimizer(
    judge=my_custom_judge,
    feedback_dataset="human_feedback_traces",
)

# Train on historical feedback
optimizer.fit()

# The optimized judge now retrieves relevant guidelines dynamically
optimized_judge = optimizer.get_optimized_judge()

# Use in evaluation
result = optimized_judge.evaluate(trace)
```

[Learn more about MemAlign](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/memalign/)

## 4. Configuring and Building a Judge with Judge Builder UI

<video controls autoplay muted loop playsinline width="100%">
  <source src={require("/img/releases/3.9.0/judge_builder_ui.mp4").default} type="video/mp4" />
</video>

A new visual interface lets you create and test custom LLM judge prompts without writing code. Iterate quickly on judge criteria and scoring rubrics with immediate feedback, test judges on sample traces before deploying to production, and export validated judges to the Python SDK for programmatic integration.

Navigate to the "Judges" section in the MLflow UI and click "Create Judge." Define your evaluation criteria, scoring rubric, and test your judge against sample traces. Once satisfied, export the configuration to use with the MLflow SDK.

[Learn more about Judge Builder](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge/)

## 5. Continuous Online Monitoring with MLflow LLM Judges

Automatically run LLM judges on incoming traces without writing any code, enabling continuous quality monitoring of your agents in production. Detect quality issues in real-time as traces flow through your system, leverage pre-defined judges for common evaluations like safety, relevance, groundedness, and correctness, and get actionable assessments attached directly to your traces.

Go to the "Judges" tab in your experiment, select from pre-defined judges or use your custom judges, and configure which traces to evaluate. Assessments are automatically attached to matching traces as they arrive.

[Learn more about Agent Evaluation](https://mlflow.org/docs/latest/genai/eval-monitor/)

## 6. Distributed Tracing for Tracking End-to-end Requests

Track requests across multiple services with context propagation, enabling end-to-end visibility into distributed AI systems. Maintain trace continuity across microservices and external API calls, debug issues that span multiple services with a unified trace view, and understand latency and errors at each step of your distributed pipeline.

Use the `mlflow.tracing.distributed` module to inject and extract trace context:

```python
from mlflow.tracing.distributed import inject_trace_context, extract_trace_context
import requests

# Service A: Inject context into outgoing request
def call_downstream_service(data):
    headers = {}
    inject_trace_context(headers)
    response = requests.post(
        "http://service-b/process",
        json=data,
        headers=headers,
    )
    return response.json()

# Service B: Extract context from incoming request
def process_request(request):
    extract_trace_context(request.headers)
    # Continue tracing in this service
    with mlflow.start_span(name="process") as span:
        result = do_processing(request.json)
    return result
```

[Learn more about Distributed Tracing](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/distributed-tracing/)

## Full Changelog

For a comprehensive list of changes, see the [release change log](https://github.com/mlflow/mlflow/releases/tag/v3.9.0).

## What's Next

### Get Started

Install MLflow 3.9.0 to try these new features:

```bash
pip install mlflow==3.9.0
```

### Share Your Feedback

We'd love to hear about your experience with these new features:

- [GitHub Issues](https://github.com/mlflow/mlflow/issues) - Report bugs or request features
- [MLflow Roadmap](https://github.com/mlflow/mlflow/discussions/19855) - See what's coming next and share your ideas
- ⭐ [Star us on GitHub](https://github.com/mlflow/mlflow) - Show your support for the project

### Learn More

- [Join our upcoming webinar](https://luma.com/mlflow-204) to see these features in action
- Check out the [MLflow documentation](https://mlflow.org/docs/latest) for detailed guides
