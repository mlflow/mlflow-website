---
title: "5 Things AI Engineers Should Know About Coding Agent and MLflow"
description: Discover how MLflow brings observability, evaluation, and AI-powered assistance to Claude Code - turning your coding agent from a black box into a transparent, testable tool.
slug: mlflow-claude-code
authors: [yuki-watanabe]
tags: [genai, evaluation, tracing, claude-code]
thumbnail: /img/blog/mlflow-claude-code-thumbnail.png
image: /img/blog/mlflow-claude-code-thumbnail.png
---

Claude Code is a powerful coding agent that help you write code, automate workflow, design features, and so much more. It has been disrupting the software engineering process and becoming an indispensable tool for developers.

[MLflow](https://mlflow.org/), an the open-source platform for adding observability and control into AI/ML workflows. In this blog, we will show you how to enhance your Claude Code experience with MLflow and 10x your productivity as an AI engineer.

## 1. Tracing and Monitoring Claude Code

Claude Code is powerful, but it can also be a black box. You give it a task, it gives you a result, but you have no visibility into how it got there. But **what did it actually do under the hood?** How many tokens did it burn? Which tools did it call, and in what order? If you're using customization knobs like skills, sub-agents, batch operations, how can you prove they are actually used and improve the output?

You could scroll through the conversation history, compare between the previous and the current result, however, it is very inefficient.

Here is where MLflow comes in. MLflow's [Tracing](https://mlflow.org/docs/latest/genai/tracing/index.html) capability gives you full transparency into Claude Code sessions, tools to systematically evaluate agent outputs. Furthermore, it connects such data back into Claude Code itself, so that is can do the heavy lifting for you. Here are three things you can add to your Claude Code workflow to make it even more powerful.

To enable tracing for your Claude Code sessions, run a single command in your project.

```bash
mlflow autolog claude
```

That's it. Run this and use Claude Code as you normally would, and MLflow captures things like input text passed to Claude Code, context change during the session, tool usage, token counts, latency, and more. Every interaction becomes a structured trace you can search, filter, and analyze. This works for both CLI sessions and the SDK (programmatic usage).

(To setup globally at user level, setup the hook manually by following the [guide](https://mlflow.org/docs/latest/genai/tracing/claude-code/index.html#setup) here)

![Claude Code Trace Screenshot](/img/blog/claude-code-trace.png)

Important statistics and metadata are automatically extracted and logged as trace attributes, making it easy to monitor key metrics like token usage, latency, and tool calls across all your sessions. You can even set up custom dashboards to track these metrics over time.

![Claude Code Trace Dashboard Screenshot](/img/blog/tool-call-dashboard.png)

For more detailed setup instructions, see the [docs](https://mlflow.org/docs/latest/genai/tracing/claude-code/index.html).

### 2. Systematic Evaluation for Claude Code Operations

Tracing Claude Code unlocks another powerful capability: testing and evaluation. Claude Code is highly customizable, you can define your own skills and sub-agents to fit your needs. However, many people just add more and more tools, skills, commands, without actually testing whether they are useful or not. The more complex the setup becomes, the harder it is to maintain them and ensure a change does not break the existing functionality.

With MLflow, you can run arbitrary testing criteria against Claude Code by using its [Trace Evaluation](https://mlflow.org/docs/latest/genai/eval-monitor/running-evaluation/traces/) capability. Once traces are recoded into MLflow by the `mlflow autolog claude` command, you can run run various built-in scorers, LLM judges, and custom code scorers.


```python
@scorer
def tool_recall(trace, expectations) -> Feedback:
    """
    Check if expected tools were used in the trace.
    Returns recall: (tools used that were expected) / (total expected tools)
    """
    expected_tools = set(expectations.get("expected_tools", []))

    # Get actual tools from trace spans
    tool_spans = trace.search_spans(span_type="TOOL")
    actual_tools = {span.name.replace("tool_", "") for span in tool_spans}

    # Calculate recall
    matched = expected_tools & actual_tools
    recall = len(matched) / len(expected_tools)
    return Feedback(value=recall, rationale=f"Expected: {expected_tools}, Found: {actual_tools}")

@scorer
def num_permission_block(trace) -> int:
    """
    Check how many times the agent encountered permission block.
    """
    permission_block = 0
    for span in trace.search_spans(span_type="TOOL"):
        if span.outputs and isinstance(span.outputs, dict) and "result" in span.outputs:
            result = span.outputs["result"]
            if "requires approval" in result.lower() or "was blocked." in result.lower():
                permission_block += 1
    return permission_block
```



```python
import mlflow
from mlflow.genai.scorers import ConversationCompleteness, RelevanceToQuery

# Load generated Claude Code traces from MLflow
traces = mlflow.search_traces(experiment_ids=["<experiment-id>"], max_results=20)

# Run evaluation against the traces
results = mlflow.genai.evaluate(
    data=traces,
    scorers=[
        tool_recall,
        num_permission_block,
        ConversationCompleteness(),
        RelevanceToQuery(),
    ],
)
```

<img
  src={require("./eval-results.png").default}
  alt="MLflow evaluation results showing LLM-judge assessments with pass/fail metrics for agent outputs"
  width="100%"
/>

Each agent output is assessed across multiple criteria, with clear indicators and detailed rationale from the judge. This is especially powerful for **skills and sub-agents that run unattended**. You build the automation, define evaluation criteria, and MLflow tells you whether it's working reliably across every dimension you care about.




## 3. Claude Code can use MLflow as a Tool

### MLflow MCP Server

For programmatic access, MLflow exposes 9 trace management tools via the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). Add this to your `.claude/settings.json`:

```json
{
  "mcpServers": {
    "mlflow": {
      "type": "stdio",
      "command": "mlflow",
      "args": ["server", "mcp"]
    }
  }
}
```

Now Claude Code can search traces, log feedback, query metrics, and manage experiments -- all through natural language. The data flows both ways: MLflow observes Claude Code, and Claude Code analyzes MLflow data.

[MCP Server docs](https://mlflow.org/docs/latest/genai/mcp/) | [Skills repo](https://github.com/mlflow/skills)

## 4. Automate Workflow with MLflow Skills

Install production-ready skills that give Claude Code deep MLflow expertise:

```bash
npx skills add mlflow/skills
```

This adds 8 skills covering trace analysis, agent evaluation, metrics querying, and debugging. Instead of writing evaluation code yourself, you can just say:

- _"Evaluate my agent against the golden dataset"_ -- it writes and runs the scorer pipeline
- _"Debug trace tr-abc123 -- why did the agent fail?"_ -- it pulls the trace, analyzes the tool calls, and explains the root cause
- _"Show me the slowest traces from the last 24 hours"_ -- it queries MLflow and summarizes the results

Claude Code knows the MLflow APIs, understands trace structures, and can write the queries for you.



## 5. MLflow Assistant, Powered by Claude Code

Automatic testing and evaluation is powerful, but doing it right is not easy. It requires you to read the documentation, understand the API, and write the code yourself. This is where many practitioners just give up and go back to manual testing even though they know it is not efficient. However, what if Claude Code could do that for you?

Claude Code is capable of doing that. However, just telling it to do so does not work well, because it has limited context about MLflow and how to use it. Fortunately, there are easy ways to enhance your Claude Code to do MLflow related tasks for you.

Skills and MCP are powerful, but there's still friction: you're constantly switching between the MLflow UI and your terminal, copying trace IDs, run IDs, and model names back and forth. You spot a failing trace in the UI, switch to the CLI, paste the ID, ask Claude Code to debug it, then switch back to verify. It works, but it's tedious.

MLflow Assistant eliminates that context-switching entirely. It's Claude Code, embedded directly in the MLflow UI.

<video controls autoplay muted loop playsinline width="100%">
  <source
    src={require("/img/releases/3.9.0/mlflow_assistant.mp4").default}
    type="video/mp4"
  />
</video>

When you open the Assistant panel, MLflow automatically passes context about what you're looking at -- the current experiment, trace, run, or evaluation results. No more copying IDs. Just ask:

- _"Why did this trace fail?"_ -- while viewing the trace
- _"Set up evaluation for this experiment"_ -- while browsing results
- _"What's causing the latency spike in these runs?"_ -- while looking at the dashboard

Because the Assistant is powered by Claude Code, it doesn't just understand MLflow data -- it understands your project's codebase too. It can connect a failing trace back to the actual code that produced it, suggest fixes, and set up tracing or evaluation pipelines tailored to your stack. The combination of MLflow's UI context and Claude Code's deep codebase awareness is where it gets truly powerful.

It uses your existing Claude Code subscription -- no extra API keys, no additional cost. Everything runs locally with full transparency.

[Assistant docs](https://mlflow.org/docs/latest/genai/getting-started/try-assistant.html)



## Get Started

Three integrations, one theme: **Claude Code is more powerful when you can observe, evaluate, and automate with confidence**.

Here's how to start in under two minutes:

```bash
# Install MLflow
pip install mlflow

# Enable Claude Code tracing
mlflow autolog claude

# Install MLflow skills for Claude Code
npx skills add mlflow/skills
```

Then open the MLflow UI (`mlflow ui`) and try the Assistant.

If this is useful, give us a star on GitHub: **[github.com/mlflow/mlflow](https://github.com/mlflow/mlflow)**

Have questions or feedback? [Open an issue](https://github.com/mlflow/mlflow/issues) or join the conversation in the [MLflow community](https://github.com/mlflow/mlflow/discussions).
