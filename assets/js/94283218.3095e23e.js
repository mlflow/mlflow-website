"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[230],{10625:(e,i,t)=>{t.d(i,{A:()=>n});const n=t.p+"assets/images/eval_comparison-62c599c5481b4c16b0bff741b9a8725e.png"},17559:(e,i,t)=>{t.d(i,{A:()=>n});const n="data:image/svg+xml;base64,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"},22881:(e,i,t)=>{t.d(i,{A:()=>n});const n=t.p+"assets/images/prompt_comparison-18be634f5c59086cba4dd9eddd10667b.png"},27482:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>n,toc:()=>d});var n=t(58152),a=t(74848),o=t(28453);const s={title:"Systematic Prompt Optimization for OpenAI Agents with GEPA",slug:"mlflow-prompt-optimization",tags:["mlflow","genai","prompt-optimization","openai","agents","evaluation","gepa"],authors:["mlflow-maintainers"],thumbnail:"/img/blog/prompt-opt-thumbnail.svg",image:"/img/blog/prompt-opt-thumbnail.svg"},r=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Challenge: Complex Question Answering",id:"the-challenge-complex-question-answering",level:2},{value:"The Solution: Automated Prompt Optimization",id:"the-solution-automated-prompt-optimization",level:2},{value:"Building an OpenAI Agent QA System",id:"building-an-openai-agent-qa-system",level:2},{value:"1. Setup and Dependencies",id:"1-setup-and-dependencies",level:3},{value:"2. Create and Register Your Base Prompt",id:"2-create-and-register-your-base-prompt",level:3},{value:"3. Initialize the OpenAI Agent",id:"3-initialize-the-openai-agent",level:3},{value:"4. Create a Prediction Function",id:"4-create-a-prediction-function",level:3},{value:"5. Baseline Evaluation",id:"5-baseline-evaluation",level:3},{value:"6. Optimize the Prompt",id:"6-optimize-the-prompt",level:3},{value:"7. Evaluate the Optimized Prompt",id:"7-evaluate-the-optimized-prompt",level:3},{value:"Understanding the Optimized Prompt",id:"understanding-the-optimized-prompt",level:2},{value:"Original Prompt:",id:"original-prompt",level:3},{value:"Optimized Prompt:",id:"optimized-prompt",level:3},{value:"Key Improvements Identified by GEPA:",id:"key-improvements-identified-by-gepa",level:3},{value:"Tracking Everything in MLflow",id:"tracking-everything-in-mlflow",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Further Reading",id:"further-reading",level:2}];function p(e){const i={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(i.p,{children:["Prompt engineering is critical for building reliable AI systems, but it's fraught with challenges. Manual iteration is time-consuming, lacks systematic guarantees for improvement, and often yields inconsistent results. It is even harder if your system has multiple different prompts. To address this, automatic joint prompt optimization algorithms such as ",(0,a.jsx)(i.a,{href:"https://github.com/gepa-ai/gepa?tab=readme-ov-file",children:"GEPA"})," and ",(0,a.jsx)(i.a,{href:"https://dspy.ai/api/optimizers/MIPROv2/",children:"MIPRO"})," have been developed. While ",(0,a.jsx)(i.a,{href:"https://dspy.ai/",children:"DSPy"})," has made these optimization techniques accessible within its framework, applying them to other agent frameworks\u2014such as OpenAI Agents SDK, LangChain, or Pydantic AI\u2014has historically required significant integration effort."]}),"\n",(0,a.jsxs)(i.p,{children:["MLflow changes this equation. With ",(0,a.jsx)(i.code,{children:"mlflow.genai.optimize_prompts"}),", you can now systematically optimize prompts, regardless of which agent framework you are using\u2014",(0,a.jsx)(i.strong,{children:"as long as you manage your prompts in MLflow Prompt Registry"}),"."]}),"\n",(0,a.jsxs)(i.p,{children:["In this blog post, we'll demonstrate the complete workflow using the OpenAI Agent framework on a question-answering task with the HotpotQA dataset. We'll show how automated optimization with the ",(0,a.jsx)(i.a,{href:"https://github.com/gepa-ai/gepa?tab=readme-ov-file",children:"GEPA"})," algorithm achieved a 10% accuracy improvement, but the approach applies broadly to any GenAI application you're building."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"Prompt Comparison",src:t(22881).A+"",width:"2292",height:"1508"})}),"\n",(0,a.jsx)(i.h2,{id:"the-challenge-complex-question-answering",children:"The Challenge: Complex Question Answering"}),"\n",(0,a.jsx)(i.p,{children:"Question answering systems often struggle with complex queries that require reasoning across multiple pieces of information. Consider this example from the HotpotQA dataset:"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Question:"}),' "Which publishing company has published Bizarre and a sister publication devoted to the anomalous phenomena popularised by Charles Fort?"']}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Context (10 documents):"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Document 1: Fortean Times is a British monthly magazine devoted to the anomalous phenomena popularised by Charles Fort... it is now published by Dennis Publishing Ltd."}),"\n",(0,a.jsx)(i.li,{children:"Document 2: Charles Fort Charles Hoy Fort (August 6, 1874 \u2013 May 3, 1932) was an American writer and researcher who specialized in anomalous phenomena..."}),"\n",(0,a.jsx)(i.li,{children:'Document 3: Bob Rickard Robert "Bob" J M Rickard is the founder and editor of the UK magazine "Fortean Times: The Journal of Strange Phenomena"...'}),"\n",(0,a.jsx)(i.li,{children:'Document 4: Bizarre was a British alternative magazine published from 1997 to 2015. It was published by Dennis Publishing, and was a sister publication to the "Fortean Times".\n....'}),"\n"]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Expected Answer:"}),' "Dennis Publishing"']}),"\n",(0,a.jsx)(i.p,{children:"This requires the agent to:"}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsx)(i.li,{children:'Identify that "Fortean Times" is devoted to phenomena popularized by Charles Fort (Document 1)'}),"\n",(0,a.jsx)(i.li,{children:'Recognize that "Bizarre" was published by Dennis Publishing (Document 4)'}),"\n",(0,a.jsx)(i.li,{children:"Connect that Bizarre and Fortean Times are sister publications (Document 4)"}),"\n",(0,a.jsx)(i.li,{children:'Synthesize the information to answer "Dennis Publishing"'}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"Getting models to consistently provide the correct format and reasoning for such questions is non-trivial."}),"\n",(0,a.jsx)(i.h2,{id:"the-solution-automated-prompt-optimization",children:"The Solution: Automated Prompt Optimization"}),"\n",(0,a.jsxs)(i.p,{children:["Rather than manually iterating on prompts through trial and error, MLflow's ",(0,a.jsx)(i.code,{children:"optimize_prompts"})," provides a systematic approach to improve prompt quality using the GEPA optimizer."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"MLflow Prompt Optimization Workflow",src:t(17559).A+"",width:"1000",height:"350"})}),"\n",(0,a.jsx)(i.h2,{id:"building-an-openai-agent-qa-system",children:"Building an OpenAI Agent QA System"}),"\n",(0,a.jsx)(i.p,{children:"Let's walk through building a complete question-answering system using the OpenAI Agent framework and optimizing it with MLflow."}),"\n",(0,a.jsx)(i.h3,{id:"1-setup-and-dependencies",children:"1. Setup and Dependencies"}),"\n",(0,a.jsx)(i.p,{children:"First, install the required packages:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-bash",children:"pip install openai-agents mlflow datasets openai gepa\n"})}),"\n",(0,a.jsx)(i.p,{children:"Set up your environment:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'import asyncio\nimport os\n\nimport mlflow\nfrom agents import Agent, Runner\nfrom datasets import load_dataset\nfrom mlflow.genai import evaluate\nfrom mlflow.genai.optimize import GepaPromptOptimizer\nfrom mlflow.genai.scorers import Equivalence\n\n# Configure MLflow\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("HotpotQA Optimization")\n\nmlflow.openai.autolog()\n\n# Avoid hanging due to the conflict between async and threading (not necessary for sync agents)\nos.environ["MLFLOW_GENAI_EVAL_MAX_WORKERS"] = "1"\n\n# If running on notebooks\nimport nest_asyncio\nnest_asyncio.apply()\n'})}),"\n",(0,a.jsx)(i.p,{children:"Start your MLflow tracking server:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-bash",children:"mlflow ui --backend-store-uri sqlite:///mlruns.db\n"})}),"\n",(0,a.jsx)(i.h3,{id:"2-create-and-register-your-base-prompt",children:"2. Create and Register Your Base Prompt"}),"\n",(0,a.jsx)(i.p,{children:"Start with a simple, straightforward prompt template:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'prompt_template = """You are a question answering assistant. Answer questions based ONLY on the provided context.\n\nIMPORTANT INSTRUCTIONS:\n- For yes/no questions, answer ONLY "yes" or "no"\n- Do NOT include phrases like "based on the context" or "according to the documents"\n\nContext:\n{{context}}\n\nQuestion: {{question}}\n\nAnswer:"""\n\n# Register the prompt in MLflow\nbase_prompt = mlflow.genai.register_prompt(\n    name="hotpotqa-user-prompt",\n    template=prompt_template,\n)\n'})}),"\n",(0,a.jsx)(i.p,{children:"The MLflow Prompt Registry provides version control for your prompts, making it easy to track changes and roll back if needed."}),"\n",(0,a.jsx)(i.h3,{id:"3-initialize-the-openai-agent",children:"3. Initialize the OpenAI Agent"}),"\n",(0,a.jsx)(i.p,{children:"Set up your agent."}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'agent = Agent(\n    name="HotpotQA Question Answerer",\n    model="gpt-4o-mini",\n)\n'})}),"\n",(0,a.jsx)(i.h3,{id:"4-create-a-prediction-function",children:"4. Create a Prediction Function"}),"\n",(0,a.jsx)(i.p,{children:"The prediction function formats the context and question using the prompt template, then runs the agent:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Create a wrapper for `predict_fn` to run the agent with different prompts\ndef create_predict_fn(prompt_uri: str):\n    prompt = mlflow.genai.load_prompt(prompt_uri)\n\n    @mlflow.trace\n    def predict_fn(context: str, question: str) -> str:\n        """Predict function that uses the agent with the MLflow prompt."""\n        # Use prompt.format() with template variables\n        user_message = prompt.format(context=context, question=question)\n\n        # Run your agent\n        result = asyncio.run(Runner.run(agent, user_message))\n\n        return result.final_output\n\n    return predict_fn\n'})}),"\n",(0,a.jsx)(i.h3,{id:"5-baseline-evaluation",children:"5. Baseline Evaluation"}),"\n",(0,a.jsxs)(i.p,{children:["Before optimizing, establish a baseline by evaluating the agent on a validation set. Here, we use the ",(0,a.jsx)(i.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.genai.html#mlflow.genai.scorers.Equivalence",children:"Equivalence"})," built-in scorer that evaluates the semantic and formatting similarity between the system outputs and expected outputs, but you can use any Scorer objects. See ",(0,a.jsx)(i.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/scorers/",children:"Scorer Overview"})," for more information."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'def prepare_hotpotqa_data(num_samples: int, split: str = "validation") -> list[dict]:\n    """Load and prepare HotpotQA data for MLflow GenAI (evaluate/optimize)."""\n    print(f"\\nLoading HotpotQA dataset ({split} split)...")\n    dataset = load_dataset("hotpot_qa", "distractor", split=split)\n    dataset = dataset.select(range(0, min(num_samples, len(dataset))))\n\n    data = []\n    for example in dataset:\n        # Format context from HotpotQA\n        context_text = "\\n\\n".join([\n            f"Document {i+1}: {title}\\n{\' \'.join(sentences)}"\n            for i, (title, sentences) in enumerate(zip(example["context"]["title"], example["context"]["sentences"]))\n        ])\n\n        data.append({\n            "inputs": {\n                "context": context_text,\n                "question": example["question"],\n            },\n            "expectations": {\n                "expected_response": example["answer"],\n            }\n        })\n\n    print(f"Prepared {len(data)} samples")\n    return data\n\n\ndef run_benchmark(\n    prompt_uri: str,\n    num_samples: int,\n    split: str = "validation",\n) -> dict:\n    """Run the agent on HotpotQA benchmark using mlflow.genai.evaluate()."""\n\n    # Prepare evaluation data\n    eval_data = prepare_hotpotqa_data(num_samples, split)\n\n    # Create prediction function\n    predict_fn = create_predict_fn(prompt_uri)\n\n    # Run evaluation\n    print(f"\\nRunning evaluation on {len(eval_data)} samples...\\n")\n\n    results = evaluate(\n        data=eval_data,\n        predict_fn=predict_fn,\n        scorers=[Equivalence(model="openai:/gpt-4o-mini")],\n    )\n\n    # Extract metrics\n    accuracy = results.metrics.get("equivalence/mean", 0.0) / 100.0\n\n    return {\n        "accuracy": accuracy,\n        "metrics": results.metrics,\n        "results": results,\n    }\n\n\n# Run baseline evaluation\nbaseline_metrics = run_benchmark(base_prompt.uri, num_samples=100)\n\nprint(f"Baseline Accuracy: {baseline_metrics[\'accuracy\']:.1%}")\n# Output: Baseline Accuracy: 50.0%\n'})}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"Baseline",src:t(46730).A+"",width:"3032",height:"1798"})}),"\n",(0,a.jsx)(i.h3,{id:"6-optimize-the-prompt",children:"6. Optimize the Prompt"}),"\n",(0,a.jsx)(i.p,{children:"Now comes the exciting part - using MLflow to automatically improve the prompt:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Prepare training data using shared function\ntrain_data = prepare_hotpotqa_data(num_samples=100, split="train")\n\n# Run optimization\nresult = mlflow.genai.optimize_prompts(\n    predict_fn=create_predict_fn(base_prompt.uri),\n    train_data=train_data,\n    prompt_uris=[base_prompt.uri],\n    optimizer=GepaPromptOptimizer(\n        reflection_model="openai:/gpt-4o",\n        max_metric_calls=500,\n    ),\n    scorers=[\n        Equivalence(model="openai:/gpt-4o-mini"),\n    ],\n    enable_tracking=True,\n)\n\n# Get the optimized prompt URI\noptimized_prompt_uri = result.optimized_prompts[0].uri\nprint(f"  Base prompt: {base_prompt.uri}")\nprint(f"  Optimized prompt: {optimized_prompt_uri}")\n'})}),"\n",(0,a.jsx)(i.p,{children:"The optimization process:"}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Evaluates"})," the current prompt on training examples"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Analyzes"})," failure patterns and common issues"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Generates"})," improved prompt variations"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Tests"})," these variations to find the best performer"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Iterates"})," until reaching the maximum metric calls or convergence"]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"Optimization Run",src:t(53298).A+"",width:"1697",height:"740"})}),"\n",(0,a.jsx)(i.h3,{id:"7-evaluate-the-optimized-prompt",children:"7. Evaluate the Optimized Prompt"}),"\n",(0,a.jsx)(i.p,{children:"Let's see how much we improved:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"# Evaluate optimized prompt on the same validation set\noptimized_metrics = run_benchmark(optimized_prompt_uri, num_samples=100)\n\nprint(f\"Optimized Accuracy: {optimized_metrics['accuracy']:.1%}\")\n# Output: Optimized Accuracy: 60.0%\n\nimprovement = optimized_metrics['accuracy'] - baseline_metrics['accuracy']\nprint(f\"Improvement: {improvement:+.1%}\")\n# Output: Improvement: +10.0%\n"})}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"Eval Comparison",src:t(10625).A+"",width:"3030",height:"1782"})}),"\n",(0,a.jsx)(i.h2,{id:"understanding-the-optimized-prompt",children:"Understanding the Optimized Prompt"}),"\n",(0,a.jsx)(i.p,{children:"Let's compare the original and optimized prompts:"}),"\n",(0,a.jsx)(i.h3,{id:"original-prompt",children:"Original Prompt:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{children:'You are a question answering assistant. Answer questions based ONLY on the provided context.\n\nIMPORTANT INSTRUCTIONS:\n- For yes/no questions, answer ONLY "yes" or "no"\n- Do NOT include phrases like "based on the context" or "according to the documents"\n\nContext:\n{{context}}\n\nQuestion: {{question}}\n\nAnswer:\n'})}),"\n",(0,a.jsx)(i.h3,{id:"optimized-prompt",children:"Optimized Prompt:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{children:"You are a question answering assistant. Your job is to answer questions using ONLY the information given in the provided Context block of documents. Do not use outside knowledge.\n\nTask and approach:\n- You may need to use multi-hop reasoning across documents. Find the entity or event mentioned in one document, then retrieve the requested attribute from the relevant document, making sure all constraints in the question are satisfied.\n- Always extract the minimal answer string directly from the context, preserving the original wording and formatting where applicable.\n\nAnswer type decision:\n- If and only if the question is a yes/no question (starts with Is/Are/Was/Were/Do/Does/Did/Can/Could/Will/Would or is clearly answerable by yes/no), answer with exactly one word: yes or no.\n- For all other questions (what/who/which/where/when/why/how/how many, etc.), provide the specific answer phrase (entity, number, date, place, etc.) from the context.\n\nExtraction and disambiguation rules:\n- Use multi-hop reasoning across documents when needed. Cross-reference facts across different documents to satisfy all constraints in the question.\n- Disambiguate carefully. Pay attention to qualifiers such as nationality, profession, location, time period, roles, and other descriptors. Choose the answer that satisfies ALL constraints.\n- Prefer information that directly relates to the specific instance/timeframe asked in the question if multiple similar items exist in the context.\n\nStrict output format:\n- Provide only the minimal answer string. No explanations or extra words.\n- Do NOT include phrases like \u201cbased on the context\u201d or \u201caccording to the documents\u201d.\n- Do NOT add quotes, punctuation, or extra sentences.\n- Preserve capitalization and surface form exactly as it appears in the context when returning names, titles, and labeled entities.\n- For measurement questions (length/width/height/distance/area/duration, etc.), return the full measurement expression exactly as it appears in the context, including units and any descriptive words such as long, wide, tall, hours, minutes (e.g., 6.213 km long).\n- For dates and times, return the date/time in the exact format used in the context.\n- For location-specific questions:\n  - If asked \u201cwhich city\u201d, return only the city name.\n  - If asked \u201cwhich US state/country\u201d, return only the state or country name.\n- For list questions, return a concise, comma-separated list in the order supported by the context (only if the question explicitly asks for multiple items).\n- If the context does not contain enough information to answer, reply with unknown (do not guess).\n\nGuidance for multi-hop and pattern matching (use only if supported by the given context):\n- Track/event length: If asked \u201cWhat is the length of the track where [event] was staged?\u201d, find the document about the event to identify the track name, then retrieve the track\u2019s length from the track\u2019s document, and return the exact measurement phrase (e.g., 6.213 km long).\n- Film identification with constraints: If asked to identify a film by director and starring actors (e.g., \u201cSouth Korean actor X starred in what 2016 movie directed by Yeon Sang-ho and starring Gong Yoo, Jung Yu-mi, and Ma Dong-seok?\u201d), locate the film document that matches those constraints and return the exact film title as it appears.\n- Geographic containment: If asked about a base located in an area of a county \u201cin which US state?\u201d, locate the base\u2019s document that states the state and return only the state name.\n\nTemplate:\nContext:\n{{context}}\n\nQuestion: {{question}}\n\nAnswer:\n"})}),"\n",(0,a.jsx)(i.h3,{id:"key-improvements-identified-by-gepa",children:"Key Improvements Identified by GEPA:"}),"\n",(0,a.jsx)(i.p,{children:"The improvements achieved by GEPA include: clarifying the explicit output format so the prompt defines exactly what makes a valid answer; enhancing yes/no question detection with examples of indicative question words; allowing for multi-document reasoning by specifying that information can be combined across documents; instructing the model to handle names by using their full canonical forms with titles; addressing notable edge cases by directly targeting specific failure patterns uncovered during optimization; and enforcing strict format preservation, requiring the answer to match the spelling, capitalization, and formatting found in the context."}),"\n",(0,a.jsx)(i.h2,{id:"tracking-everything-in-mlflow",children:"Tracking Everything in MLflow"}),"\n",(0,a.jsx)(i.p,{children:"Throughout this workflow, MLflow automatically tracks:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Prompts"}),": All versions with timestamps and metadata"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Runs"}),": Each optimization run with its configuration"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Metrics"}),": Baseline and optimized accuracy scores"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Traces"}),": Detailed execution traces of your agent"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Training Time"}),": Optimization with 100 training samples and 500 max metric calls took approximately 30 minutes."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Scaling"}),": For production systems, it is recommended to start with smaller sample sizes (50-100) for faster iteration. Use validation sets to verify that improvements generalize beyond the training data. To improve efficiency and reduce costs, consider caching predictions to avoid redundant API calls."]}),"\n",(0,a.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsxs)(i.p,{children:["Manual prompt engineering is time-consuming and often yields suboptimal results. MLflow's ",(0,a.jsx)(i.code,{children:"optimize_prompts"})," provides a systematic, data-driven approach to improve prompt quality automatically."]}),"\n",(0,a.jsx)(i.p,{children:"In our HotpotQA experiment, we observed a 10% absolute accuracy improvement (from 50% to 60%) after optimization. This workflow enabled systematic optimization rather than relying on trial and error, and provided full experiment tracking for reproducibility."}),"\n",(0,a.jsx)(i.p,{children:"The combination of OpenAI's Agent framework for execution and MLflow's optimization capabilities creates a powerful workflow for building production-ready AI systems."}),"\n",(0,a.jsx)(i.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.a,{href:"https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/",children:"MLflow Prompt Optimization Documentation"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.a,{href:"https://mlflow.org/docs/latest/genai/prompt-registry/",children:"MLflow Prompt Registry"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.a,{href:"https://github.com/openai/openai-agents-python",children:"OpenAI Agent Framework"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.a,{href:"https://hotpotqa.github.io/",children:"HotpotQA Dataset"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.a,{href:"https://mlflow.org/blog/mlflow-prompt-evaluate",children:"Building and Managing LLM Systems with MLflow"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/",children:"LLM as Judge"})}),"\n"]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsxs)(i.p,{children:["Have questions or face issues? Please file a report on ",(0,a.jsx)(i.a,{href:"https://github.com/mlflow/mlflow/issues",children:"MLflow's GitHub Issues"}),"."]})]})}function m(e={}){const{wrapper:i}={...(0,o.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},28453:(e,i,t)=>{t.d(i,{R:()=>s,x:()=>r});var n=t(96540);const a={},o=n.createContext(a);function s(e){const i=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),n.createElement(o.Provider,{value:i},e.children)}},46730:(e,i,t)=>{t.d(i,{A:()=>n});const n=t.p+"assets/images/baseline-0e6089e93fb2a12f75da2ee2ea99e5b3.png"},53298:(e,i,t)=>{t.d(i,{A:()=>n});const n=t.p+"assets/images/optimization_run-0a1919d490c7f08bab06293d309c13a6.png"},58152:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/mlflow-prompt-optimization","source":"@site/blog/2025-10-24-mlflow-prompt-optimization/index.md","title":"Systematic Prompt Optimization for OpenAI Agents with GEPA","description":"Prompt engineering is critical for building reliable AI systems, but it\'s fraught with challenges. Manual iteration is time-consuming, lacks systematic guarantees for improvement, and often yields inconsistent results. It is even harder if your system has multiple different prompts. To address this, automatic joint prompt optimization algorithms such as GEPA and MIPRO have been developed. While DSPy has made these optimization techniques accessible within its framework, applying them to other agent frameworks\u2014such as OpenAI Agents SDK, LangChain, or Pydantic AI\u2014has historically required significant integration effort.","date":"2025-10-24T00:00:00.000Z","tags":[{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"prompt-optimization","permalink":"/mlflow-website/blog/tags/prompt-optimization"},{"inline":true,"label":"openai","permalink":"/mlflow-website/blog/tags/openai"},{"inline":true,"label":"agents","permalink":"/mlflow-website/blog/tags/agents"},{"inline":true,"label":"evaluation","permalink":"/mlflow-website/blog/tags/evaluation"},{"inline":true,"label":"gepa","permalink":"/mlflow-website/blog/tags/gepa"}],"readingTime":10.325,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"Systematic Prompt Optimization for OpenAI Agents with GEPA","slug":"mlflow-prompt-optimization","tags":["mlflow","genai","prompt-optimization","openai","agents","evaluation","gepa"],"authors":["mlflow-maintainers"],"thumbnail":"/img/blog/prompt-opt-thumbnail.svg","image":"/img/blog/prompt-opt-thumbnail.svg"},"unlisted":false,"nextItem":{"title":"Rapidly Prototype and Evaluate Agents with Claude Agent SDK and MLflow","permalink":"/mlflow-website/blog/mlflow-autolog-claude-agents-sdk"}}')}}]);