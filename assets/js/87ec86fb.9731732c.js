"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3570],{6326:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/margaret-hogan-used-the-black-sounding-optophone-to-read-a-book-b2a7ff0a74da291404563de637f5e43c.jpg"},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(96540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}},43235:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/word_grouping_semantic_entity_labeling-76ffcbf4ade5af5360a63e6415bc08b0.png"},65762:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var i=t(70333),a=t(74848),s=t(28453);const r={title:"Building and Managing an LLM-based OCR System with MLflow",slug:"mlflow-prompt-evaluate",tags:["mlflow","genai","evaluation","prompt-registry","tracing"],authors:["allison-bennett","shyam-sankararaman","michael-berk","mlflow-maintainers"],thumbnail:"/img/blog/prompt-evaluate/prompt-evaluate.png"},o=void 0,l={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[{value:"What is OCR?",id:"what-is-ocr",level:2},{value:"The Challenge",id:"the-challenge",level:2},{value:"OCR Use Case:",id:"ocr-use-case",level:2},{value:"1. Set Up",id:"1-set-up",level:3},{value:"2. Observe the Data",id:"2-observe-the-data",level:3},{value:"3. MLflow Tracking, Autolog and Tracing",id:"3-mlflow-tracking-autolog-and-tracing",level:3},{value:"4. Loading inputs and Prompting",id:"4-loading-inputs-and-prompting",level:3},{value:"5. Setting up the LLM",id:"5-setting-up-the-llm",level:3},{value:"Prompt Registry",id:"prompt-registry",level:4},{value:"6. Defining and Evaluating Performance",id:"6-defining-and-evaluating-performance",level:3},{value:"Conclusion and Next Steps",id:"conclusion-and-next-steps",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Building GenAI tools presents a unique set of challenges. As we evaluate accuracy, iterate on prompts, and enable collaboration, we often encounter bottlenecks that slow down our progress toward production."}),"\n",(0,a.jsx)(n.p,{children:"In this blog, we explore how MLflow's GenAI capabilities help us streamline development and unlock value for both technical and non-technical contributors when building an LLM-based Optical Character Recognition (OCR) tool."}),"\n",(0,a.jsx)(n.h2,{id:"what-is-ocr",children:"What is OCR?"}),"\n",(0,a.jsx)(n.p,{children:"Optical Character Recognition, or OCR, is the process of extracting text from scanned documents and images. The resulting text is machine-encoded, editable, and searchable, unlocking a wide range of downstream use cases."}),"\n",(0,a.jsx)(n.p,{children:"Here, we leverage multi-modal LLMs to extract formatted text from scanned documents. Unlike traditional OCR tools such as PyTesseract, LLM-based methods offer greater flexibility for complex layouts, handwritten content, and context-aware extraction. While these methods may require more computational resources and careful prompt engineering, they provide significant advantages for advanced use cases."}),"\n",(0,a.jsxs)(n.p,{children:["Fun fact: The earliest form of OCR, the Optophone, was introduced in 1914 to help blind individuals read printed text without Braille.\n",(0,a.jsx)(n.img,{alt:"Optophone",src:t(6326).A+"",width:"360",height:"254"})]}),"\n",(0,a.jsx)(n.h2,{id:"the-challenge",children:"The Challenge"}),"\n",(0,a.jsx)(n.p,{children:"We face several recurring challenges when building an LLM-based OCR application."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prompt Iteration and Versioning:"})," Prompts need to be updated and tweaked to improve extraction quality. A new prompt can introduce performance regression, but we do not save the old version. Without rigorous versioning, it's hard to roll back or compare."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Debugging Unexpected Results:"})," Unexpected results may show up periodically in our OCR attempts. We need a way to understand why. Without detailed traceability, it is difficult to diagnose whether the issue is with the prompt, the model, or the data (e.g., a new document strucutre)."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Evaluating and Comparing Models:"})," Accuracy in OCR can mean many things. We may want to measure correct field extraction, formatting, or even business logic compliance. In order to compare different model or prompt strategies, we need a way to define and track what matters."]}),"\n",(0,a.jsx)(n.p,{children:"MLflow addresses these directly in our workflow."}),"\n",(0,a.jsx)(n.h2,{id:"ocr-use-case",children:"OCR Use Case:"}),"\n",(0,a.jsx)(n.p,{children:"Our task is to create a document parsing tool for text extraction (OCR) using LLMs, using MLflow features to address our challenges. The data consists of scanned documents and their corresponding text extracted as JSON."}),"\n",(0,a.jsxs)(n.p,{children:["We use the ",(0,a.jsx)(n.a,{href:"https://guillaumejaume.github.io/FUNSD/",children:"FUNSD dataset"}),", which contains around 200 fully annotated forms, structured as semantic entity labels and word groupings."]}),"\n",(0,a.jsx)(n.p,{children:"Example:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Annotated form example",src:t(43235).A+"",width:"1196",height:"752"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'{\n        "form": [\n        {\n            "id": 0,\n            "text": "Registration No.",\n            "box": [94,169,191,186],\n            "linking": [\n                [0,1]\n            ],\n            "label": "question",\n            "words": [\n                {\n                    "text": "Registration",\n                    "box": [94,169,168,186]\n                },\n                {\n                    "text": "No.",\n                    "box": [170,169,191,183]\n                }\n            ]\n        },\n        {\n            "id": 1,\n            "text": "533",\n            "box": [209,169,236,182],\n            "label": "answer",\n            "words": [\n                {\n                    "box": [209,169,236,182\n                    ],\n                    "text": "533"\n                }\n            ],\n            "linking": [\n                [0,1]\n            ]\n        }\n    ]\n    }\n'})}),"\n",(0,a.jsxs)(n.p,{children:["For a detailed description of each entry, refer to the ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/1905.13538.pdf",children:"original paper"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"1-set-up",children:"1. Set Up"}),"\n",(0,a.jsx)(n.p,{children:"Install the required packages:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openai mlflow tiktoken aiohttp -qU\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"openai:"})," For interacting with OpenAI models and APIs",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"mlflow:"})," For experiment tracking, model management, and GenAI workflow tools",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"tiktoken:"})," For efficient tokenization, especially useful with OpenAI models",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"aiohttp:"})," For asynchronous HTTP requests, enabling efficient API calls"]}),"\n",(0,a.jsx)(n.p,{children:"For this tutorial we use OpenAI, but the approach extends to other LLM providers.\nWe can prompt the user to type the OpenAI API key without echoing using getpass():"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nfrom getpass import getpass\n\nos.environ["OPENAI_API_KEY"] = getpass("Your OpenAI API Key: ")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-observe-the-data",children:"2. Observe the Data"}),"\n",(0,a.jsx)(n.p,{children:"Let's read a randomly selected image and its corresponding annotation JSON file. The following utils functions faciliate this task."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Any, Mapping, Sequence, TypedDict, Literal\nimport pandas as pd\nimport base64\nimport json\nimport random\nimport re\n\nfrom PIL import Image as PILImage\n\n\nDATA_DIRECTORY = Path("./data")\nANNOTATIONS_DIRECTORY = DATA_DIRECTORY / "annotations"\nIMAGES_DIRECTORY = DATA_DIRECTORY / "images"\n\n\nclass Word(TypedDict):\n    text: str\n\nclass Item(TypedDict, total=False):\n    id: str | int\n    label: Literal["question", "answer", "other"]\n    words: list[Word]\n    linking: list[tuple[str | int, str | int]]\n\n@dataclass(frozen=True)\nclass QAPair:\n    question: str\n    answer: str\n\ndef _flatten_form(form: Any) -> list[Item]:\n    """\n    Flattens the \'form\' section into a simple list of items.\n    Accepts list[Item], list[list[Item]], or a single list-like page.\n    """\n    if isinstance(form, list):\n\n        if form and all(isinstance(page, list) for page in form):\n            return [item for page in form for item in page]  # 2D -> 1D\n        return form\n\n    raise TypeError("Expected \'form\' to be a list or list of lists.")\n\ndef extract_qa_pairs(items: Sequence[Mapping[str, Any]]) -> list[QAPair]:\n    """\n    Robustly extract Q&A pairs. Supports multiple answers per question.\n    Does not assume unique question text; uses ids to match.\n    """\n    by_id: dict[Any, Mapping[str, Any]] = {}\n    for it in items:\n        if "id" in it:\n            by_id[it["id"]] = it\n\n    pairs: list[QAPair] = []\n\n    for it in items:\n        if it.get("label") != "question":\n            continue\n        links = it.get("linking") or []\n        q_words = it.get("words") or []\n        q_text = " ".join(w.get("text", "") for w in q_words).strip()\n\n        for link in links:\n            if not isinstance(link, (list, tuple)) or len(link) != 2:\n                continue\n            q_id, a_id = link\n            if q_id != it.get("id"):\n                # Skip foreign link edges\n                continue\n            ans = by_id.get(a_id)\n            if not ans or ans.get("label") != "answer":\n                continue\n            a_words = ans.get("words") or []\n            a_text = " ".join(w.get("text", "") for w in a_words).strip()\n            if q_text and a_text:\n                pairs.append(\n                    QAPair(\n                        question=q_text,\n                        answer=a_text,\n                    )\n                )\n    return pairs\n\ndef load_annotation_file(file_name: str | Path, directory: Path = ANNOTATIONS_DIRECTORY) -> list[QAPair]:\n    """\n    Load one annotation JSON and return extracted Q&A pairs.\n    Always returns a list; empty if none found.\n    """\n    file_path = (directory / file_name) if isinstance(file_name, str) else file_name\n    if file_path.suffix.lower() != ".json":\n        file_path = file_path.with_suffix(".json")\n\n    with file_path.open("r", encoding="utf-8") as f:\n        data = json.load(f)\n    form = data.get("form")\n    items = _flatten_form(form)\n    return extract_qa_pairs(items)\n\ndef choose_random_annotation_names(\n    directory: Path = ANNOTATIONS_DIRECTORY,\n    n: int = 1,\n    suffix: str = ".json",\n    seed: int | None = None,\n) -> list[str]:\n    """\n    Returns a list of basenames (without extension). Raises if directory missing or empty.\n    Deterministic when seed is provided.\n    """\n    if not directory.exists():\n        raise FileNotFoundError(f"Directory {directory} does not exist.")\n    files = sorted(p for p in directory.iterdir() if p.is_file() and p.suffix.lower() == suffix)\n    if not files:\n        raise FileNotFoundError(f"No {suffix} files found in {directory}.")\n\n    if seed is not None:\n        rnd = random.Random(seed)\n        selected = rnd.sample(files, k=min(n, len(files)))\n    else:\n        selected = random.sample(files, k=min(n, len(files)))\n    return [p.stem for p in selected]\n\n\ndef compress_image(\n    file_path: str | Path,\n    *,\n    quality: int = 40,\n    max_size: tuple[int, int] = (1000, 1000),\n) -> bytes:\n    """\n    Resize and convert to JPEG with the given quality. Returns JPEG bytes.\n    """\n    file_path = Path(file_path)\n    with PILImage.open(file_path) as img:\n        img = img.convert("RGB")\n        img.thumbnail(max_size)\n        buf = BytesIO()\n        img.save(buf, format="JPEG", quality=quality, optimize=True)\n        return buf.getvalue()\n\ndef get_image_bytes_or_b64(\n    file_name: str | Path,\n    *,\n    directory: Path = IMAGES_DIRECTORY,\n    as_base64: bool = False,\n    quality: int = 40,\n    max_size: tuple[int, int] = (1000, 1000),\n) -> bytes | str:\n    """\n    Returns compressed JPEG bytes or a base64 string. File extension is coerced to .png on input,\n    but compression always outputs JPEG bytes/base64.\n    """\n    path = (directory / file_name) if isinstance(file_name, str) else file_name\n    if path.suffix.lower() != ".png":\n        path = path.with_suffix(".png")\n\n    jpeg_bytes = compress_image(path, quality=quality, max_size=max_size)\n    if as_base64:\n        return base64.b64encode(jpeg_bytes).decode("utf-8")\n    return jpeg_bytes\n\n\n_key_chars_re = re.compile(r"[^\\w\\s\\-_]")  # keep word chars, space, dash, underscore\n_ws_re = re.compile(r"\\s+")\n\ndef clean_key(key: str) -> str:\n    """Remove unwanted characters, collapse whitespace, trim, and convert to UPPER_SNAKE-ish."""\n    key = _key_chars_re.sub("", key)\n    key = _ws_re.sub(" ", key).strip()\n    # If you prefer underscores: key = key.replace(" ", "_")\n    return key.upper()\n\ndef normalize_dict_keys(obj: Any) -> Any:\n    """\n    Recursively normalize mapping keys; preserves lists/tuples;\n    """\n    from collections.abc import Mapping as ABMapping, Sequence as ABSequence\n\n    if isinstance(obj, ABMapping):\n        return {clean_key(str(k)): normalize_dict_keys(v) for k, v in obj.items()}\n    if isinstance(obj, (list, tuple)):\n        return obj.__class__(normalize_dict_keys(v) for v in obj)\n    return obj\n\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Let\u2019s take a moment to break down the ",(0,a.jsx)(n.code,{children:"_extract_qa_pairs"})," function:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"We create an ID lookup dictionary to find items by their ID"}),"\n",(0,a.jsx)(n.li,{children:'We identify "question" items that have linked answer pair(s) in the form of (q_id, a_id)'}),"\n",(0,a.jsx)(n.li,{children:"We construct structured QAPair objects by extracting text from both question and answer words"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["We can then call ",(0,a.jsx)(n.code,{children:"load_annotation_file"})," to load an annotation JSON file and return question-answer pair (structured QAPair object)."]}),"\n",(0,a.jsxs)(n.p,{children:["For LLM inputs, we favor small, predictable payloads over lossless fidelity. We use ",(0,a.jsx)(n.code,{children:"get_image_bytes_or_b64"})," to read PNG images from a directory and convert to either compressed JPEG bytes using ",(0,a.jsx)(n.code,{children:"_compress_image"})," or base64 depending depending on the boolean as_base64."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Bandwidth & Cost:"})," PNG scans of forms are often 5\u201310\xd7 larger than a JPEG of the same resolution. Since we send images as base64 (with \u224833% overhead), shrinking bytes directly reduces request size, latency, and API cost\u2014especially when batch-evaluating many pages."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Normalization:"}),' Converting to 8-bit RGB and flattening transparency removes PNG quirks (alpha, indexed palettes, 16-bit depth, etc.), so every image reaches the model in a consistent form. This eliminates "works on my machine" ingestion issues across providers.']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Throughput Over Perfection:"})," Our pages are high-contrast forms, so modest JPEG compression keeps text legible while dramatically shrinking files. For this tutorial, we also downscale to a max side of 1000 px, which is typically sufficient for field labels while speeding up end-to-end runs and evals."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Trade-offs & Not Converting:"}),"\nWith tiny type, low-contrast scans, or while using classical OCR (e.g., Tesseract), we prefer lossless formats (PNG/TIFF) or use higher JPEG quality. Compression artifacts can blur thin strokes and hurt accuracy."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"random_files = choose_random_annotation_names(n=1, seed=42)\nimage_bytes = get_image_bytes_or_b64(random_files[0], as_base64=True)\nload_annotation_file(random_files[0])\n\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-mlflow-tracking-autolog-and-tracing",children:"3. MLflow Tracking, Autolog and Tracing"}),"\n",(0,a.jsxs)(n.p,{children:["Before we make a call to OpenAI, we need to set up ",(0,a.jsx)(n.a,{href:"https://mlflow.org/docs/latest/ml/tracking",children:"MLflow Tracking"})," to ensure every experiment, prompt, and result is recorded and traceable."]}),"\n",(0,a.jsxs)(n.p,{children:["We will also enable ",(0,a.jsx)(n.a,{href:"https://mlflow.org/docs/latest/ml/tracking/autolog",children:"MLflow Autolog"})," to simplify the tracking process by reducing the need for manual log statements. Below, we point MLflow to a local SQLite database as the backend store, where metrics, parameters, artifacts, and other useful information are logged automatically."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.set_tracking_uri("sqlite:///mlflow_runs.db")\nmlflow.set_experiment("ocr-initial-experiment")\nmlflow.openai.autolog()\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/",children:"MLflow Tracing"})," provides us with end-to-end observability. We gain a comprehensive view of each step from prompt construction and model inference to tool calls and final outputs. If we notice various failed attemped in our OCR tool, we can use the MLflow UI to inspect traces, compare inputs and outputs, and identify whether the issue is with the prompt, the model, or the data structure"]}),"\n",(0,a.jsxs)(n.p,{children:["In order to access the MLflow UI, we need to run the following. The UI will start on ",(0,a.jsx)(n.code,{children:"http://localhost:5000"})," by default:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mlflow ui --backend-store-uri sqlite:///mlflow_runs.db\n\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"MLflow UI showing the tracing for each LLM execution",src:t(71412).A+"",width:"3446",height:"1736"})}),"\n",(0,a.jsx)(n.h3,{id:"4-loading-inputs-and-prompting",children:"4. Loading inputs and Prompting"}),"\n",(0,a.jsx)(n.p,{children:'We start by defining a system prompt for extracting the contents of the images into lists of "questions" and "answers" using an LLM. We will use a "first pass" prompt, deliberately made short and minimally descriptive so that subsequent improvements can be made later on. These are tracked under the MLflow experiment runs when the LLM completion calls are invoked for each image file. We convert the QAPair object into a dictionary for easier comparison with the LLM response during evaluation.'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from collections import defaultdict\n\n_files = choose_random_annotation_names(n=5, seed=42)\n#Store multiple answers per question as list\nannotation_items = []\nfor file_name in _files:\n    file_dict = defaultdict(list)\n    for pair in load_annotation_file(file_name):\n        file_dict[pair.question].append(pair.answer)\n    annotation_items.append(dict(file_dict))\n\nannotation_normalized = normalize_dict_keys(annotation_items)\nimages = [get_image_bytes_or_b64(file, as_base64=True) for file in _files]\n\nsystem_prompt = """You are an expert at Optical Character Recognition (OCR). Extract the questions and answers from the image."""\n\n'})}),"\n",(0,a.jsx)(n.h3,{id:"5-setting-up-the-llm",children:"5. Setting up the LLM"}),"\n",(0,a.jsx)(n.p,{children:"On the OpenAI side, we initialize the client and send a prompt to the LLM, instructing it to act as an OCR expert. The expected output is a Structured Output containing a list of key-value pairs. To enforce this structure, we can define Pydantic models that validate the response format. Let's try to invoke the LLM and log the execution and see what the response looks like."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel\nfrom openai import OpenAI\n\n# Define Pydantic models for structured output in the form of key-value pair accounting for duplicate questions\nclass KeyValueModel(BaseModel):\n    key: str\n    value: list[str]\n\nclass KeyValueList(BaseModel):\n    pairs: list[KeyValueModel]\n\nclient = OpenAI()\n\ndef get_completion(inputs: str) -> str:\n    completion = client.chat.completions.parse(\n        model="gpt-4o-mini",\n        response_format=KeyValueList,\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {\n                "role": "user",\n                "content": [\n                    { "type": "text", "text": "what\'s in this image?" },\n                    {\n                        "type": "image_url",\n                        "image_url": {\n                            "url": f"data:image/jpeg;base64,{inputs}",\n                        },\n                    },\n                ],\n            }\n        ],\n    )\n\n\n    generated_response = {pair.key: pair.value for pair in completion.choices[0].message.parsed.pairs}\n    return normalize_dict_keys(generated_response)\n\nwith mlflow.start_run() as run:\n    predicted = get_completion(images[0])\n    print(predicted)\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"The following screenshot represents the extracted questions and answers for this specific image."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Screenshot of LLM completion response",src:t(71944).A+"",width:"2796",height:"548"})}),"\n",(0,a.jsx)(n.h4,{id:"prompt-registry",children:"Prompt Registry"}),"\n",(0,a.jsxs)(n.p,{children:["Prompt engineering is central to LLM-based OCR, but creating an initial prompt is often not sufficient. In order to track which prompt version produced which results as we iterate, we will use ",(0,a.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry",children:"MLflow Prompt Registry"}),". This allows us to register, version, and add tags to prompts."]}),"\n",(0,a.jsx)(n.p,{children:"Here's an example of a prompt template, specifically instructing the LLM to generate results in our expected format."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'new_template = """You are an expert at Optical Character Recognition (OCR). Extract the questions and answers from the image as a JSON object, where the keys are questions and the values are answers. If there are no questions or answers, return an empty JSON object {}.\n"""\n'})}),"\n",(0,a.jsxs)(n.p,{children:["This initial prompt can be registered along with a prompt name, commit message, and relevant tags. Once registered, it can later be retrieved using ",(0,a.jsx)(n.code,{children:"mlflow.genai.load_prompt()"})," for reuse or further improvements."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\n# Register a new prompt for OCR question-answer extraction\nnew_prompt = mlflow.genai.register_prompt(\n    name="ocr-question-answer",\n    template=new_template,\n    commit_message="Initial commit",\n    tags={\n        "author": "author@example.com",\n        "task": "ocr",\n        "language": "en",\n    },\n)\n\nprompt = mlflow.genai.load_prompt(name_or_uri="ocr-question-answer", version=new_prompt.version)\nsystem_prompt = prompt.template\n'})}),"\n",(0,a.jsx)(n.h3,{id:"6-defining-and-evaluating-performance",children:"6. Defining and Evaluating Performance"}),"\n",(0,a.jsxs)(n.p,{children:["As ML engineers, we ensure that the OCR application using the LLM is robustly evaluated against ground truth. When evaluating an OCR system, we care about more than just accuracy. We may look at format compliance, business logic, or field extraction results. ",(0,a.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/",children:"MLflow Evaluate"})," allows us to define ",(0,a.jsx)(n.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.metrics.html",children:"built-in and custom metrics"})," that align with our use case."]}),"\n",(0,a.jsxs)(n.p,{children:["While MLflow supports LLM-as-a-judge metrics, for this OCR example it's better and cheaper to use deterministic metrics. For example, we can define a custom metric ",(0,a.jsx)(n.code,{children:"key_value_accuracy"})," to check if key-value pair of the generated output correctly matches that of the ground truth."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.metrics.base import MetricValue\nfrom mlflow.models import make_metric\n\ndef batch_completion(df: pd.DataFrame) -> pd.Series:\n    result = [get_completion(image) for image in df["inputs"]]\n    return pd.Series(result)\n\n\ndef key_value_accuracy(predictions: pd.Series, truth: pd.Series) -> MetricValue:\n    """\n    Calculate accuracy scores by comparing predicted dictionaries with ground truth dictionaries.\n    Both predictions and truth are expected to be dict[str, list[str]] format.\n    """\n    scores = []\n\n    for pred_dict, truth_dict in zip(predictions, truth):\n        if not isinstance(pred_dict, dict) or not isinstance(truth_dict, dict):\n            scores.append(0.0)\n            continue\n\n        correct = 0\n        total_answers = 0\n\n        for question, truth_answers in truth_dict.items():\n            total_answers += len(truth_answers)\n\n            if question in pred_dict:\n                pred_answers = pred_dict[question]\n\n                # Count how many truth answers are correctly predicted\n                for truth_ans in truth_answers:\n                    if truth_ans in pred_answers:\n                        correct += 1\n\n        scores.append(correct / total_answers if total_answers > 0 else 0.0)\n\n    return MetricValue(\n        scores=scores,\n        aggregate_results={\n            "mean": sum(scores) / len(scores) if scores else 0.0,\n            "p90": sorted(scores)[int(len(scores) * 0.9)] if scores else 0.0\n        }\n    )\n\ncustom_key_value_accuracy = make_metric(\n    eval_fn=key_value_accuracy,\n    greater_is_better=True,\n    name="key_value_accuracy",\n)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["After defining this custom metric, we can evaluate it over a dataframe, which includes a subset of base64-encoded images and their corresponding ground truth dictionaries. Using the ",(0,a.jsx)(n.code,{children:"batch_completion"})," function, we run a batch completion request on this subset, retrieving outputs in the predefined Structured Output format."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'results = mlflow.models.evaluate(\n    model=batch_completion,\n    data=pd.DataFrame({"inputs": images, "truth": annotation_normalized}),\n    targets="truth",\n    model_type="text",\n    predictions="predictions",\n    extra_metrics=[custom_key_value_accuracy]\n)\n\nprint("Custom metric results:", results.metrics)\neval_table = results.tables["eval_results_table"]\nprint("\\n Per-row scores:")\nprint(eval_table[[\'key_value_accuracy/score\']])\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"MLflow UI showing the metric key_value_accuracy computed for a single run",src:t(69578).A+"",width:"3406",height:"1336"})}),"\n",(0,a.jsxs)(n.p,{children:["This metric requires an exact match between the key-value pairs in the ground truth and the LLM-generated output. Upon reviewing the individual scores for each image, we observe that the model performs the worst on the last image. Specifically, one of the keys is incorrectly generated as ",(0,a.jsx)(n.code,{children:"CHAINS - ACCEPTANCEMERCHANDISING"})," instead of ",(0,a.jsx)(n.code,{children:"CHAINS ACCEPTANCE/ MERCHANDISING"}),". A similar pattern can be observed in other keys where different topics are improperly separated or unnecessarily paraphrased."]}),"\n",(0,a.jsxs)(n.p,{children:["To address this, we can refine the prompt template by explicitly instructing the model to separate distinct topics using the ",(0,a.jsx)(n.code,{children:"/"})," separator. Additionally, we can instruct the LLM to avoid paraphrasing the topics in the keys and instead focus on maintaining exactness to the image text."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'updated_template = """\\\nYou are an expert at key information extraction and OCR. Extract the questions and answers from the image, where the keys are questions and the values are answers.\n\n\nQuestion refers to a field in the form that takes in information. Answer refers to the information\nthat is filled in the field.\n\nFollow these rules:\n- Only use the information present in the text and do not paraphrase.\n- If the keys have multiple topics, separate them with a slash (/)\n{{ additional_rules }}\n"""\n'})}),"\n",(0,a.jsx)(n.p,{children:"The next step is to register the updated prompt template, load it back later and format it with any additional rule before rerunning the evaluation."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'updated_prompt = mlflow.genai.register_prompt(\n    name="ocr-question-answer",\n    template=updated_template,\n    commit_message="Update commit",\n    tags={\n        "author": "author@example.com",\n        "task": "ocr",\n        "language": "en",\n    },\n)\n\n# Load the updated prompt and format it with additional rules\n\nprompt = mlflow.genai.load_prompt(name_or_uri="ocr-question-answer", version=updated_prompt.version)\nsystem_prompt = prompt.format(additional_rules="Use exact formatting you see in the form.")\n\n\nresults_updated = mlflow.models.evaluate(\n    model=batch_completion,\n    data=pd.DataFrame({"inputs": images, "truth": annotation_normalized}),\n    targets="truth",\n    model_type="text",\n    predictions="predictions",\n    extra_metrics=[custom_key_value_accuracy]\n)\n\nprint("Custom metric results:", results_updated.metrics)\neval_table_updated = results_updated.tables["eval_results_table"]\nprint("\\n Per-row scores:")\nprint(eval_table_updated[[\'key_value_accuracy/score\']])\n'})}),"\n",(0,a.jsx)(n.p,{children:"This updated prompt may lead to a marginal improvement in the metric for each image. However, there are still mismatches in values between the ground truth and the LLM response. As showcased above using Prompt Registry, iteratively refining the prompt can address these issues, leading to better alignment with the expected output."}),"\n",(0,a.jsx)(n.h2,{id:"conclusion-and-next-steps",children:"Conclusion and Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"By leveraging MLflow GenAI capabilities, we efficiently manage prompts and evaluate models for our OCR tool. With all runs, prompts, and metrics logged, we can compare different models or prompt strategies side-by-side in the MLflow UI. This enables data-driven decisions, justifies model selection, and enables both technical and non-technical contributors to collaborate, iterate, and deploy AI solutions confidently."}),"\n",(0,a.jsx)(n.p,{children:"We can take several directions to further enhance our workflow and outcomes:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Expand Your Custom Metrics:"})," Scale out your custom evaluation metrics to more accurately capture the requirements of our specific OCR problem. This allows us to measure what truly matters for the use case, such as domain-specific accuracy, formatting compliance, or business logic adherence."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Experiment with Multiple LLMs:"})," Take advantage of MLflow\u2019s ability to track and compare experiments by iterating with different LLMs. We can view and analyze results side-by-side in the MLflow UI, making it easier to identify which model best fits our needs and to justify model selection with clear, data-driven evidence."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Utilize Tracing and Model Logging:"})," Leverage MLflow\u2019s tracing and model logging features to gain end-to-end visibility into our GenAI workflows. By capturing detailed traces and logs, we can iteratively refine our models and prompts, diagnose issues, and ensure reproducibility\u2014all within the context of our custom metrics."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Expand Governance and Access Control"}),": Implement robust governance practices to ensure secure, compliant, and auditable management of our GenAI assets and workflows. This is especially important for scaling in enterprise or regulated environments."]}),"\n",(0,a.jsx)(n.p,{children:"These are just a few of the many ways we can build on this solution. Whether we are aiming to improve model performance, streamline collaboration, or scale our solution to new domains, these MLflow capabilities support us in our GenAI development."}),"\n",(0,a.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://mlflow.org/blog/ai-observability-mlflow-tracing",children:"Practical AI Observability: Getting Started with MLflow Tracing"}),(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.a,{href:"https://mlflow.org/blog/custom-tracing",children:"Beyond Autolog: Add MLflow Tracing to a New LLM Provider"}),(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.a,{href:"https://mlflow.org/blog/llm-as-judge",children:"LLM as Judge"})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},69578:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/new_eval_metrics-f4f9d78ac0ec4f4cdd219609148ba487.png"},70333:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/mlflow-prompt-evaluate","source":"@site/blog/2025-08-30-mlflow-prompt-evaluate/index.md","title":"Building and Managing an LLM-based OCR System with MLflow","description":"Building GenAI tools presents a unique set of challenges. As we evaluate accuracy, iterate on prompts, and enable collaboration, we often encounter bottlenecks that slow down our progress toward production.","date":"2025-08-30T00:00:00.000Z","tags":[{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"evaluation","permalink":"/mlflow-website/blog/tags/evaluation"},{"inline":true,"label":"prompt-registry","permalink":"/mlflow-website/blog/tags/prompt-registry"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":16.275,"hasTruncateMarker":false,"authors":[{"name":"Allison Bennett","title":"Sr. Solutions Engineer at Databricks","url":"https://www.linkedin.com/in/allison-b-ba269889/","imageURL":"/mlflow-website/img/authors/allison_bennett.png","key":"allison-bennett","page":null},{"name":"Shyam Sankararaman","title":"Specialist Solutions Architect at Databricks","url":"https://www.linkedin.com/in/shyampr16/","imageURL":"/mlflow-website/img/authors/shyam_sankararaman.png","key":"shyam-sankararaman","page":null},{"name":"Michael Berk","title":"Sr. Resident Solutions Architect at Databricks","url":"https://www.linkedin.com/in/-michael-berk/","imageURL":"/mlflow-website/img/authors/michael_berk.png","key":"michael-berk","page":null},{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"Building and Managing an LLM-based OCR System with MLflow","slug":"mlflow-prompt-evaluate","tags":["mlflow","genai","evaluation","prompt-registry","tracing"],"authors":["allison-bennett","shyam-sankararaman","michael-berk","mlflow-maintainers"],"thumbnail":"/img/blog/prompt-evaluate/prompt-evaluate.png"},"unlisted":false,"prevItem":{"title":"Beyond Manually Crafted LLM Judges: Automate Building Domain-Specific Evaluators with MLflow","permalink":"/mlflow-website/blog/custom-llm-judges-make-judge"},"nextItem":{"title":"Assessment-focused UIs in MLflow","permalink":"/mlflow-website/blog/mlflow-assessment-ui"}}')},71412:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/eval_traces-ca5f936f5ad94243b92c3ecdd55ee36a.png"},71944:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/llm_output-7501cf263168fd1dc13da1ba36499f4f.png"}}]);