"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1069],{7974:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"mlflow","metadata":{"permalink":"/mlflow-website/blog/mlflow","source":"@site/blog/2024-08-06-langgraph-pyfunc/index.md","title":"LangGraph with Custom PyFunc","description":"In this blog, we\'ll guide you through creating a LangGraph chatbot within an MLflow custom PyFunc. By combining MLflow with LangGraph\'s ability to create and manage cyclical graphs, you can create powerful stateful, multi-actor applications in a scalable fashion.","date":"2024-08-06T00:00:00.000Z","formattedDate":"August 6, 2024","tags":[{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":9.9,"hasTruncateMarker":false,"authors":[{"name":"Michael Berk","title":"Sr. Resident Solutions Architect at Databricks","url":"https://www.linkedin.com/in/-michael-berk/","imageURL":"/img/authors/michael_berk.png","key":"michael-berk"},{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"LangGraph with Custom PyFunc","tags":["genai","mlops"],"slug":"mlflow","authors":["michael-berk","mlflow-maintainers"],"thumbnail":"img/blog/release-candidates.png"},"unlisted":false,"nextItem":{"title":"PyFunc in Practice","permalink":"/mlflow-website/blog/pyfunc-in-practice"}},"content":"In this blog, we\'ll guide you through creating a LangGraph chatbot within an MLflow custom PyFunc. By combining MLflow with LangGraph\'s ability to create and manage cyclical graphs, you can create powerful stateful, multi-actor applications in a scalable fashion.\\n\\nThroughout this post we will demonstrate how to leverage MLflow\'s ChatModel to create a serializable and servable MLflow model which can easily be tracked, versioned, and deployed on a variety of servers.\\n\\n### What is a Custom PyFunc?\\n\\nWhile MLflow strives to cover many popular machine learning libraries, there has been a proliferation of open source packages. If users want MLflow\'s myriad benefits paired with a package that doesn\'t have native support, users can create a [custom PyFunc model](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html or https://mlflow.org/blog/custom-pyfunc).\\nCustom PyFunc models allow you to integrate any Python code, providing flexibility in defining GenAI apps and AI models. These models can be easily logged, managed, and deployed using the typical MLflow APIs, enhancing flexibility and portability in machine learning workflows.\\n\\nWithin the category of custom PyFunc models, MLflow supports a specialized model called [ChatModel](https://mlflow.org/docs/latest/llms/transformers/tutorials/conversational/pyfunc-chat-model.html). It extends the base PyFunc functionality to specifically support messages. For this demo, we will use ChatModel to create a LangGraph chatbot.\\n\\n### What is LangGraph?\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits:\\n\\n- **Cycles and Branching**: Implement loops and conditionals in your apps.\\n- **Persistence**: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\\n- **Human-in-the-Loop**: Interrupt graph execution to approve or edit next action planned by the agent.\\n- **Streaming Support**: Stream outputs as they are produced by each node (including token streaming).\\n- **Integration with LangChain**: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\\n\\nLangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\\n\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nFor a full walkthrough, check out the [LangGraph Quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/) and for more on the fundamentals of design with LangGraph, check out the [conceptual guides](https://langchain-ai.github.io/langgraph/concepts/#human-in-the-loop).\\n\\n## 1 - Setup\\n\\nFirst, we must install the required dependencies. We will use OpenAI for our LLM in this example, but using LangChain with LangGraph makes it easy to substitute any alternative supported LLM or LLM provider.\\n\\n```python\\n%%capture\\n%pip install langgraph==0.2.3 langsmith==0.1.98 mlflow>=2.15.1\\n%pip install -U typing_extensions\\n%pip install langchain_openai==0.1.21\\n```\\n\\nNext, let\'s get our relevant secrets. `getpass`, as demonstrated in the [LangGraph quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/#setup) is a great way to insert your keys into an interactive jupyter environment.\\n\\n```python\\nimport os\\n\\n# Set required environment variables for authenticating to OpenAI and LangSmith\\n# Check additional MLflow tutorials for examples of authentication if needed\\n# https://mlflow.org/docs/latest/llms/openai/guide/index.html#direct-openai-service-usage\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set the OPENAI_API_KEY environment variable.\\"\\nassert \\"LANGSMITH_API_KEY\\" in os.environ, \\"Please set the LANGSMITH_API_KEY environment variable.\\"\\n```\\n\\n## 2 - Custom Utilities\\n\\nWhile this is a demo, it\'s good practice to separate reusable utilities into a separate file/directory. Below we create three general utilities that theoretically would valuable when building additional MLflow + LangGraph implementations.\\n\\nNote that we use the magic `%%writefile` command to create a new file in a jupyter notebook context. If you\'re running this outside of an interactive notebook, simply create the file below, omitting the `%%writefile {FILE_NAME}.py` line.\\n\\n```python\\n%%writefile langgraph_utils.py\\n# omit this line if directly creating this file; this command is purely for running within Jupyter\\n\\nimport os\\nfrom typing import Union, List, Dict\\n\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage,\\n    messages_from_dict,\\n)\\nfrom mlflow.types.llm import ChatMessage\\n\\n\\ndef validate_langgraph_environment_variables():\\n    \\"\\"\\"Ensure that required secrets and project environment variables are present.\\"\\"\\"\\n\\n    # Validate enviornment variable secrets are present\\n    required_secrets = [\\"OPENAI_API_KEY\\", \\"LANGSMITH_API_KEY\\"]\\n\\n    if missing_keys := [key for key in required_secrets if not os.environ.get(key)]:\\n        raise ValueError(f\\"The following keys are missing: {missing_keys}\\")\\n\\n    # Add project environent variables if not present\\n    os.environ[\\"LANCHAIN_TRACING_V2\\"] = os.environ.get(\\"LANGCHAIN_TRACING_V2\\", \\"true\\")\\n    os.environ[\\"LANGCHAIN_PROJECT\\"] = os.environ.get(\\n        \\"LANGCHAIN_TRACING_V2\\", \\"LangGraph MLflow Tutorial\\"\\n    )\\n\\n\\ndef _format_mlflow_chat_message_for_langraph_message(\\n    chat_message: ChatMessage,\\n) -> Dict:\\n    mlflow_role_to_langgraph_type = {\\n        \\"user\\": \\"human\\",\\n        \\"assistant\\": \\"ai\\",\\n        \\"system\\": \\"system\\",\\n    }\\n\\n    if role_clean := mlflow_role_to_langgraph_type.get(chat_message.role):\\n        return {\\"type\\": role_clean, \\"data\\": {\\"content\\": chat_message.content}}\\n    else:\\n        raise ValueError(f\\"Incorrect role specified: {chat_message.role}\\")\\n\\n\\ndef mlflow_chat_message_to_langgraph_message(\\n    chat_message: List[ChatMessage],\\n) -> List[Union[AIMessage, HumanMessage, SystemMessage]]:\\n    \\"\\"\\"Convert MLflow messages (list of mlflow.types.llm.ChatMessage) to LangGraph messages.\\n\\n    This utility is required because LangGraph messages have a different structure and type\\n    than MLflow ChatMessage. If we pass the payload coming into our `predict()` method directly\\n    into the LangGraph graph, we\'ll get an error.\\n    \\"\\"\\"\\n    # NOTE: This is a simplified example for demonstration purposes\\n    if isinstance(chat_message, list):\\n        list_of_parsed_dicts = [\\n            _format_mlflow_chat_message_for_langraph_message(d) for d in chat_message\\n        ]\\n        return messages_from_dict(list_of_parsed_dicts)\\n    else:\\n        raise ValueError(f\\"Invalid _dict type: {type(chat_message)}\\")\\n\\n```\\n\\nBy the end of this step, you should see a new file in your current directory with the name `langgraph_utils.py`.\\n\\nNote that it\'s best practice to add unit tests and properly organize your project into logically structured directories.\\n\\n## 3 - Custom PyFunc ChatModel\\n\\nGreat! Now that we have some reusable utilities located in `./langgraph_utils.py`, we are ready to declare a custom PyFunc and log the model. However, before writing more code, let\'s provide some quick background on the **Model from Code** feature.\\n\\n### 3.1 - Create our Model-From-Code File\\n\\nHistorically, MLflow\'s process of saving a custom `pyfunc` model uses a mechanism that has some frustrating drawbacks: `cloudpickle`. Prior to the release of support for saving a model as a Python script in MLflow 2.12.2 (known as the [models from code](https://mlflow.org/docs/latest/models.html#models-from-code) feature), logging a defined `pyfunc` involved pickling an instance of that model. Along with the pickled model artifact, MLflow will store the signature, which can be passed or inferred from the `model_input` parameter. It will also log inferred model dependencies to help you serve the model in a new environment.\\n\\nPickle is an easy-to-use serialization mechanism, but it has a variety of limitations:\\n\\n- **Limited Support for Some Data Types**: `cloudpickle` may struggle with serializing certain complex or low-level data types, such as file handles, sockets, or objects containing these types, which can lead to errors or incorrect deserialization.\\n- **Version Compatibility Issues**: Serialized objects with `cloudpickle` may not be deserializable across different versions of `cloudpickle` or Python, making long-term storage or sharing between different environments risky.\\n- **Recursion Depth for Nested Dependencies**: `cloudpickle` can serialize objects with nested dependencies (e.g., functions within functions, or objects that reference other objects). However, deeply nested dependencies can hit the recursion depth limit imposed by Python\'s interpreter.\\n- **Mutable Object States that Cannot be Serialized**: `cloudpickle` struggles to serialize certain mutable objects whose states change during runtime, especially if these objects contain non-serializable elements like open file handles, thread locks, or custom C extensions. Even if `cloudpickle` can serialize the object structure, it may fail to capture the exact state or may not be able to deserialize the state accurately, leading to potential data loss or incorrect behavior upon deserialization.\\n\\nTo get around this issue, we must perform the following steps:\\n\\n1. Create an additional Python file in our directory.\\n2. In that file, create a function that creates a [CompiledStateGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-1-build-a-basic-chatbot), which is DAG-based stateful chatbot.\\n3. Also in that file, create a [MLflow custom PyFunc](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html). Note that in our case, we\'re using a [custom ChatModel](https://mlflow.org/docs/latest/llms/transformers/tutorials/conversational/pyfunc-chat-model.html#Customizing-the-model).\\n4. Also in that file, set the custom ChatModel to be accessible by [MLflow model from code](https://mlflow.org/docs/latest/models.html#models-from-code) via the [mlflow.models.set_model()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.set_model) command.\\n5. In a different file, log the **path** to the file created in steps 1-3 instead of the model object.\\n\\nBy passing a Python file, we simply can load the model from that Python code, thereby bypassing all the headaches associated with serialization and `cloudpickle`.\\n\\n```python\\n%%writefile graph_chain.py\\n# omit this line if directly creating this file; this command is purely for running within Jupyter\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.graph.state import CompiledStateGraph\\n\\n# Our custom utilities\\nfrom langgraph_utils import (\\n    mlflow_chat_message_to_langgraph_message,\\n    validate_langgraph_environment_variables,\\n)\\n\\nimport mlflow\\nfrom mlflow.types.llm import ChatMessage, ChatParams, ChatResponse\\n\\nimport random\\nfrom typing import Annotated, List\\nfrom typing_extensions import TypedDict\\n\\n\\ndef load_graph() -> CompiledStateGraph:\\n    \\"\\"\\"Create example chatbot from LangGraph Quickstart.\\"\\"\\"\\n\\n    class State(TypedDict):\\n        messages: Annotated[list, add_messages]\\n\\n    graph_builder = StateGraph(State)\\n    llm = ChatOpenAI()\\n\\n    def chatbot(state: State):\\n        return {\\"messages\\": [llm.invoke(state[\\"messages\\"])]}\\n\\n    graph_builder.add_node(\\"chatbot\\", chatbot)\\n    graph_builder.add_edge(START, \\"chatbot\\")\\n    graph_builder.add_edge(\\"chatbot\\", END)\\n    return graph_builder.compile()\\n\\n\\nclass LangGraphChatModel(mlflow.pyfunc.ChatModel):\\n    def load_context(self, context):\\n        self.graph = load_graph()\\n\\n    def predict(\\n        self, context, messages: List[ChatMessage], params: ChatParams\\n    ) -> ChatResponse:\\n\\n        # Format mlflow ChatMessage as LangGraph messages\\n        messages = mlflow_chat_message_to_langgraph_message(messages)\\n\\n        # Query the model\\n        response = self.graph.invoke({\\"messages\\": messages})\\n\\n        # Extract the response text\\n        text = response[\\"messages\\"][-1].content\\n\\n        # NB: chat session ID should be handled on the client side. Here we\\n        # create a placeholder for demonstration purposes. Furthermore, if you\\n        # need persistance between model sessions, it\'s a good idea to\\n        # write your session history to a database.\\n        id = f\\"some_meaningful_id_{random.randint(0, 100)}\\"\\n\\n        # Format the response to be compatible with MLflow\\n        response = {\\n            \\"id\\": id,\\n            \\"model\\": \\"MyChatModel\\",\\n            \\"choices\\": [\\n                {\\n                    \\"index\\": 0,\\n                    \\"message\\": {\\"role\\": \\"assistant\\", \\"content\\": text},\\n                    \\"finish_reason\\": \\"stop\\",\\n                }\\n            ],\\n            \\"usage\\": {},\\n        }\\n\\n        return ChatResponse(**response)\\n\\n\\n# Set our model to be accessible by MLflow model from code\\nmlflow.models.set_model(LangGraphChatModel())\\n```\\n\\n### 3.2 - Log our Model-From-Code\\n\\nAfter creating this ChatModel implementation in we leverage the standard MLflow APIs to log the model. However, as noted above, instead of passing a model object, we pass the path `str` to the file containing our `mlflow.models.set_model()` command.\\n\\n```python\\nimport mlflow\\n\\n# Save the model\\nwith mlflow.start_run() as run:\\n    # Log the model to the mlflow tracking server\\n    mlflow.pyfunc.log_model(\\n        python_model=\\"graph_chain.py\\", # Path to our custom model\\n        artifact_path=\\"langgraph_model\\",\\n    )\\n\\n    # Store the run id for later loading\\n    run_id = run.info.run_id\\n```\\n\\n## 4 - Use the Logged Model\\n\\nNow that we have successfully logged a model, we can load it and leverage it for inference.\\n\\nIn the code below, we demonstrate that our chain has chatbot functionality!\\n\\n```python\\nimport mlflow\\n\\n# Load the model\\n# NOTE: you need the run_id from the above step or another model URI format\\nloaded_model = mlflow.pyfunc.load_model(f\\"runs:/{run_id}/langgraph_model\\")\\n\\n# Show inference and message history\\nprint(\\"-------- Message 1 -----------\\")\\nmessage = \\"What\'s my name?\\"\\npayload = {\\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": message}]}\\nresponse = loaded_model.predict(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {response[\'choices\'][-1][\'message\'][\'content\']}\\")\\n\\n# print(\\"\\\\n-------- Message 2 -----------\\")\\nmessage = \\"My name is Morpheus.\\"\\nmessage_history = [choice[\'message\'] for choice in response[\'choices\']]\\npayload = {\\"messages\\": message_history + [{\\"role\\": \\"user\\", \\"content\\": message}]}\\nresponse = loaded_model.predict(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {response[\'choices\'][-1][\'message\'][\'content\']}\\")\\n\\n# # print(\\"\\\\n-------- Message 3 -----------\\")\\nmessage = \\"What\'s my name?\\"\\nmessage_history = [choice[\'message\'] for choice in response[\'choices\']]\\npayload = {\\"messages\\": message_history + [{\\"role\\": \\"user\\", \\"content\\": message}]}\\nresponse = loaded_model.predict(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {response[\'choices\'][-1][\'message\'][\'content\']}\\")\\n```\\n\\nOuput:\\n\\n```text\\n-------- Message 1 -----------\\nUser: What\'s my name?\\nAgent: I\'m sorry, I don\'t know your name. Can you please tell me?\\n\\n-------- Message 2 -----------\\nUser: My name is Morpheus.\\nAgent: Nice to meet you, Morpheus! How can I assist you today?\\n\\n-------- Message 3 -----------\\nUser: What\'s my name?\\nAgent: Your name is Morpheus!\\n```\\n\\n## 5 - Summary\\n\\nThere are many logical extensions of the this tutorial, however the MLflow components can remain largely unchanged. Some examples include persisting chat history to a database, implementing a more complex langgraph object, productionizing this solution, and much more!\\n\\nTo summarize, here\'s what was covered in this tutorial:\\n\\n- Creating a simple LangGraph chain.\\n- Declaring a custom MLflow PyFunc ChatModel that wraps the above LangGraph chain with pre/post-processing logic.\\n- Leveraging MLflow [model from code](https://mlflow.org/docs/latest/models.html#models-from-code) functionality to log our Custom PyFunc.\\n- Loading the Custom PyFunc via the standard MLflow APIs.\\n\\nHappy coding!"},{"id":"pyfunc-in-practice","metadata":{"permalink":"/mlflow-website/blog/pyfunc-in-practice","source":"@site/blog/2024-07-26-pyfunc-in-practice/index.md","title":"PyFunc in Practice","description":"Creative Applications of MLflow Pyfunc in Machine Learning Projects","date":"2024-07-26T00:00:00.000Z","formattedDate":"July 26, 2024","tags":[{"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"label":"ensemble-models","permalink":"/mlflow-website/blog/tags/ensemble-models"}],"readingTime":22.055,"hasTruncateMarker":true,"authors":[{"name":"Hugo Carvalho","title":"Machine Learning Analyst at adidas","url":"https://www.linkedin.com/in/hugodscarvalho/","imageURL":"/img/authors/hugo_carvalho.png","key":"hugo-carvalho"},{"name":"Joana Ferreira","title":"Machine Learning Engineer at adidas","url":"https://www.linkedin.com/in/joanaferreira96/","imageURL":"/img/authors/joana_ferreira.png","key":"joana-ferreira"},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey"},{"name":"Filipe Miranda","title":"Sr. Data Engineer at adidas","url":"https://www.linkedin.com/in/filipe-miranda-b576b186/","imageURL":"/img/authors/filipe_miranda.png","key":"filipe-miranda"}],"frontMatter":{"title":"PyFunc in Practice","description":"Creative Applications of MLflow Pyfunc in Machine Learning Projects","tags":["pyfunc","mlflow","ensemble-models"],"slug":"pyfunc-in-practice","authors":["hugo-carvalho","joana-ferreira","rahul-pandey","filipe-miranda"],"thumbnail":"img/blog/pyfunc-in-practice.png"},"unlisted":false,"prevItem":{"title":"LangGraph with Custom PyFunc","permalink":"/mlflow-website/blog/mlflow"},"nextItem":{"title":"Introducing MLflow Tracing","permalink":"/mlflow-website/blog/mlflow-tracing"}},"content":"If you\'re looking to fully leverage the capabilities of `mlflow.pyfunc` and understand how it can be utilized in a Machine Learning project, this blog post will guide you through the process. MLflow PyFunc offers creative freedom and flexibility, allowing the development of complex systems encapsulated as models in MLflow that follow the same lifecycle as traditional ones. This blog will showcase how to create multi-model setups, seamlessly connect to databases, and implement your own custom fit method in your MLflow PyFunc model.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nThis blog post demonstrates the capabilities of [MLflow PyFunc](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and how it can be utilized to build a multi-model setup encapsulated as a PyFunc flavor model in MLflow. This approach allows ensemble models to follow the same lifecycle as traditional [Built-In Model Flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors) in MLflow.\\n\\nBut first, let\'s use an analogy to get you familiarized with the concept of ensemble models and why you should consider this solution in your next Machine Learning project.\\n\\nImagine you are in the market to buy a house. Would you make a decision based solely on the first house you visit and the advice of a single real estate agent? Of course not! The process of buying a house involves considering multiple factors and gathering information from various sources to make an informed decision.\\n\\nThe house buying process explained:\\n\\n- **Identify Your Needs**: Determine whether you want a new or used house, the type of house, the model, and the year of construction.\\n- **Research**: Look for a list of available houses, check for discounts and offers, read customer reviews, and seek opinions from friends and family.\\n- **Evaluate**: Consider the performance, location, neighborhood amenities, and price range.\\n- **Compare**: Compare multiple houses to find the best fit for your needs and budget.\\n\\nIn short, you wouldn\u2019t directly reach a conclusion but would instead make a decision considering all the aforementioned factors before deciding on the best choice.\\n\\nEnsemble models in Machine Learning operate on a similar idea. Ensemble learning helps improve Machine Learning results by combining several models to improve predictive performance compared to a single model. The performance increase can be due to several factors such as the reduction in variance by averaging multiple models or reducing bias by focusing on errors of previous models. There are several types of ensemble learning techniques exists such as:\\n\\n- **Averaging**\\n- **Weighted Averaging**\\n- **Bagging**\\n- **Boosting**\\n- **Stacking**\\n\\nHowever, developing such systems requires careful management of the lifecycle of ensemble models, as integrating diverse models can be highly complex. This is where MLflow PyFunc becomes invaluable. It offers the flexibility to build complex systems, treating the entire ensemble as a model that adheres to the same lifecycle processes as traditional models. Essentially, MLflow PyFunc allows the creation of custom methods tailored to ensemble models, serving as an alternative to the built-in MLflow flavors available for popular frameworks such as scikit-learn, PyTorch, and LangChain.\\n\\nThis blog utilizes the house price dataset from [Kaggle](https://www.kaggle.com/) to demonstrate the development and management of ensemble models through MLflow.\\n\\nWe will leverage various tools and technologies to highlight the capabilities of MLflow PyFunc models. Before delving into the ensemble model itself, we will explore how these components integrate to create a robust and efficient Machine Learning pipeline.\\n\\n### Components of the Project\\n\\n**DuckDB**  \\nDuckDB is a high-performance analytical database system designed to be fast, reliable, portable, and easy to use. In this project, it showcases the integration of a database connection within the model context, facilitating efficient data handling directly within the model. [Learn more about DuckDB](https://duckdb.org/).\\n\\n**scikit-learn (sklearn)**  \\nscikit-learn is a Machine Learning library for Python that provides efficient tools for data analysis and modelling. In this project, it is used to develop and evaluate various Machine Learning models that are integrated into our ensemble model. [Learn more about scikit-learn](https://scikit-learn.org/).\\n\\n**MLflow**  \\nMLflow is an open-source platform for managing the end-to-end Machine Learning lifecycle, including experimentation, reproducibility, and deployment. In this project, it tracks experiments, manages model versions, and facilitates the deployment of MLflow PyFunc models in a similar manner to how we are familiar with individual flavors. [Learn more about MLflow](https://mlflow.org/).\\n\\n> **Note:** To reproduce this project, please refer to the official MLflow documentation for more details on setting up a simple local [MLflow Tracking Server](https://mlflow.org/docs/latest/tracking/server.html).\\n\\n## Creating the Ensemble Model\\n\\nCreating a MLflow PyFunc ensemble model requires additional steps compared to using the built-in flavors for logging and working with popular Machine Learning frameworks.\\n\\nTo implement an ensemble model, you need to define an `mlflow.pyfunc` model, which involves creating a Python class that inherits from the `PythonModel` class and implementing its constructor and class methods. While the basic creation of a PyFunc model only requires implementing the `predict` method, an ensemble model requires additional methods to manage the models and obtain multi-model predictions. After instantiating the ensemble model, you must use the custom `fit` method to train the ensemble model\'s sub-models. Similar to an out-of-the-box MLflow model, you need to log the model along with its artifacts during the training run and then register the model in the MLflow Model Registry. A model alias `production` will also be added to the model to streamline both model updates and inference. Model aliases allow you to assign a mutable, named reference to a specific version of a registered model. By assigning the alias to a particular model version, it can be easily referenced via a model URI or the model registry API. This setup allows for seamless updates to the model version used for inference without changing the serving workload code. For more details, refer to [Deploy and Organize Models with Aliases and Tags](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags).\\n\\nThe following sections, as depicted in the diagram, detail the implementation of each method for the ensemble model, providing a comprehensive understanding of defining, managing, and utilizing an ensemble model with MLflow PyFunc.\\n\\n![Ensemble Model Architecture](ensemble-model-architecture.png)\\n\\nBefore delving into the detailed implementation of each method, let\'s first review the skeleton of our `EnsembleModel` class. This skeleton serves as a blueprint for understanding the structure of the ensemble model. The subsequent sections will provide an overview and code for both the default methods provided by MLflow PyFunc and the custom methods implemented for the ensemble model.\\n\\nHere is the skeleton of the `EnsembleModel` class:\\n\\n```python\\nimport mlflow\\n\\nclass EnsembleModel(mlflow.pyfunc.PythonModel):\\n    \\"\\"\\"Ensemble model class leveraging Pyfunc for multi-model integration in MLflow.\\"\\"\\"\\n\\n    def __init__(self):\\n        \\"\\"\\"Initialize the EnsembleModel instance.\\"\\"\\"\\n        ...\\n\\n    def add_strategy_and_save_to_db(self):\\n        \\"\\"\\"Add strategies to the DuckDB database.\\"\\"\\"\\n        ...\\n\\n    def feature_engineering(self):\\n        \\"\\"\\"Perform feature engineering on input data.\\"\\"\\"\\n        ...\\n\\n    def initialize_models(self):\\n        \\"\\"\\"Initialize models and their hyperparameter grids.\\"\\"\\"\\n        ...\\n\\n    def fit(self):\\n        \\"\\"\\"Train the ensemble of models.\\"\\"\\"\\n        ...\\n\\n    def predict(self):\\n        \\"\\"\\"Predict using the ensemble of models.\\"\\"\\"\\n        ...\\n\\n    def load_context(self):\\n        \\"\\"\\"Load the preprocessor and models from the MLflow context.\\"\\"\\"\\n        ...\\n```\\n\\n### Initializing the EnsembleModel\\n\\nThe constructor method in the ensemble model is crucial for setting up its essential elements. It establishes key attributes such as the preprocessor, a dictionary to store trained models, the path to a DuckDB database, and a pandas DataFrame for managing different ensemble strategies. Additionally, it takes advantage of the `initialize_models` method to define the sub-models integrated into the ensemble.\\n\\n```python\\nimport pandas as pd\\n\\ndef __init__(self):\\n    \\"\\"\\"\\n    Initializes the EnsembleModel instance.\\n\\n    Sets up an empty preprocessing pipeline, a dictionary for fitted models,\\n    and a DataFrame to store strategies. Also calls the method to initialize sub-models.\\n    \\"\\"\\"\\n    self.preprocessor = None\\n    self.fitted_models = {}\\n    self.db_path = None\\n    self.strategies = pd.DataFrame(columns=[\\"strategy\\", \\"model_list\\", \\"weights\\"])\\n    self.initialize_models()\\n```\\n\\n### Adding Strategies and Saving to the Database\\n\\nThe custom-defined `add_strategy_and_save_to_db` method enables the addition of new ensemble strategies to the model and their storage in a DuckDB database. This method accepts a pandas DataFrame containing the strategies and the database path as inputs. It appends the new strategies to the existing ones and saves them in the database specified during the initialization of the ensemble model. This method facilitates the management of various ensemble strategies and ensures their persistent storage for future use.\\n\\n```python\\nimport duckdb\\nimport pandas as pd\\n\\ndef add_strategy_and_save_to_db(self, strategy_df: pd.DataFrame, db_path: str) -> None:\\n    \\"\\"\\"Add strategies from a DataFrame and save them to the DuckDB database.\\n\\n    Args:\\n        strategy_df (pd.DataFrame): DataFrame containing strategies.\\n        db_path (str): Path to the DuckDB database.\\n    \\"\\"\\"\\n    # Update the instance-level database path for the current object\\n    self.db_path = db_path\\n\\n    # Attempt to concatenate new strategies with the existing DataFrame\\n    try:\\n        self.strategies = pd.concat([self.strategies, strategy_df], ignore_index=True)\\n    except Exception as e:\\n        # Print an error message if any exceptions occur during concatenation\\n        print(f\\"Error concatenating DataFrames: {e}\\")\\n        return  # Exit early to prevent further errors\\n\\n    # Use context manager for the database connection\\n    try:\\n        with duckdb.connect(self.db_path) as con:\\n            # Register the strategies DataFrame as a temporary table in DuckDB\\n            con.register(\\"strategy_df\\", self.strategies)\\n\\n            # Drop any existing strategies table and create a new one with updated strategies\\n            con.execute(\\"DROP TABLE IF EXISTS strategies\\")\\n            con.execute(\\"CREATE TABLE strategies AS SELECT * FROM strategy_df\\")\\n    except Exception as e:\\n        # Print an error message if any exceptions occur during database operations\\n        print(f\\"Error executing database operations: {e}\\")\\n```\\n\\nThe following example demonstrates how to use this method to add strategies to the database.\\n\\n```python\\nimport pandas as pd\\n\\n# Initialize ensemble model\\nensemble_model = EnsembleModel()\\n\\n# Define strategies for the ensemble model\\nstrategy_data = {\\n    \\"strategy\\": [\\"average_1\\"],\\n    \\"model_list\\": [\\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\"],\\n    \\"weights\\": [\\"1\\"],\\n}\\n\\n# Create a DataFrame to hold the strategy information\\nstrategies_df = pd.DataFrame(strategy_data)\\n\\n# Add strategies to the database\\nensemble_model.add_strategy_and_save_to_db(strategies_df, \\"models/strategies.db\\")\\n```\\n\\nThe DataFrame `strategy_data` includes:\\n\\n- **strategy**: The name of the strategy for model predictions.\\n- **model_list**: A comma-separated list of model names included in the strategy.\\n- **weights**: A comma-separated list of weights assigned to each model in the `model_list`. If not provided, implies equal weights or default values.\\n\\n| strategy  | model_list                                                     | weights |\\n| --------- | -------------------------------------------------------------- | ------- |\\n| average_1 | random_forest,xgboost,decision_tree,gradient_boosting,adaboost | 1       |\\n\\n### Feature Engineering\\n\\nThe `feature_engineering` method preprocesses input data by handling missing values, scaling numerical features, and encoding categorical features. It applies different transformations to both numerical and categorical features, and returns the processed features as a NumPy array. This method is crucial for preparing data in a suitable format for model training, ensuring consistency and enhancing model performance.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ndef feature_engineering(self, X: pd.DataFrame) -> np.ndarray:\\n    \\"\\"\\"\\n    Applies feature engineering to the input data X, including imputation, scaling, and encoding.\\n\\n    Args:\\n        X (pd.DataFrame): Input features with potential categorical and numerical columns.\\n\\n    Returns:\\n        np.ndarray: Processed feature array after transformations.\\n    \\"\\"\\"\\n    # Convert columns with \'object\' dtype to \'category\' dtype for proper handling of categorical features\\n    X = X.apply(\\n        lambda col: col.astype(\\"category\\") if col.dtypes == \\"object\\" else col\\n    )\\n\\n    # Identify categorical and numerical features from the DataFrame\\n    categorical_features = X.select_dtypes(include=[\\"category\\"]).columns\\n    numerical_features = X.select_dtypes(include=[\\"number\\"]).columns\\n\\n    # Define the pipeline for numerical features: imputation followed by scaling\\n    numeric_transformer = Pipeline(\\n        steps=[\\n            (\\n                \\"imputer\\",\\n                SimpleImputer(strategy=\\"median\\"),\\n            ),  # Replace missing values with the median\\n            (\\n                \\"scaler\\",\\n                StandardScaler(),\\n            ),  # Standardize features by removing the mean and scaling to unit variance\\n        ]\\n    )\\n\\n    # Define the pipeline for categorical features: imputation followed by one-hot encoding\\n    categorical_transformer = Pipeline(\\n        steps=[\\n            (\\n                \\"imputer\\",\\n                SimpleImputer(strategy=\\"most_frequent\\"),\\n            ),  # Replace missing values with the most frequent value\\n            (\\n                \\"onehot\\",\\n                OneHotEncoder(handle_unknown=\\"ignore\\"),\\n            ),  # Encode categorical features as a one-hot numeric array\\n        ]\\n    )\\n\\n    # Create a ColumnTransformer to apply the appropriate pipelines to the respective feature types\\n    preprocessor = ColumnTransformer(\\n        transformers=[\\n            (\\n                \\"num\\",\\n                numeric_transformer,\\n                numerical_features,\\n            ),  # Apply the numeric pipeline to numerical features\\n            (\\n                \\"cat\\",\\n                categorical_transformer,\\n                categorical_features,\\n            ),  # Apply the categorical pipeline to categorical features\\n        ]\\n    )\\n\\n    # Fit and transform the input data using the preprocessor\\n    X_processed = preprocessor.fit_transform(X)\\n\\n    # Store the preprocessor for future use in the predict method\\n    self.preprocessor = preprocessor\\n    return X_processed\\n```\\n\\n### Initializing Models\\n\\nThe `initialize_models` method sets up a dictionary of various Machine Learning models along with their hyperparameter grids. This includes models such as `RandomForest`, `XGBoost`, `DecisionTree`, `GradientBoosting`, and `AdaBoost`. This step is crucial for preparing the ensemble\u2019s sub-models and specifying the hyperparameters to adjust during training, ensuring that each model is configured correctly and ready for training.\\n\\n```python\\nfrom sklearn.ensemble import (\\n    AdaBoostRegressor,\\n    GradientBoostingRegressor,\\n    RandomForestRegressor,\\n)\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom xgboost import XGBRegressor\\n\\ndef initialize_models(self) -> None:\\n    \\"\\"\\"\\n    Initializes a dictionary of models along with their hyperparameter grids for grid search.\\n    \\"\\"\\"\\n    # Define various regression models with their respective hyperparameter grids for tuning\\n    self.models = {\\n        \\"random_forest\\": (\\n            RandomForestRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [None, 10, 20]},\\n        ),\\n        \\"xgboost\\": (\\n            XGBRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [3, 6, 10]},\\n        ),\\n        \\"decision_tree\\": (\\n            DecisionTreeRegressor(random_state=42),\\n            {\\"max_depth\\": [None, 10, 20]},\\n        ),\\n        \\"gradient_boosting\\": (\\n            GradientBoostingRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [3, 5, 7]},\\n        ),\\n        \\"adaboost\\": (\\n            AdaBoostRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"learning_rate\\": [0.01, 0.1, 1.0]},\\n        ),\\n    }\\n```\\n\\n### Defining a Custom `fit` Method to Train and Save Multi-Models\\n\\nAs already highlighted in the previous method, a key feature of MLflow PyFunc models is the ability to define custom methods, providing significant flexibility and customization for various tasks. In the multi-model PyFunc setup, the `fit` method is essential for customizing and optimizing multiple sub-models. It manages the training and fine-tuning of algorithms such as `RandomForestRegressor`, `XGBRegressor`, `DecisionTreeRegressor`, `GradientBoostingRegressor`, and `AdaBoostRegressor`. For demonstration purposes, Grid Search is used, which, while straightforward, can be computationally intensive and time-consuming, especially for ensemble models. To enhance efficiency, advanced optimization methods such as Bayesian optimization are recommended. Tools like [Optuna](https://optuna.org/) and [Hyperopt](https://hyperopt.github.io/hyperopt/) leverage probabilistic models to intelligently navigate the search space, significantly reducing the number of evaluations needed to identify optimal configurations.\\n\\n```python\\nimport os\\n\\nimport joblib\\nimport pandas as pd\\nfrom sklearn.model_selection import GridSearchCV\\n\\ndef fit(\\n        self, X_train_processed: pd.DataFrame, y_train: pd.Series, save_path: str\\n    ) -> None:\\n    \\"\\"\\"\\n    Trains the ensemble of models using the provided preprocessed training data.\\n\\n    Args:\\n        X_train_processed (pd.DataFrame): Preprocessed feature matrix for training.\\n        y_train (pd.Series): Target variable for training.\\n        save_path (str): Directory path where trained models will be saved.\\n    \\"\\"\\"\\n    # Create the directory for saving models if it does not exist\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    # Iterate over each model and its parameter grid\\n    for model_name, (model, param_grid) in self.models.items():\\n        # Perform GridSearchCV to find the best hyperparameters for the current model\\n        grid_search = GridSearchCV(\\n            model, param_grid, cv=5, n_jobs=-1, scoring=\\"neg_mean_squared_error\\"\\n        )\\n        grid_search.fit(\\n            X_train_processed, y_train\\n        )  # Fit the model with the training data\\n\\n        # Save the best estimator from GridSearchCV\\n        best_model = grid_search.best_estimator_\\n        self.fitted_models[model_name] = best_model\\n\\n        # Save the trained model to disk\\n        joblib.dump(best_model, os.path.join(save_path, f\\"{model_name}.pkl\\"))\\n```\\n\\n### Defining a Custom `predict` Method to Aggregate Multi-model Predictions\\n\\nTo streamline the inference process, every PyFunc model should define a custom `predict` method as the single entry point for inference. This approach abstracts the model\'s internal workings at inference time, whether dealing with a custom PyFunc model or an out-of-the-box MLflow built-in flavor for popular ML frameworks.\\n\\nThe custom `predict` method for the ensemble model is designed to collect and combine predictions from the sub-models, supporting various aggregation strategies (e.g., average, weighted). The process involves the following steps:\\n\\n1. Load the sub-model predictions aggregation strategy based on the user-defined approach.\\n2. Load the models to be used for inference.\\n3. Preprocess the input data.\\n4. Collect predictions from individual models.\\n5. Aggregate the model predictions according to the specified strategy.\\n\\n```python\\nimport duckdb\\nimport joblib\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ndef predict(self, context, model_input: pd.DataFrame) -> np.ndarray:\\n    \\"\\"\\"\\n    Predicts the target variable using the ensemble of models based on the selected strategy.\\n\\n    Args:\\n        context: MLflow context object.\\n        model_input (pd.DataFrame): Input features for prediction.\\n\\n    Returns:\\n        np.ndarray: Array of predicted values.\\n\\n    Raises:\\n        ValueError: If the strategy is unknown or no models are fitted.\\n    \\"\\"\\"\\n    # Check if the \'strategy\' column is present in the input DataFrame\\n    if \\"strategy\\" in model_input.columns:\\n        # Extract the strategy and drop it from the input features\\n        print(f\\"Strategy: {model_input[\'strategy\'].iloc[0]}\\")\\n        strategy = model_input[\\"strategy\\"].iloc[0]\\n        model_input.drop(columns=[\\"strategy\\"], inplace=True)\\n    else:\\n        # Default to \'average\' strategy if none is provided\\n        strategy = \\"average\\"\\n\\n    # Load the strategy details from the pre-loaded strategies DataFrame\\n    loaded_strategy = self.strategies[self.strategies[\\"strategy\\"] == strategy]\\n\\n    if loaded_strategy.empty:\\n        # Raise an error if the specified strategy is not found\\n        raise ValueError(\\n            f\\"Strategy \'{strategy}\' not found in the pre-loaded strategies.\\"\\n        )\\n\\n    # Parse the list of models to be used for prediction\\n    model_list = loaded_strategy[\\"model_list\\"].iloc[0].split(\\",\\")\\n\\n    # Transform input features using the preprocessor, if available\\n    if self.preprocessor is None:\\n        # Feature engineering is required if the preprocessor is not set\\n        X_processed = self.feature_engineering(model_input)\\n    else:\\n        # Use the existing preprocessor to transform the features\\n        X_processed = self.preprocessor.transform(model_input)\\n\\n    if not self.fitted_models:\\n        # Raise an error if no models are fitted\\n        raise ValueError(\\"No fitted models found. Please fit the models first.\\")\\n\\n    # Collect predictions from all models specified in the strategy\\n    predictions = np.array(\\n        [self.fitted_models[model].predict(X_processed) for model in model_list]\\n    )\\n\\n    # Apply the specified strategy to combine the model predictions\\n    if \\"average\\" in strategy:\\n        # Calculate the average of predictions from all models\\n        return np.mean(predictions, axis=0)\\n    elif \\"weighted\\" in strategy:\\n        # Extract weights from the strategy and normalize them\\n        weights = [float(w) for w in loaded_strategy[\\"weights\\"].iloc[0].split(\\",\\")]\\n        weights = np.array(weights)\\n        weights /= np.sum(weights)  # Ensure weights sum to 1\\n\\n        # Compute the weighted average of predictions\\n        return np.average(predictions, axis=0, weights=weights)\\n    else:\\n        # Raise an error if an unknown strategy is encountered\\n        raise ValueError(f\\"Unknown strategy: {strategy}\\")\\n```\\n\\n### Defining a `load context` custom method to initialize the Ensemble Model\\n\\nWhen loading the ensemble model using `mlflow.pyfunc.load_model`, the custom `load_context` method is executed to handle the required model initialization steps before inference.\\n\\nThis initialization process includes:\\n\\n1. Loading model artifacts, including both the pre-trained models and the preprocessor, using the context object that contains the artifacts references.\\n2. Fetching strategies definitions from DuckDB Database.\\n\\n```python\\nimport duckdb\\nimport joblib\\nimport pandas as pd\\n\\ndef load_context(self, context) -> None:\\n    \\"\\"\\"\\n    Loads the preprocessor and models from the MLflow context.\\n\\n    Args:\\n        context: MLflow context object which provides access to saved artifacts.\\n    \\"\\"\\"\\n    # Load the preprocessor if its path is specified in the context artifacts\\n    preprocessor_path = context.artifacts.get(\\"preprocessor\\", None)\\n    if preprocessor_path:\\n        self.preprocessor = joblib.load(preprocessor_path)\\n\\n    # Load each model from the context artifacts and store it in the fitted_models dictionary\\n    for model_name in self.models.keys():\\n        model_path = context.artifacts.get(model_name, None)\\n        if model_path:\\n            self.fitted_models[model_name] = joblib.load(model_path)\\n        else:\\n            # Print a warning if a model is not found in the context artifacts\\n            print(\\n                f\\"Warning: {model_name} model not found in artifacts. Initialized but not fitted.\\"\\n            )\\n\\n    # Reconnect to the DuckDB database to load the strategies\\n    conn = duckdb.connect(self.db_path)\\n    # Fetch strategies from the DuckDB database into the strategies DataFrame\\n    self.strategies = conn.execute(\\"SELECT * FROM strategies\\").fetchdf()\\n    # Close the database connection\\n    conn.close()\\n```\\n\\n### Bringing It All Together\\n\\nHaving explored each method in detail, the next step is to integrate them to observe the complete implementation in action. This will offer a comprehensive view of how the components interact to achieve the project\'s objectives.\\n\\nYou can use the skeleton provided in the [Creating the Ensemble Model](#creating-the-ensemble-model) section to assemble the entire `EnsembleModel` class. Each method was demonstrated with its specific dependencies included. Now, you just need to combine these methods into the class definition, following the outline given. Feel free to add any custom logic that fits your specific use case or enhances the functionality of the ensemble model.\\n\\nAfter everything has been encapsulated in a PyFunc model, the lifecycle of the ensemble model closely mirrors that of a traditional MLflow model. The following diagram depicts the lifecycle of the model.\\n\\n![Ensemble Model Lifecycle](ensemble-model-lifecycle.png)\\n\\n## MLflow Tracking\\n\\n### Using the `fit` Method to Train Sub-Models\\n\\nOnce the data is preprocessed, we use the custom `fit` method to train all the sub-models in our Ensemble Model. This method applies grid search to find the best hyperparameters for each sub-model, fits them to the training data, and saves the trained models for future use.\\n\\n> **Note:** For the following block of code, you might need to set the MLflow Tracking Server if you\'re not using Managed MLflow. In the [Components of the Project](#components-of-the-project), there\'s a note about setting up a simple local MLflow Tracking Server. For this step of the project, you\'ll need to point MLflow to the server\u2019s URI that has been configured and is currently running. Don\'t forget to set the server URI variable `remote_server_uri`. You can refer to the official MLflow documentation for more details on [Logging to a Tracking Server](https://mlflow.org/docs/latest/tracking/server.html#logging-to-a-tracking-server).\\n\\n```python\\nimport datetime\\nimport os\\n\\nimport joblib\\nimport mlflow\\nimport pandas as pd\\nfrom mlflow.models.signature import infer_signature\\nfrom sklearn.model_selection import train_test_split\\n\\n# Initialize the MLflow client\\nclient = mlflow.MlflowClient()\\n\\n# Set the URI of your MLflow Tracking Server\\nremote_server_uri = \\"...\\"  # Replace with your server URI\\n\\n# Point MLflow to your MLflow Tracking Server\\nmlflow.set_tracking_uri(remote_server_uri)\\n\\n# Set the experiment name for organizing runs in MLflow\\nmlflow.set_experiment(\\"Ensemble Model\\")\\n\\n# Load dataset from the provided URL\\ndata = pd.read_csv(\\n    \\"https://github.com/zobi123/Machine-Learning-project-with-Python/blob/master/Housing.csv?raw=true\\"\\n)\\n\\n# Separate features and target variable\\nX = data.drop(\\"price\\", axis=1)\\ny = data[\\"price\\"]\\n\\n# Split dataset into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)\\n\\n# Create a directory to save the models and related files\\nos.makedirs(\\"models\\", exist_ok=True)\\n\\n# Initialize and train the EnsembleModel\\nensemble_model = EnsembleModel()\\n\\n# Preprocess the training data using the defined feature engineering method\\nX_train_processed = ensemble_model.feature_engineering(X_train)\\n\\n# Fit the models with the preprocessed training data and save them\\nensemble_model.fit(X_train_processed, y_train, save_path=\\"models\\")\\n\\n# Infer the model signature using a small example from the training data\\nexample_input = X_train[:1]  # Use a single sample for signature inference\\nexample_input[\\"strategy\\"] = \\"average\\"\\nexample_output = y_train[:1]\\nsignature = infer_signature(example_input, example_output)\\n\\n# Save the preprocessing pipeline to disk\\njoblib.dump(ensemble_model.preprocessor, \\"models/preprocessor.pkl\\")\\n\\n# Define strategies for the ensemble model\\nstrategy_data = {\\n    \\"strategy\\": [\\n        \\"average_1\\",\\n        \\"average_2\\",\\n        \\"weighted_1\\",\\n        \\"weighted_2\\",\\n        \\"weighted_3\\",\\n        \\"weighted_4\\",\\n    ],\\n    \\"model_list\\": [\\n        \\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\",\\n        \\"decision_tree\\",\\n        \\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\",\\n        \\"random_forest,xgboost,gradient_boosting\\",\\n        \\"decision_tree,adaboost\\",\\n        \\"xgboost,gradient_boosting\\",\\n    ],\\n    \\"weights\\": [\\"1\\", \\"1\\", \\"0.2,0.3,0.1,0.2,0.2\\", \\"0.4,0.4,0.2\\", \\"0.5,0.5\\", \\"0.7,0.3\\"],\\n}\\n\\n# Create a DataFrame to hold the strategy information\\nstrategies_df = pd.DataFrame(strategy_data)\\n\\n# Add strategies to the database\\nensemble_model.add_strategy_and_save_to_db(strategies_df, \\"models/strategies.db\\")\\n\\n# Define the Conda environment configuration for the MLflow model\\nconda_env = {\\n    \\"name\\": \\"mlflow-env\\",\\n    \\"channels\\": [\\"conda-forge\\"],\\n    \\"dependencies\\": [\\n        \\"python=3.8\\",\\n        \\"scikit-learn=1.3.0\\",\\n        \\"xgboost=2.0.3\\",\\n        \\"joblib=1.2.0\\",\\n        \\"pandas=1.5.3\\",\\n        \\"numpy=1.23.5\\",\\n        \\"duckdb=1.0.0\\",\\n        {\\n            \\"pip\\": [\\n                \\"mlflow==2.14.1\\",\\n            ]\\n        },\\n    ],\\n}\\n\\n# Get current timestamp\\ntimestamp = datetime.datetime.now().isoformat()\\n\\n# Log the model using MLflow\\nwith mlflow.start_run(run_name=timestamp) as run:\\n    # Log parameters, artifacts, and model signature\\n    mlflow.log_param(\\"model_type\\", \\"EnsembleModel\\")\\n\\n    artifacts = {\\n        model_name: os.path.join(\\"models\\", f\\"{model_name}.pkl\\")\\n        for model_name in ensemble_model.models.keys()\\n    }\\n    artifacts[\\"preprocessor\\"] = os.path.join(\\"models\\", \\"preprocessor.pkl\\")\\n    artifacts[\\"strategies_db\\"] = os.path.join(\\"models\\", \\"strategies.db\\")\\n\\n    mlflow.pyfunc.log_model(\\n        artifact_path=\\"ensemble_model\\",\\n        python_model=ensemble_model,\\n        artifacts=artifacts,\\n        conda_env=conda_env,\\n        signature=signature,\\n    )\\n\\n    print(f\\"Model logged in run {run.info.run_id}\\")\\n```\\n\\n### Registering the Model with MLflow\\n\\nFollowing the completion of model training, the subsequent step involves registering the ensemble model with MLflow. This process entails logging the trained models, preprocessing pipelines, and associated strategies into the MLflow Tracking Server. This ensures that all components of the ensemble model are systematically saved and versioned, facilitating reproducibility and traceability.\\n\\nMoreover, we will assign to this initial version of the model a production alias. This designation establishes a baseline model against which future iterations can be assessed. By marking this version as the `production` model, we can effectively benchmark improvements and confirm that subsequent versions yield measurable advancements over this established baseline.\\n\\n```python\\n# Register the model in MLflow and assign a production alias\\nmodel_uri = f\\"runs:/{run.info.run_id}/ensemble_model\\"\\nmodel_details = mlflow.register_model(model_uri=model_uri, name=\\"ensemble_model\\")\\n\\nclient.set_registered_model_alias(\\n\\tname=\\"ensemble_model\\", alias=\\"production\\", version=model_details.version\\n)\\n```\\n\\nThe following illustration demonstrates the complete lifecycle of our ensemble model within the MLflow UI up until this step.\\n\\n![Ensemble Model within MLflow UI](ensemble-model-mlflow-ui.gif)\\n\\n### Using the `predict` Method to Perform Inference\\n\\nWith the ensemble model registered in the MLflow Model Registry, it can now be utilized to predict house prices by aggregating the predictions from the various sub-models within the ensemble.\\n\\n```python\\nimport pandas as pd\\n\\nimport mlflow\\nfrom sklearn.metrics import r2_score\\n\\n# Load the registered model using its alias\\nloaded_model = mlflow.pyfunc.load_model(\\n\\tmodel_uri=f\\"models:/ensemble_model@production\\"\\n)\\n\\n# Define the different strategies for evaluation\\nstrategies = [\\n    \\"average_1\\",\\n    \\"average_2\\",\\n    \\"weighted_1\\",\\n    \\"weighted_2\\",\\n    \\"weighted_3\\",\\n    \\"weighted_4\\",\\n]\\n\\n# Initialize a DataFrame to store the results of predictions\\nresults_df = pd.DataFrame()\\n\\n# Iterate over each strategy, make predictions, and calculate R^2 scores\\nfor strategy in strategies:\\n    # Create a test DataFrame with the current strategy\\n    X_test_with_params = X_test.copy()\\n    X_test_with_params[\\"strategy\\"] = strategy\\n\\n    # Use the loaded model to make predictions\\n    y_pred = loaded_model.predict(X_test_with_params)\\n\\n    # Calculate R^2 score for the predictions\\n    r2 = r2_score(y_test, y_pred)\\n\\n    # Store the results and R^2 score in the results DataFrame\\n    results_df[strategy] = y_pred\\n    results_df[f\\"r2_{strategy}\\"] = r2\\n\\n# Add the actual target values to the results DataFrame\\nresults_df[\\"y_test\\"] = y_test.values\\n```\\n\\nSimilar to out-of-the-box MLflow models, you begin by loading the ensemble model using `mlflow.pyfunc.load_model` to generate the house price predictions. After defining the different strategies for aggregating sub-model predictions and creating the model input containing both the housing data features and aggregation strategy, simply call the ensemble model\'s `predict` method to get the aggregated house price prediction.\\n\\n### Evaluating Model Performance with Different Strategies\\n\\nTo evaluate the performance of our ensemble model, we calculated the average R\xb2 scores for different aggregation strategies. These strategies include both simple averaging and weighted combinations of sub-models, with varying configurations of models and their respective weights. By comparing the R\xb2 scores, we can assess which strategies provide the most accurate predictions.\\n\\nThe bar graph below illustrates the average R\xb2 scores for each strategy. Higher values indicate better predictive performance. As shown in the graph, the ensemble strategies generally outperform individual models as depicted in our second strategy that is relying on a single `DecisionTree` (average_2), demonstrating the effectiveness of aggregating predictions from multiple sub-models. This visual comparison highlights the benefits of using an ensemble approach, particularly with weighted strategies that optimize the contribution of each sub-model.\\n\\n![Ensemble Model Evaluation](ensemble-model-evaluation.png)\\n\\n## Summary\\n\\nThis blog post highlights the capabilities of mlflow.pyfunc and its application in a Machine Learning project. This powerful feature of MLflow provides creative freedom and flexibility, enabling teams to build complex systems encapsulated as models within MLflow, following the same lifecycle as traditional models. The post showcases the creation of ensemble model setups, seamless integration with DuckDB, and the implementation of custom methods using this versatile module.\\n\\nBeyond offering a structured approach to achieving desired outcomes, this blog demonstrates practical possibilities based on hands-on experience, discussing potential challenges and their solutions.\\n\\n## Additional resources\\n\\nExplore the following resources for a deeper understanding of MLflow PyFunc models:\\n\\n- [Custom MLflow Models with mlflow.pyfunc](https://mlflow.org/blog/custom-pyfunc)\\n- [Understanding PyFunc in MLflow](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part2-pyfunc-components.html)\\n- [Building Custom Python Function Models with MLflow](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n- [Deploy an MLflow PyFunc model with Model Serving](https://mlflow.org/docs/latest/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.html)"},{"id":"mlflow-tracing","metadata":{"permalink":"/mlflow-website/blog/mlflow-tracing","source":"@site/blog/2024-06-10-mlflow-tracing/index.md","title":"Introducing MLflow Tracing","description":"We\'re excited to announce the release of a powerful new feature in MLflow: MLflow Tracing.","date":"2024-06-10T00:00:00.000Z","formattedDate":"June 10, 2024","tags":[{"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"},{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":3.825,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"Introducing MLflow Tracing","tags":["tracing","genai","mlops"],"slug":"mlflow-tracing","authors":["mlflow-maintainers"],"thumbnail":"img/blog/trace-intro.gif"},"unlisted":false,"prevItem":{"title":"PyFunc in Practice","permalink":"/mlflow-website/blog/pyfunc-in-practice"},"nextItem":{"title":"Deep Learning with MLflow (Part 2)","permalink":"/mlflow-website/blog/deep-learning-part-2"}},"content":"We\'re excited to announce the release of a powerful new feature in MLflow: [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html).\\nThis feature brings comprehensive instrumentation capabilities to your GenAI applications, enabling you to gain deep insights into the execution of your\\nmodels and workflows, from simple chat interfaces to complex multi-stage Retrieval Augmented Generation (RAG) applications.\\n\\n> NOTE: MLflow Tracing has been released in MLflow 2.14.0 and is not available in previous versions.\\n\\n## Introducing MLflow Tracing\\n\\nTracing is a critical aspect of understanding and optimizing complex applications, especially in the realm of machine learning and artificial intelligence.\\nWith the release of MLflow Tracing, you can now easily capture, visualize, and analyze detailed execution traces of your GenAI applications.\\nThis new feature aims to provide greater visibility and control over your applications\' performance and behavior, aiding in everything from fine-tuning to debugging.\\n\\n## What is MLflow Tracing?\\n\\nMLflow Tracing offers a variety of methods to enable [tracing](https://mlflow.org/docs/latest/llms/tracing/overview.html) in your applications:\\n\\n- **Automated Tracing with LangChain**: A fully automated integration with [LangChain](https://www.langchain.com/) allows you to activate tracing simply by enabling `mlflow.langchain.autolog()`.\\n- **Manual Trace Instrumentation with High-Level Fluent APIs**: Use decorators, function wrappers, and context managers via the fluent API to add tracing functionality with minimal code modifications.\\n- **Low-Level Client APIs for Tracing**: The MLflow client API provides a thread-safe way to handle trace implementations for fine-grained control of what and when data is recorded.\\n\\n## Getting Started with MLflow Tracing\\n\\n### LangChain Automatic Tracing\\n\\nThe easiest way to get started with MLflow Tracing is through the built-in integration with LangChain. By enabling autologging, traces are automatically logged to the active MLflow experiment when calling invocation APIs on chains. Here\u2019s a quick example:\\n\\n```python\\nimport os\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI\\nimport mlflow\\n\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set your OPENAI_API_KEY environment variable.\\"\\n\\nmlflow.set_experiment(\\"LangChain Tracing\\")\\nmlflow.langchain.autolog(log_models=True, log_input_examples=True)\\n\\nllm = OpenAI(temperature=0.7, max_tokens=1000)\\nprompt_template = \\"Imagine you are {person}, and you are answering a question: {question}\\"\\nchain = prompt_template | llm\\n\\nchain.invoke({\\"person\\": \\"Richard Feynman\\", \\"question\\": \\"Why should we colonize Mars?\\"})\\nchain.invoke({\\"person\\": \\"Linus Torvalds\\", \\"question\\": \\"Can I set everyone\'s access to sudo?\\"})\\n\\n```\\n\\nAnd this is what you will see after invoking the chains when navigating to the **LangChain Tracing** experiment in the MLflow UI:\\n\\n![Traces in UI](tracing-ui.gif)\\n\\n### Fluent APIs for Manual Tracing\\n\\nFor more control, you can use MLflow\u2019s fluent APIs to manually instrument your code. This approach allows you to capture detailed trace data with minimal changes to your existing code.\\n\\n#### Trace Decorator\\n\\nThe trace decorator captures the inputs and outputs of a function:\\n\\n```python\\nimport mlflow\\n\\nmlflow.set_experiment(\\"Tracing Demo\\")\\n\\n@mlflow.trace\\ndef some_function(x, y, z=2):\\n    return x + (y - z)\\n\\nsome_function(2, 4)\\n```\\n\\n#### Context Handler\\n\\nThe context handler is ideal for supplementing span information with additional data at the point of information generation:\\n\\n```python\\nimport mlflow\\n\\n@mlflow.trace\\ndef first_func(x, y=2):\\n    return x + y\\n\\n@mlflow.trace\\ndef second_func(a, b=3):\\n    return a * b\\n\\ndef do_math(a, x, operation=\\"add\\"):\\n    with mlflow.start_span(name=\\"Math\\") as span:\\n        span.set_inputs({\\"a\\": a, \\"x\\": x})\\n        span.set_attributes({\\"mode\\": operation})\\n        first = first_func(x)\\n        second = second_func(a)\\n        result = first + second if operation == \\"add\\" else first - second\\n        span.set_outputs({\\"result\\": result})\\n        return result\\n\\ndo_math(8, 3, \\"add\\")\\n```\\n\\n### Comprehensive Tracing with Client APIs\\n\\nFor advanced use cases, the MLflow client API offers fine-grained control over trace management. These APIs allows you to create, manipulate, and retrieve traces programmatically, albeit with additional complexity throughout the implementation.\\n\\n#### Starting and Managing Traces with the Client APIs\\n\\n```python\\nfrom mlflow import MlflowClient\\n\\nclient = MlflowClient()\\n\\n# Start a new trace\\nroot_span = client.start_trace(\\"my_trace\\")\\nrequest_id = root_span.request_id\\n\\n# Create a child span\\nchild_span = client.start_span(\\n    name=\\"child_span\\",\\n    request_id=request_id,\\n    parent_id=root_span.span_id,\\n    inputs={\\"input_key\\": \\"input_value\\"},\\n    attributes={\\"attribute_key\\": \\"attribute_value\\"},\\n)\\n\\n# End the child span\\nclient.end_span(\\n    request_id=child_span.request_id,\\n    span_id=child_span.span_id,\\n    outputs={\\"output_key\\": \\"output_value\\"},\\n    attributes={\\"custom_attribute\\": \\"value\\"},\\n)\\n\\n# End the root span (trace)\\nclient.end_trace(\\n    request_id=request_id,\\n    outputs={\\"final_output_key\\": \\"final_output_value\\"},\\n    attributes={\\"token_usage\\": \\"1174\\"},\\n)\\n```\\n\\n## Diving Deeper into Tracing\\n\\nMLflow Tracing is designed to be flexible and powerful, supporting various use cases from simple function tracing to complex, asynchronous workflows.\\n\\nTo learn more about this feature, [read the guide](https://mlflow.org/docs/latest/llms/tracing/index.html), [review the API Docs](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-fluent-python-apis) and [get started with the LangChain integration](https://mlflow.org/docs/latest/llms/tracing/index.html#langchain-automatic-tracing) today!\\n\\n## Join Us on This Journey\\n\\nThe introduction of MLflow Tracing marks a significant milestone in our mission to provide comprehensive tools for managing machine learning workflows. We\u2019re excited about the possibilities this new feature opens up and look forward to your [feedback](https://github.com/mlflow/mlflow/issues) and [contributions](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).\\n\\nFor those in our community with a passion for sharing knowledge, we invite you to [collaborate](https://github.com/mlflow/mlflow-website/blob/main/CONTRIBUTING.md). Whether it\u2019s writing tutorials, sharing use-cases, or providing feedback, every contribution enriches the MLflow community.\\n\\nStay tuned for more updates, and as always, happy coding!"},{"id":"deep-learning-part-2","metadata":{"permalink":"/mlflow-website/blog/deep-learning-part-2","source":"@site/blog/2024-04-26-deep-learning-part-2/index.md","title":"Deep Learning with MLflow (Part 2)","description":"Using MLflow\'s Deep Learning tracking features for fine tuning an LLM","date":"2024-04-26T00:00:00.000Z","formattedDate":"April 26, 2024","tags":[{"label":"Deep Learning","permalink":"/mlflow-website/blog/tags/deep-learning"}],"readingTime":11.64,"hasTruncateMarker":true,"authors":[{"name":"Puneet Jain","title":"Sr. Specialist Solutions Architect at Databricks","url":"https://www.linkedin.com/in/puneetjain159/","imageURL":"/img/authors/puneet.png","key":"puneet-jain"},{"name":"Avinash Sooriyarachchi","title":"Sr. Solutions Architect at Databricks","url":"https://www.linkedin.com/in/avi-data-ml/","imageURL":"/img/authors/avinash.png","key":"avinash-sooriyarachchi"},{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/img/authors/abe.png","key":"abe-omorogbe"},{"name":"Ben Wilson","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/benjamin-wilson-arch/","imageURL":"/img/authors/ben.png","key":"ben"}],"frontMatter":{"title":"Deep Learning with MLflow (Part 2)","description":"Using MLflow\'s Deep Learning tracking features for fine tuning an LLM","slug":"deep-learning-part-2","authors":["puneet-jain","avinash-sooriyarachchi","abe-omorogbe","ben"],"tags":["Deep Learning"],"thumbnail":"img/blog/dl-blog-2.png"},"unlisted":false,"prevItem":{"title":"Introducing MLflow Tracing","permalink":"/mlflow-website/blog/mlflow-tracing"},"nextItem":{"title":"MLflow Release Candidates","permalink":"/mlflow-website/blog/release-candidates"}},"content":"In the realm of deep learning, finetuning of pre-trained Large Language Models (LLMs) on private datasets is an excellent customization\\noption to increase a model\u2019s relevancy for a specific task. This practice is not only common, but also essential for developing specialized\\nmodels, particularly for tasks like text classification and summarization.\\n\\nIn such scenarios, tools like MLflow are invaluable. Tracking tools like MLflow help to ensure that every aspect of the training\\nprocess - metrics, parameters, and artifacts - are reproducibly tracked and logged, allowing for the analysis, comparison, and sharing of tuning iterations.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this blog post, we are going to be using [MLflow 2.12](https://mlflow.org/releases/2.12.1) and the\\n[recently introduced MLflow Deep Learning features](https://mlflow.org/blog/deep-learning-part-1) to track all the important aspects of fine\\ntuning a large language model for text classification, including the use of automated logging of training checkpoints in order to simplify\\nthe process of resumption of training.\\n\\n## Use Case: Fine Tuning a Transformer Model for Text Classification\\n\\nThe example scenario that we\'re using within this blog utilizes the [unfair-TOS](https://huggingface.co/datasets/coastalcph/lex_glue/viewer/unfair_tos) dataset.\\n\\nIn today\u2019s world, it\u2019s hard to find a service, platform, or even a consumer good that doesn\u2019t have a legally-binding terms of service connected\\nwith it. These encyclopedic size agreements, filled with dense legal jargon and sometimes baffling levels of specificity, are so large that\\nmost people simply accept them without reading them. However, reports have indicated over time that occasionally, some suspiciously unfair\\nterms are embedded within them.\\n\\nAddressing unfair clauses in Terms of Service (TOS) agreements through machine learning (ML) is particularly relevant due to the pressing\\nneed for transparency and fairness in legal agreements that affect consumers. Consider the following clause from an example TOS\\nagreement: **\\"We may revise these Terms from time to time. The changes will not be retroactive, and the most current version of the Terms, which will always...\\"**\\nThis clause stipulates that the service provider may suspend or terminate the service at any time for any reason,\\nwith or without notice. Most people would consider this to be quite unfair.\\n\\nWhile this sentence is buried quite deeply within a fairly dense document, an ML algorithm is not burdened by the exhaustion that a human\\nwould have for combing through the text and identifying clauses that might seem a bit unfair. By automatically identifying potentially\\nunfair clauses, a transformers-based Deep Learning (DL) model can help protect consumers from exploitative practices, ensure greater compliance with legal standards,\\nand foster trust between service providers and users.\\n\\nA base pre-trained transformer model, without specialized fine-tuning, faces several challenges in accurately identifying unfair Terms of Service clauses.\\nFirstly, it lacks the domain-specific knowledge essential for understanding complex legal language. Secondly, its training objectives are\\ntoo general to capture the nuanced interpretation required for legal analysis. Lastly, it may not effectively recognize the subtle\\ncontextual meanings that determine the fairness of contractual terms, making it less effective for this specialized task.\\n\\nUsing prompt engineering to address the identification of unfair Terms of Service clauses with a closed-source Large language model\\ncan be prohibitively expensive. This approach requires extensive trial and error to refine prompts without the ability to tweak\\nthe underlying model mechanics. Each iteration can consume significant computational resources , especially when using\\n[few-shot prompting](https://www.promptingguide.ai/techniques/fewshot), leading to escalating costs without guaranteeing a corresponding\\nincrease in accuracy or effectiveness.\\n\\nIn this context, the use of the **RoBERTa-base** model is particularly effective, provided that it is fine-tuned. This model is robust\\nenough to handle complex tasks like discerning embedded instructions within texts, yet it is sufficiently compact to be fine-tuned\\non modest hardware, such as an Nvidia T4 GPU.\\n\\n### What is PEFT?\\n\\n[Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/en/index) approaches are advantageous as they involve\\nkeeping the bulk of the pre-trained model parameters fixed while either only training a few additional layers or modifying the parameters used\\nwhen interacting with the model\'s weights. This methodology not only conserves memory during training, but also significantly reduces the overall training time. When\\ncompared with the alternative of fine-tuning a base model\'s weights in order to customize its performance for a specific targeted task, the PEFT\\napproach can save significant cost in both time and money, while providing an equivalent or better performance results with less data than is required\\nfor a comprehensive fine-tuning training task.\\n\\n## Integrating Hugging-Face models and the PyTorch Lightning framework\\n\\n[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) integrates seamlessly with\\n[Hugging Face\'s Transformers library](https://huggingface.co/docs/transformers/en/index), enabling streamlined model training workflows\\nthat capitalize on Lightning\'s easy-to-use Higher level API\u2019s and HF\'s state-of-the-art pre-trained models. The combination of Lightning with transformers\u2019\\n[PEFT module](https://huggingface.co/blog/peft) enhances productivity and scalability by reducing code complexity and enabling the use of\\nhigh-quality pre-optimized models for a range of diverse NLP tasks.\\n\\nBelow is an example of configuring the PEFT-based fine tuning of a base model using PyTorch Lightning and HuggingFace\'s `peft` module.\\n\\n```python\\nfrom typing import List\\nfrom lightning import LightningModule\\nfrom peft import get_peft_model, LoraConfig, TaskType\\nfrom transformers import AutoModelForSequenceClassification\\n\\n\\nclass TransformerModule(LightningModule):\\n    def __init__(\\n        self,\\n        pretrained_model: str,\\n        num_classes: int = 2,\\n        lora_alpha: int = 32,\\n        lora_dropout: float = 0.1,\\n        r: int = 8,\\n        lr: float = 2e-4\\n    ):\\n        super().__init__()\\n        self.model = self.create_model(pretrained_model, num_classes, lora_alpha, lora_dropout, r)\\n        self.lr = lr\\n        self.save_hyperparameters(\\"pretrained_model\\")\\n\\n    def create_model(self, pretrained_model, num_classes, lora_alpha, lora_dropout, r):\\n        \\"\\"\\"Create and return the PEFT model with the given configuration.\\n\\n        Args:\\n            pretrained_model: The path or identifier for the pretrained model.\\n            num_classes: The number of classes for the sequence classification.\\n            lora_alpha: The alpha parameter for LoRA.\\n            lora_dropout: The dropout rate for LoRA.\\n            r: The rank of LoRA adaptations.\\n\\n        Returns:\\n            Model: A model configured with PEFT.\\n        \\"\\"\\"\\n        model = AutoModelForSequenceClassification.from_pretrained(\\n            pretrained_model_name_or_path=pretrained_model,\\n            num_labels=num_classes\\n        )\\n        peft_config = LoraConfig(\\n            task_type=TaskType.SEQ_CLS,\\n            inference_mode=False,\\n            r=r,\\n            lora_alpha=lora_alpha,\\n            lora_dropout=lora_dropout\\n        )\\n        return get_peft_model(model, peft_config)\\n\\n    def forward(self, input_ids: List[int], attention_mask: List[int], label: List[int]):\\n        \\"\\"\\"Calculate the loss by passing inputs to the model and comparing against ground truth labels.\\n\\n        Args:\\n            input_ids: List of token indices to be fed to the model.\\n            attention_mask: List to indicate to the model which tokens should be attended to, and which should not.\\n            label: List of ground truth labels associated with the input data.\\n\\n        Returns:\\n            torch.Tensor: The computed loss from the model as a tensor.\\n        \\"\\"\\"\\n        return self.model(\\n            input_ids=input_ids,\\n            attention_mask=attention_mask,\\n            labels=label\\n        )\\n```\\n\\nAdditional references for the full implementation can be [seen within the companion repository here](https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/custom_module/fine_tune_clsify_head.py)\\n\\n## Configuring MLflow for PEFT-based fine-tuning\\n\\nBefore initiating the training process, it\'s crucial to configure MLFlow so that all system metrics, loss metrics, and parameters are logged for the training run.\\nAs of MLFlow 2.12, auto-logging for TensorFlow and PyTorch now includes support for checkpointing model weights during training, giving a snapshot of the model\\nweights at defined epoch frequencies in order to provide for training resumption in the case of an error or loss of the compute environment.\\nBelow is an example of how to enable this feature:\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.enable_system_metrics_logging()\\nmlflow.pytorch.autolog(checkpoint_save_best_only = False, checkpoint_save_freq=\'epoch\')\\n```\\n\\nIn the code above we are doing the following:\\n\\n- **Enabling System Metrics Logging**: The system resources will be logged to MLflow in order to understand where bottlenecks in memory, CPU, GPU, disk usage, and network traffic are throughout the training process.\\n\\n![MLflow UI System Metrics](sys_metrics.png)\\n\\n- **Configuring Auto Logging to log parameters, metrics and checkpoints for all epochs**: Deep learning involves experimenting with various model architectures and hyperparameter settings. Auto logging plays a crucial role in systematically recording these experiments, making it easier to compare different runs and determine which configurations yield the best results. Checkpoints are logged at every epoch, enabling detailed evaluations of all intermediate epochs during the initial exploration phase of the project. However, it is generally not advisable to log all epochs during late-stage development to avoid excessive data writes and latency in the final training stages.\\n\\n![System Metrics Logged](epoch_logging.png)\\n\\nThe auto-logged checkpoint metrics and model artifacts will be viewable in the MLflow UI as the model trains, as shown below:\\n\\n![Metrics logged with each epoch](checkpoint_metrics.png)\\n\\n## The Importance of Logging and Early-stopping\\n\\nThe integration of the Pytorch Lightning `Trainer` callback with MLflow is crucial within this training exercise. The integration allows for comprehensive\\ntracking and logging of metrics, parameters, and artifacts during model finetuning without having to explicitly call MLflow logging APIs. Additionally,\\nthe autologging API allows for modifying the default logging behavior, permitting changes to the logging frequency, allowing for logging to occur at each\\nepoch, after a specified number of epochs, or at explicitly defined steps.\\n\\n### Early stopping\\n\\nEarly stopping is a critical regularization technique in neural network training, designed to assist in preventing overfitting through the act of\\nhalting training when validation performance plateaus. Pytorch Lightning includes APIs that allow for an easy high-level control of training cessation,\\nas demonstrated below.\\n\\n### Configuring Pytorch Trainer Callback with Early stopping\\n\\nThe example below shows the configuration of the `Trainer` object within `Lightning` to leverage early stopping to prevent overfitting. Once configured, the\\ntraining is executed by calling `fit` on the `Trainer` object. By providing the `EarlyStopping` callback, in conjunction with MLflow\'s autologging, the\\nappropriate number of epochs will be used, logged, and tracked without any additional effort.\\n\\n```python\\nfrom dataclasses import dataclass, field\\nimport os\\n\\nfrom data import LexGlueDataModule\\nfrom lightning import Trainer\\nfrom lightning.pytorch.callbacks import EarlyStopping\\nimport mlflow\\n\\n\\n@dataclass\\nclass TrainConfig:\\n    pretrained_model: str = \\"bert-base-uncased\\"\\n    num_classes: int = 2\\n    lr: float = 2e-4\\n    max_length: int = 128\\n    batch_size: int = 256\\n    num_workers: int = os.cpu_count()\\n    max_epochs: int = 10\\n    debug_mode_sample: int | None = None\\n    max_time: dict[str, float] = field(default_factory=lambda: {\\"hours\\": 3})\\n    model_checkpoint_dir: str = \\"/local_disk0/tmp/model-checkpoints\\"\\n    min_delta: float = 0.005\\n    patience: int = 4\\n\\ntrain_config = TrainConfig()\\n\\n# Instantiate the custom Transformer class for PEFT training\\nnlp_model = TransformerModule(\\n        pretrained_model=train_config.pretrained_model,\\n        num_classes=train_config.num_classes,\\n        lr=train_config.lr,\\n    )\\n\\ndatamodule = LexGlueDataModule(\\n        pretrained_model=train_config.pretrained_model,\\n        max_length=train_config.max_length,\\n        batch_size=train_config.batch_size,\\n        num_workers=train_config.num_workers,\\n        debug_mode_sample=train_config.debug_mode_sample,\\n    )\\n\\n# Log system metrics while training loop is running\\nmlflow.enable_system_metrics_logging()\\n\\n# Automatically log per-epoch parameters, metrics, and checkpoint weights\\nmlflow.pytorch.autolog(checkpoint_save_best_only = False)\\n\\n# Define the Trainer configuration\\ntrainer = Trainer(\\n   callbacks=[\\n       EarlyStopping(\\n           monitor=\\"Val_F1_Score\\",\\n           min_delta=train_config.min_delta,\\n           patience=train_config.patience,\\n           verbose=True,\\n           mode=\\"max\\",\\n       )\\n   ],\\n   default_root_dir=train_config.model_checkpoint_dir,\\n   fast_dev_run=bool(train_config.debug_mode_sample),\\n   max_epochs=train_config.max_epochs,\\n   max_time=train_config.max_time,\\n   precision=\\"32-true\\"\\n)\\n\\n# Execute the training run\\ntrainer.fit(model=nlp_model, datamodule=datamodule)\\n```\\n\\n## Visualization and Sharing Capabilities within MLflow\\n\\nThe newly introduced DL-specific visualization capabilities introduced in MLflow 2.12 enable you to make comparisons between different runs and artifacts over epochs.\\nWhen comparing training runs, MLflow is capable of generating useful visualization that can be integrated into dashboards, facilitating\\neasy sharing. Additionally, the centralized storage of metrics, in conjunction with parameters, allows for effective analysis of the training\\nefficacy, as shown in the image below.\\n\\n![Epoch Run Compare](compare.png)\\n\\n## When to stop training?\\n\\nWhen training DL models, it is important to understand when to stop. Efficient training (for minimizing the overall cost incurred for\\nconducting training) and optimal model performance rely heavily on preventing a model from overfitting on the training data. A model\\nthat trains for too long will invariably become quite good at effectively \u2018memorizing\u2019 the training data, resulting in a reduction in\\nthe performance of the model when presented with novel data. A straightforward way to evaluate this behavior is to ensure that\\nvalidation data set metrics (scoring loss metrics on data that is not in the training data set) are captured during the training\\nloop. Integrating the MLflow callback into the PyTorch Lightning Trainer allows for iterative logging of loss metrics at\\nconfigurable iterations, enabling an easily debuggable evaluation of the training performance, ensuring that stopping criteria\\ncan be enforced at the appropriate time to prevent overfitting.\\n\\n### Evaluating epoch checkpoints of Fine Tuned Models with MLflow\\n\\nWith your training process meticulously tracked and logged by MLflow, you have the flexibility to retrieve and test your model at\\nany arbitrary checkpoint. To do this, you can use the mlflow.pytorch.load_model() API to load the model from a specific run\\nand use the `predict()` method for evaluation.\\n\\nIn the example below, we will load the model checkpoint from the 3rd epoch and use the `Lightning` train module to generate predictions based on the\\ncheckpoint state of the saved training epoch.\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.pytorch.autolog(disable = True)\\n\\nrun_id = \'<Add the run ID>\'\\n\\nmodel = mlflow.pytorch.load_checkpoint(TransformerModule, run_id, 3)\\n\\nexamples_to_test = [\\"We reserve the right to modify the service price at any time and retroactively apply the adjusted price to historical service usage.\\"]\\n\\ntrain_module = Trainer()\\ntokenizer = AutoTokenizer.from_pretrained(train_config.pretrained_model)\\ntokens = tokenizer(examples_to_test,\\n                  max_length=train_config.max_length,\\n                  padding=\\"max_length\\",\\n                  truncation=True)\\nds = Dataset.from_dict(dict(tokens))\\nds.set_format(\\n            type=\\"torch\\", columns=[\\"input_ids\\", \\"attention_mask\\"]\\n        )\\n\\ntrain_module.predict(model, dataloaders = DataLoader(ds))\\n```\\n\\n## Summary\\n\\nThe integration of MLflow into the finetuning process of pre-trained language models, particularly for applications like custom\\nnamed entity recognition, text classification and instruction-following represents a significant advancement in managing and\\noptimizing deep learning workflows. Leveraging the autologging and tracking capabilities of MLflow in these workstreams not only\\nenhances the reproducibility and efficiency of model development, but also fosters a collaborative environment where insights\\nand improvements can be easily shared and implemented.\\n\\nAs we continue to push the boundaries of what these models can achieve, tools like MLflow will be instrumental in harnessing their full potential.\\n\\nIf you\'re interested in seeing the full example in its entirety, feel free to [see the full example implementation](https://github.com/puneet-jain159/deeplearning_with_mlfow)\\n\\n### Check out the code\\n\\nThe code we provide will delve into additional aspects such as training from a checkpoint, integrating MLflow and TensorBoard, and utilizing Pyfunc for model wrapping, among others. These resources are specifically tailored for implementation on [Databricks Community Edition](https://mlflow.org/blog/databricks-ce). The main runner notebook\\nwithin the full example repository [can be found here](https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/train.ipynb).\\n\\n## Get Started with MLflow 2.12 Today\\n\\nDive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\\nMLflow is here to elevate your ML workflow to new heights. [Get started now with MLflow\'s cutting-edge tools and features](https://mlflow.org/releases/2.12.1).\\n\\n## Feedback\\n\\nWe value your input! Our feature prioritization is guided by feedback from the MLflow late 2023 survey. Please fill out our\\n[Spring 2024 survey](https://surveys.training.databricks.com/jfe/form/SV_3jGIliwGC0g5xTU), and by participating, you can help ensure that the features\\nyou want most are implemented in MLflow."},{"id":"release-candidates","metadata":{"permalink":"/mlflow-website/blog/release-candidates","source":"@site/blog/2024-04-17-release-candidates.md","title":"MLflow Release Candidates","description":"We are excited to announce the implementation of a release candidate process for MLflow!","date":"2024-04-17T00:00:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"}],"readingTime":2.82,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"MLflow Release Candidates","tags":["mlflow"],"slug":"release-candidates","authors":["mlflow-maintainers"],"thumbnail":"img/blog/release-candidates.png"},"unlisted":false,"prevItem":{"title":"Deep Learning with MLflow (Part 2)","permalink":"/mlflow-website/blog/deep-learning-part-2"},"nextItem":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","permalink":"/mlflow-website/blog/deep-learning-part-1"}},"content":"We are excited to announce the implementation of a release candidate process for MLflow!\\nThe pace of feature development in MLflow is faster now than ever before and the core maintainer team has even more exciting things planned in the near future! However, with an increased velocity on major feature development comes with a risk of breaking things. As the maintainers of such a widely used project, we are cognizant of the disruptive nature of regressions and we strive to avoid them as much as we can. Aside from new feature development work, our primary goal is in ensuring the stability of production systems. While we do have the aspirational goal of moving fast(er), we certainly don\'t want to move fast and break things. With that goal in mind, we\'ve decided to introduce a release candidate (RC) process. The RC process allows us to introduce new features and fixes in a controlled environment before they become part of the official release.\\n\\n## How It Works\\n\\nStarting from MLflow 2.13.0, new MLflow major and minor releases will be tagged as release candidates (e.g., `2.13.0rc0`) in PyPI two weeks before they are officially released.\\n\\nThe release candidate process involves several key stages:\\n\\n- Feature Development Freeze: Prior to cutting the RC branch and announcing its availability, we will freeze the RC branch from feature commits. Once the branch is cut, only bug fix and stability PRs will be permitted to be merged, ensuring that unexpected, late-arriving, potentially regression-causing merges are not permitted to destabilize the forthcoming release.\\n- Pre-Release Announcement: We will announce upcoming features and improvements, providing our community with a roadmap of what to expect.\\n- Release Candidate Rollout: A release candidate version will be made available for testing, accompanied by detailed release notes outlining the changes.\\n- Community Testing and Feedback: We encourage our users to test the release candidate in their environments and share their feedback with us by filing issue reports on the MLflow Github repository. This feedback is invaluable for identifying issues and ensuring the final release aligns with user needs (i.e., we didn\'t break your workflows).\\n- Final Release: After incorporating feedback and making necessary adjustments, we will proceed with the final release. This version will include all updates tested in the RC phase, offering a polished and stable experience for all users.\\n\\nThis approach provides several benefits:\\n\\n- Enhanced Stability: By rigorously testing release candidates, we can identify and address potential issues early, reducing the likelihood of disruptions in production environments.\\n- Community Feedback: The RC phase offers you, a member of the MLflow community, the opportunity to provide feedback on upcoming changes. This collaborative approach ensures that the final release aligns with the needs and expectations of our users.\\n- Gradual Adoption: Users can choose to experiment with new features in a release candidate without committing to a full upgrade. This flexibility supports cautious integration and thorough evaluation in various environments.\\n\\n## Get Involved\\n\\nYour participation is crucial to the success of this process. We invite you to join us in testing upcoming release candidates and sharing your insights. Together, we can ensure that MLflow continues to serve as a reliable foundation for your machine learning projects.\\nStay tuned for announcements regarding our first release candidate. We look forward to your contributions and feedback as we take this important step toward a more stable and dependable MLflow."},{"id":"deep-learning-part-1","metadata":{"permalink":"/mlflow-website/blog/deep-learning-part-1","source":"@site/blog/2024-03-05-deep-learning-part-1/index.md","title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","description":"Highlighting the recent improvements in MLflow for Deep Learning workflows","date":"2024-03-05T00:00:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Deep Learning","permalink":"/mlflow-website/blog/tags/deep-learning"}],"readingTime":5.175,"hasTruncateMarker":true,"authors":[{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/img/authors/abe.png","key":"abe-omorogbe"},{"name":"Hubert Zub","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/hubert-zub/","imageURL":"/img/authors/hubert.png","key":"hubert-zub"},{"name":"Yun Park","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/yunpark93/","imageURL":"/img/authors/yun.png","key":"yun-park"},{"name":"Chen Qian","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/thomas-chen-qian/","imageURL":"/img/authors/chen.png","key":"chen-qian"},{"name":"Jesse Chan","title":"Software Engineer at Databricks","key":"jesse-chan"}],"frontMatter":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","description":"Highlighting the recent improvements in MLflow for Deep Learning workflows","slug":"deep-learning-part-1","authors":["abe-omorogbe","hubert-zub","yun-park","chen-qian","jesse-chan"],"tags":["Deep Learning"],"thumbnail":"img/blog/dl-chart-grouping.gif"},"unlisted":false,"prevItem":{"title":"MLflow Release Candidates","permalink":"/mlflow-website/blog/release-candidates"},"nextItem":{"title":"2023 Year in Review","permalink":"/mlflow-website/blog/mlflow-year-in-review"}},"content":"In the quickly evolving world of artificial intelligence, where generative AI has taken center stage, the landscape of machine learning is\\nevolving at an unprecedented pace. There has been a surge in the use of cutting-edge deep learning (DL) libraries like\\n[Transformers](https://huggingface.co/docs/transformers/index), [Tensorflow](https://www.tensorflow.org/),\\nand [PyTorch](https://pytorch.org/) to fine-tune these generative AI models for enhanced performance.\\nAs this trend accelerates, it\'s become clear that the tools used to build these models must rapidly evolve as well, particularly when it comes\\nto managing and optimizing these deep learning workloads. MLflow offers a practical solution for managing the complexities of these machine learning projects.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn collaboration with [MosaicML](https://www.mosaicml.com/) and the broader ML community, MLflow is thrilled to unveil a set of eagerly awaited enhancements.\\nThis latest release ([MLflow 2.11](https://www.mlflow.org/releases/2.11.0)) introduces updated tracking UI capabilities in direct response to\\n[the feedback](https://www.linkedin.com/posts/mlflow-org_qualtrics-survey-qualtrics-experience-management-activity-7128154257924513793-RCDG?utm_source=share&utm_medium=member_desktop)\\nand needs of MLflow enthusiasts. These updates are not just incremental; they represent a leap forward in addressing the needs of MLflow users doing Deep Learning.\\n\\nThe evolution of enhanced Deep Learning capabilities is a testament to MLflow\'s commitment to serving the open-source community, ensuring that its offerings\\nare not just keeping pace, but setting the pace in the rapidly evolving domain of machine learning.\\n\\n## Deep Learning API Improvements\\n\\nLeveraging valuable insights from our user community, we\'ve implemented critical enhancements to the effective scale of metrics logging and the inclusion of\\nsystem-related metric logging within our platform. These improvements encompass expanded scalability options, support for logging more iterations and the\\nlogging of system metrics.\\n\\n### System Metrics\\n\\nThis feature allows you to [monitor system metrics](https://mlflow.org/docs/latest/system-metrics/index.html?highlight=system) and identify any hardware issues that might be impacting performance.\\nMetrics such as CPU utilization, Memory usage, disk usage etc., from all nodes in your cluster can now be logged and visualized within the MLflow UI.\\n\\n![System Metrics](system-metrics.png)\\n\\n### Improved Logging Performance\\n\\nWe recently introduced both asynchronous and batch logging, making it easier to log both\\n[parallel and distributed](https://mlflow.org/docs/latest/tracking/tracking-api.html#parallel-runs) DL training sessions. Additionally, the MLflow Client\\nnow supports up to **1 million** steps (iterations) when logging metrics, allowing users to log more steps during long-running DL jobs.\\n\\n![Parallel Runs](parallel-runs.png)\\n\\n### Checkpointing for Deep Learning\\n\\n[TensorFlow](https://mlflow.org/releases/2.11.0#autologging-for-tensorflow-and-pytorch-now-supports-checkpointing-of-model-weights:~:text=both%20PyTorch%20and-,TensorFlow,-for%20automatic%20model)\\nand [PyTorch](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) now support model weight checkpointing when\\nusing autologging.\\n\\n![DL Checkpointing](dl-checkpointing.png)\\n\\n## User Experience and Productivity Enhancements\\n\\nWe have introduced substantial improvements to user experience and feature organization within our platform. These enhancements include more\\nsophisticated user interfaces and an intuitive redesign of the run details page, the addition of chart groups and metric aggregation, all\\naimed at simplifying navigation and enhancing productivity especially for Deep Learning use cases.\\n\\n### Metric Aggregation\\n\\nWe\'ve enhanced the UI with metric aggregation, enabling you to aggregate metrics across multiple runs based on\\ndatasets, tags, or parameters. These improvements significantly improve the time it takes to understand training results when working\\nwith large DL models, enabling more nuanced and comprehensive analysis of overarching trends in model performance across multiple dimensions.\\n\\n![DL Metric Aggregation](dl-metric-aggregation.gif)\\n\\n### Chart Grouping Functionality\\n\\nYou can now easily categorize and organize your metrics, such as training, testing, and system metrics into\\nnamed groups within the MLflow UI. This organization allows for a comprehensive overview of all metrics, enabling quicker access and\\nbetter management, particularly when handling experiments with many metrics.\\n\\n![DL Chart Grouping](dl-chart-grouping.gif)\\n\\n### Slash (\\"/\\") Logging Syntax\\n\\nTo further streamline metric organization, we\'ve implemented a new logging syntax that uses slashes\\n(\\"/\\") to group metrics. For example, using mlflow.log_metric(\\"x/y/score\\", 100) helps in structuring and segregating different types\\nof data or metrics into hierarchical groups, making it easier to navigate and interpret the logs, especially when dealing with complex\\nmodels and experiments.\\n\\n```python\\n\\nmlflow.log_metric(\'SVR/val_MAPE\', mean_absolute_percentage_error(test_y, pred_y))\\n\\n```\\n\\n![DL Slash Logging](dl-slash-logging.png)\\n\\n### Chart Searching\\n\\nWe\'ve significantly enhanced the search functionality within our platform, enabling more robust and intuitive searching\\nacross charts, parameters, and metrics. This upgrade allows for quicker and more precise retrieval of specific data points, streamlining the\\nprocess of analyzing and comparing different aspects of your experiments.\\n\\n![DL Chart Searching](dl-chart-searching.gif)\\n\\n### Run Details Redesign\\n\\nWe reorganized the Run Details UI to a modular tabbed layout, added new drag and drop UI functionality so that you can\\ncan now render logged tables. This enhancement will make it easier to organize your runs and experiments.\\n\\n![DL Run Details Redesign](dl-run-details.gif)\\n\\n## Getting Started Updates\\n\\nFollowing extensive feedback from our user community, we\'ve introduced significant updates to enhance the\\n[getting started](https://www.mlflow.org/docs/latest/getting-started/index.html) documentation within MLflow. These updates include a\\n[comprehensive overhaul](https://www.mlflow.org/docs/latest/deep-learning/index.html) of our documentation for easier navigation and\\n[enriched guidance](https://www.mlflow.org/docs/latest/deep-learning/pytorch/quickstart/pytorch_quickstart.html), along with a streamlined\\n[login API](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login). These enhancements, reflecting our commitment to improving the\\nuser experience and workflow, aim to empower our users to achieve more with greater speed and ease.\\n\\n### New Tutorials and Docs\\n\\nWe\'ve overhauled our documentation to offer a more comprehensive, user-friendly experience with practical examples\\nto support both newcomers and experienced practitioners with the information they need to start a Deep Learning project.\\n\\n![Deep Learning Docs](dl-docs.png)\\n\\n### Seamless login with mlflow.login()\\n\\nWe\'ve streamlined our authentication processes.\\n[This method](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#method-2-use-free-hosted-tracking-server-databricks-community-edition)\\nprovides a simple way to connect MLflow to your tracking server without having to leave your development environment.\\n[Try it out today](https://mlflow.org/blog/databricks-ce)\\n\\n![Login Update](login-update.png)\\n\\n## Get Started Today\\n\\nDive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\\nMLflow is here to elevate your data science workflow to new heights.\\n[Get started now with MLflow\'s cutting-edge tools and features](https://mlflow.org/releases/2.11.0).\\n\\n```bash\\npip install mlflow==2.11\\n\\nmlflow ui --port 8080\\n```\\n\\n```python\\nimport mlflow\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n# Set our tracking server uri for logging\\nmlflow.set_tracking_uri(uri=\\"http://127.0.0.1:8080\\")\\n\\nmlflow.autolog()\\n\\ndb = load_diabetes()\\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\\n\\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\\n# MLflow triggers logging automatically upon model fitting\\nrf.fit(X_train, y_train)\\n```\\n\\n## Feedback\\n\\nWe value your input! Our [feature roadmap](https://github.com/orgs/mlflow/projects/4) prioritization is guided by feedback from the [MLflow late 2023 survey](https://www.linkedin.com/feed/update/urn:li:activity:7128154257924513793), [GitHub Issues](https://github.com/mlflow/mlflow) and [Slack](https://mlflow.org/slack). Look out for our next survey later this year, by participating you can help ensure that the features you want are implemented in MLflow. You can also create an [issue on GitHub](https://github.com/mlflow/mlflow) or join our [Slack](https://mlflow.org/slack)."},{"id":"mlflow-year-in-review","metadata":{"permalink":"/mlflow-website/blog/mlflow-year-in-review","source":"@site/blog/2024-01-26-mlflow-year-in-review/index.md","title":"2023 Year in Review","description":"MLflow year-end recap","date":"2024-01-26T00:00:00.000Z","formattedDate":"January 26, 2024","tags":[{"label":"MLflow","permalink":"/mlflow-website/blog/tags/m-lflow"},{"label":"2023","permalink":"/mlflow-website/blog/tags/2023"},{"label":"Linux Foundation","permalink":"/mlflow-website/blog/tags/linux-foundation"}],"readingTime":6.625,"hasTruncateMarker":true,"authors":[{"name":"Carly Akerly","title":"OSS Marketing Consultant at The Linux Foundation","url":"https://www.linkedin.com/in/carlyakerly/","imageURL":"/img/authors/carly.png","key":"carly-akerly"}],"frontMatter":{"title":"2023 Year in Review","description":"MLflow year-end recap","slug":"mlflow-year-in-review","authors":["carly-akerly"],"tags":["MLflow","2023","Linux Foundation"],"thumbnail":"img/blog/2023-year-in-review.png"},"unlisted":false,"prevItem":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","permalink":"/mlflow-website/blog/deep-learning-part-1"},"nextItem":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","permalink":"/mlflow-website/blog/databricks-ce"}},"content":"With more than **16 million** monthly downloads, MLflow has established itself as a leading open-source MLOps platform worldwide.\\nThis achievement underscores the robustness of MLflow and the active community that consistently refines and improves it.\\n\\nThe past year marked a significant milestone for MLflow, particularly in Generative AI. Its integration and support for Large Language Models\\n(LLMs) stood out. This strategic decision has propelled MLflow to the forefront of the AI revolution, establishing itself as the premier GenAI\\nplatform that enables users to create more intelligent, efficient, and adaptable AI models and applications.\\n\\n\x3c!-- truncate --\x3e\\n\\n![16 Million Downloads!](download-graph.png)\\n\\n## 2023: A Year of GenAI and Innovation\\n\\nLast year was remarkable for MLflow, particularly in integrating LLMs and other generative AI tools. MLflow has evolved significantly by offering\\na unified platform and workflow for traditional ML, deep learning, and GenAI applications. This integration ensures unparalleled efficiency and\\ninnovation. MLflow\'s dedication to improving LLM support has revolutionized how users create and oversee AI workflows, establishing it as an\\nindispensable tool for building advanced machine learning applications.\\n\\n### Integrations with Leading AI Tools\\n\\nMLflow has successfully incorporated support for popular AI services/frameworks such as [Hugging Face](https://huggingface.co/),\\n[LangChain](https://www.langchain.com/), and [OpenAI](https://openai.com/), while offering a unified and framework-agnostic interface for\\npackaging, evaluating, and deploying them. These integrations have opened new horizons for MLflow users, allowing them to leverage the capabilities\\nof these advanced AI tools seamlessly within their MLflow workflows.\\n\\n![GenAI Integrations](integrations.png)\\n\\n#### Model Packaging for LLMs\\n\\nRecognizing the surge in LLM popularity and utility, MLflow has focused on enhancing packaging support for these models. With MLflow\u2019s new built-in\\nmodel flavors for [Hugging Face](https://www.mlflow.org/docs/latest/llms/transformers/index.html), [LangChain](https://www.mlflow.org/docs/latest/llms/langchain/index.html)\\n, and [OpenAI](https://www.mlflow.org/docs/latest/llms/openai/index.html), users can log and deploy their LLMs and generative AI applications within minutes.\\n\\n#### Retrieval Augmented Generation (RAG) and MLflow Integration\\n\\n[Retrieval Augmented Generation (RAG)](https://mlflow.org/docs/latest/llms/rag/index.html) represents an impactful method in natural language processing.\\nIt combines pre-trained models with retrieval mechanisms to access a dataset of documents that fetch validated and curated content as opposed to relying\\non pure generation. This approach significantly improves generated responses\' contextual relevance and factual accuracy. With\\n[mlflow.evaluate()](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html), users can compare RAG systems across prompts, models, vector\\ndatabases, and more. See further details in the blog post:\\n[\\"Evaluating Retrieval Augmented Generation (RAG) Systems with MLflow\\"](https://medium.com/@dliden/evaluating-retrieval-augmented-generation-rag-systems-with-mlflow-cf09a74faadb).\\n\\n![RAG with MLflow](rag.webp)\\n\\n#### MLflow Deployment Server in MLflow 2.9.0\\n\\nThe [MLflow Deployment Server](https://www.mlflow.org/docs/latest/llms/deployments/index.html) simplifies LLM usage and management from various providers\\nlike OpenAI, MosaicML, Anthropic, Hugging Face, Cohere, MLflow models, and more. Besides supporting popular SaaS LLM providers, the MLflow Deployment Server\\nintegrates with MLflow model serving, enabling users to serve their own LLM or fine-tuned foundation models within their serving infrastructure.\\nThe MLflow Deployment Server also provides a unified inference API across different providers and services, making it much easier to query and compose\\nthem together. It uses securely stored keys from a centralized location, so users no longer need to share sensitive API keys with each member of their\\norganization. This simplifies how we interact with language models, adding an extra layer of security for managing API keys.\\n\\n#### Enhanced MLflow Evaluate API in MLflow 2.8.0\\n\\nThe [MLflow Evaluate API](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) underwent significant feature enhancements to support LLM\\nworkflows better and incorporate multiple new evaluation modes, including support for\\n[LLM-as-a-judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge). This upgraded API enables a more refined\\nand thorough analysis of LLM performance.\\n\\n#### Prompt Engineering UI in MLflow 2.7.0\\n\\nMLflow introduced the [Prompt Engineering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html), a tool specifically designed for efficient prompt\\ndevelopment, testing, and assessment in Large Language Models (LLMs). This user-friendly interface and comprehensive toolkit have notably improved the\\naccessibility and efficiency of prompt engineering within LLM workflows.\\n\\n![Prompt Engineering UI](prompt-engineering.png)\\n\\n## Community Growth and Engagement\\n\\nThe introduction of the MLflow blog in 2023 was a new addition to the MLflow website. This fresh section signifies a crucial stride toward boosting\\ncommunity involvement and fostering knowledge exchange within the MLflow ecosystem. The blog serves as a direct avenue for sharing updates about new\\nfeatures, improvements, and the future trajectory of the MLflow project.\\n\\nMLflow surpassed 45,000 followers in 2023! Not only this, across [X](https://twitter.com/MLflow?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\\nand [LinkedIn](https://www.linkedin.com/company/mlflow-org/), MLflow had over 1 million impressions, the number of times our\\ncontent was displayed to users. When it came to MLflow contributor growth, the MLflow contributor count grew from 530 to 690 in 2023.\\n\\n### MLflow Docs Overhaul\\n\\nWe have undertaken a massive initiative to reimagine how our users interact with our content. The primary goal is to enhance clarity, improve navigation,\\nand provide more in-depth resources for our community, in addition to refreshing the look and feel. The overhaul of the MLflow documentation is a significant\\nmilestone, but it\u2019s just the beginning. We have a roadmap full of exciting updates, new content, and features. Whether it\u2019s writing tutorials, sharing use cases,\\nor providing feedback, every contribution enriches the MLflow community.\\n\\n![Docs Overhaul](docs-overhaul.png)\\n\\n### 2023 Events\\n\\nMLflow made a substantial impact at two significant events: **NeurIPS 2023** and the **Data+AI Summit 2023**. These events underscored MLflow\'s commitment\\nto contributing to the evolving discourse in machine learning and AI, emphasizing its pivotal role in shaping the future of these dynamic fields.\\nThe Data+AI Summit occurred in June 2023 and featured various MLflow-related sessions. Notably, two sessions stood out:\\n\\n- [Advancements in Open Source LLM Tooling, Including MLflow](https://www.youtube.com/watch?v=WpudXKAZQNI): Explored MLflow\'s seamless integration\\n  with leading generative AI tools like Hugging Face, LangChain, and OpenAI. It highlighted how these integrations enable effortless construction of AI workflows.\\n- [How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse](https://www.youtube.com/watch?v=MYqXfMqEUq4): Offered a\\n  comprehensive insight into how the Texas Rangers baseball team leveraged MLflow and Databricks to revolutionize their approach to data analytics.\\n\\n![Big Data Baseball](baseball.png)\\n\\nIn December 2023, MLflow participated in the 37th Annual Conference of Neural Information Processing Systems (NeurIPS) held in New Orleans, LA. NeurIPS\\nstands as one of the most prestigious conferences in machine learning and computational neuroscience.\\nFor those seeking guidance on fine-tuning a Large Language Model for general-purpose instruction following, the session\\n[\\"LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms\\"](https://arxiv.org/abs/2311.13133) at NeurIPS presented valuable insights.\\n\\n### Stay Plugged In\\n\\nIf you are interested in joining the MLflow community, we\u2019d love to connect! Join us on\\n[Slack](https://mlflow-users.slack.com/ssb/redirect), [Google Groups](https://groups.google.com/g/mlflow-users), and [GitHub](https://github.com/mlflow/mlflow/).\\nWe have a roadmap full of exciting updates, new content, and features. Whether it\u2019s writing tutorials, developing code, sharing use-cases, or providing feedback, let\u2019s work together!\\nAre you already an MLflow contributor? The newly launched MLflow Ambassador Program is a great way to boost your involvement. As an MLflow Ambassador,\\nyou will serve as one of our esteemed global ambassadors, pivotal in propelling the adoption and amplifying awareness of MLflow within the global data\\ncommunity. We invite you to submit an application [here](https://forms.gle/adAPNvH6aVq4diPF9).\\n\\n![Ambassador Program](ambassador-program.png)\\n\\n### Looking Forward\\n\\n\u201cIn 2024, we\'re launching new initiatives to engage, support, and expand our community. MLflow is thrilled to broaden its horizons this year through strategic\\ncollaboration and partnership\u201d, says Ben Wilson, Software Engineer at Databricks. \u201cThis collaboration will unlock fresh opportunities for our users and\\nsignificantly contribute to MLflow\'s evolution. Stay tuned for an announcement about this exciting effort.\u201d\\n\\nThe year 2023 marked a transformative period for MLflow. By embracing the latest ML and GenAI advancements, MLflow improved its platform and made substantial\\ncontributions to the wider AI and machine learning community. To our MLflow community, we extend our deepest gratitude.\\n\\nYou have been instrumental in driving MLflow\'s success over the past year. Whether it\'s enhancing existing features, exploring new integrations, or sharing\\nyour expertise, your contributions are the lifeblood of the MLflow community. If you\'re interested in contributing to MLflow,\\n[this guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md) is an excellent starting point. Looking ahead, we\'re excited about the myriad\\npossibilities and new frontiers we can explore together.\\n\\nMLflow is poised to continue its path of growth and innovation, cementing its role as a leader in managing machine learning and GenAI workflows across the\\nentire lifecycle. We\'re eager to keep pushing the boundaries of what\'s achievable in AI and strive to create an innovative, inclusive, and open future."},{"id":"databricks-ce","metadata":{"permalink":"/mlflow-website/blog/databricks-ce","source":"@site/blog/2024-01-25-databricks-ce/index.md","title":"Streamline your MLflow Projects with Free Hosted MLflow","description":"A guide to using Databricks Community Edition with integrated managed MLflow","date":"2024-01-25T00:00:00.000Z","formattedDate":"January 25, 2024","tags":[{"label":"managed mlflow","permalink":"/mlflow-website/blog/tags/managed-mlflow"},{"label":"getting started","permalink":"/mlflow-website/blog/tags/getting-started"}],"readingTime":5.27,"hasTruncateMarker":true,"authors":[{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/img/authors/abe.png","key":"abe-omorogbe"}],"frontMatter":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","description":"A guide to using Databricks Community Edition with integrated managed MLflow","slug":"databricks-ce","thumbnail":"img/blog/databricks-ce.png","authors":["abe-omorogbe"],"tags":["managed mlflow","getting started"]},"unlisted":false,"prevItem":{"title":"2023 Year in Review","permalink":"/mlflow-website/blog/mlflow-year-in-review"},"nextItem":{"title":"Custom MLflow Models with mlflow.pyfunc","permalink":"/mlflow-website/blog/custom-pyfunc"}},"content":"If you\'re new to MLflow and want to get started with a fully-managed and completely free deployment of MLflow, this blog will show you how to get started using MLflow in minutes.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Streamline Your ML Projects: Get Started with Hosted MLflow for Free\\n\\nExplore the world of big data and machine learning with [Databricks Community Edition (CE)](https://community.cloud.databricks.com/), a free, limited[^1] version of the Databricks platform.\\nIdeal for beginners and those new to Databricks and MLflow, this edition streamlines the learning curve by offering a managed environment. It eliminates the complexity of manually\\nsetting up a tracking server. Databricks CE includes hosted MLflow, enabling efficient management and visualization of your MLflow experiments. This makes it a prime choice for\\ndeveloping machine learning projects in a user-friendly interface, allowing you to connect from your favorite IDE, notebook environment, or even from within Databricks CE\'s notebooks.\\n\\n[^1]: The Model Registry and Model Deployment features are not available in the Databricks Community Edition.\\n\\n### Benefits of Using Databricks CE for MLflow\\n\\nMLflow is an open-source framework compatible with any platform, yet it offers distinct benefits when used on Databricks (including the Community Edition, CE) compared to other platforms. These advantages include:\\n\\n1. **Cost-Effective**: Free of charge, MLflow on Databricks CE is perfect for educational purposes and small-scale projects.\\n\\n2. **Simple Setup**: Gain access to a fully managed tracking server and user interface from any location. To connect to Databricks CE, just execute `mlflow.login()`.\\n\\n3. **Easy Sharing**: In the Databricks ecosystem, sharing your notebooks is straightforward and hassle-free.\\n\\n4. **Seamless Integration**: Databricks CE allows for direct storage and visualization of MLflow experiments, runs, and models.\\n\\n5. **Scalability**: MLflow on Databricks CE provides an easy path to scale your projects. It also integrates seamlessly with a wide range of data tools available on the Databricks platform.\\n\\n### Scenario\\n\\nIn this blog, we will walk through running ML experiments on your local device and tracking them on an [MLflow tracking server hosted on Databricks CE](https://mlflow.org/docs/latest/tracking.html#common-setups)\\n\\nTo give you an idea of the options available for running MLflow, the figure below shows what is possible for common setup configurations.\\n\\n![Remote Tracking Server](remote-tracking-server.png)\\n\\nFor this blog, we\'re showing #3, using a remote (fully managed) tracking server.\\n\\n### Step-by-Step Guide\\n\\n#### 1. Creating a Databricks CE Account\\n\\nIf you haven\'t already, you can [sign up for a free account](https://www.databricks.com/try-databricks#account). The process is quick, typically taking no more than 3 minutes.\\n\\nFill out the signup form and select \u201cGet started with Community Edition.\u201d\\n\\n![Databricks CE Signup Page](ce-signup.png)\\n\\nOnce signed up, you\'ll get information on how to set a password that you can use to login to CE with[^2].\\n\\n[^2]: Databricks CE only supports basic authorization signin (username / password). For more advanced and secure authorization setups, only the full Databricks product supports those.\\n\\n#### 2. Installing Dependencies\\n\\nBefore you start, ensure that you have the necessary packages installed. Run the following command in your favorite IDE or notebook from your device:\\n\\n```bash\\n%pip install -q mlflow databricks-sdk\\n```\\n\\n#### 3. Setting Up Databricks CE Authentication\\n\\nThe main advantage of Databricks Community Edition (CE) is its convenience: it offers an MLflow tracking server without requiring\\n[local infrastructure setup](https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server.html). You can easily access this server through the\\n[mlflow.login()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login) function after creating your CE account, streamlining the process for MLflow experiment tracking.\\n\\nTo authenticate with Databricks CE, use the [mlflow.login()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login) function. This will prompt you for:\\n\\n- **Databricks Host**: `https://community.cloud.databricks.com/`\\n\\n- **Username**: Your Databricks CE email address.\\n\\n- **Password**: Your Databricks CE password.\\n\\nUpon successful authentication, you will see a confirmation message.\\n\\n```python\\nimport mlflow\\n\\nmlflow.login()\\n\\n# Follow the prompts for authentication\\n```\\n\\n#### 4. Connect to Hosted MLflow and Track Experiments with Databricks CE\\n\\nAfter you login from your local machine, start an experiment with [mlflow.set_experiment()](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=mlflow%20set_experiment#mlflow.set_experiment) and log some metrics. For instance:\\n\\n```python\\nmlflow.set_experiment(\\"/Users/\\\\<email>/check-databricks-ce-connection\\")\\n\\nwith mlflow.start_run():\\n\\n\xa0\xa0\xa0\xa0mlflow.log_metric(\\"foo\\", 1)\\n\\n\xa0\xa0\xa0\xa0mlflow.log_metric(\\"bar\\", 2)\\n```\\n\\n> **Note**: The Databricks environment requires you to set experiments with the directory (from root)\\n\\n    `/Users/{your email address for your account}/{name of your experiment}`, which is different from the behavior in self-hosted MLflow (and when running MLFlow locally).\\n\\n#### 5. Viewing Your Experiment in Databricks CE\\n\\nNow let\u2019s navigate to Databricks CE to view the experiment result. Log in to your [Databricks CE](https://community.cloud.databricks.com/)\\naccount, and click on the top left to select machine learning in the drop down list. Finally, click on the experiment icon. See the screenshots below:\\n\\nNavigate to the Machine Learning Section\\n\\n![Navigate to ML Section of Databricks CE](navigate-to-experiments.png)\\n\\nNavigate to the MLflow UI\\n\\n![Navigate to the MLflow UI on Databricks CE](navigate-to-mlflow-ui.png)\\n\\nIn the \u201cExperiments\u201d view, you should be able to find the experiment `/Users/{your email}/check-databricks-ce-connection`, similar to:\\n\\n![Experiment view of Databricks MLflow server](view-experiment.png)\\n\\nClicking on the run name, which in this example is \'youthful-lamb-287\' (note that you will see a different, randomly generated name in your CE console),\\nwill take you to the run view that looks similar to the following:\\n\\n![Run view of Databricks MLflow server](view-run.png)\\n\\nIn the run view, you will see our dummy metrics `\u201cfoo\u201d` and `\u201cbar\u201d` have been logged successfully.\\n\\n#### 6. Run any MLflow tutorial in Databricks CE\\n\\nIf you want to try a tutorial from the MLflow website, you can use Databricks CE to quickly test (and modify, if you\'re inclined) the tutorial. For example, if you wanted to test\\nthe [Creating Custom Pyfunc tutorial](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.html):\\n\\n1. Click Workspace and\xa0 select \u201cImport notebook\u201d\\n\\n![Import a Notebook](import-notebook.png)\\n\\n2. Use the `URL` option to import the notebook directly from the MLflow documentation website. For this example, to import, replace the last element of the url\\n   from `html` to `ipynb`. This can be done with any of the tutorial or guide notebooks that are hosted on the MLflow website.\\n\\n   .../notebooks/basic-pyfunc.~~html~~ &rarr; .../notebooks/basic-pyfunc.**ipynb**[^3]\\n\\n[^3]: Or you can [download the notebook](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb) and manually load it in the UI by selecting `File` instead of `URL`.\\n\\n![Select the Notebook for Importing](import-notebook-2.png)\\n\\n### Conclusion\\n\\nDatabricks Community Edition (CE) offers an accessible and collaborative platform for MLflow experiment tracking, presenting several advantages. Its setup process is effortless\\nand quick, providing a user-friendly experience. Additionally, it\'s free to use, making it an ideal choice for beginners, learners, and small-scale projects.\\n\\n### Getting started\\n\\nTry out the notebook on [Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2830662238121329/3266358972198675/8538262732615206/latest.html)\\n\\n### Further Reading\\n\\n- Learn more about [different methods to setup your tracking server](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#minute-tracking-server-overv)\\n\\n- Learn more about running [Tutorial Notebooks ](https://mlflow.org/docs/latest/getting-started/running-notebooks/index.html)with Databricks CE"},{"id":"custom-pyfunc","metadata":{"permalink":"/mlflow-website/blog/custom-pyfunc","source":"@site/blog/2024-01-23-custom-pyfunc/index.md","title":"Custom MLflow Models with mlflow.pyfunc","description":"A guide for creating custom MLflow models","date":"2024-01-23T00:00:00.000Z","formattedDate":"January 23, 2024","tags":[{"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"label":"models","permalink":"/mlflow-website/blog/tags/models"}],"readingTime":15.29,"hasTruncateMarker":true,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/img/authors/daniel_liden.png","key":"daniel-liden"}],"frontMatter":{"title":"Custom MLflow Models with mlflow.pyfunc","description":"A guide for creating custom MLflow models","slug":"custom-pyfunc","authors":["daniel-liden"],"tags":["pyfunc","models"],"thumbnail":"img/blog/custom-pyfunc.png"},"unlisted":false,"prevItem":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","permalink":"/mlflow-website/blog/databricks-ce"},"nextItem":{"title":"MLflow AI Gateway renamed to MLflow Deployments for LLMs","permalink":"/mlflow-website/blog/ai-gateway-rename"}},"content":"If you\'re looking to learn about all of the flexibility and customization that is possible within\\nMLflow\'s custom models, this blog will help you on your journey in understanding more about how to\\nleverage this powerful and highly customizable model storage format.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Welcome](./header.png)\\n\\nMLflow offers built-in methods for logging and working with models from many popular machine\\nlearning and generative AI frameworks and model providers, such as scikit-learn, PyTorch,\\nHuggingFace transformers, and LangChain. For example,\\n[mlflow.sklearn.log_model](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)\\nwill log a scikit-learn model as an MLflow artifact without requiring you to define custom methods for\\nprediction or for handling artifacts.\\n\\nIn some cases, however, you might be working in a framework for which MLflow does not have\\nbuilt-in methods, or you might want something different than the model\u2019s default prediction\\noutputs. In those cases, MLflow allows you to create custom models to work with essentially\\nany framework and to integrate custom logic to existing supported frameworks.\\n\\nIn its simplest form, all that\u2019s required is to define a custom predict method and log the model.\\nThe following example defines a simple pyfunc model that just returns the square of its input:\\n\\n```python\\nimport mlflow\\n\\n# Define a custom model\\nclass MyModel(mlflow.pyfunc.PythonModel):\\n    def predict(self, context, model_input):\\n        # Directly return the square of the input\\n        return model_input**2\\n\\n\\n# Save the model\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"model\\",\\n        python_model=MyModel()\\n    )\\n\\n# Load the model\\nloaded_model = mlflow.pyfunc.load_model(\\n    model_uri=model_info.model_uri\\n)\\n\\n# Predict\\nloaded_model.predict(2)\\n```\\n\\nLet\u2019s dig into how this works, starting with some basic concepts.\\n\\n## Models and Model Flavors\\n\\n![Models and Flavors](models-and-flavors.png)\\n\\nAn MLflow model is a directory that includes everything needed to reproduce a machine learning model\\nacross different environments. Aside from the stored model itself, the most important component\\nstored is an `MLmodel` YAML file that specifies the model\u2019s supported model flavors.\\nA [model flavor](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part1-named-flavors.html#components-of-a-model-in-mlflow)\\nis a set of rules specifying how MLflow can interact with the model (i.e., save it, load it, and\\nget predictions from it).\\n\\nWhen you create a custom model in MLflow, it has the `python_function` or pyfunc model flavor,\\nwhich is a kind of \u201cuniversal translator\u201d across formats in MLflow. When you save a model in MLflow\\nusing a built-in model flavor, e.g. with [mlflow.sklearn.log_model](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model),\\nthat model also has the pyfunc model flavor in addition to its framework-specific flavor.\\nHaving both framework-specific and pyfunc model flavors allows you to use the model via the\\nframework\u2019s native API (e.g., `scikit-learn`) or via the pyfunc flavor\u2019s framework-agnostic inference API.\\n\\nModels with the pyfunc flavor are loaded as instances of the [mlflow.pyfunc.PyfuncModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.PyFuncModel)\\nclass, which exposes a standardized predict method. This enables straightforward inference through a single\\nfunction call, regardless of the underlying model\'s implementation details.\\n\\n## Defining Custom MLflow Pyfunc Models\\n\\nSaving a model from any supported machine learning framework as an MLflow model results in the\\ncreation of a pyfunc model flavor that provides a framework-agnostic interface for managing and\\nusing the model. But what if you\u2019re using a framework without an MLflow integration, or you\u2019re\\ntrying to elicit some custom behavior from a model? [Custom pyfunc models](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models)\\nallow you to work with essentially any framework and to integrate custom logic.\\n\\nTo implement a custom pyfunc model, define a new Python class inheriting from the PythonModel class\\nand implement the necessary methods. Minimally, this will involve implementing a custom predict\\nmethod. Next, create an instance of your model and log or save the model. Once you\u2019ve loaded the\\nsaved or logged model, you can use it for predictions.\\n\\n![Creating a custom model](custom-model-creation.png)\\n\\nLet\u2019s work through a few examples, each adding a little more complexity and highlighting different\\naspects of defining a custom pyfunc model. We\u2019ll cover four main techniques for implementing custom\\nbehaviors in pyfunc models:\\n\\n1. Implementing a custom `predict` method\\n2. Implementing a custom `__init__` method\\n3. Implementing a custom `load_context` method\\n4. Implementing user-defined custom methods\\n\\n![Pyfunc model customization](custom-pyfunc-types.png)\\n\\n### Defining a custom `predict` method\\n\\nAt a minimum, a pyfunc model should specify a custom predict method that defines what happens when\\nwe call `model.predict`. Here\u2019s an example of a custom model that applies a simple linear\\ntransformation to the model inputs, multiplying each input by two and adding three:\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass SimpleLinearModel(PythonModel):\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(2 * model_input + 3)\\n\\n\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"linear_model\\",\\n        python_model=SimpleLinearModel(),\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nNote that you can (and should) also include a [signature](https://mlflow.org/docs/latest/models.html#model-signature)\\nand an [input example](https://mlflow.org/docs/latest/models.html#model-input-example) when saving/logging a\\nmodel. If you pass an input example, the signature will be inferred automatically. The model\\nsignature provides a way for MLflow to enforce correct usage of your model.\\n\\nOnce we\u2019ve defined the model path and saved an instance of the model, we can load the saved model\\nand use it to generate predictions:\\n\\n```python\\n# Now the model can be loaded and used for predictions\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\nmodel_input = pd.DataFrame([1, 2, 3])  # Example input data\\nprint(loaded_model.predict(model_input))  # Outputs transformed data\\n```\\n\\nWhich will return:\\n\\n```text\\n:    0\\n: 0  5\\n: 1  7\\n: 2  9\\n```\\n\\nNote that if a custom `predict` method is all you need\u2014that is, if your model does not have any\\nartifacts that require special handling\u2014you can save or log the `predict` method directly without\\nneeding to wrap it in a Python class:\\n\\n```python\\nimport mlflow\\nimport pandas as pd\\n\\n\\ndef predict(model_input):\\n    \\"\\"\\"\\n    Applies a simple linear transformation\\n    to the input data. For example, y = 2x + 3.\\n    \\"\\"\\"\\n    # Assuming model_input is a pandas DataFrame with one column\\n    return pd.DataFrame(2 * model_input + 3)\\n\\n\\n# Pass predict method as python_model argument to save/log model\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"simple_function\\",\\n        python_model=predict,\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nNote that with this approach, we **must include** an input example along with the custom predict\\nmethod. We also have to modify the predict method such that it takes only one input (i.e., no self or context).\\nRunning this example and then loading with the same code as the preceding code block will retain the same output as\\nthe example using a class definiton.\\n\\n### Parameterizing the custom model\\n\\nNow suppose we want to parameterize the custom linear function model so that it can be used with\\ndifferent slopes and intercepts. We can define the `__init__` method to set up custom parameters,\\nas in the following example. Note that the custom model class\u2019s `__init__` method should not be used\\nto load external resources like data files or pretrained models; these are handled in the\\n`load_context` method, which we\u2019ll discuss shortly.\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass ParameterizedLinearModel(PythonModel):\\n    def __init__(self, slope, intercept):\\n        \\"\\"\\"\\n        Initialize the parameters of the model. Note that we are not loading\\n        any external resources here, just setting up the parameters.\\n        \\"\\"\\"\\n        self.slope = slope\\n        self.intercept = intercept\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(self.slope * model_input + self.intercept)\\n\\n\\nlinear_model = ParameterizedLinearModel(10, 20)\\n\\n# Saving the model with mlflow\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"parameter_model\\",\\n        python_model=linear_model,\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nAgain, we can load this model and make some predictions:\\n\\n```python\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\nmodel_input = pd.DataFrame([1, 2, 3])  # Example input data\\nprint(loaded_model.predict(model_input))  # Outputs transformed data\\n```\\n\\n```text\\n:     0\\n: 0  30\\n: 1  40\\n: 2  50\\n```\\n\\nThere are many cases where we might want to parameterize a model in this manner. We can define\\nvariables in the `__init__` method to:\\n\\n- Set model hyperparameters.\\n- A/B test models with different parameter sets.\\n- Set user-specific customizations.\\n- Toggle features.\\n- Set, e.g., access credentials and endpoints for models that access external APIs.\\n\\nIn some cases, we may want to be able to pass parameters at inference time rather than when we\\ninitialize the model. This can be accomplished with\\n[model inference params](https://mlflow.org/docs/latest/models.html#model-inference-params). To use\\ninference params, we must pass a valid model signature including `params`. Here\u2019s how to adapt the\\npreceding example to use inference params:\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.models import infer_signature\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass LinearFunctionInferenceParams(PythonModel):\\n    def predict(self, context, model_input, params):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        slope = params[\\"slope\\"]\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(slope * model_input + params[\\"intercept\\"])\\n\\n\\n# Set default params\\nparams = {\\"slope\\": 2, \\"intercept\\": 3}\\n\\n# Define model signature\\nsignature = infer_signature(model_input=pd.DataFrame([10, 20, 30]), params=params)\\n\\n# Save the model with mlflow\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"model_with_params\\",\\n        python_model=LinearFunctionInferenceParams(),\\n        signature=signature,\\n    )\\n```\\n\\nAfter loading the model as before, you can now pass a `params` argument to the `predict` method,\\nenabling you to use the same loaded model for different combinations of parameters:\\n\\n```python\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\n\\nparameterized_predictions = loaded_model.predict(\\n    pd.DataFrame([10, 20, 30]), params={\\"slope\\": 2, \\"intercept\\": 10}\\n)\\nprint(parameterized_predictions)\\n```\\n\\n```text\\n:     0\\n: 0  30\\n: 1  50\\n: 2  70\\n```\\n\\n### Loading external resources with `load_context`\\n\\nCustom models often require external files such as model weights in order to perform inference.\\nThese files, or artifacts, must be handled carefully to avoid unnecessarily loading files into\\nmemory or errors during model serialization. When building custom pyfunc models in MLflow, you can\\nuse the `load_context` method to handle model artifacts correctly.\\n\\nThe `load_context` method receives a `context` object containing artifacts the model can use during\\ninference. You can specify these artifacts using the `artifacts` argument when saving or logging\\nmodels, making them accessible to the `load_context` method via the `context.artifacts` dictionary.\\n\\nIn practice, the `load_context` method often initializes the model called by the `predict` method by\\nhandling the loading of model artifacts.\\n\\nThis raises an important question: why do we load artifacts and define the model in the `load_context`\\nmethod and not in `__init__` or directly in `predict`? Correct usage of `load_context` is essential\\nfor the maintainability, efficiency, scalability, and portability of MLflow pyfunc models. This is because:\\n\\n- The `load_context` method is executed once when the model is loaded via `mlflow.pyfunc.load_model`.\\n  This setup ensures that resource-intensive processes defined within this method, such as loading\\n  large model files, are not repeated unnecessarily. If artifact loading is done in the predict\\n  method, it will occur every single time a prediction is made. This is highly inefficient for\\n  resource-intensive models.\\n- Saving or logging an MLflow `pyfunc` model involves serializing the Python model class (the subclass\\n  of [mlflow.pyfunc.PythonModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel)\\n  you created) and its attributes. Complex ML models are not always compatible with the methods used\\n  to serialize the Python object, which can lead to errors if they are created as attributes of the Python object.\\n\\nAs an example, suppose we want to load a large language model (LLM) in the `gguf` model format\\n(a file format designed for storing models for inference) and run it with the\\n[ctransformers library](https://pypi.org/project/ctransformers). At the time of writing, there is\\nno built-in model flavor that lets us use `gguf` models for inference, so we\u2019ll create a custom\\npyfunc model that loads the required libraries and model files in the `load_context` method.\\nSpecifically, we\u2019re going to load a quantized version of the [AWQ version of the Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ).\\n\\nFirst, we\u2019ll download the model snapshot using the huggingface hub cli:\\n\\n```bash\\nhuggingface-cli download TheBloke/Mistral-7B-v0.1-GGUF \\\\\\n                mistral-7b-v0.1.Q4_K_M.gguf \\\\\\n                --local-dir /path/to/mistralfiles/ \\\\\\n                --local-dir-use-symlinks False\\n```\\n\\nAnd then we\u2019ll define our custom `pyfunc` model. Note the addition of the `load_context` method:\\n\\n```python\\nimport ctransformers\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\nclass CTransformersModel(PythonModel):\\n    def __init__(self, gpu_layers):\\n        \\"\\"\\"\\n        Initialize with GPU layer configuration.\\n        \\"\\"\\"\\n        self.gpu_layers = gpu_layers\\n        self.model = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Load the model from the specified artifacts directory.\\n        \\"\\"\\"\\n        model_file_path = context.artifacts[\\"model_file\\"]\\n\\n        # Load the model\\n        self.model = ctransformers.AutoModelForCausalLM.from_pretrained(\\n            model_path_or_repo_id=model_file_path,\\n            gpu_layers=self.gpu_layers,\\n        )\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Perform prediction using the loaded model.\\n        \\"\\"\\"\\n        if self.model is None:\\n            raise ValueError(\\n                \\"The model has not been loaded. \\"\\n                \\"Ensure that \'load_context\' is properly executed.\\"\\n            )\\n        return self.model(model_input)\\n```\\n\\nThere\u2019s a lot going on here, so let\u2019s break it down. Here are the key points:\\n\\n- As before, we use the `__init__` method to parameterize the model (in this case, to set the\\n  `gpu_layers` argument for the model).\\n- The purpose of the `load_context` method is to load the artifacts required for use in the\\n  `predict` method. In this case, we need to load the model and its weights.\\n- You\u2019ll notice that we reference `context.artifacts[\\"model_file\\"]`. This comes from the artifacts\\n  argument to `mlflow.pyfunc.save_model` or `mlflow.pyfunc.load_model`, as shown in the following\\n  code snippet. This is an important part of working with `pyfunc` models. The `predict` and\\n  `load_context` methods can access the artifacts defined in the artifacts argument to the\\n  `save_model` or `log_model` method via the `context.artifacts` object. `load_context` is executed\\n  when the model is loaded via `load_model`; as described earlier, this provides a way to ensure that\\n  the potentially time-consuming initialization of a model does not occur each time the model is used\\n  for prediction.\\n\\nNow we can initialize and save an instance of the model. Note the artifacts argument to the\\n`save_model` function:\\n\\n```python\\n# Create an instance of the model\\nmistral_model = CTransformersModel(gpu_layers=50)\\n\\n# Log the model using mlflow with the model file as an artifact\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"mistral_model\\",\\n        python_model=mistral_model,\\n        artifacts={\\"model_file\\": model_file_path},\\n        pip_requirements=[\\n            \\"ctransformers==0.2.27\\",\\n        ],\\n    )\\n\\n# Load the saved model\\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\\n\\n# Make a prediction with the model\\nloaded_model.predict(\\"Question: What is the MLflow Pyfunc model flavor?\\")\\n```\\n\\nTo recap: correct use of the `load_context` method helps to ensure efficient handling of model\\nartifacts and prevents errors in serialization that could result from attempting to define artifacts\\nas model attributes.\\n\\n### Defining custom methods\\n\\nYou can define your own methods in the custom `pyfunc` model to handle tasks like preprocessing\\ninputs or post-processing outputs. These custom methods can then be called by the predict method.\\nKeep in mind that these custom methods, just like `__init__` and `predict`, should **not be used** for\\nloading artifacts. Loading artifacts is the exclusive role of the `load_context` method.\\n\\nFor example, we can modify the `CTransformersModel` to incorporate some prompt formatting as follows:\\n\\n```python\\nimport ctransformers\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\nclass CTransformersModel(PythonModel):\\n    def __init__(self, gpu_layers):\\n        \\"\\"\\"\\n        Initialize with GPU layer configuration.\\n        \\"\\"\\"\\n        self.gpu_layers = gpu_layers\\n        self.model = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Load the model from the specified artifacts directory.\\n        \\"\\"\\"\\n        model_file_path = context.artifacts[\\"model_file\\"]\\n\\n        # Load the model\\n        self.model = ctransformers.AutoModelForCausalLM.from_pretrained(\\n            model_path_or_repo_id=model_file_path,\\n            gpu_layers=self.gpu_layers,\\n        )\\n\\n    @staticmethod\\n    def _format_prompt(prompt):\\n        \\"\\"\\"\\n        Formats the user\'s prompt\\n        \\"\\"\\"\\n        formatted_prompt = (\\n            \\"Question: What is an MLflow Model?\\\\n\\\\n\\"\\n            \\"Answer: An MLflow Model is a directory that includes \\"\\n            \\"everything that is needed to reproduce a machine \\"\\n            \\"learning model across different environments. \\"\\n            \\"It is essentially a container holding the trained model \\"\\n            \\"files, dependencies, environment details, input examples, \\"\\n            \\"and additional metadata. The directory also includes an \\"\\n            \\"MLmodel YAML file, which describes the different \\"\\n            f\\"flavors of the model.\\\\n\\\\nQuestion: {prompt}\\\\nAnswer: \\"\\n        )\\n\\n        return formatted_prompt\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Perform prediction using the loaded model.\\n        \\"\\"\\"\\n        if self.model is None:\\n            raise ValueError(\\n                \\"Model was not loaded. Ensure that \'load_context\' \\"\\n                \\"is properly executed.\\"\\n            )\\n        return self.model(self._format_prompt(model_input))\\n```\\n\\nNow the `predict` method can access the private `_format_prompt` static method to apply custom formatting to the prompts.\\n\\n### Dependencies and Source Code\\n\\nThe custom `CTransformersModel` defined above uses the `ctransformers` library. There are a few\\ndifferent approaches for making sure this library (and any other source code, including from your\\nlocal device) is correctly loaded with your model. Correctly specifying dependencies is essential\\nfor ensuring that custom models work as expected across environments.\\n\\nThere are three main approaches to be aware of for specifying dependencies:\\n\\n- Define pip requirements explicitly with the `pip_requirements` argument to `save_model` or `log_model`.\\n- Add extra pip requirements to an automatically generated set of requirements with the\\n  `extra_pip_requirements` argument to `save_model` or `log_model`.\\n- Define a Conda environment with the `conda_env` argument to `save_model` or `log_model`.\\n\\nEarlier, we used the first approach to specify that the `ctransformers` library was needed:\\n\\n```python\\n# Log the model using mlflow with the model file as an artifact\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.save_model(\\n        artifact_path=\\"mistralmodel\\",\\n        python_model=mistral_model,\\n        artifacts={\\"model_file\\": \\"path/to/mistral/model/on/local/filesystem\\"},\\n        pip_requirements=[\\n            \\"ctransformers==0.2.27\\",\\n        ],\\n    )\\n```\\n\\nIf you do not specify dependencies explicitly, MLflow will attempt to infer the correct set of\\nrequirements and environment details. To enable greater accuracy, it is **strongly recommended** to\\ninclude an `input_example` when saving or logging your model due to the internal execution of a\\nsample inference step that will capture any loaded library references associated with the inference\\nexecution, enabling a higher probability that the correct dependencies will be recorded.\\n\\nYou can also work with custom code on your own filesystem with the `code_path` argument.\\n`code_path` takes a list of paths to Python file dependencies and prepends them to the system\\npath before the model is loaded, so the custom pyfunc model can import from these modules.\\n\\nSee the documentation for the [log_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.log_model) and\\n[save_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.save_model)\\nfunctions for more details on the accepted formats for `pip`, `Conda`, and local code requirements.\\n\\n### Summary: Custom Pyfunc Models in MLflow\\n\\nMLflow has built-in methods for working with models from many popular machine learning frameworks,\\nsuch as [scikit-learn](https://www.mlflow.org/docs/latest/models.html#scikit-learn-sklearn),\\n[PyTorch](https://www.mlflow.org/docs/latest/models.html#pytorch-pytorch), and\\n[Transformers](https://www.mlflow.org/docs/latest/llms/transformers/index.html). You can define your own custom\\n`mlflow.pyfunc` model when you want to work with models that do not yet have built-in model\\nflavors, or when you want to implement a custom predict method for models with built-in model flavors.\\n\\nThere are several ways to customize `pyfunc` models to get the desired behavior. Minimally, you can\\nimplement a custom `predict` method. If your model requires saving or loading artifacts, you should also\\nimplement a `load_context` method. For further customization, you can use the `__init__` method for\\nsetting custom attributes and define your own custom methods for pre- and post-processing.\\nCombining these approaches gives you the ability to flexibly define custom logic for your machine\\nlearning models.\\n\\n### Further Learning\\n\\nInterested in learning more about custom `pyfunc` implementations? You can visit:\\n\\n- [Custom Pyfuncs for Advanced LLMs with MLflow](https://www.mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html)\\n- [Build Custom Python Function Models for traditional ML](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n- [Custom PyFunc notebook examples](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/index.html)"},{"id":"ai-gateway-rename","metadata":{"permalink":"/mlflow-website/blog/ai-gateway-rename","source":"@site/blog/2023-12-01-ai-gateway-rename.md","title":"MLflow AI Gateway renamed to MLflow Deployments for LLMs","description":"If you are currently using the MLflow AI Gateway, please read this in full to get critically important information about this feature!","date":"2023-12-01T00:00:00.000Z","formattedDate":"December 1, 2023","tags":[{"label":"ai","permalink":"/mlflow-website/blog/tags/ai"}],"readingTime":1.885,"hasTruncateMarker":true,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"MLflow AI Gateway renamed to MLflow Deployments for LLMs","tags":["ai"],"slug":"ai-gateway-rename","authors":["mlflow-maintainers"],"thumbnail":"img/blog/ai-gateway.png"},"unlisted":false,"prevItem":{"title":"Custom MLflow Models with mlflow.pyfunc","permalink":"/mlflow-website/blog/custom-pyfunc"},"nextItem":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","permalink":"/mlflow-website/blog/mlflow-autolog"}},"content":"If you are currently using the MLflow AI Gateway, please read this in full to get critically important information about this feature!\\n\\n# \ud83d\udd14 Important Update Regarding the MLflow AI Gateway\\n\\nPlease note that the feature previously known as the `MLflow AI Gateway`, which was in an experimental phase, has undergone significant updates and improvements.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introducing \\"MLflow Deployments for LLMs\\"\\n\\nThis feature, while still in experimental status, has been renamed and migrated to utilize the `deployments` API.\\n\\n## \ud83d\udd11 Key Changes\\n\\n**New Name, Enhanced Features**: The transition from \\"MLflow AI Gateway\\" to \\"MLflow Deployments for LLMs\\" reflects not only a change in name but also substantial enhancements in usability and **standardization** for API endpoints for Large Language Models.\\n\\n**API Changes**: With this move, there are changes to the API endpoints and configurations. Users are encouraged to review the updated API documentation to familiarize themselves with the new structure.\\n\\n**Migration Path**: For existing projects using \\"MLflow AI Gateway\\", a migration guide is available to assist with the transition to \\"MLflow Deployments for LLMs\\". This guide provides step-by-step instructions and best practices to ensure a smooth migration.\\n\\n\u26a0\ufe0f **Action Required**: Users who have been utilizing the experimental \\"MLflow AI Gateway\\" should plan to migrate to \\"MLflow Deployments for LLMs\\". While we aim to make this transition as seamless as possible, manual changes to your code and deployment configurations will be necessary. This new namespace for deploying the previously-known-as AI Gateway will be released in version 2.9.0. The old AI Gateway references will remain active but will enter a deprecated state. _We will be removing the entire AI Gateway namespace in a future release_.\\n\\n## \ud83d\udcda Resources and Support\\n\\n**Updated Documentation**: Detailed documentation for \\"MLflow Deployments for LLMs\\" is available [here](pathname:///docs/latest/llms/deployments/index.html). It includes comprehensive information about the modifications to API interfaces, updates to the input and output structures for queries and responses, API utilization, and the updated configuration options.\\n\\n**Community and Support**: If you have any questions or need assistance, please reach out to the maintainers [on GitHub](https://github.com/mlflow/mlflow/issues).\\n\\nWe are excited about these advancements and strongly believe that leveraging the deployments API will offer a more robust, efficient, and scalable solution for managing your Large Language Model deployments. Thank you for your continued support and collaboration!"},{"id":"mlflow-autolog","metadata":{"permalink":"/mlflow-website/blog/mlflow-autolog","source":"@site/blog/2023-11-30-mlflow-autolog/index.md","title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","description":"Looking to learn more about the autologging functionality included in MLflow? Look no further than this primer on the basics of using this powerful and time-saving feature!","date":"2023-11-30T00:00:00.000Z","formattedDate":"November 30, 2023","tags":[{"label":"autolog","permalink":"/mlflow-website/blog/tags/autolog"}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/img/authors/daniel_liden.png","key":"daniel-liden"}],"frontMatter":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","slug":"mlflow-autolog","tags":["autolog"],"authors":["daniel-liden"],"thumbnail":"img/blog/mlflow-autolog.png"},"unlisted":false,"prevItem":{"title":"MLflow AI Gateway renamed to MLflow Deployments for LLMs","permalink":"/mlflow-website/blog/ai-gateway-rename"},"nextItem":{"title":"MLflow Docs Overhaul","permalink":"/mlflow-website/blog/mlflow-docs-overhaul"}},"content":"Looking to learn more about the autologging functionality included in MLflow? Look no further than this primer on the basics of using this powerful and time-saving feature!\\n\\n# Introduction to [mlflow.autolog](https://www.mlflow.org/docs/latest/tracking/autolog.html)\\n\\nRobust logging practices are central to the iterative development and improvement of machine learning models. Carefully tracking metrics, parameters, and artifacts can be challenging when working with complex machine learning libraries or when experimenting with multiple different frameworks with varying APIs and selections of different objects and values to track.\\n\\n\x3c!-- truncate --\x3e\\n\\nMLflow\u2019s **automatic logging functionality** offers a simple solution that is compatible with many widely-used machine learning libraries, such as [PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html), [Scikit-learn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog), and [XGBoost](https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog). Using `mlflow.autolog()` instructs MLflow to capture essential data without requiring the user to specify what to capture manually. It is an accessible and powerful entrypoint for MLflow\u2019s logging capabilities.\\n\\nTo enable automatic logging, simply add the following line to your machine learning scripts/notebooks, before initiating activities like model training or evaluation that may include information or artifacts you would like to log:\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.autolog()\\n```\\n\\n## Autolog features\\n\\nWhen a data science workflow includes `mlflow.autolog()`, MLflow will automatically log:\\n\\n- **Metrics**: standard training and evaluation measures such as accuracy and F1 score;\\n- **Parameters**: hyperparameters, such as learning rate and number of estimators; and\\n- **Artifacts**: important files, such as trained models.\\n\\nMLflow\u2019s automatic logging captures details tailored to the specific activities of the library being used: different libraries will result in different logged objects and data. In addition, MLflow logs key metadata such as software versions, a git commit hash, and the file name from which the run was initiated. By documenting the system\'s state during model training, MLflow aims to facilitate environment reproducibility and provide audit lineage, minimizing the possibility of inference issues that could arise from package regressions or deprecations in newer library versions.\\n\\nThe specifics of what is captured through automatic logging depend on the libraries used. Additionally, MLflow captures contextual metadata such as software versions, git commit hash, and the name of the file from which the run was launched. By logging as much detail as possible about the state of the system that trained the model, MLflow can offer environment reproducibility and audit lineage, minimizing the possibility of inference issues resulting from, for example, package regressions or deprecations.\\n\\n## Basic Usage of `mlflow.autolog`\\n\\nYou can access auto logging functionality in two different ways:\\n\\n1. Via the `mlflow.autolog()` function, which enables and configures automatic logging across all supported libraries. This provides a broad, one-size-fits-all approach when working with multiple libraries and is ideal for prototyping and exploratory analysis of a machine learning pipeline.\\n2. Via the library-specific autolog functions, such as `mlflow.sklearn.autolog()`, which enable finer-grained logging configuration for individual libraries. For example, `mlflow.pytorch.autolog()` includes the `log_every_n_epoch` and `log_every_n_step` arguments for specifying how often to log metrics.\\n\\nRegardless of which of these two approaches you use, you do not need to manually initialize an MLflow run with [start_run()](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run) in order to have a run created and for your model, parameters, and metrics to be captured in MLflow.\\n\\n### Example\\n\\n```python\\nimport mlflow\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Generate a 3-class classification problem\\nX, y = datasets.make_classification(\\n    n_samples=1000,\\n    class_sep=0.5,\\n    random_state=42,\\n    n_classes=3,\\n    n_informative=3,\\n)\\n\\n# Split the data into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Enable autolog\\nmlflow.autolog()  # or mlflow.sklearn.autolog()\\n\\n# Initialize the classifier with n_estimators=200 and max_depth=10\\nclf = RandomForestClassifier(n_estimators=200, max_depth=10)\\n\\n# Fit the classifier to the data.\\n# The `fit` method is patched to perform autologging. When engaged in training, a\\n# run is created and the parameters are logged.\\n# After the fit is complete, the model artifact is logged to the run.\\nclf.fit(X_train, y_train)\\n\\n# Score the model on the data\\n# The current active run is retrieved during calling `score` and the loss metrics are logged\\n# to the active MLflow run.\\nclf.score(X_val, y_val)\\n\\n# Visualize the automatically logged run results to validate what we recorded\\nmlflow.last_active_run()\\n```\\n\\nThe above logs model parameters, metrics, and the model to an MLflow run. The output result of the final statement ([mlflow.last_active_run()](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.last_active_run)) in the above example, which will return data from the run on model metrics, parameters, and logged artifacts (results truncated) is as shown below:\\n\\n```text\\n<Run: data=<RunData:\\nmetrics={\'RandomForestClassifier_score_X_val\': 0.72,\\n         \'training_accuracy_score\': 0.99625,\\n         \'training_f1_score\': 0.9962547564333545,\\n         \'training_log_loss\': 0.3354604497935824,\\n         \'training_precision_score\': 0.9962921348314606,\\n         \'training_recall_score\': 0.99625,\\n         \'training_roc_auc\': 0.9998943433719795,\\n         \'training_score\': 0.99625\\n         },\\n params={\'bootstrap\': \'True\',\\n         \'ccp_alpha\': \'0.0\',\\n         \'class_weight\': \'None\',\\n         \'criterion\': \'gini\',\\n         \'max_depth\': \'10\',\\n         \'max_features\': \'sqrt\',\\n         \'max_leaf_nodes\': \'None\',\\n         [...],\\n         },\\ntags={\'estimator_class\': \'sklearn.ensemble._forest.RandomForestClassifier\',\\n      \'estimator_name\': \'RandomForestClassifier\',\\n      \'mlflow.autologging\': \'sklearn\',\\n      [...]\\n}, [...]>>\\n```\\n\\nYou can also access these in the mlflow ui by executing [mlflow ui](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) on a command line terminal.\\n\\n![The MLflow Tracking UI](./autolog_in_ui.png)\\n\\nThe MLflow UI also allows you to graphically compare different metrics and parameters across multiple runs, including those generated via `mlflow.autolog`.\\n\\n![Run comparison of autologged runs in the MLflow UI](./autolog_compare_runs.png)\\n\\n## Configuration and Customization\\n\\nThe automatic logging functions take many arguments that give the user greater control over logging behavior. For example, `mlflow.autolog()` includes `log_models` and `log_datasets` arguments (both `True` by default) that specify whether models and dataset information should be saved to the MLflow run, enabling you to specify what is actually logged. To disable automatic logging of datasets while continuing to log all the usual elements, simply disable the autologging of datasets feature by setting `mlflow.autolog(log_datasets=False)` before fitting a model. You can also adjust the behavior of library-specific autolog functions: for example, the `mlflow.sklearn.autolog()` function includes a `max_tuning_runs` argument that specifies how many nested runs to capture when performing hyperparameter searches.\\n\\n`mlflow.autolog()` can be used in combination with the library-specific autolog functions to control the logging behavior for specific libraries. The library-specific autolog call will always supersede `mlflow.autolog()`, regardless of the order in which they are called. For example, combining `mlflow.autolog()` with `mlflow.sklearn.autolog(disable=True)` will result in automatic logging for all supported libraries except for `scikit-learn`.\\n\\nIt is important to consult the documentation for the specific framework(s) you are using in order to understand what is logged automatically and what configuration options are available. See the [further reading section below](#further-reading) for links.\\n\\n## Conclusion and Next Steps\\n\\nMLflow\'s autologging capabilities and library-specific automatic logging functions provide a straightforward starting point for MLflow tracking with little or no required configuration. They log key metrics, parameters, and artifacts from many popular machine learning libraries, allowing users to track their machine learning workflows without writing custom tracking code.\\n\\nThey are not, however, the right solution for all use cases. If you only need to track a handful of specific metrics, enabling automatic logging may be inefficient, resulting in much more generated data and stored artifacts than needed. Furthermore, automatic logging is not available for every possible framework and custom values one might want to track. In such cases, it might be necessary to [manually specify what to track](https://mlflow.org/docs/latest/tracking/tracking-api.html#logging-functions).\\n\\n## Further Reading\\n\\n- [MLflow Documentation on Automatic Logging](https://mlflow.org/docs/latest/tracking/autolog.html)\\n- [Python API reference for mlflow.autolog](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.autolog)\\n- Python API references for library-specific autolog functions\\n  - [PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\\n  - [Tensorflow](https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.autolog)\\n  - [Scikit-learn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog)\\n  - [XGBoost](https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog)\\n  - [PySpark](https://mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog)\\n  - [Statsmodels](https://mlflow.org/docs/latest/python_api/mlflow.statsmodels.html#mlflow.statsmodels.autolog)\\n  - [LightGBM](https://mlflow.org/docs/latest/python_api/mlflow.lightgbm.html#mlflow.lightgbm.autolog)\\n  - [Paddle](https://mlflow.org/docs/latest/python_api/mlflow.paddle.html#mlflow.paddle.autolog)\\n  - [Fastai](https://mlflow.org/docs/latest/python_api/mlflow.fastai.html#mlflow.fastai.autolog)"},{"id":"mlflow-docs-overhaul","metadata":{"permalink":"/mlflow-website/blog/mlflow-docs-overhaul","source":"@site/blog/2023-10-31-mlflow-docs-overhaul.md","title":"MLflow Docs Overhaul","description":"The MLflow Documentation is getting an upgrade.","date":"2023-10-31T00:00:00.000Z","formattedDate":"October 31, 2023","tags":[{"label":"docs","permalink":"/mlflow-website/blog/tags/docs"}],"readingTime":5.11,"hasTruncateMarker":true,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"MLflow Docs Overhaul","tags":["docs"],"slug":"mlflow-docs-overhaul","authors":["mlflow-maintainers"],"thumbnail":"img/blog/docs-overhaul.png"},"unlisted":false,"prevItem":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","permalink":"/mlflow-website/blog/mlflow-autolog"}},"content":"The MLflow Documentation is getting an upgrade.\\n\\n## Overhauling the MLflow Docs\\n\\nWe\'re thrilled to announce a comprehensive overhaul of the MLflow Docs. This initiative is not just about refreshing the look and feel but about reimagining how our users interact with our content. Our primary goal is to enhance clarity, improve navigation, and provide more in-depth resources for our community.\\n\\n## A Renewed Focus on User Experience\\n\\nThe MLflow documentation has always been an essential resource for our users. Over time, we\'ve received invaluable feedback, and we\'ve listened. The modernization effort is a direct response to the needs and preferences of our community.\\n\\n\x3c!-- truncate --\x3e\\n\\nAlong with working on covering new cutting-edge features as part of this documentation overhaul, we\'re working on addressing the complexity of getting started. As the first part of a series of tutorials and guides focusing on the initial learning phase, we\'ve created a new [getting started guide](https://www.mlflow.org/docs/latest/getting-started/logging-first-model/index.html), the first of many in a new series we\'re working on in an effort to teach the fundamentals of using MLflow. We feel that more in-depth instructional tutorials for learning the concepts and tools of MLflow will help to enhance the user experience for not only new users, but experienced users who need a refresher of how to do certain tasks.\\n\\nThere are more of these coming in the future!\\n\\n### **Easier Navigation**\\n\\nOur first order of business is to declutter and reorganize. This is going to be a process, though. With some of the monolithic pages ([Mlflow Models](https://www.mlflow.org/docs/2.7.1/models.html)), this will be more of a marathon than a sprint.\\n\\nWe\'ve introduced a [new main navigation page](https://www.mlflow.org/docs/latest/index.html) in an effort to help steer you to the content that you\'re looking for based on end-use domain, rather than component of MLflow. We\'re hoping that this helps to bring new feature content and useful examples to your awareness, limiting the amount of exploratory discovery needed to understand how to use these new features.\\n\\nAnother priority for us was to make major new features easier to discover. While the [release notes](https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md) are useful, particularly for Engineers who are maintaining integrations with, or are managing a deployment of, MLflow, they\'re not particularly user-friendly for an end-user of MLflow. We felt that a curated list of major new features would help to distill the information in our release notes, so we built the [new features](https://www.mlflow.org/docs/latest/new-features/index.html) page. We sincerely hope it helps to reduce the amount of effort needed to know what new major features have been released.\\n\\n### **Interactive Learning with Notebooks**\\n\\nIn today\'s fast-paced tech world, interactive learning is becoming the norm. Recognizing this trend, we\'re embedding viewable notebooks directly within the docs. But we\'re not stopping there. These notebooks are downloadable, allowing you to run, modify, and experiment with them locally. It\'s a hands-on approach to learning, bridging the gap between theory and practice.\\n\\n### **In-depth Tutorials and Guides**\\n\\nWhile our previous documentation provided a solid foundation, we felt there was room for more detailed explorations. We\'re introducing comprehensive [tutorials](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html) and [guides](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html) that delve deep into MLflow\'s features, showing how to solve actual problems. These first new tutorials and guides are just the start. We\'re going to be spending a lot of time and effort on making much more of MLflow documented in this way, helping to dramatically reduce the amount of time you have to spend figuring out how to leverage features in MLflow.\\n\\n## Diving Deeper: Expanding on Guides and Tutorials\\n\\nOur dedication to simplifying the usage of MLflow shines through in our revamped tutorials and guides. We\'re not just providing instructions; we\'re offering [deep dives](https://www.mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html), [best practices](https://www.mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/index.html), and real-world applications. What you see in the MLflow 2.8.0 release is just the beginning. We\'re going to be heavily focusing on creating more content, showing the best way to leverage the many features and services within MLflow, all the while endeavoring to make it easier than ever to manage any ML project you\'re working on.\\n\\n- **LLMs**: With all of the [new LLM-focused features](https://www.mlflow.org/docs/latest/llms/llm-evaluate/notebooks/rag-evaluation.html) we\'ve been releasing in the past year, we feel the need to create easier getting started guides,\\n  [in-depth tutorials](https://www.mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html), runnable examples, and more teaching-oriented step-by-step introductions to these features.\\n\\n- **Tracking and the MLflow UI**: Our expanded section on tracking will cover everything from setting up your first experiment to advanced tracking techniques. The MLflow UI, an integral part of the platform, will also get its spotlight, ensuring you can make the most of its features.\\n\\n- **Model Registry**: The model registry is where MLflow truly shines, and our new guides will ensure you can harness its full power. From organizing models to version control, we\'ll cover it all.\\n\\n- **Recipes and LLM-focused Features**: MLflow\'s versatility is one of its strengths. Our new content will explore the breadth of features available, from recipes to LLM-focused tools like the AI Gateway, LLM Evaluation, and the PromptLab UI.\\n\\n## The Transformative Power of Interactive Notebooks\\n\\nInteractive notebooks have revolutionized data science and machine learning. By integrating them into our documentation, we aim to provide a holistic learning experience. You can see code in action, understand its impact, and then experiment on their own. It\'s a dynamic way to grasp complex concepts, ensuring that you not only understand but can also apply your knowledge in your actual project code.\\n\\n## Join Us on This Journey\\n\\nThe overhaul of the MLflow documentation is a significant milestone, but it\'s just the beginning. We have a roadmap full of exciting updates, new content, and features. And for those in our community with a passion for sharing knowledge, we have a message: We\'d love to [collaborate](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md)! Whether it\'s writing tutorials, sharing use-cases, or providing feedback, every contribution enriches the MLflow community.\\n\\nIn conclusion, our commitment to providing top-notch documentation is a new primary focus of the maintainer group. We believe that well-documented features, combined with interactive learning tools, can significantly enhance the experience of using any tool. We want to put in the effort and time to make sure that your journey with using MLflow is as simple and powerful as it can be.\\n\\nStay tuned for more updates, and as always, happy coding!"}]}')}}]);