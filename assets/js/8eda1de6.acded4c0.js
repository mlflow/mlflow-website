"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2201],{5465:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/graph-37d109015dac66cc4ba91020cf15d09a.png"},8391:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var s=t(63867),r=t(74848),a=t(28453);const i={title:"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine",description:"MLflow Models from Code and MLflow Tracing applied to AI Workflows",tags:["pyfunc","mlflow","sql-generator","models-from-code","tracing"],slug:"from-natural-language-to-sql",authors:["hugo-carvalho","joana-ferreira","rahul-pandey"],thumbnail:"/img/blog/from-natural-language-to-sql.png"},o=void 0,l={authorsImageUrls:[void 0,void 0,void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Multi-Lingual Query Engine using LangGraph",id:"multi-lingual-query-engine-using-langgraph",level:2},{value:"Key LangGraph features:",id:"key-langgraph-features",level:3},{value:"AI Workflow Overview",id:"ai-workflow-overview",level:2},{value:"Components",id:"components",level:3},{value:"OpenAI",id:"openai",level:4},{value:"FAISS Vector Store",id:"faiss-vector-store",level:4},{value:"Step 1: Load SQL Documentation",id:"step-1-load-sql-documentation",level:5},{value:"Step 2: Split the Text into Manageable Chunks",id:"step-2-split-the-text-into-manageable-chunks",level:5},{value:"Step 3: Generate Embedding Model",id:"step-3-generate-embedding-model",level:5},{value:"Step 4: Create and Store Embeddings in FAISS Vector Store",id:"step-4-create-and-store-embeddings-in-faiss-vector-store",level:5},{value:"SQLite Database",id:"sqlite-database",level:4},{value:"SQLite Database Initialization",id:"sqlite-database-initialization",level:4},{value:"SQL Generation Chain",id:"sql-generation-chain",level:4},{value:"Workflow Setup and Initialization",id:"workflow-setup-and-initialization",level:4},{value:"Defining <code>GraphState</code>",id:"defining-graphstate",level:5},{value:"Workflow Compilation Function",id:"workflow-compilation-function",level:5},{value:"Node Descriptions",id:"node-descriptions",level:4},{value:"1. Translate Input",id:"1-translate-input",level:5},{value:"2. Pre-safety Check",id:"2-pre-safety-check",level:5},{value:"3. Schema Extract",id:"3-schema-extract",level:5},{value:"4. Context Check",id:"4-context-check",level:5},{value:"5. Generate",id:"5-generate",level:5},{value:"6. Post-safety Check",id:"6-post-safety-check",level:5},{value:"7. SQL Check",id:"7-sql-check",level:5},{value:"8. Run Query",id:"8-run-query",level:5},{value:"Decision Step: Determine Next Action",id:"decision-step-determine-next-action",level:5},{value:"Workflow Orchestration and Conditional Logic",id:"workflow-orchestration-and-conditional-logic",level:4},{value:"Logging the Model in MLflow",id:"logging-the-model-in-mlflow",level:2},{value:"Step 1: Create our Model from Code File",id:"step-1-create-our-model-from-code-file",level:3},{value:"Step 2: Log with MLflow Models from Code feature",id:"step-2-log-with-mlflow-models-from-code-feature",level:3},{value:"Use the Logged Multi-Lingual Query Engine in <code>main.py</code>",id:"use-the-logged-multi-lingual-query-engine-in-mainpy",level:2},{value:"Project File Structure",id:"project-file-structure",level:2},{value:"MLflow Tracing",id:"mlflow-tracing",level:2},{value:"Viewing Traces in MLflow",id:"viewing-traces-in-mlflow",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["If you're looking to build a Multi-Lingual Query Engine that combines natural language to SQL generation with query execution while fully leveraging MLflow\u2019s features, this blog post is your guide. We\u2019ll explore how to leverage ",(0,r.jsx)(n.strong,{children:"MLflow Models from Code"})," to enable seamless tracking and versioning of AI Workflows. Additionally, we\u2019ll deep dive into ",(0,r.jsx)(n.strong,{children:"MLflow\u2019s Tracing"})," feature, which introduces observability into the many different components of an AI Workflow by tracking inputs, outputs, and metadata at every intermediate step."]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"SQL is a fundamental skill for managing and accessing data within relational databases. However, constructing complex SQL queries to answer intricate data questions can be challenging and time-consuming. This complexity can make it difficult to fully leverage data effectively. Natural language to SQL (NL2SQL) systems help in solving this problem by providing a translation from natural language to SQL commands allowing non-technical people to interact with data: users can just ask questions in a natural language they are comfortable speaking and these systems will assist them in returning the appropriate information."}),"\n",(0,r.jsx)(n.p,{children:"However, there are also a number of problems that remain when creating a NL2SQL system such as semantic ambiguity, schema mapping or error handling and user feedback. Therefore, it is very important that while building such systems, we must put some guardrails instead of completely relying on LLM."}),"\n",(0,r.jsxs)(n.p,{children:['In this blog post, we\u2019ll walk you through the process of building a Multi-Lingual Query Engine. This engine supports natural language inputs in multiple languages, generates an SQL query based on the translated user input, and executes the query. Let\'s jump into an example: using a database containing information about a company\'s customers, products, and orders, a user might ask a question in any language, such as "Quantos clientes temos por pa\xeds?" (Portuguese for "How many customers do we have per country?"). The AI Workflow translates the input into English, outputting "How many customers do we have per country?". It then validates the input for safety, checks if the question can be answered using the database schema, generates the appropriate SQL query (e.g., ',(0,r.jsx)(n.code,{children:"SELECT COUNT(CustomerID) AS NumberOfCustomers, Country FROM Customers GROUP BY Country;"}),"), and validates the query to ensure no harmful commands (e.g., DROP) are present. Finally, it executes the query against the database to retrieve the results."]}),"\n",(0,r.jsxs)(n.p,{children:["We\u2019ll start by demonstrating how to leverage ",(0,r.jsx)(n.a,{href:"https://www.langchain.com/langgraph",children:"LangGraph\u2019s"})," capabilities to build a dynamic AI workflow. This workflow integrates OpenAI and external data sources, such as a Vector Store and a SQLite database, to process user input, perform safety checks, query databases, and generate meaningful responses."]}),"\n",(0,r.jsxs)(n.p,{children:["Throughout this post, we\u2019ll leverage ",(0,r.jsx)(n.a,{href:"https://mlflow.org/docs/latest/model/models-from-code.html",children:"MLflow\u2019s Models from Code"})," feature to enable seamless tracking and versioning of AI Workflows. Additionally, we\u2019ll deep dive into ",(0,r.jsx)(n.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html",children:"MLflow\u2019s Tracing"})," feature, designed to enhance the observability of the many different components of an AI workflow by tracking inputs, outputs, and metadata associated with each intermediate step. This enables easy identification of bugs and unexpected behaviors, providing greater transparency over the workflow."]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.p,{children:["To set up and run this project, ensure the following ",(0,r.jsx)(n.strong,{children:"Python packages"})," are installed:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"faiss-cpu"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"langchain"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"langchain-core"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"langchain-openai"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"langgraph"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"langchain-community"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"pydantic >=2"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"typing_extensions"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"python-dotenv"})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Additionally, an ",(0,r.jsx)(n.strong,{children:"MLflow Tracking Server"})," is required to log and manage experiments, models, and traces effectively. For local setup, refer to the official MLflow documentation for instructions on ",(0,r.jsx)(n.a,{href:"https://mlflow.org/docs/latest/tracking/server.html",children:"configuring a simple MLflow Tracking Server"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Finally, ensure that your OpenAI API key is saved within a .env file in the project directory. This allows the application to securely access the OpenAI services required for building the AI workflow. The .env file should include a line like:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"OPENAI_API_KEY=your_openai_api_key\n"})}),"\n",(0,r.jsx)(n.h2,{id:"multi-lingual-query-engine-using-langgraph",children:"Multi-Lingual Query Engine using LangGraph"}),"\n",(0,r.jsxs)(n.p,{children:["The Multi-Lingual Query Engine leverages the ",(0,r.jsx)(n.a,{href:"https://langchain-ai.github.io/langgraph/",children:"LangGraph"})," library, an AI orchestration tool designed to create stateful, multi-agent, and cyclical graph architectures for applications powered by LLMs."]}),"\n",(0,r.jsx)(n.p,{children:"Compared to other AI orchestrators, LangGraph offers three core benefits: cycles, controllability, and persistence. It allows the definition of AI workflows with cycles, which are essential for implementing retry mechanisms like the SQL query generation retries in the Multi-Lingual Query Engine (where the query loops back for regeneration if validation fails). This makes LangGraph the ideal tool for building our Multi-Lingual Query Engine."}),"\n",(0,r.jsx)(n.h3,{id:"key-langgraph-features",children:"Key LangGraph features:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Stateful Architecture"}),": The engine maintains a dynamic snapshot of the graph\u2019s execution status. This snapshot acts as a shared resource across nodes, enabling efficient decision-making and real-time updates at each node execution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multi-Agent Design"}),": The AI Workflow includes multiple interactions with OpenAI and other external tools throughout the workflow."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cyclical Graph Structure"}),": The graph\u2019s cyclical nature introduces a robust retry mechanism. This mechanism dynamically addresses failures by looping back to previous stages when needed, ensuring continuous graph execution. (Details of this mechanism will be discussed later.)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"ai-workflow-overview",children:"AI Workflow Overview"}),"\n",(0,r.jsx)(n.p,{children:"The Multi-Lingual Query Engine\u2019s advanced AI workflow is composed of interconnected nodes and edges, each representing a crucial stage:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Translation Node"}),": Converts the user\u2019s input into English."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pre-safety Check"}),": Ensures user input is free from toxic or inappropriate content and does not contain harmful SQL commands (e.g., ",(0,r.jsx)(n.code,{children:"DELETE"}),", ",(0,r.jsx)(n.code,{children:"DROP"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Database Schema Extraction"}),": Retrieves the schema of the target database to understand its structure and available data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Relevancy Validation"}),": Validates the user\u2019s input against the database schema to ensure alignment with the database\u2019s context."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SQL Query Generation"}),": Generates an SQL query based on the user\u2019s input and the current database schema."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Post-safety Check"}),": Ensures the generated SQL Query does not contain harmful SQL commands (e.g., ",(0,r.jsx)(n.code,{children:"DELETE"}),", ",(0,r.jsx)(n.code,{children:"DROP"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SQL Query Validation"}),": Executes the SQL query in a rollback-safe environment to ensure its validity before running it."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Dynamic State Evaluation"}),": Determines the next steps based on the current state. If the SQL query validation fails, it loops back to Stage 5 to regenerate the query."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Query Execution and Result Retrieval"}),": Executes the SQL query and returns the results if it\u2019s a ",(0,r.jsx)(n.code,{children:"SELECT"})," statement."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The retry mechanism is introduced in Stage 8, where the system dynamically evaluates the current graph state. Specifically, when the SQL query validation node (Stage 7) detects an issue, the state triggers a loop back to the SQL Generation node (Stage 5) for a new SQL Generation attempt (with a maximum of 3 attempts)."}),"\n",(0,r.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,r.jsx)(n.p,{children:"The Multi-Lingual Query Engine interacts with several external components to transform natural language user inputs into SQL queries and execute them in a safe and robust manner. In this section, we will take a detailed look at the key AI Workflow components: OpenAI, Vector Store, SQLite Database, and SQL Generation Chain."}),"\n",(0,r.jsx)(n.h4,{id:"openai",children:"OpenAI"}),"\n",(0,r.jsxs)(n.p,{children:["OpenAI, more specifically the ",(0,r.jsx)(n.code,{children:"gpt-4o-mini"})," language model, plays a crucial role in multiple stages of the workflow. It provides the intelligence required for:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Translation"}),": Translates user input into English. If the text is already in English, it simply repeats the input."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Safety Checks"}),": Analyzes user input to ensure that it does not contain toxic or inappropriate content."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Relevance Checks"}),": Evaluates whether the user's question is relevant given the database schema."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SQL Generation"}),": Generates valid and executable SQL queries based on user input, SQL generation documentation, and the database schema."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Details on OpenAI implementation will be provided later on in the ",(0,r.jsx)(n.a,{href:"#node-descriptions",children:"Node Descriptions"})," section."]}),"\n",(0,r.jsx)(n.h4,{id:"faiss-vector-store",children:"FAISS Vector Store"}),"\n",(0,r.jsxs)(n.p,{children:["To build an effective natural language to SQL engine capable of generating accurate and executable SQL queries, we leverage Langchain's FAISS Vector Store feature. This setup allows the system to search and extract SQL query generation guidelines from ",(0,r.jsx)(n.a,{href:"https://www.w3schools.com/sql/",children:"W3Schools SQL documents"})," previously stored in the Vector Database, enhancing the success of SQL query generation."]}),"\n",(0,r.jsxs)(n.p,{children:["For demo purposes, we are using FAISS, an in-memory vector store where vectors are stored directly in RAM. This provides fast access but means data is not persisted between runs. For a more scalable solution that enables embeddings to be stored and shared across multiple projects, we recommend alternatives like ",(0,r.jsx)(n.a,{href:"https://aws.amazon.com/what-is/opensearch/",children:"AWS OpenSearch"}),", ",(0,r.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs/vector-search/overview",children:"Vertex AI Vector Search"}),", ",(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/search/vector-search-overview",children:"Azure Vector Search"}),", or ",(0,r.jsx)(n.a,{href:"https://docs.databricks.com/en/generative-ai/vector-search.html",children:"Mosaic AI Vector Search"}),". These cloud-based solutions offer persistent storage, automatic scaling, and seamless integration with other cloud services, making them well-suited for large-scale applications."]}),"\n",(0,r.jsx)(n.h5,{id:"step-1-load-sql-documentation",children:"Step 1: Load SQL Documentation"}),"\n",(0,r.jsxs)(n.p,{children:["The first step in creating a FAISS Vector Store with SQL query generation guidelines is to load SQL documentation from the ",(0,r.jsx)(n.a,{href:"https://www.w3schools.com/sql/",children:"W3Schools SQL page"})," using LangChain's ",(0,r.jsx)(n.code,{children:"RecursiveUrlLoader"}),". This tool retrieves the documentation, allowing us to use it as a knowledge base for our engine."]}),"\n",(0,r.jsx)(n.h5,{id:"step-2-split-the-text-into-manageable-chunks",children:"Step 2: Split the Text into Manageable Chunks"}),"\n",(0,r.jsxs)(n.p,{children:["The loaded SQL documentation is a lengthy text, making it difficult to be effectively ingested by the LLM. To address this, the next step involves splitting the text into smaller, manageable chunks using Langchain's ",(0,r.jsx)(n.code,{children:"RecursiveCharacterTextSplitter"}),". By splitting the text into chunks of 500 characters with a 50-character overlap, we ensure the language model has sufficient context while minimizing the risk of losing important information that spans across chunks. The ",(0,r.jsx)(n.code,{children:"split_text"})," method applies this splitting process, storing the resulting pieces in a list called 'documents'."]}),"\n",(0,r.jsx)(n.h5,{id:"step-3-generate-embedding-model",children:"Step 3: Generate Embedding Model"}),"\n",(0,r.jsx)(n.p,{children:"The third step is to create a model that converts these chunks into embeddings (vectorized numerical representations of each text chunk). Embeddings enable the system to compare the similarity between chunks and the user's input, facilitating the retrieval of the most relevant matches for SQL query generation."}),"\n",(0,r.jsx)(n.h5,{id:"step-4-create-and-store-embeddings-in-faiss-vector-store",children:"Step 4: Create and Store Embeddings in FAISS Vector Store"}),"\n",(0,r.jsxs)(n.p,{children:["Finally, we create and store the embeddings using FAISS. The ",(0,r.jsx)(n.code,{children:"FAISS.from_texts"})," method takes all the chunks, computes their embeddings, and stores them in a high speed searchable vector database. This searchable database allows the engine to efficiently retrieve relevant SQL guidelines, significantly improving the success rate of executable SQL query generation."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nimport os\n\nfrom bs4 import BeautifulSoup as Soup\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\n\n\ndef setup_vector_store(logger: logging.Logger):\n    """Setup or load the vector store."""\n    if not os.path.exists("data"):\n        os.makedirs("data")\n\n    vector_store_dir = "data/vector_store"\n\n    if os.path.exists(vector_store_dir):\n        # Load the vector store from disk\n        logger.info("Loading vector store from disk...")\n        vector_store = FAISS.load_local(\n            vector_store_dir,\n            OpenAIEmbeddings(),\n            allow_dangerous_deserialization=True,\n        )\n    else:\n        logger.info("Creating new vector store...")\n        # Load SQL documentation\n        url = "https://www.w3schools.com/sql/"\n        loader = RecursiveUrlLoader(\n            url=url, max_depth=2, extractor=lambda x: Soup(x, "html.parser").text\n        )\n        docs = loader.load()\n\n        # Split documents into chunks\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50,\n            separators=["\\n\\n", "\\n", ".", "!", "?", ",", " ", ""],\n        )\n\n        documents = []\n        for doc in docs:\n            splits = text_splitter.split_text(doc.page_content)\n            for i, split in enumerate(splits):\n                documents.append(\n                    {\n                        "content": split,\n                        "metadata": {"source": doc.metadata["source"], "chunk": i},\n                    }\n                )\n\n        # Compute embeddings and create vector store\n        embedding_model = OpenAIEmbeddings()\n        vector_store = FAISS.from_texts(\n            [doc["content"] for doc in documents],\n            embedding_model,\n            metadatas=[doc["metadata"] for doc in documents],\n        )\n\n        # Save the vector store to disk\n        vector_store.save_local(vector_store_dir)\n        logger.info("Vector store created and saved to disk.")\n\n    return vector_store\n'})}),"\n",(0,r.jsx)(n.h4,{id:"sqlite-database",children:"SQLite Database"}),"\n",(0,r.jsx)(n.p,{children:"The SQLite database is a key component of the Multi-Lingual Query Engine serving as the structured data repository. SQLite offers a lightweight, fast, and self-contained relational database engine that requires no server setup or installation. Its compact size (under 500KB) and zero-configuration nature make it incredibly easy to use, while its platform-agnostic database format ensures seamless portability across different systems. As a local disk database, SQLite was the ideal choice for avoiding the complexity of setting up MySQL or PostgreSQL, while still providing a reliable, full-featured SQL engine with outstanding performance."}),"\n",(0,r.jsx)(n.p,{children:"The SQLite database supports efficient SQL query generation, validation and execution by enabling:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Schema Extraction"}),": Suplying schema information for user\u2019s input context validation (Stage 4) and executable SQL Query Generation (Stage 5)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Query Execution"}),": Executing SQL queries in a rollback-safe environment in Validation Stage (Stage 7) and in Query Execution Stage (Stage 9) fetching results for ",(0,r.jsx)(n.code,{children:"SELECT"})," statements and committing changes for other query types."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"sqlite-database-initialization",children:"SQLite Database Initialization"}),"\n",(0,r.jsxs)(n.p,{children:["The database is initialized using the ",(0,r.jsx)(n.code,{children:"setup_database"})," function when the AI Workflow is initialized. This process involves:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Setting the SQLite Database Connection"}),": Establishes a connection to the SQLite database, enabling data interaction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Table Creation"}),": Defines and creates the necessary database tables for the AI Workflow."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Data Population"}),": Populates the tables with sample data to support query execution and validation stages."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nimport os\n\nimport sqlite3\n\n\ndef create_connection(db_file="data/database.db"):\n    """Create a database connection to the SQLite database."""\n    conn = sqlite3.connect(db_file)\n    return conn\n\n\ndef create_tables(conn):\n    """Create tables in the database."""\n    cursor = conn.cursor()\n    # Create Customers table\n    cursor.execute(\n        """\n    CREATE TABLE IF NOT EXISTS Customers (\n        CustomerID INTEGER PRIMARY KEY,\n        CustomerName TEXT,\n        ContactName TEXT,\n        Address TEXT,\n        City TEXT,\n        PostalCode TEXT,\n        Country TEXT\n    )\n    """\n    )\n\n    # Create Orders table\n    cursor.execute(\n        """\n    CREATE TABLE IF NOT EXISTS Orders (\n        OrderID INTEGER PRIMARY KEY,\n        CustomerID INTEGER,\n        OrderDate TEXT,\n        FOREIGN KEY (CustomerID) REFERENCES Customers (CustomerID)\n    )\n    """\n    )\n\n    # Create OrderDetails table\n    cursor.execute(\n        """\n    CREATE TABLE IF NOT EXISTS OrderDetails (\n        OrderDetailID INTEGER PRIMARY KEY,\n        OrderID INTEGER,\n        ProductID INTEGER,\n        Quantity INTEGER,\n        FOREIGN KEY (OrderID) REFERENCES Orders (OrderID),\n        FOREIGN KEY (ProductID) REFERENCES Products (ProductID)\n    )\n    """\n    )\n\n    # Create Products table\n    cursor.execute(\n        """\n    CREATE TABLE IF NOT EXISTS Products (\n        ProductID INTEGER PRIMARY KEY,\n        ProductName TEXT,\n        Price REAL\n    )\n    """\n    )\n\n    conn.commit()\n\n\ndef populate_tables(conn):\n    """Populate tables with sample data if they are empty."""\n    cursor = conn.cursor()\n\n    # Populate Customers table if empty\n    cursor.execute("SELECT COUNT(*) FROM Customers")\n    if cursor.fetchone()[0] == 0:\n        customers = []\n        for i in range(1, 51):\n            customers.append(\n                (\n                    i,\n                    f"Customer {i}",\n                    f"Contact {i}",\n                    f"Address {i}",\n                    f"City {i % 10}",\n                    f"{10000 + i}",\n                    f"Country {i % 5}",\n                )\n            )\n        cursor.executemany(\n            """\n        INSERT INTO Customers (CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n        """,\n            customers,\n        )\n\n    # Populate Products table if empty\n    cursor.execute("SELECT COUNT(*) FROM Products")\n    if cursor.fetchone()[0] == 0:\n        products = []\n        for i in range(1, 51):\n            products.append((i, f"Product {i}", round(10 + i * 0.5, 2)))\n        cursor.executemany(\n            """\n        INSERT INTO Products (ProductID, ProductName, Price)\n        VALUES (?, ?, ?)\n        """,\n            products,\n        )\n\n    # Populate Orders table if empty\n    cursor.execute("SELECT COUNT(*) FROM Orders")\n    if cursor.fetchone()[0] == 0:\n        orders = []\n        from datetime import datetime, timedelta\n\n        base_date = datetime(2023, 1, 1)\n        for i in range(1, 51):\n            order_date = base_date + timedelta(days=i)\n            orders.append(\n                (\n                    i,\n                    i % 50 + 1,  # CustomerID between 1 and 50\n                    order_date.strftime("%Y-%m-%d"),\n                )\n            )\n        cursor.executemany(\n            """\n        INSERT INTO Orders (OrderID, CustomerID, OrderDate)\n        VALUES (?, ?, ?)\n        """,\n            orders,\n        )\n\n    # Populate OrderDetails table if empty\n    cursor.execute("SELECT COUNT(*) FROM OrderDetails")\n    if cursor.fetchone()[0] == 0:\n        order_details = []\n        for i in range(1, 51):\n            order_details.append(\n                (\n                    i,\n                    i % 50 + 1,  # OrderID between 1 and 50\n                    i % 50 + 1,  # ProductID between 1 and 50\n                    (i % 5 + 1) * 2,  # Quantity between 2 and 10\n                )\n            )\n        cursor.executemany(\n            """\n        INSERT INTO OrderDetails (OrderDetailID, OrderID, ProductID, Quantity)\n        VALUES (?, ?, ?, ?)\n        """,\n            order_details,\n        )\n\n    conn.commit()\n\n\ndef setup_database(logger: logging.Logger):\n    """Setup the database and return the connection."""\n    db_file = "data/database.db"\n    if not os.path.exists("data"):\n        os.makedirs("data")\n\n    db_exists = os.path.exists(db_file)\n\n    conn = create_connection(db_file)\n\n    if not db_exists:\n        logger.info("Setting up the database...")\n        create_tables(conn)\n        populate_tables(conn)\n    else:\n        logger.info("Database already exists. Skipping setup.")\n\n    return conn\n'})}),"\n",(0,r.jsx)(n.h4,{id:"sql-generation-chain",children:"SQL Generation Chain"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"SQL Generation Chain"})," (",(0,r.jsx)(n.code,{children:"sql_gen_chain"}),") is the backbone of automated SQL query generation in our workflow. This chain leverages LangChain's modular capabilities and OpenAI's advanced natural language processing to transform user questions into precise and executable SQL queries."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Core Features"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prompt-Driven Generation"}),": Begins with a thoughtfully designed prompt that integrates the database schema and documentation snippets, ensuring queries are contextually accurate."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Structured Responses"}),": Delivers outputs in a predefined format, including:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"description"})," of the query's purpose."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The corresponding ",(0,r.jsx)(n.strong,{children:"SQL code"})," ready for execution."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Adaptable and Reliable"}),": Uses ",(0,r.jsx)(n.code,{children:"gpt-4o-mini"})," for robust, consistent query generation, minimizing manual effort and errors."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This chain is a critical component in our workflow, enabling seamless integration of SQL query generation with downstream processes, ensuring accuracy, and significantly improving efficiency."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel, Field\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nclass SQLQuery(BaseModel):\n    """Schema for SQL query solutions to questions."""\n    description: str = Field(description="Description of the SQL query")\n    sql_code: str = Field(description="The SQL code block")\n\ndef get_sql_gen_chain():\n    """Set up the SQL generation chain."""\n    sql_gen_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                "system",\n                """You are a SQL assistant with expertise in SQL query generation. \\n\nAnswer the user\'s question based on the provided documentation snippets and the database schema provided below. Ensure any SQL query you provide is valid and executable. \\n\nStructure your answer with a description of the query, followed by the SQL code block. Here are the documentation snippets:\\n{retrieved_docs}\\n\\nDatabase Schema:\\n{database_schema}""",\n            ),\n            ("placeholder", "{messages}"),\n        ]\n    )\n\n    # Initialize the OpenAI LLM\n    llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")\n\n    # Create the code generation chain\n    sql_gen_chain = sql_gen_prompt | llm.with_structured_output(SQLQuery)\n\n    return sql_gen_chain\n'})}),"\n",(0,r.jsx)(n.h4,{id:"workflow-setup-and-initialization",children:"Workflow Setup and Initialization"}),"\n",(0,r.jsxs)(n.p,{children:["Before delving into the workflow nodes, it's crucial to set up the necessary components and define the structure of the workflow. This section explains the initialization of essential libraries, logging, and the custom ",(0,r.jsx)(n.code,{children:"GraphState"})," class, as well as the main workflow compilation function."]}),"\n",(0,r.jsxs)(n.h5,{id:"defining-graphstate",children:["Defining ",(0,r.jsx)(n.code,{children:"GraphState"})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"GraphState"})," class is a custom ",(0,r.jsx)(n.code,{children:"TypedDict"})," that maintains the state information as the workflow progresses. It acts as a shared data structure across the nodes, ensuring continuity and consistency. Key fields include:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"error"})}),": Tracks whether an error has occurred."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"messages"})}),": Stores a list of user and system messages."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"generation"})}),": Holds the generated SQL query."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"iterations"})}),": Tracks the number of retry attempts in case of errors."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"results"})}),": Stores the SQL execution results, if any."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"no_records_found"})}),": Flags if no records are returned by the query."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"translated_input"})}),": Contains the user's translated input."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"database_schema"})}),": Maintains the database schema for context validation."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nimport re\nfrom typing import List, Optional\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, StateGraph\nfrom sql_generation import get_sql_gen_chain\nfrom typing_extensions import TypedDict\n\n# Initialize the logger\n_logger = logging.getLogger(__name__)\n_logger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")\nhandler.setFormatter(formatter)\n_logger.addHandler(handler)\n\nclass GraphState(TypedDict):\n    error: str  # Tracks if an error has occurred\n    messages: List  # List of messages (user input and assistant messages)\n    generation: Optional[str]  # Holds the generated SQL query\n    iterations: int  # Keeps track of how many times the workflow has retried\n    results: Optional[List]  # Holds the results of SQL execution\n    no_records_found: bool  # Flag for whether any records were found in the SQL result\n    translated_input: str  # Holds the translated user input\n    database_schema: str  # Holds the extracted database schema for context checking\n'})}),"\n",(0,r.jsx)(n.h5,{id:"workflow-compilation-function",children:"Workflow Compilation Function"}),"\n",(0,r.jsxs)(n.p,{children:["The main function, ",(0,r.jsx)(n.code,{children:"get_workflow"}),", is responsible for defining and compiling the workflow. Key components include:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"conn"})," and ",(0,r.jsx)(n.code,{children:"cursor"})]}),": Used for database connectivity and query execution."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"vector_store"})}),": A vector database for contextual retrieval."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"max_iterations"})}),": Sets a limit on retry attempts to prevent infinite loops."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"sql_gen_chain"})}),": Retrieves the SQL generation chain from ",(0,r.jsx)(n.code,{children:"sql_generation"})," for producing SQL queries based on contextual inputs."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"ChatOpenAI"})}),": Initializes the OpenAI ",(0,r.jsx)(n.code,{children:"gpt-4o-mini"})," model for tasks like safety checks and query translation."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def get_workflow(conn, cursor, vector_store):\n    """Define and compile the LangGraph workflow."""\n\n    # Max iterations: defines how many times the workflow should retry in case of errors\n    max_iterations = 3\n\n    # SQL generation chain: this is a chain that will generate SQL based on retrieved docs\n    sql_gen_chain = get_sql_gen_chain()\n\n    # Initialize OpenAI LLM for translation and safety checks\n    llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")\n\n    # Define the individual nodes of the workflow\n'})}),"\n",(0,r.jsxs)(n.p,{children:["This function acts as the entry point for creating a complete workflow using ",(0,r.jsx)(n.code,{children:"StateGraph"}),". Individual nodes within the workflow will be defined and connected in subsequent sections."]}),"\n",(0,r.jsx)(n.h4,{id:"node-descriptions",children:"Node Descriptions"}),"\n",(0,r.jsx)(n.h5,{id:"1-translate-input",children:"1. Translate Input"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"translate_input"})," node translates user queries into English to standardize processing and ensure compatibility with downstream nodes. Translating user input as the first step in the AI Workflow ensures task segregation and improves observability. Task segregation simplifies the workflow by isolating translation from the other dowstream tasks like user input safety validation and SQL generation. Improved observability provides clear traces in MLflow, making it easier to debug and monitor the process."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input: ",(0,r.jsx)(n.em,{children:'"Quantos pedidos foram realizados em Novembro?"'})]}),"\n",(0,r.jsxs)(n.li,{children:["Translated: ",(0,r.jsx)(n.em,{children:'"How many orders were made in November?"'})]}),"\n",(0,r.jsxs)(n.li,{children:["Input: ",(0,r.jsx)(n.em,{children:'"Combien de ventes avons-nous enregistr\xe9es en France ?"'})]}),"\n",(0,r.jsxs)(n.li,{children:["Translated: ",(0,r.jsx)(n.em,{children:'"How many sales did we record in France?"'})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def translate_input(state: GraphState) -> GraphState:\n    """\n    Translates user input to English using an LLM. If the input is already in English,\n    it is returned as is. This ensures consistent input for downstream processing.\n\n    Args:\n        state (GraphState): The current graph state containing user messages.\n\n    Returns:\n        GraphState: The updated state with the translated input.\n    """\n    _logger.info("Starting translation of user input to English.")\n    messages = state["messages"]\n    user_input = messages[-1][1]  # Get the latest user input\n\n    # Translation prompt for the model\n    translation_prompt = f"""\n    Translate the following text to English. If the text is already in English, repeat it exactly without any additional explanation.\n\n    Text:\n    {user_input}\n    """\n\n    # Call the OpenAI LLM to translate the text\n    translated_response = llm.invoke(translation_prompt)\n    translated_text = translated_response.content.strip()  # Access the \'content\' attribute and strip any extra spaces\n\n    # Update state with the translated input\n    state["translated_input"] = translated_text\n    _logger.info("Translation completed successfully. Translated input: %s", translated_text)\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"2-pre-safety-check",children:"2. Pre-safety Check"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"pre_safety_check"})," node ensures early detection of disallowed SQL operations and inappropriate content in the user's input. While the check for harmful SQL commands (e.g., ",(0,r.jsx)(n.code,{children:"CREATE"}),", ",(0,r.jsx)(n.code,{children:"DELETE"}),", ",(0,r.jsx)(n.code,{children:"DROP"}),", ",(0,r.jsx)(n.code,{children:"INSERT"}),", ",(0,r.jsx)(n.code,{children:"UPDATE"}),") will occur again later in the workflow, specifically after generating the SQL query, this pre-safety check is crucial for identifying potential issues at the input stage. By doing so, it prevents unnecessary computation and offers immediate feedback to the user."]}),"\n",(0,r.jsxs)(n.p,{children:["While the use of a disallow list for harmful SQL operations provides a quick way to safeguard against destructive queries, maintaining a comprehensive disallow list can become hard to manage when dealing with complex SQL backends like T-SQL. An alternative approach is adopting an allowlist, restricting queries to only safe operations (e.g., ",(0,r.jsx)(n.code,{children:"SELECT"}),", ",(0,r.jsx)(n.code,{children:"JOIN"}),"). This approach ensures a more robust solution by narrowing down permissible actions rather than attempting to block every risky command."]}),"\n",(0,r.jsxs)(n.p,{children:["To achieve an enterprise-grade solution, the project could leverage frameworks like ",(0,r.jsx)(n.a,{href:"https://github.com/unitycatalog/unitycatalog/blob/main/ai/core/README.md",children:"Unity Catalog"}),", which provide a centralized and robust approach to managing security-related functions, such as the ",(0,r.jsx)(n.code,{children:"pre_safety_check"})," for AI workflows. By registering and managing reusable functions within such a framework, you can enforce consistent and reliable behavior across all AI workflows, enhancing both security and scalability."]}),"\n",(0,r.jsx)(n.p,{children:"Additionally, the node leverages the LLM to analyze the input for offensive or inappropriate content. If unsafe queries or inappropriate content are detected, the state is updated with an error flag and transparent feedback is provided, safeguarding the workflow from malicious or destructive elements early on."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Examples:"})}),"\n"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Disallowed Operations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," ",(0,r.jsx)(n.em,{children:'"DROP TABLE customers;"'})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," ",(0,r.jsx)(n.em,{children:'"Your query contains disallowed SQL operations and cannot be processed."'})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Input:"}),' _"SELECT _ FROM orders;"*']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," ",(0,r.jsx)(n.em,{children:'"Query allowed."'})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Inappropriate Content:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," ",(0,r.jsx)(n.em,{children:"\"Show me orders where customers have names like 'John the Idiot';\""})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," ",(0,r.jsx)(n.em,{children:'"Your query contains inappropriate content and cannot be processed."'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," ",(0,r.jsx)(n.em,{children:'"Find total sales by region."'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," ",(0,r.jsx)(n.em,{children:'"Input is safe to process."'})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def pre_safety_check(state: GraphState) -> GraphState:\n    """\n    Perform safety checks on the user input to ensure that no dangerous SQL operations\n    or inappropriate content is present. The function checks for SQL operations like\n    DELETE, DROP, and others, and also evaluates the input for toxic or unsafe content.\n\n    Args:\n        state (GraphState): The current graph state containing the translated user input.\n\n    Returns:\n        GraphState: The updated state with error status and messages if any issues are found.\n    """\n    _logger.info("Performing safety check.")\n    translated_input = state["translated_input"]\n    messages = state["messages"]\n    error = "no"\n\n    # List of disallowed SQL operations (e.g., DELETE, DROP)\n    disallowed_operations = [\'CREATE\', \'DELETE\', \'DROP\', \'INSERT\', \'UPDATE\', \'ALTER\', \'TRUNCATE\', \'EXEC\', \'EXECUTE\']\n    pattern = re.compile(r\'\\b(\' + \'|\'.join(disallowed_operations) + r\')\\b\', re.IGNORECASE)\n\n    # Check if the input contains disallowed SQL operations\n    if pattern.search(translated_input):\n        _logger.warning("Input contains disallowed SQL operations. Halting the workflow.")\n        error = "yes"\n        messages += [("assistant", "Your query contains disallowed SQL operations and cannot be processed.")]\n    else:\n        # Check if the input contains inappropriate content\n        safety_prompt = f"""\n        Analyze the following input for any toxic or inappropriate content.\n\n        Respond with only "safe" or "unsafe", and nothing else.\n\n        Input:\n        {translated_input}\n        """\n        safety_invoke = llm.invoke(safety_prompt)\n        safety_response = safety_invoke.content.strip().lower()  # Access the \'content\' attribute and strip any extra spaces\n\n        if safety_response == "safe":\n            _logger.info("Input is safe to process.")\n        else:\n            _logger.warning("Input contains inappropriate content. Halting the workflow.")\n            error = "yes"\n            messages += [("assistant", "Your query contains inappropriate content and cannot be processed.")]\n\n    # Update state with error status and messages\n    state["error"] = error\n    state["messages"] = messages\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"3-schema-extract",children:"3. Schema Extract"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"schema_extract"})," node dynamically retrieves the database schema, including table names and column details, by querying metadata. The formatted schema is stored in the state, enabling validation of user queries while adapting to the current database structure."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input: Request for schema extraction.",(0,r.jsx)(n.br,{}),"\n","Schema Output:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Customers(CustomerID (INTEGER), CustomerName (TEXT), ContactName (TEXT), Address (TEXT), City (TEXT), PostalCode (TEXT), Country (TEXT))"}),"\n",(0,r.jsx)(n.li,{children:"Orders(OrderID (INTEGER), CustomerID (INTEGER), OrderDate (TEXT))"}),"\n",(0,r.jsx)(n.li,{children:"OrderDetails(OrderDetailID (INTEGER), OrderID (INTEGER), ProductID (INTEGER), Quantity (INTEGER))"}),"\n",(0,r.jsx)(n.li,{children:"Products(ProductID (INTEGER), ProductName (TEXT), Price (REAL))"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def schema_extract(state: GraphState) -> GraphState:\n    """\n    Extracts the database schema, including all tables and their respective columns,\n    from the connected SQLite database. This function retrieves the list of tables and\n    iterates through each table to gather column definitions (name and data type).\n\n    Args:\n        state (GraphState): The current graph state, which will be updated with the database schema.\n\n    Returns:\n        GraphState: The updated state with the extracted database schema.\n    """\n    _logger.info("Extracting database schema.")\n\n    # Extract the schema from the database\n    cursor.execute("SELECT name FROM sqlite_master WHERE type=\'table\';")\n    tables = cursor.fetchall()\n    schema_details = []\n\n    # Loop through each table and retrieve column information\n    for table_name_tuple in tables:\n        table_name = table_name_tuple[0]\n        cursor.execute(f"PRAGMA table_info({table_name});")\n        columns = cursor.fetchall()\n\n        # Format column definitions\n        column_defs = \', \'.join([f"{col[1]} ({col[2]})" for col in columns])\n        schema_details.append(f"- {table_name}({column_defs})")\n\n    # Save the schema in the state\n    database_schema = \'\\n\'.join(schema_details)\n    state["database_schema"] = database_schema\n    _logger.info(f"Database schema extracted:\\n{database_schema}")\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"4-context-check",children:"4. Context Check"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"context_check"})," node validates user queries by comparing them against the extracted database schema to ensure alignment and relevance. Queries that do not correspond to the schema are flagged as irrelevant, preventing resource waste and enabling user feedback for query reformulation."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input: ",(0,r.jsx)(n.em,{children:'"What is the average order value?"'}),(0,r.jsx)(n.br,{}),"\n","Schema Match: Input is relevant to the database schema."]}),"\n",(0,r.jsxs)(n.li,{children:["Input: ",(0,r.jsx)(n.em,{children:'"Show me data from the inventory table."'}),(0,r.jsx)(n.br,{}),"\n","Response: ",(0,r.jsx)(n.em,{children:'"Your question is not related to the database and cannot be processed."'})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def context_check(state: GraphState) -> GraphState:\n    """\n    Checks whether the user\'s input is relevant to the database schema by comparing\n    the user\'s question with the database schema. Uses a language model to determine if\n    the question can be answered using the provided schema.\n\n    Args:\n        state (GraphState): The current graph state, which contains the translated input\n                            and the database schema.\n\n    Returns:\n        GraphState: The updated state with error status and messages if the input is irrelevant.\n    """\n    _logger.info("Performing context check.")\n\n    # Extract relevant data from the state\n    translated_input = state["translated_input"]\n    messages = state["messages"]\n    error = "no"\n    database_schema = state["database_schema"]  # Get the schema from the state\n\n    # Use the LLM to determine if the input is relevant to the database schema\n    context_prompt = f"""\n    Determine whether the following user input is a question that can be answered using the database schema provided below.\n\n    Respond with only "relevant" if the input is relevant to the database schema, or "irrelevant" if it is not.\n\n    User Input:\n    {translated_input}\n\n    Database Schema:\n    {database_schema}\n    """\n\n    # Call the LLM for context check\n    llm_invoke = llm.invoke(context_prompt)\n    llm_response = llm_invoke.content.strip().lower()  # Access the \'content\' attribute and strip any extra spaces and lower case\n\n    # Process the response from the LLM\n    if llm_response == "relevant":\n        _logger.info("Input is relevant to the database schema.")\n    else:\n        _logger.info("Input is not relevant. Halting the workflow.")\n        error = "yes"\n        messages += [("assistant", "Your question is not related to the database and cannot be processed.")]\n\n    # Update the state with error and messages\n    state["error"] = error\n    state["messages"] = messages\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"5-generate",children:"5. Generate"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"generate"})," node constructs SQL queries from natural language input by retrieving relevant documentation from the vector store and leveraging a pre-defined SQL generation chain. It aligns the query with the user\u2019s intent and schema context, updating the state with the generated SQL and its description."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input: ",(0,r.jsx)(n.em,{children:'"Find total sales."'}),(0,r.jsx)(n.br,{}),"\n","Generated SQL: ",(0,r.jsx)(n.em,{children:'"SELECT SUM(Products.Price * OrderDetails.Quantity) AS TotalSales FROM OrderDetails LEFT JOIN Products ON OrderDetails.ProductID = Products.ProductID;"'})]}),"\n",(0,r.jsxs)(n.li,{children:["Input: ",(0,r.jsx)(n.em,{children:'"List all customers in New York."'}),(0,r.jsx)(n.br,{}),"\n","Generated SQL: ",(0,r.jsx)(n.em,{children:"\"SELECT name FROM customers WHERE location = 'New York';\""})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def generate(state: GraphState) -> GraphState:\n    """\n    Generates an SQL query based on the user\'s input. The node retrieves relevant documents from\n    the vector store and uses a generation chain to produce an SQL query.\n\n    Args:\n        state (GraphState): The current graph state, which contains the translated input and\n                            other relevant data such as messages and iteration count.\n\n    Returns:\n        GraphState: The updated state with the generated SQL query and related messages.\n    """\n    _logger.info("Generating SQL query.")\n\n    # Extract relevant data from the state\n    messages = state["messages"]\n    iterations = state["iterations"]\n    translated_input = state["translated_input"]\n    database_schema = state["database_schema"]\n\n    # Retrieve relevant documents from the vector store based on the translated user input\n    docs = vector_store.similarity_search(translated_input, k=4)\n    retrieved_docs = "\\n\\n".join([doc.page_content for doc in docs])\n\n    # Generate the SQL query using the SQL generation chain\n    sql_solution = sql_gen_chain.invoke(\n        {\n            "retrieved_docs": retrieved_docs,\n            "database_schema": database_schema,\n            "messages": [("user", translated_input)],\n        }\n    )\n\n    # Save the generated SQL query in the state\n    messages += [\n        (\n            "assistant",\n            f"{sql_solution.description}\\nSQL Query:\\n{sql_solution.sql_code}",\n        )\n    ]\n    iterations += 1\n\n    # Log the generated SQL query\n    _logger.info("Generated SQL query:\\n%s", sql_solution.sql_code)\n\n    # Update the state with the generated SQL query and updated message list\n    state["generation"] = sql_solution\n    state["messages"] = messages\n    state["iterations"] = iterations\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"6-post-safety-check",children:"6. Post-safety Check"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"post_safety_check"})," node ensures the generated SQL query is safe by performing a final validation for harmful SQL commands. While the earlier pre-safety check identifies disallowed operations in user inputs, this post-safety check verifies that the SQL query produced after generation adheres to security guidelines. This two-step approach ensures that even if disallowed operations are inadvertently introduced during query generation, they can be caught and flagged. If unsafe queries are detected, the node halts the workflow, updates the state with an error flag, and provides feedback to the user."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Examples:"})}),"\n"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Disallowed Operations:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generated Query:"})," ",(0,r.jsx)(n.em,{children:'"DROP TABLE orders;"'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," ",(0,r.jsx)(n.em,{children:'"The generated SQL query contains disallowed SQL operations: DROP and cannot be processed."'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generated Query:"})," ",(0,r.jsx)(n.em,{children:'"SELECT name FROM customers;"'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," ",(0,r.jsx)(n.em,{children:'"Query is valid."'})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def post_safety_check(state: GraphState) -> GraphState:\n    """\n    Perform safety checks on the generated SQL query to ensure that it doesn\'t contain disallowed operations\n    such as CREATE, DELETE, DROP, etc. This node checks the SQL query generated earlier in the workflow.\n\n    Args:\n        state (GraphState): The current graph state containing the generated SQL query.\n\n    Returns:\n        GraphState: The updated state with error status and messages if any issues are found.\n    """\n    _logger.info("Performing post-safety check on the generated SQL query.")\n\n    # Retrieve the generated SQL query from the state\n    sql_solution = state.get("generation", {})\n    sql_query = sql_solution.get("sql_code", "").strip()\n    messages = state["messages"]\n    error = "no"\n\n    # List of disallowed SQL operations\n    disallowed_operations = [\'CREATE\', \'DELETE\', \'DROP\', \'INSERT\', \'UPDATE\', \'ALTER\', \'TRUNCATE\', \'EXEC\', \'EXECUTE\']\n    pattern = re.compile(r\'\\b(\' + \'|\'.join(disallowed_operations) + r\')\\b\', re.IGNORECASE)\n\n    # Check if the generated SQL query contains disallowed SQL operations\n    found_operations = pattern.findall(sql_query)\n    if found_operations:\n        _logger.warning(\n            "Generated SQL query contains disallowed SQL operations: %s. Halting the workflow.",\n            ", ".join(set(found_operations))\n        )\n        error = "yes"\n        messages += [("assistant", f"The generated SQL query contains disallowed SQL operations: {\', \'.join(set(found_operations))} and cannot be processed.")]\n    else:\n        _logger.info("Generated SQL query passed the safety check.")\n\n    # Update state with error status and messages\n    state["error"] = error\n    state["messages"] = messages\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"7-sql-check",children:"7. SQL Check"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"sql_check"})," node ensures the generated SQL query is safe and syntactically valid by executing it within a transactional savepoint. Any changes are rolled back after validation, with errors flagged and detailed feedback provided to maintain query integrity."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input SQL: ",(0,r.jsx)(n.em,{children:"\"SELECT name FROM customers WHERE city = 'New York';\""}),(0,r.jsx)(n.br,{}),"\n","Validation: Query is valid."]}),"\n",(0,r.jsxs)(n.li,{children:["Input SQL: ",(0,r.jsx)(n.em,{children:'"SELECT MONTH(date) AS month, SUM(total) AS total_sales FROM orders GROUP BY MONTH(date);"'}),(0,r.jsx)(n.br,{}),"\n","Response: ",(0,r.jsx)(n.em,{children:'"Your SQL query failed to execute: no such function: MONTH."'})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def sql_check(state: GraphState) -> GraphState:\n    """\n    Validates the generated SQL query by attempting to execute it on the database.\n    If the query is valid, the changes are rolled back to ensure no data is modified.\n    If there is an error during execution, the error is logged and the state is updated accordingly.\n\n    Args:\n        state (GraphState): The current graph state, which contains the generated SQL query\n                             and the messages to communicate with the user.\n\n    Returns:\n        GraphState: The updated state with error status and messages if the query is invalid.\n    """\n    _logger.info("Validating SQL query.")\n\n    # Extract relevant data from the state\n    messages = state["messages"]\n    sql_solution = state["generation"]\n    error = "no"\n\n    sql_code = sql_solution.sql_code.strip()\n\n    try:\n        # Start a savepoint for the transaction to allow rollback\n        conn.execute(\'SAVEPOINT sql_check;\')\n        # Attempt to execute the SQL query\n        cursor.execute(sql_code)\n        # Roll back to the savepoint to undo any changes\n        conn.execute(\'ROLLBACK TO sql_check;\')\n        _logger.info("SQL query validation: success.")\n    except Exception as e:\n        # Roll back in case of error\n        conn.execute(\'ROLLBACK TO sql_check;\')\n        _logger.error("SQL query validation failed. Error: %s", e)\n        messages += [("user", f"Your SQL query failed to execute: {e}")]\n        error = "yes"\n\n    # Update the state with the error status\n    state["error"] = error\n    state["messages"] = messages\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"8-run-query",children:"8. Run Query"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"run_query"})," node executes the validated SQL query, connecting to the database to retrieve results. It updates the state with the query output, ensuring the data is formatted for further analysis or reporting while implementing robust error handling."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input SQL: ",(0,r.jsx)(n.em,{children:"\"SELECT COUNT(*) FROM Customers WHERE City = 'New York';\""}),(0,r.jsx)(n.br,{}),"\n","Query Result: ",(0,r.jsx)(n.em,{children:'"(0,)"'})]}),"\n",(0,r.jsxs)(n.li,{children:['Input SQL: _"SELECT SUM(Products.Price * OrderDetails.Quantity) AS TotalSales FROM OrderDetails LEFT JOIN Products ON OrderDetails.ProductID = Products.ProductID;"*',(0,r.jsx)(n.br,{}),"\n",'Query Result: _"(6925.0,)"_']}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def run_query(state: GraphState) -> GraphState:\n    """\n    Executes the generated SQL query on the database and retrieves the results if it is a SELECT query.\n    For non-SELECT queries, commits the changes to the database. If no records are found for a SELECT query,\n    the `no_records_found` flag is set to True.\n\n    Args:\n        state (GraphState): The current graph state, which contains the generated SQL query and other relevant data.\n\n    Returns:\n        GraphState: The updated state with the query results, or a flag indicating if no records were found.\n    """\n    _logger.info("Running SQL query.")\n\n    # Extract the SQL query from the state\n    sql_solution = state["generation"]\n    sql_code = sql_solution.sql_code.strip()\n    results = None\n    no_records_found = False  # Flag to indicate no records found\n\n    try:\n        # Execute the SQL query\n        cursor.execute(sql_code)\n\n        # For SELECT queries, fetch and store results\n        if sql_code.upper().startswith("SELECT"):\n            results = cursor.fetchall()\n            if not results:\n                no_records_found = True\n                _logger.info("SQL query execution: success. No records found.")\n            else:\n                _logger.info("SQL query execution: success.")\n        else:\n            # For non-SELECT queries, commit the changes\n            conn.commit()\n            _logger.info("SQL query execution: success. Changes committed.")\n    except Exception as e:\n        _logger.error("SQL query execution failed. Error: %s", e)\n\n    # Update the state with results and flag for no records found\n    state["results"] = results\n    state["no_records_found"] = no_records_found\n\n    return state\n'})}),"\n",(0,r.jsx)(n.h5,{id:"decision-step-determine-next-action",children:"Decision Step: Determine Next Action"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"decide_next_step"})," function acts as a control point in the workflow, deciding what action should be taken next based on the current state. It evaluates the ",(0,r.jsx)(n.code,{children:"error"})," status and the number of iterations performed so far to determine if the query should be run, the workflow should be finished, or if the system should retry generating the SQL query."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Process:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["If there is no error (",(0,r.jsx)(n.code,{children:'error == "no"'}),"), the system proceeds with running the SQL query."]}),"\n",(0,r.jsxs)(n.li,{children:["If the maximum number of iterations (",(0,r.jsx)(n.code,{children:"max_iterations"}),") has been reached, the workflow ends."]}),"\n",(0,r.jsx)(n.li,{children:"If an error occurred and the maximum iterations haven't been reached, the system will retry the query generation."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Workflow Decisions:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No Error, Proceed with Query"}),": If no errors are found in the previous steps, the workflow moves forward to run the query."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Maximum Iterations Reached, End Workflow"}),": If the workflow has already attempted a set number of times (",(0,r.jsx)(n.code,{children:"max_iterations"}),"), it terminates."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error Detected, Retry SQL Generation"}),": If an error occurs and the system has not yet reached the retry limit, it will attempt to regenerate the SQL query."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Code:"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def decide_next_step(state: GraphState) -> str:\n    """\n    Determines the next step in the workflow based on the current state, including whether the query\n    should be run, the workflow should be finished, or if the query generation needs to be retried.\n\n    Args:\n        state (GraphState): The current graph state, which contains error status and iteration count.\n\n    Returns:\n        str: The next step in the workflow, which can be "run_query", "generate", or END.\n    """\n    _logger.info("Deciding next step based on current state.")\n\n    error = state["error"]\n    iterations = state["iterations"]\n\n    if error == "no":\n        _logger.info("Error status: no. Proceeding with running the query.")\n        return "run_query"\n    elif iterations >= max_iterations:\n        _logger.info("Maximum iterations reached. Ending the workflow.")\n        return END\n    else:\n        _logger.info("Error detected. Retrying SQL query generation.")\n        return "generate"\n'})}),"\n",(0,r.jsx)(n.h4,{id:"workflow-orchestration-and-conditional-logic",children:"Workflow Orchestration and Conditional Logic"}),"\n",(0,r.jsxs)(n.p,{children:["To define the orchestration of tasks in our system, we construct a workflow graph using the ",(0,r.jsx)(n.code,{children:"StateGraph"})," class. Each task is represented as a node, and the transitions between tasks are defined as edges. Conditional edges are used to control the flow based on the state of the workflow."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def get_workflow(conn, cursor, vector_store):\n    """Define and compile the LangGraph workflow."""\n\n    # Max iterations: defines how many times the workflow should retry in case of errors\n    max_iterations = 3\n\n    # SQL generation chain: this is a chain that will generate SQL based on retrieved docs\n    sql_gen_chain = get_sql_gen_chain()\n\n    # Initialize OpenAI LLM for translation and safety checks\n    llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")\n\n    # Define the individual nodes of the workflow\n    ... # Insert nodes code defined above here\n\n    # Build the workflow graph\n    workflow = StateGraph(GraphState)\n\n    # Define workflow nodes\n    workflow.add_node("translate_input", translate_input)  # Translate user input to structured format\n    workflow.add_node("pre_safety_check", pre_safety_check)  # Perform a pre-safety check on input\n    workflow.add_node("schema_extract", schema_extract)  # Extract the database schema\n    workflow.add_node("context_check", context_check)  # Validate input relevance to context\n    workflow.add_node("generate", generate)  # Generate SQL query\n    workflow.add_node("post_safety_check", post_safety_check)  # Perform a post-safety check on generated SQL query\n    workflow.add_node("sql_check", sql_check)  # Validate the generated SQL query\n    workflow.add_node("run_query", run_query)  # Execute the SQL query\n\n    # Define workflow edges\n    workflow.add_edge(START, "translate_input")  # Start at the translation step\n    workflow.add_edge("translate_input", "pre_safety_check")  # Move to safety checks\n\n    # Conditional edge after safety check\n    workflow.add_conditional_edges(\n        "pre_safety_check",  # Start at the pre_safety_check node\n        lambda state: "schema_extract" if state["error"] == "no" else END,  # Decide next step\n        {"schema_extract": "schema_extract", END: END}  # Map states to nodes\n    )\n\n    workflow.add_edge("schema_extract", "context_check")  # Proceed to context validation\n\n    # Conditional edge after context check\n    workflow.add_conditional_edges(\n        "context_check",  # Start at the context_check node\n        lambda state: "generate" if state["error"] == "no" else END,  # Decide next step\n        {"generate": "generate", END: END}\n    )\n\n    workflow.add_edge("generate", "post_safety_check")  # Proceed to post-safety check\n\n    # Conditional edge after post-safety check\n    workflow.add_conditional_edges(\n            "post_safety_check",  # Start at the post_safety_check node\n            lambda state: "sql_check" if state["error"] == "no" else END,  # If no error, proceed to sql_check, else END\n            {"sql_check": "sql_check", END: END},\n        )\n\n    # Conditional edge after SQL validation\n    workflow.add_conditional_edges(\n        "sql_check",  # Start at the sql_check node\n        decide_next_step,  # Function to determine the next step\n        {\n            "run_query": "run_query",  # If SQL is valid, execute the query\n            "generate": "generate",  # If retry is needed, go back to generation\n            END: END  # Otherwise, terminate the workflow\n        }\n    )\n\n    workflow.add_edge("run_query", END)  # Final step is to end the workflow\n\n    # Compile and return the workflow application\n    app = workflow.compile()\n\n    return app\n'})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Start to ",(0,r.jsx)(n.code,{children:"translate_input"})]}),":",(0,r.jsx)(n.br,{}),"\n","The workflow begins by translating user input into a structured format."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"translate_input"})," to ",(0,r.jsx)(n.code,{children:"pre_safety_check"})]}),":",(0,r.jsx)(n.br,{}),"\n","After translation, the workflow proceeds to check the safety of the input."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"pre_safety_check"})," Conditional Rule"]}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["If the input passes the pre-safety check (",(0,r.jsx)(n.code,{children:'state["error"] == "no"'}),"), the workflow moves to ",(0,r.jsx)(n.code,{children:"schema_extract"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["If the input fails the pre-safety check, the workflow terminates (",(0,r.jsx)(n.code,{children:"END"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"schema_extract"})," to ",(0,r.jsx)(n.code,{children:"context_check"})]}),":",(0,r.jsx)(n.br,{}),"\n","The schema is extracted, and then the workflow validates the input\u2019s relevance to the database context."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"context_check"})," Conditional Rule"]}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["If the input is relevant (",(0,r.jsx)(n.code,{children:'state["error"] == "no"'}),"), the workflow moves to ",(0,r.jsx)(n.code,{children:"generate"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["If not, the workflow terminates (",(0,r.jsx)(n.code,{children:"END"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"generate"})," to ",(0,r.jsx)(n.code,{children:"post_safety_check"})]}),":",(0,r.jsx)(n.br,{}),"\n","The workflow generates an SQL query and sends it for validation.\\"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"post_safety_check"})," Conditional Rule"]}),": - If the input passes the post-safety check (",(0,r.jsx)(n.code,{children:'state["error"] == "no"'}),"), the workflow moves to ",(0,r.jsx)(n.code,{children:"sql_check"}),". - If the input fails the post-safety check, the workflow terminates (",(0,r.jsx)(n.code,{children:"END"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"sql_check"})," Conditional Rule"]}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["If the query is valid, the workflow proceeds to ",(0,r.jsx)(n.code,{children:"run_query"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["If the query needs adjustments and the number of iteractions is lower than 3, the workflow loops back to ",(0,r.jsx)(n.code,{children:"generate"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["If validation fails and the number of iteractions is higher than 3, the workflow terminates (",(0,r.jsx)(n.code,{children:"END"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"run_query"})," to ",(0,r.jsx)(n.code,{children:"END"})]}),":",(0,r.jsx)(n.br,{}),"\n","Once the query is executed, the workflow ends."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The schema above provides a representation of the LangGraph Nodes and Edges:"}),"\n",(0,r.jsx)("img",{src:t(5465).A,alt:"Graph Representation of our Agent",width:"50%"}),"\n",(0,r.jsx)(n.h2,{id:"logging-the-model-in-mlflow",children:"Logging the Model in MLflow"}),"\n",(0,r.jsxs)(n.p,{children:["Now that we have built a Multi-Lingual Query Engine using LangGraph, we are ready to log the model using MLflow\u2019s ",(0,r.jsx)(n.a,{href:"https://mlflow.org/docs/latest/model/models-from-code.html",children:"Models from Code"}),". Logging the model into MLflow allows us to treat the Multi-Lingual Query Engine as a traditional ML model, enabling seamless tracking, versioning, and packaging for deployment across diverse serving infrastructures. MLflow\u2019s Models from Code strategy, where we log the code that represents the model, contrasts with MLflow object-based logging, where a model object is created, serialized, and logged as a pickle or JSON object."]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-our-model-from-code-file",children:"Step 1: Create our Model from Code File"}),"\n",(0,r.jsxs)(n.p,{children:["So far, we have defined the ",(0,r.jsx)(n.code,{children:"get_workflow"})," function in a file named ",(0,r.jsx)(n.code,{children:"workflow.py"}),". In this step, we will create a new file, ",(0,r.jsx)(n.code,{children:"sql_model.py"}),", which introduces the ",(0,r.jsx)(n.code,{children:"SQLGenerator"})," class. This script will:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Import the ",(0,r.jsx)(n.code,{children:"get_workflow"})," function from ",(0,r.jsx)(n.code,{children:"workflow.py"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Define the ",(0,r.jsx)(n.code,{children:"SQLGenerator"})," class as a ",(0,r.jsx)(n.code,{children:"PythonModel"}),", including a predict method that utilizes the ",(0,r.jsx)(n.code,{children:"get_workflow"})," function to initialize the LangGraph workflow."]}),"\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.code,{children:"mlflow.models.set_model"})," to designate the ",(0,r.jsx)(n.code,{children:"SQLGenerator PythonModel"})," class as the model of interest for MLflow."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom definitions import REMOTE_SERVER_URI\nfrom workflow import get_workflow\n\nmlflow.set_tracking_uri(REMOTE_SERVER_URI)\n\n\nclass SQLGenerator(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return get_workflow(\n            model_input["conn"], model_input["cursor"], model_input["vector_store"]\n        )\n\n\nmlflow.models.set_model(SQLGenerator())\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-log-with-mlflow-models-from-code-feature",children:"Step 2: Log with MLflow Models from Code feature"}),"\n",(0,r.jsx)(n.p,{children:"With the SQLGenerator custom Python model defined, the next step is to log it using MLflow's Models from Code feature. This involves using the log model standard API specifying the path to the sql_model.py script and using the code_paths parameter to include workflow.py as a dependency. This approach ensures that all necessary code files are available when the model is loaded in a different environment or on another machine."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom definitions import (\n    EXPERIMENT_NAME,\n    MODEL_ALIAS,\n    REGISTERED_MODEL_NAME,\n    REMOTE_SERVER_URI,\n)\nfrom mlflow import MlflowClient\n\nclient = MlflowClient(tracking_uri=REMOTE_SERVER_URI)\n\nmlflow.set_tracking_uri(REMOTE_SERVER_URI)\nmlflow.set_experiment(EXPERIMENT_NAME)\n\nwith mlflow.start_run():\n    logged_model_info = mlflow.pyfunc.log_model(\n        python_model="sql_model.py",\n        artifact_path="sql_generator",\n        registered_model_name=REGISTERED_MODEL_NAME,\n        code_paths=["workflow.py"],\n    )\n\nclient.set_registered_model_alias(\n    REGISTERED_MODEL_NAME, MODEL_ALIAS, logged_model_info.registered_model_version\n)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["In the MLflow UI, the stored model includes both the ",(0,r.jsx)(n.code,{children:"sql_model.py"})," and ",(0,r.jsx)(n.code,{children:"workflow.py"})," scripts as artifacts within the run. This logging from code feature not only records the model's parameters and metrics but also captures the code defining its functionality. This ensures observability, seamless tracking, and straightforward debugging directly through the UI. However, it is crucial to ensure that sensitive elements, such as API keys or credentials, are never hardcoded into these scripts. Since the code is stored as-is, any sensitive information included may lead to token leakage and pose security risks. Instead, sensitive data should be securely managed using environment variables, secret management systems, or other secure methods."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"model_as_code_artifact",src:t(30058).A+"",width:"1915",height:"962"})}),"\n",(0,r.jsxs)(n.h2,{id:"use-the-logged-multi-lingual-query-engine-in-mainpy",children:["Use the Logged Multi-Lingual Query Engine in ",(0,r.jsx)(n.code,{children:"main.py"})]}),"\n",(0,r.jsxs)(n.p,{children:["After logging the model, it can be loaded back from the MLflow Tracking Server using the model URI and the standard ",(0,r.jsx)(n.code,{children:"mlflow.pyfunc.load_model"})," API. When the model is loaded, the ",(0,r.jsx)(n.code,{children:"workflow.py"})," script will be executed along with the ",(0,r.jsx)(n.code,{children:"sql_model.py"})," script, ensuring that the ",(0,r.jsx)(n.code,{children:"get_workflow"})," function is available when the ",(0,r.jsx)(n.code,{children:"predict"})," method is called."]}),"\n",(0,r.jsx)(n.p,{children:"Executing the code below, we demonstrate that our Multi-Lingual Query Engine is able to perform natural language to SQL generation and query execution."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import os\nimport logging\n\nimport mlflow\nfrom database import setup_database\nfrom definitions import (\n    EXPERIMENT_NAME,\n    MODEL_ALIAS,\n    REGISTERED_MODEL_NAME,\n    REMOTE_SERVER_URI,\n)\nfrom dotenv import load_dotenv\nfrom vector_store import setup_vector_store\n\nmlflow.set_tracking_uri(REMOTE_SERVER_URI)\nmlflow.set_experiment(EXPERIMENT_NAME)\nmlflow.langchain.autolog()\n\n# Initialize the logger\n_logger = logging.getLogger(__name__)\n_logger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")\nhandler.setFormatter(formatter)\n_logger.addHandler(handler)\n\n\ndef main():\n    # Load environment variables from .env file\n    load_dotenv()\n\n    # Access secrets using os.getenv\n    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")\n\n    # Setup database and vector store\n    conn = setup_database()\n    cursor = conn.cursor()\n    vector_store = setup_vector_store()\n\n    # Load the model\n    model_uri = f"models:/{REGISTERED_MODEL_NAME}@{MODEL_ALIAS}"\n    model = mlflow.pyfunc.load_model(model_uri)\n    model_input = {"conn": conn, "cursor": cursor, "vector_store": vector_store}\n    app = model.predict(model_input)\n\n    # save image\n    app.get_graph().draw_mermaid_png(\n        output_file_path="sql_agent_with_safety_checks.png"\n    )\n\n    # Example user interaction\n    _logger.info("Welcome to the SQL Assistant!")\n    while True:\n        question = input("\\nEnter your SQL question (or type \'exit\' to quit): ")\n        if question.lower() == "exit":\n            break\n\n        # Initialize the state with all required keys\n        initial_state = {\n            "messages": [("user", question)],\n            "iterations": 0,\n            "error": "",\n            "results": None,\n            "generation": None,\n            "no_records_found": False,\n            "translated_input": "",  # Initialize translated_input\n        }\n\n        solution = app.invoke(initial_state)\n\n        # Check if an error was set during the safety check\n        if solution["error"] == "yes":\n            _logger.info("\\nAssistant Message:\\n")\n            _logger.info(solution["messages"][-1][1])  # Display the assistant\'s message\n            continue  # Skip to the next iteration\n\n        # Extract the generated SQL query from solution["generation"]\n        sql_query = solution["generation"].sql_code\n        _logger.info("\\nGenerated SQL Query:\\n")\n        _logger.info(sql_query)\n\n        # Extract and display the query results\n        if solution.get("no_records_found"):\n            _logger.info("\\nNo records found matching your query.")\n        elif "results" in solution and solution["results"] is not None:\n            _logger.info("\\nQuery Results:\\n")\n            for row in solution["results"]:\n                _logger.info(row)\n        else:\n            _logger.info("\\nNo results returned or query did not execute successfully.")\n\n    _logger.info("Goodbye!")\n\n\nif __name__ == "__main__":\n    main()\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"project-file-structure",children:"Project File Structure"}),"\n",(0,r.jsx)(n.p,{children:"The Project follows a simple file structure:"}),"\n",(0,r.jsx)("img",{src:t(70993).A,alt:"file structure",width:"60%"}),"\n",(0,r.jsx)(n.h2,{id:"mlflow-tracing",children:"MLflow Tracing"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html",children:"MLflow Automated Tracing"})," provides fully automated integrations with various GenAI libraries such as LangChain, OpenAI, LlamaIndex, DSPy, and AutoGen. Since our AI Workflow is built using LangGraph, we can activate automated LangChain tracing by enabling ",(0,r.jsx)(n.code,{children:"mlflow.langchain.autolog()"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"With LangChain autologging, traces are automatically logged to the active MLflow experiment whenever invocation APIs are called on chains. This seamless integration ensures that every interaction is captured for monitoring and analysis."}),"\n",(0,r.jsx)(n.h3,{id:"viewing-traces-in-mlflow",children:"Viewing Traces in MLflow"}),"\n",(0,r.jsx)(n.p,{children:'Traces can be easily accessed by navigating to the MLflow experiment of interest and clicking on the "Tracing" tab. Once inside, selecting a specific trace provides detailed execution information.'}),"\n",(0,r.jsx)(n.p,{children:"Each trace includes:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execution Graphs"}),": Visualizations of the workflow steps."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inputs and Outputs"}),": Detailed logs of data processed at each step."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"By leveraging MLflow Tracing, we gain granular visibility into the whole graph execution. AI workflows graphs can often feel like a black box, making it challenging to debug and understand what is happening at each step. However, with just a single line of code to enable tracing, MLflow provides clear and detailed insights into the workflow, allowing developers to effectively debug, monitor, and optimize every node of the graph ensuring that our Multi-Lingual Query Engine remains transparent, auditable, and scalable."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"mlflow_tracing_gif",src:t(35624).A+"",width:"2048",height:"1067"})}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Throughout this post, we explored the process of building and managing a Multi-Lingual Query Engine using LangGraph and MLflow. By integrating LangGraph\u2019s dynamic AI workflows with MLflow\u2019s robust lifecycle management and tracing features, we\u2019ve created a system that not only delivers accurate and efficient natural language to SQL generation and execution but is also transparent and scalable."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var s=t(96540);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}},30058:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/model_as_code-95d70b56ca83f408bfefac190d0a51cc.png"},35624:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/mlflow_trace-ded83c22f153c03b7179262b98dc7d68.gif"},63867:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/from-natural-language-to-sql","source":"@site/blog/2025-01-23-from-natural-language-to-sql/index.md","title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","description":"MLflow Models from Code and MLflow Tracing applied to AI Workflows","date":"2025-01-23T00:00:00.000Z","tags":[{"inline":true,"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"inline":true,"label":"sql-generator","permalink":"/mlflow-website/blog/tags/sql-generator"},{"inline":true,"label":"models-from-code","permalink":"/mlflow-website/blog/tags/models-from-code"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":39.44,"hasTruncateMarker":true,"authors":[{"name":"Hugo Carvalho","title":"Machine Learning Analyst at adidas","url":"https://www.linkedin.com/in/hugodscarvalho/","imageURL":"/mlflow-website/img/authors/hugo_carvalho.png","key":"hugo-carvalho","page":null},{"name":"Joana Ferreira","title":"Machine Learning Engineer at adidas","url":"https://www.linkedin.com/in/joanaferreira96/","imageURL":"/mlflow-website/img/authors/joana_ferreira.png","key":"joana-ferreira","page":null},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/mlflow-website/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey","page":null}],"frontMatter":{"title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","description":"MLflow Models from Code and MLflow Tracing applied to AI Workflows","tags":["pyfunc","mlflow","sql-generator","models-from-code","tracing"],"slug":"from-natural-language-to-sql","authors":["hugo-carvalho","joana-ferreira","rahul-pandey"],"thumbnail":"/img/blog/from-natural-language-to-sql.png"},"unlisted":false,"prevItem":{"title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","permalink":"/mlflow-website/blog/custom-tracing"},"nextItem":{"title":"MLflow Tracing in Jupyter Notebooks","permalink":"/mlflow-website/blog/mlflow-tracing-in-jupyter"}}')},70993:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/file_structure-93a5eec4529aff31b94a1e91072ff864.png"}}]);