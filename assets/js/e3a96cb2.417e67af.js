"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([["6665"],{20476(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var r=t(7862),i=t(74848),a=t(28453);let o={title:"AI Observability for Every TypeScript LLM Stack",description:"New integrations for Vercel AI SDK, LangChain.js, Mastra, Anthropic, and Gemini make it extremely easy to add observability into your TypeScript LLM stacks with MLflow.",slug:"typescript-enhancement",authors:["mlflow-maintainers"],tags:["genai","tracing","typescript","langchain"],thumbnail:"/img/blog/mlflow-typescript-enhancement.png",image:"/img/blog/mlflow-typescript-enhancement.png"},s,l={authorsImageUrls:[void 0]},c=[{value:"Set up MLflow",id:"set-up-mlflow",level:2},{value:"Vercel AI SDK / Next.js",id:"vercel-ai-sdk--nextjs",level:2},{value:"Mastra",id:"mastra",level:2},{value:"LangChain.js / LangGraph.js",id:"langchainjs--langgraphjs",level:2},{value:"OpenAI SDK",id:"openai-sdk",level:2},{value:"Anthropic SDK",id:"anthropic-sdk",level:2},{value:"Gemini SDK",id:"gemini-sdk",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){let n={a:"a",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("div",{style:{display:"flex",justifyContent:"center"},children:(0,i.jsx)("img",{src:t(62435).A,style:{width:"100%",borderRadius:"15px",marginBottom:"40px"},alt:"Vercel AI SDK tracing demo in MLflow"})}),"\n",(0,i.jsxs)(n.p,{children:["TypeScript and JavaScript continue to dominate full-stack AI development, and MLflow 3.6 doubles down on that reality. In addition to the OpenAI SDK hooks we launched earlier this year, we now ship automatic tracing integrations for ",(0,i.jsx)(n.strong,{children:"Vercel AI SDK"}),", ",(0,i.jsx)(n.strong,{children:"LangChain.js"}),", ",(0,i.jsx)(n.strong,{children:"LangGraph.js"}),", ",(0,i.jsx)(n.strong,{children:"Mastra"}),", ",(0,i.jsx)(n.strong,{children:"Anthropic"}),", and ",(0,i.jsx)(n.strong,{children:"Gemini"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"In practical terms, any traces emitted from modern JS LLM stacks now land in the same MLflow experiment UI as your Python services, complete with prompt/response payloads, token usage, tool results, and error metadata."}),"\n",(0,i.jsx)("div",{style:{display:"flex",justifyContent:"center"},children:(0,i.jsx)("img",{src:"https://github.com/mlflow/mlflow/blob/b39e1a8b6f12704638504747a164649fcff9aacd/docs/static/images/llms/tracing/vercel-ai-tracing.gif?raw=True",style:{width:"100%",borderRadius:"15px",marginBottom:"20px",marginTop:"20px"},alt:"Vercel AI SDK tracing demo in MLflow"})}),"\n",(0,i.jsx)(n.h2,{id:"set-up-mlflow",children:"Set up MLflow"}),"\n",(0,i.jsx)(n.p,{children:"Getting MLflow running beside a JS stack no longer requires a Python environment:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/self-hosting/#docker-compose",children:"Docker Compose bundle"})," \u2013 For the easiest local or small-team deployment, MLflow's official Docker Compose setup is highly recommended. To get started, simply clone the ",(0,i.jsx)(n.a,{href:"https://github.com/mlflow/mlflow",children:"GitHub repository"}),", then navigate into the ",(0,i.jsx)(n.code,{children:"docker-compose"})," directory provided in the repository. Run ",(0,i.jsx)(n.code,{children:"docker compose up -d"})," to launch the entire MLflow Tracking stack: this automatically starts the MLflow Tracking server along with a pre-configured SQL backend (such as PostgreSQL), both pre-wired for seamless ingestion of OpenTelemetry Protocol (OTLP) traces."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/self-hosting/#cloud-services",children:"Managed solutions"})," \u2013 MLflow is available on many cloud providers, such as Databricks, AWS SageMaker Experiments, Azure ML, Nebius, etc. You can point ",(0,i.jsx)(n.code,{children:"MLFLOW_TRACKING_URI"})," at Databricks\u2019 free edition or another managed MLflow service, and start emitting traces without hosting anything yourself."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"vercel-ai-sdk--nextjs",children:"Vercel AI SDK / Next.js"}),"\n",(0,i.jsxs)(n.p,{children:["Vercel AI SDK tracing is built on top of the OpenTelemetry and designed for minimal instrumentation effort. Since MLflow tracking server is ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/opentelemetry",children:"OpenTelemetry-compatible"}),", you can directly send Vercel AI SDK traces to MLflow tracking server with minimal configuration."]}),"\n",(0,i.jsx)(n.p,{children:"The following code snippet shows how to set up tracing for LLM applications using Vercel AI SDK and Next.js."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",metastring:'title=".env.local"',children:"OTEL_EXPORTER_OTLP_ENDPOINT=<your-mlflow-tracking-server-endpoint>\nOTEL_EXPORTER_OTLP_TRACES_HEADERS=x-mlflow-experiment-id=<your-experiment-id>\nOTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",metastring:'title="instrumentation.ts"',children:'import { registerOTel } from "@vercel/otel";\n\nexport async function register() {\n  registerOTel({ serviceName: "next-app" });\n}\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",metastring:'title="route.ts"',children:'import { openai } from "@ai-sdk/openai";\nimport { generateText } from "ai";\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const { text } = await generateText({\n    model: openai("gpt-4o-mini"),\n    maxOutputTokens: 100,\n    prompt,\n    experimental_telemetry: { isEnabled: true },\n  });\n\n  return new Response(JSON.stringify({ text }), {\n    headers: { "Content-Type": "application/json" },\n  });\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["See ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/vercelai",children:"Vercel AI SDK Tracing"})," for more details."]}),"\n",(0,i.jsx)(n.h2,{id:"mastra",children:"Mastra"}),"\n",(0,i.jsxs)(n.p,{children:["Mastra includes a built-in OpenTelemetry exporter, making it easy to send tracing data directly to the MLflow tracking server. To enable tracing, simply add an ",(0,i.jsx)(n.code,{children:"observability"})," configuration block to your Mastra setup. By providing an OpenTelemetry exporter instance that points to your MLflow server, all your Mastra workflows\u2014including the full hierarchy of chains and tools\u2014are automatically traced and visualized in MLflow. This setup preserves the structure and relationships between your workflow components, so you can see detailed traces of end-to-end workflow execution right in the MLflow UI."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import { OtelExporter } from "@mastra/otel-exporter";\n\nexport const mastra = new Mastra({\n  workflows: { weatherWorkflow },\n  ...\n\n  // Add the following observability configuration to enable OpenTelemetry tracing.\n  observability: {\n    configs: {\n      otel: {\n        serviceName: "mastra-app",\n        exporters: [new OtelExporter({\n          provider: {\n            custom: {\n              // Specify tracking server URI with the `/v1/traces` path.\n              endpoint: "http://localhost:5000/v1/traces",\n              // Set the MLflow experiment ID in the header.\n              headers: { "x-mlflow-experiment-id": "<your-experiment-id>"},\n              // MLflow supports HTTP/Protobuf protocol.\n              protocol: "http/protobuf"\n            }\n          }\n        })]\n      }\n    }\n  },\n});\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Launch ",(0,i.jsx)(n.code,{children:"npm run dev"}),", talk to the playground, and refresh the MLflow UI to see the spans."]}),"\n",(0,i.jsxs)(n.p,{children:["See ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/mastra",children:"Mastra Tracing"})," for more details."]}),"\n",(0,i.jsx)(n.h2,{id:"langchainjs--langgraphjs",children:"LangChain.js / LangGraph.js"}),"\n",(0,i.jsxs)(n.p,{children:["LangChain.js instrumentation rides on OpenTelemetry. Configure a tracer once and MLflow ingests every span\u2014chains, agents, retrievers, tool calls, and async flows\u2014through the OTLP ",(0,i.jsx)(n.code,{children:"/v1/traces"})," endpoint."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import {\n  NodeTracerProvider,\n  SimpleSpanProcessor,\n} from "@opentelemetry/sdk-trace-node";\nimport { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";\nimport { LangChainInstrumentation } from "@arizeai/openinference-instrumentation-langchain";\nimport * as CallbackManagerModule from "@langchain/core/callbacks/manager";\n\nconst exporter = new OTLPTraceExporter({\n  url: "http://localhost:5000/v1/traces",\n  headers: { "x-mlflow-experiment-id": "123" },\n});\n\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(exporter));\nprovider.register();\n\nnew LangChainInstrumentation().manuallyInstrument(CallbackManagerModule);\n'})}),"\n",(0,i.jsxs)(n.p,{children:["See ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langchain",children:"LangChain Tracing"})," or ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langgraph",children:"LangGraph Tracing"})," for more details."]}),"\n",(0,i.jsx)(n.h2,{id:"openai-sdk",children:"OpenAI SDK"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.npmjs.com/package/mlflow-openai",children:"mlflow-openai"})," wraps the official OpenAI SDK so you can trace every call to OpenAI and capture messages, tool calls, token usage, and errors."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import { OpenAI } from "openai";\nimport { tracedOpenAI } from "mlflow-openai";\nimport * as mlflow from "mlflow-tracing";\n\nmlflow.init({ trackingUri: "http://localhost:5000", experimentId: "openai" });\n\nconst client = tracedOpenAI(new OpenAI({ apiKey: process.env.OPENAI_API_KEY }));\n\nconst response = await client.chat.completions.create({\n  model: "gpt-4o-mini",\n  messages: [{ role: "user", content: "What\'s the weather like?" }],\n});\n'})}),"\n",(0,i.jsxs)(n.p,{children:["See ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/openai",children:"OpenAI Tracing"})," for more details."]}),"\n",(0,i.jsx)(n.h2,{id:"anthropic-sdk",children:"Anthropic SDK"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.npmjs.com/package/mlflow-anthropic",children:"mlflow-anthropic"})," wraps the official SDK so you can trace Claude calls (sync or async) in a single line."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import Anthropic from "@anthropic-ai/sdk";\nimport { tracedAnthropic } from "mlflow-anthropic";\nimport * as mlflow from "mlflow-tracing";\n\nmlflow.init({\n  trackingUri: "http://localhost:5000",\n  experimentId: "anthropic",\n});\n\nconst client = tracedAnthropic(new Anthropic());\nconst message = await client.messages.create({\n  model: "claude-sonnet-4-5-20250929",\n  max_tokens: 1024,\n  messages: [\n    { role: "user", content: "Summarize the new TypeScript release." },\n  ],\n});\n'})}),"\n",(0,i.jsxs)(n.p,{children:["See ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/anthropic",children:"Anthropic Tracing"})," for more details."]}),"\n",(0,i.jsx)(n.h2,{id:"gemini-sdk",children:"Gemini SDK"}),"\n",(0,i.jsxs)(n.p,{children:["Google's GenAI SDK is supported via ",(0,i.jsx)(n.a,{href:"https://www.npmjs.com/package/mlflow-gemini",children:"mlflow-gemini"}),", covering ",(0,i.jsx)(n.code,{children:"models.generateContent()"})," plus function-calling metadata."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import { GoogleGenAI } from "@google/genai";\nimport { tracedGemini } from "mlflow-gemini";\n\nconst client = tracedGemini(\n  new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY }),\n);\nconst response = await client.models.generateContent({\n  model: "gemini-2.5-flash",\n  contents: "List three GenAI observability KPIs.",\n});\n'})}),"\n",(0,i.jsxs)(n.p,{children:["See ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/listing/gemini",children:"Gemini Tracing"})," for more details."]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/integrations/",children:"Tracing integrations catalog"})," - Find the tracing integration for your favorite framework."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/tracing/quickstart/typescript-openai/",children:"TypeScript SDK quickstart"})," - Get started with the MLflow TypeScript SDK."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/",children:"Evaluation and Monitoring"})," - Evaluate and monitor your AI applications with MLflow."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Tell us which JavaScript framework you want next by opening an ",(0,i.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/issues",children:"issue"}),"!"]})]})}function d(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},62435(e,n,t){t.d(n,{A:()=>r});let r=t.p+"assets/images/mlflow-typescript-enhancement-5659f495e77ab32181cb86ed11aae764.png"},28453(e,n,t){t.d(n,{R:()=>o,x:()=>s});var r=t(96540);let i={},a=r.createContext(i);function o(e){let n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}},7862(e){e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/typescript-enhancement","source":"@site/blog/2025-12-24-typescript-enhancement/index.mdx","title":"AI Observability for Every TypeScript LLM Stack","description":"New integrations for Vercel AI SDK, LangChain.js, Mastra, Anthropic, and Gemini make it extremely easy to add observability into your TypeScript LLM stacks with MLflow.","date":"2025-12-24T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"},{"inline":true,"label":"typescript","permalink":"/mlflow-website/blog/tags/typescript"},{"inline":true,"label":"langchain","permalink":"/mlflow-website/blog/tags/langchain"}],"readingTime":5.6,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"AI Observability for Every TypeScript LLM Stack","description":"New integrations for Vercel AI SDK, LangChain.js, Mastra, Anthropic, and Gemini make it extremely easy to add observability into your TypeScript LLM stacks with MLflow.","slug":"typescript-enhancement","authors":["mlflow-maintainers"],"tags":["genai","tracing","typescript","langchain"],"thumbnail":"/img/blog/mlflow-typescript-enhancement.png","image":"/img/blog/mlflow-typescript-enhancement.png"},"unlisted":false,"prevItem":{"title":"Introducing MLflow Agents Dashboard","permalink":"/mlflow-website/blog/mlflow-agent-dashboard"},"nextItem":{"title":"Full OpenTelemetry Support in MLflow Tracing","permalink":"/mlflow-website/blog/opentelemetry-tracing-support"}}')}}]);