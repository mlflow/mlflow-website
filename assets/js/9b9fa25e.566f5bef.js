"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1539],{6378:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var i=t(5893),o=t(1151);const s={title:"PyFunc in Practice",description:"Creative Applications of MLflow Pyfunc in Machine Learning Projects",tags:["pyfunc","mlflow","ensemble-models"],slug:"pyfunc-in-practice",authors:["hugo-carvalho","joana-ferreira","rahul-pandey","filipe-miranda"],thumbnail:"img/blog/pyfunc-in-practice.png"},a=void 0,r={permalink:"/mlflow-website/blog/pyfunc-in-practice",source:"@site/blog/2024-07-26-pyfunc-in-practice/index.md",title:"PyFunc in Practice",description:"Creative Applications of MLflow Pyfunc in Machine Learning Projects",date:"2024-07-26T00:00:00.000Z",formattedDate:"July 26, 2024",tags:[{label:"pyfunc",permalink:"/mlflow-website/blog/tags/pyfunc"},{label:"mlflow",permalink:"/mlflow-website/blog/tags/mlflow"},{label:"ensemble-models",permalink:"/mlflow-website/blog/tags/ensemble-models"}],readingTime:22.055,hasTruncateMarker:!0,authors:[{name:"Hugo Carvalho",title:"Machine Learning Analyst at adidas",url:"https://www.linkedin.com/in/hugodscarvalho/",imageURL:"/img/authors/hugo_carvalho.png",key:"hugo-carvalho"},{name:"Joana Ferreira",title:"Machine Learning Engineer at adidas",url:"https://www.linkedin.com/in/joanaferreira96/",imageURL:"/img/authors/joana_ferreira.png",key:"joana-ferreira"},{name:"Rahul Pandey",title:"Sr. Solutions Architect at adidas",url:"https://www.linkedin.com/in/rahulpandey1901/",imageURL:"/img/ambassadors/Rahul_Pandey.png",key:"rahul-pandey"},{name:"Filipe Miranda",title:"Sr. Data Engineer at adidas",url:"https://www.linkedin.com/in/filipe-miranda-b576b186/",imageURL:"/img/authors/filipe_miranda.png",key:"filipe-miranda"}],frontMatter:{title:"PyFunc in Practice",description:"Creative Applications of MLflow Pyfunc in Machine Learning Projects",tags:["pyfunc","mlflow","ensemble-models"],slug:"pyfunc-in-practice",authors:["hugo-carvalho","joana-ferreira","rahul-pandey","filipe-miranda"],thumbnail:"img/blog/pyfunc-in-practice.png"},unlisted:!1,prevItem:{title:"LangGraph with Custom PyFunc",permalink:"/mlflow-website/blog/mlflow"},nextItem:{title:"Introducing MLflow Tracing",permalink:"/mlflow-website/blog/mlflow-tracing"}},l={authorsImageUrls:[void 0,void 0,void 0,void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Components of the Project",id:"components-of-the-project",level:3},{value:"Creating the Ensemble Model",id:"creating-the-ensemble-model",level:2},{value:"Initializing the EnsembleModel",id:"initializing-the-ensemblemodel",level:3},{value:"Adding Strategies and Saving to the Database",id:"adding-strategies-and-saving-to-the-database",level:3},{value:"Feature Engineering",id:"feature-engineering",level:3},{value:"Initializing Models",id:"initializing-models",level:3},{value:"Defining a Custom <code>fit</code> Method to Train and Save Multi-Models",id:"defining-a-custom-fit-method-to-train-and-save-multi-models",level:3},{value:"Defining a Custom <code>predict</code> Method to Aggregate Multi-model Predictions",id:"defining-a-custom-predict-method-to-aggregate-multi-model-predictions",level:3},{value:"Defining a <code>load context</code> custom method to initialize the Ensemble Model",id:"defining-a-load-context-custom-method-to-initialize-the-ensemble-model",level:3},{value:"Bringing It All Together",id:"bringing-it-all-together",level:3},{value:"MLflow Tracking",id:"mlflow-tracking",level:2},{value:"Using the <code>fit</code> Method to Train Sub-Models",id:"using-the-fit-method-to-train-sub-models",level:3},{value:"Registering the Model with MLflow",id:"registering-the-model-with-mlflow",level:3},{value:"Using the <code>predict</code> Method to Perform Inference",id:"using-the-predict-method-to-perform-inference",level:3},{value:"Evaluating Model Performance with Different Strategies",id:"evaluating-model-performance-with-different-strategies",level:3},{value:"Summary",id:"summary",level:2},{value:"Additional resources",id:"additional-resources",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["If you're looking to fully leverage the capabilities of ",(0,i.jsx)(n.code,{children:"mlflow.pyfunc"})," and understand how it can be utilized in a Machine Learning project, this blog post will guide you through the process. MLflow PyFunc offers creative freedom and flexibility, allowing the development of complex systems encapsulated as models in MLflow that follow the same lifecycle as traditional ones. This blog will showcase how to create multi-model setups, seamlessly connect to databases, and implement your own custom fit method in your MLflow PyFunc model."]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["This blog post demonstrates the capabilities of ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html",children:"MLflow PyFunc"})," and how it can be utilized to build a multi-model setup encapsulated as a PyFunc flavor model in MLflow. This approach allows ensemble models to follow the same lifecycle as traditional ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/models.html#built-in-model-flavors",children:"Built-In Model Flavors"})," in MLflow."]}),"\n",(0,i.jsx)(n.p,{children:"But first, let's use an analogy to get you familiarized with the concept of ensemble models and why you should consider this solution in your next Machine Learning project."}),"\n",(0,i.jsx)(n.p,{children:"Imagine you are in the market to buy a house. Would you make a decision based solely on the first house you visit and the advice of a single real estate agent? Of course not! The process of buying a house involves considering multiple factors and gathering information from various sources to make an informed decision."}),"\n",(0,i.jsx)(n.p,{children:"The house buying process explained:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Identify Your Needs"}),": Determine whether you want a new or used house, the type of house, the model, and the year of construction."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Research"}),": Look for a list of available houses, check for discounts and offers, read customer reviews, and seek opinions from friends and family."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Evaluate"}),": Consider the performance, location, neighborhood amenities, and price range."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compare"}),": Compare multiple houses to find the best fit for your needs and budget."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In short, you wouldn\u2019t directly reach a conclusion but would instead make a decision considering all the aforementioned factors before deciding on the best choice."}),"\n",(0,i.jsx)(n.p,{children:"Ensemble models in Machine Learning operate on a similar idea. Ensemble learning helps improve Machine Learning results by combining several models to improve predictive performance compared to a single model. The performance increase can be due to several factors such as the reduction in variance by averaging multiple models or reducing bias by focusing on errors of previous models. There are several types of ensemble learning techniques exists such as:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Averaging"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Weighted Averaging"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Bagging"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Boosting"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Stacking"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"However, developing such systems requires careful management of the lifecycle of ensemble models, as integrating diverse models can be highly complex. This is where MLflow PyFunc becomes invaluable. It offers the flexibility to build complex systems, treating the entire ensemble as a model that adheres to the same lifecycle processes as traditional models. Essentially, MLflow PyFunc allows the creation of custom methods tailored to ensemble models, serving as an alternative to the built-in MLflow flavors available for popular frameworks such as scikit-learn, PyTorch, and LangChain."}),"\n",(0,i.jsxs)(n.p,{children:["This blog utilizes the house price dataset from ",(0,i.jsx)(n.a,{href:"https://www.kaggle.com/",children:"Kaggle"})," to demonstrate the development and management of ensemble models through MLflow."]}),"\n",(0,i.jsx)(n.p,{children:"We will leverage various tools and technologies to highlight the capabilities of MLflow PyFunc models. Before delving into the ensemble model itself, we will explore how these components integrate to create a robust and efficient Machine Learning pipeline."}),"\n",(0,i.jsx)(n.h3,{id:"components-of-the-project",children:"Components of the Project"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"DuckDB"}),(0,i.jsx)(n.br,{}),"\n","DuckDB is a high-performance analytical database system designed to be fast, reliable, portable, and easy to use. In this project, it showcases the integration of a database connection within the model context, facilitating efficient data handling directly within the model. ",(0,i.jsx)(n.a,{href:"https://duckdb.org/",children:"Learn more about DuckDB"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"scikit-learn (sklearn)"}),(0,i.jsx)(n.br,{}),"\n","scikit-learn is a Machine Learning library for Python that provides efficient tools for data analysis and modelling. In this project, it is used to develop and evaluate various Machine Learning models that are integrated into our ensemble model. ",(0,i.jsx)(n.a,{href:"https://scikit-learn.org/",children:"Learn more about scikit-learn"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"MLflow"}),(0,i.jsx)(n.br,{}),"\n","MLflow is an open-source platform for managing the end-to-end Machine Learning lifecycle, including experimentation, reproducibility, and deployment. In this project, it tracks experiments, manages model versions, and facilitates the deployment of MLflow PyFunc models in a similar manner to how we are familiar with individual flavors. ",(0,i.jsx)(n.a,{href:"https://mlflow.org/",children:"Learn more about MLflow"}),"."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," To reproduce this project, please refer to the official MLflow documentation for more details on setting up a simple local ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/tracking/server.html",children:"MLflow Tracking Server"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"creating-the-ensemble-model",children:"Creating the Ensemble Model"}),"\n",(0,i.jsx)(n.p,{children:"Creating a MLflow PyFunc ensemble model requires additional steps compared to using the built-in flavors for logging and working with popular Machine Learning frameworks."}),"\n",(0,i.jsxs)(n.p,{children:["To implement an ensemble model, you need to define an ",(0,i.jsx)(n.code,{children:"mlflow.pyfunc"})," model, which involves creating a Python class that inherits from the ",(0,i.jsx)(n.code,{children:"PythonModel"})," class and implementing its constructor and class methods. While the basic creation of a PyFunc model only requires implementing the ",(0,i.jsx)(n.code,{children:"predict"})," method, an ensemble model requires additional methods to manage the models and obtain multi-model predictions. After instantiating the ensemble model, you must use the custom ",(0,i.jsx)(n.code,{children:"fit"})," method to train the ensemble model's sub-models. Similar to an out-of-the-box MLflow model, you need to log the model along with its artifacts during the training run and then register the model in the MLflow Model Registry. A model alias ",(0,i.jsx)(n.code,{children:"production"})," will also be added to the model to streamline both model updates and inference. Model aliases allow you to assign a mutable, named reference to a specific version of a registered model. By assigning the alias to a particular model version, it can be easily referenced via a model URI or the model registry API. This setup allows for seamless updates to the model version used for inference without changing the serving workload code. For more details, refer to ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags",children:"Deploy and Organize Models with Aliases and Tags"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"The following sections, as depicted in the diagram, detail the implementation of each method for the ensemble model, providing a comprehensive understanding of defining, managing, and utilizing an ensemble model with MLflow PyFunc."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Ensemble Model Architecture",src:t(5707).Z+"",width:"1545",height:"1200"})}),"\n",(0,i.jsxs)(n.p,{children:["Before delving into the detailed implementation of each method, let's first review the skeleton of our ",(0,i.jsx)(n.code,{children:"EnsembleModel"})," class. This skeleton serves as a blueprint for understanding the structure of the ensemble model. The subsequent sections will provide an overview and code for both the default methods provided by MLflow PyFunc and the custom methods implemented for the ensemble model."]}),"\n",(0,i.jsxs)(n.p,{children:["Here is the skeleton of the ",(0,i.jsx)(n.code,{children:"EnsembleModel"})," class:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nclass EnsembleModel(mlflow.pyfunc.PythonModel):\n    """Ensemble model class leveraging Pyfunc for multi-model integration in MLflow."""\n\n    def __init__(self):\n        """Initialize the EnsembleModel instance."""\n        ...\n\n    def add_strategy_and_save_to_db(self):\n        """Add strategies to the DuckDB database."""\n        ...\n\n    def feature_engineering(self):\n        """Perform feature engineering on input data."""\n        ...\n\n    def initialize_models(self):\n        """Initialize models and their hyperparameter grids."""\n        ...\n\n    def fit(self):\n        """Train the ensemble of models."""\n        ...\n\n    def predict(self):\n        """Predict using the ensemble of models."""\n        ...\n\n    def load_context(self):\n        """Load the preprocessor and models from the MLflow context."""\n        ...\n'})}),"\n",(0,i.jsx)(n.h3,{id:"initializing-the-ensemblemodel",children:"Initializing the EnsembleModel"}),"\n",(0,i.jsxs)(n.p,{children:["The constructor method in the ensemble model is crucial for setting up its essential elements. It establishes key attributes such as the preprocessor, a dictionary to store trained models, the path to a DuckDB database, and a pandas DataFrame for managing different ensemble strategies. Additionally, it takes advantage of the ",(0,i.jsx)(n.code,{children:"initialize_models"})," method to define the sub-models integrated into the ensemble."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pandas as pd\n\ndef __init__(self):\n    """\n    Initializes the EnsembleModel instance.\n\n    Sets up an empty preprocessing pipeline, a dictionary for fitted models,\n    and a DataFrame to store strategies. Also calls the method to initialize sub-models.\n    """\n    self.preprocessor = None\n    self.fitted_models = {}\n    self.db_path = None\n    self.strategies = pd.DataFrame(columns=["strategy", "model_list", "weights"])\n    self.initialize_models()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"adding-strategies-and-saving-to-the-database",children:"Adding Strategies and Saving to the Database"}),"\n",(0,i.jsxs)(n.p,{children:["The custom-defined ",(0,i.jsx)(n.code,{children:"add_strategy_and_save_to_db"})," method enables the addition of new ensemble strategies to the model and their storage in a DuckDB database. This method accepts a pandas DataFrame containing the strategies and the database path as inputs. It appends the new strategies to the existing ones and saves them in the database specified during the initialization of the ensemble model. This method facilitates the management of various ensemble strategies and ensures their persistent storage for future use."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import duckdb\nimport pandas as pd\n\ndef add_strategy_and_save_to_db(self, strategy_df: pd.DataFrame, db_path: str) -> None:\n    """Add strategies from a DataFrame and save them to the DuckDB database.\n\n    Args:\n        strategy_df (pd.DataFrame): DataFrame containing strategies.\n        db_path (str): Path to the DuckDB database.\n    """\n    # Update the instance-level database path for the current object\n    self.db_path = db_path\n\n    # Attempt to concatenate new strategies with the existing DataFrame\n    try:\n        self.strategies = pd.concat([self.strategies, strategy_df], ignore_index=True)\n    except Exception as e:\n        # Print an error message if any exceptions occur during concatenation\n        print(f"Error concatenating DataFrames: {e}")\n        return  # Exit early to prevent further errors\n\n    # Use context manager for the database connection\n    try:\n        with duckdb.connect(self.db_path) as con:\n            # Register the strategies DataFrame as a temporary table in DuckDB\n            con.register("strategy_df", self.strategies)\n\n            # Drop any existing strategies table and create a new one with updated strategies\n            con.execute("DROP TABLE IF EXISTS strategies")\n            con.execute("CREATE TABLE strategies AS SELECT * FROM strategy_df")\n    except Exception as e:\n        # Print an error message if any exceptions occur during database operations\n        print(f"Error executing database operations: {e}")\n'})}),"\n",(0,i.jsx)(n.p,{children:"The following example demonstrates how to use this method to add strategies to the database."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pandas as pd\n\n# Initialize ensemble model\nensemble_model = EnsembleModel()\n\n# Define strategies for the ensemble model\nstrategy_data = {\n    "strategy": ["average_1"],\n    "model_list": ["random_forest,xgboost,decision_tree,gradient_boosting,adaboost"],\n    "weights": ["1"],\n}\n\n# Create a DataFrame to hold the strategy information\nstrategies_df = pd.DataFrame(strategy_data)\n\n# Add strategies to the database\nensemble_model.add_strategy_and_save_to_db(strategies_df, "models/strategies.db")\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The DataFrame ",(0,i.jsx)(n.code,{children:"strategy_data"})," includes:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"strategy"}),": The name of the strategy for model predictions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"model_list"}),": A comma-separated list of model names included in the strategy."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"weights"}),": A comma-separated list of weights assigned to each model in the ",(0,i.jsx)(n.code,{children:"model_list"}),". If not provided, implies equal weights or default values."]}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"strategy"}),(0,i.jsx)(n.th,{children:"model_list"}),(0,i.jsx)(n.th,{children:"weights"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"average_1"}),(0,i.jsx)(n.td,{children:"random_forest,xgboost,decision_tree,gradient_boosting,adaboost"}),(0,i.jsx)(n.td,{children:"1"})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"feature-engineering",children:"Feature Engineering"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"feature_engineering"})," method preprocesses input data by handling missing values, scaling numerical features, and encoding categorical features. It applies different transformations to both numerical and categorical features, and returns the processed features as a NumPy array. This method is crucial for preparing data in a suitable format for model training, ensuring consistency and enhancing model performance."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\ndef feature_engineering(self, X: pd.DataFrame) -> np.ndarray:\n    """\n    Applies feature engineering to the input data X, including imputation, scaling, and encoding.\n\n    Args:\n        X (pd.DataFrame): Input features with potential categorical and numerical columns.\n\n    Returns:\n        np.ndarray: Processed feature array after transformations.\n    """\n    # Convert columns with \'object\' dtype to \'category\' dtype for proper handling of categorical features\n    X = X.apply(\n        lambda col: col.astype("category") if col.dtypes == "object" else col\n    )\n\n    # Identify categorical and numerical features from the DataFrame\n    categorical_features = X.select_dtypes(include=["category"]).columns\n    numerical_features = X.select_dtypes(include=["number"]).columns\n\n    # Define the pipeline for numerical features: imputation followed by scaling\n    numeric_transformer = Pipeline(\n        steps=[\n            (\n                "imputer",\n                SimpleImputer(strategy="median"),\n            ),  # Replace missing values with the median\n            (\n                "scaler",\n                StandardScaler(),\n            ),  # Standardize features by removing the mean and scaling to unit variance\n        ]\n    )\n\n    # Define the pipeline for categorical features: imputation followed by one-hot encoding\n    categorical_transformer = Pipeline(\n        steps=[\n            (\n                "imputer",\n                SimpleImputer(strategy="most_frequent"),\n            ),  # Replace missing values with the most frequent value\n            (\n                "onehot",\n                OneHotEncoder(handle_unknown="ignore"),\n            ),  # Encode categorical features as a one-hot numeric array\n        ]\n    )\n\n    # Create a ColumnTransformer to apply the appropriate pipelines to the respective feature types\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\n                "num",\n                numeric_transformer,\n                numerical_features,\n            ),  # Apply the numeric pipeline to numerical features\n            (\n                "cat",\n                categorical_transformer,\n                categorical_features,\n            ),  # Apply the categorical pipeline to categorical features\n        ]\n    )\n\n    # Fit and transform the input data using the preprocessor\n    X_processed = preprocessor.fit_transform(X)\n\n    # Store the preprocessor for future use in the predict method\n    self.preprocessor = preprocessor\n    return X_processed\n'})}),"\n",(0,i.jsx)(n.h3,{id:"initializing-models",children:"Initializing Models"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"initialize_models"})," method sets up a dictionary of various Machine Learning models along with their hyperparameter grids. This includes models such as ",(0,i.jsx)(n.code,{children:"RandomForest"}),", ",(0,i.jsx)(n.code,{children:"XGBoost"}),", ",(0,i.jsx)(n.code,{children:"DecisionTree"}),", ",(0,i.jsx)(n.code,{children:"GradientBoosting"}),", and ",(0,i.jsx)(n.code,{children:"AdaBoost"}),". This step is crucial for preparing the ensemble\u2019s sub-models and specifying the hyperparameters to adjust during training, ensuring that each model is configured correctly and ready for training."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from sklearn.ensemble import (\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n    RandomForestRegressor,\n)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\ndef initialize_models(self) -> None:\n    """\n    Initializes a dictionary of models along with their hyperparameter grids for grid search.\n    """\n    # Define various regression models with their respective hyperparameter grids for tuning\n    self.models = {\n        "random_forest": (\n            RandomForestRegressor(random_state=42),\n            {"n_estimators": [50, 100, 200], "max_depth": [None, 10, 20]},\n        ),\n        "xgboost": (\n            XGBRegressor(random_state=42),\n            {"n_estimators": [50, 100, 200], "max_depth": [3, 6, 10]},\n        ),\n        "decision_tree": (\n            DecisionTreeRegressor(random_state=42),\n            {"max_depth": [None, 10, 20]},\n        ),\n        "gradient_boosting": (\n            GradientBoostingRegressor(random_state=42),\n            {"n_estimators": [50, 100, 200], "max_depth": [3, 5, 7]},\n        ),\n        "adaboost": (\n            AdaBoostRegressor(random_state=42),\n            {"n_estimators": [50, 100, 200], "learning_rate": [0.01, 0.1, 1.0]},\n        ),\n    }\n'})}),"\n",(0,i.jsxs)(n.h3,{id:"defining-a-custom-fit-method-to-train-and-save-multi-models",children:["Defining a Custom ",(0,i.jsx)(n.code,{children:"fit"})," Method to Train and Save Multi-Models"]}),"\n",(0,i.jsxs)(n.p,{children:["As already highlighted in the previous method, a key feature of MLflow PyFunc models is the ability to define custom methods, providing significant flexibility and customization for various tasks. In the multi-model PyFunc setup, the ",(0,i.jsx)(n.code,{children:"fit"})," method is essential for customizing and optimizing multiple sub-models. It manages the training and fine-tuning of algorithms such as ",(0,i.jsx)(n.code,{children:"RandomForestRegressor"}),", ",(0,i.jsx)(n.code,{children:"XGBRegressor"}),", ",(0,i.jsx)(n.code,{children:"DecisionTreeRegressor"}),", ",(0,i.jsx)(n.code,{children:"GradientBoostingRegressor"}),", and ",(0,i.jsx)(n.code,{children:"AdaBoostRegressor"}),". For demonstration purposes, Grid Search is used, which, while straightforward, can be computationally intensive and time-consuming, especially for ensemble models. To enhance efficiency, advanced optimization methods such as Bayesian optimization are recommended. Tools like ",(0,i.jsx)(n.a,{href:"https://optuna.org/",children:"Optuna"})," and ",(0,i.jsx)(n.a,{href:"https://hyperopt.github.io/hyperopt/",children:"Hyperopt"})," leverage probabilistic models to intelligently navigate the search space, significantly reducing the number of evaluations needed to identify optimal configurations."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\n\nimport joblib\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\ndef fit(\n        self, X_train_processed: pd.DataFrame, y_train: pd.Series, save_path: str\n    ) -> None:\n    """\n    Trains the ensemble of models using the provided preprocessed training data.\n\n    Args:\n        X_train_processed (pd.DataFrame): Preprocessed feature matrix for training.\n        y_train (pd.Series): Target variable for training.\n        save_path (str): Directory path where trained models will be saved.\n    """\n    # Create the directory for saving models if it does not exist\n    os.makedirs(save_path, exist_ok=True)\n\n    # Iterate over each model and its parameter grid\n    for model_name, (model, param_grid) in self.models.items():\n        # Perform GridSearchCV to find the best hyperparameters for the current model\n        grid_search = GridSearchCV(\n            model, param_grid, cv=5, n_jobs=-1, scoring="neg_mean_squared_error"\n        )\n        grid_search.fit(\n            X_train_processed, y_train\n        )  # Fit the model with the training data\n\n        # Save the best estimator from GridSearchCV\n        best_model = grid_search.best_estimator_\n        self.fitted_models[model_name] = best_model\n\n        # Save the trained model to disk\n        joblib.dump(best_model, os.path.join(save_path, f"{model_name}.pkl"))\n'})}),"\n",(0,i.jsxs)(n.h3,{id:"defining-a-custom-predict-method-to-aggregate-multi-model-predictions",children:["Defining a Custom ",(0,i.jsx)(n.code,{children:"predict"})," Method to Aggregate Multi-model Predictions"]}),"\n",(0,i.jsxs)(n.p,{children:["To streamline the inference process, every PyFunc model should define a custom ",(0,i.jsx)(n.code,{children:"predict"})," method as the single entry point for inference. This approach abstracts the model's internal workings at inference time, whether dealing with a custom PyFunc model or an out-of-the-box MLflow built-in flavor for popular ML frameworks."]}),"\n",(0,i.jsxs)(n.p,{children:["The custom ",(0,i.jsx)(n.code,{children:"predict"})," method for the ensemble model is designed to collect and combine predictions from the sub-models, supporting various aggregation strategies (e.g., average, weighted). The process involves the following steps:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Load the sub-model predictions aggregation strategy based on the user-defined approach."}),"\n",(0,i.jsx)(n.li,{children:"Load the models to be used for inference."}),"\n",(0,i.jsx)(n.li,{children:"Preprocess the input data."}),"\n",(0,i.jsx)(n.li,{children:"Collect predictions from individual models."}),"\n",(0,i.jsx)(n.li,{children:"Aggregate the model predictions according to the specified strategy."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import duckdb\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\ndef predict(self, context, model_input: pd.DataFrame) -> np.ndarray:\n    """\n    Predicts the target variable using the ensemble of models based on the selected strategy.\n\n    Args:\n        context: MLflow context object.\n        model_input (pd.DataFrame): Input features for prediction.\n\n    Returns:\n        np.ndarray: Array of predicted values.\n\n    Raises:\n        ValueError: If the strategy is unknown or no models are fitted.\n    """\n    # Check if the \'strategy\' column is present in the input DataFrame\n    if "strategy" in model_input.columns:\n        # Extract the strategy and drop it from the input features\n        print(f"Strategy: {model_input[\'strategy\'].iloc[0]}")\n        strategy = model_input["strategy"].iloc[0]\n        model_input.drop(columns=["strategy"], inplace=True)\n    else:\n        # Default to \'average\' strategy if none is provided\n        strategy = "average"\n\n    # Load the strategy details from the pre-loaded strategies DataFrame\n    loaded_strategy = self.strategies[self.strategies["strategy"] == strategy]\n\n    if loaded_strategy.empty:\n        # Raise an error if the specified strategy is not found\n        raise ValueError(\n            f"Strategy \'{strategy}\' not found in the pre-loaded strategies."\n        )\n\n    # Parse the list of models to be used for prediction\n    model_list = loaded_strategy["model_list"].iloc[0].split(",")\n\n    # Transform input features using the preprocessor, if available\n    if self.preprocessor is None:\n        # Feature engineering is required if the preprocessor is not set\n        X_processed = self.feature_engineering(model_input)\n    else:\n        # Use the existing preprocessor to transform the features\n        X_processed = self.preprocessor.transform(model_input)\n\n    if not self.fitted_models:\n        # Raise an error if no models are fitted\n        raise ValueError("No fitted models found. Please fit the models first.")\n\n    # Collect predictions from all models specified in the strategy\n    predictions = np.array(\n        [self.fitted_models[model].predict(X_processed) for model in model_list]\n    )\n\n    # Apply the specified strategy to combine the model predictions\n    if "average" in strategy:\n        # Calculate the average of predictions from all models\n        return np.mean(predictions, axis=0)\n    elif "weighted" in strategy:\n        # Extract weights from the strategy and normalize them\n        weights = [float(w) for w in loaded_strategy["weights"].iloc[0].split(",")]\n        weights = np.array(weights)\n        weights /= np.sum(weights)  # Ensure weights sum to 1\n\n        # Compute the weighted average of predictions\n        return np.average(predictions, axis=0, weights=weights)\n    else:\n        # Raise an error if an unknown strategy is encountered\n        raise ValueError(f"Unknown strategy: {strategy}")\n'})}),"\n",(0,i.jsxs)(n.h3,{id:"defining-a-load-context-custom-method-to-initialize-the-ensemble-model",children:["Defining a ",(0,i.jsx)(n.code,{children:"load context"})," custom method to initialize the Ensemble Model"]}),"\n",(0,i.jsxs)(n.p,{children:["When loading the ensemble model using ",(0,i.jsx)(n.code,{children:"mlflow.pyfunc.load_model"}),", the custom ",(0,i.jsx)(n.code,{children:"load_context"})," method is executed to handle the required model initialization steps before inference."]}),"\n",(0,i.jsx)(n.p,{children:"This initialization process includes:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Loading model artifacts, including both the pre-trained models and the preprocessor, using the context object that contains the artifacts references."}),"\n",(0,i.jsx)(n.li,{children:"Fetching strategies definitions from DuckDB Database."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import duckdb\nimport joblib\nimport pandas as pd\n\ndef load_context(self, context) -> None:\n    """\n    Loads the preprocessor and models from the MLflow context.\n\n    Args:\n        context: MLflow context object which provides access to saved artifacts.\n    """\n    # Load the preprocessor if its path is specified in the context artifacts\n    preprocessor_path = context.artifacts.get("preprocessor", None)\n    if preprocessor_path:\n        self.preprocessor = joblib.load(preprocessor_path)\n\n    # Load each model from the context artifacts and store it in the fitted_models dictionary\n    for model_name in self.models.keys():\n        model_path = context.artifacts.get(model_name, None)\n        if model_path:\n            self.fitted_models[model_name] = joblib.load(model_path)\n        else:\n            # Print a warning if a model is not found in the context artifacts\n            print(\n                f"Warning: {model_name} model not found in artifacts. Initialized but not fitted."\n            )\n\n    # Reconnect to the DuckDB database to load the strategies\n    conn = duckdb.connect(self.db_path)\n    # Fetch strategies from the DuckDB database into the strategies DataFrame\n    self.strategies = conn.execute("SELECT * FROM strategies").fetchdf()\n    # Close the database connection\n    conn.close()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"bringing-it-all-together",children:"Bringing It All Together"}),"\n",(0,i.jsx)(n.p,{children:"Having explored each method in detail, the next step is to integrate them to observe the complete implementation in action. This will offer a comprehensive view of how the components interact to achieve the project's objectives."}),"\n",(0,i.jsxs)(n.p,{children:["You can use the skeleton provided in the ",(0,i.jsx)(n.a,{href:"#creating-the-ensemble-model",children:"Creating the Ensemble Model"})," section to assemble the entire ",(0,i.jsx)(n.code,{children:"EnsembleModel"})," class. Each method was demonstrated with its specific dependencies included. Now, you just need to combine these methods into the class definition, following the outline given. Feel free to add any custom logic that fits your specific use case or enhances the functionality of the ensemble model."]}),"\n",(0,i.jsx)(n.p,{children:"After everything has been encapsulated in a PyFunc model, the lifecycle of the ensemble model closely mirrors that of a traditional MLflow model. The following diagram depicts the lifecycle of the model."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Ensemble Model Lifecycle",src:t(9290).Z+"",width:"2471",height:"215"})}),"\n",(0,i.jsx)(n.h2,{id:"mlflow-tracking",children:"MLflow Tracking"}),"\n",(0,i.jsxs)(n.h3,{id:"using-the-fit-method-to-train-sub-models",children:["Using the ",(0,i.jsx)(n.code,{children:"fit"})," Method to Train Sub-Models"]}),"\n",(0,i.jsxs)(n.p,{children:["Once the data is preprocessed, we use the custom ",(0,i.jsx)(n.code,{children:"fit"})," method to train all the sub-models in our Ensemble Model. This method applies grid search to find the best hyperparameters for each sub-model, fits them to the training data, and saves the trained models for future use."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," For the following block of code, you might need to set the MLflow Tracking Server if you're not using Managed MLflow. In the ",(0,i.jsx)(n.a,{href:"#components-of-the-project",children:"Components of the Project"}),", there's a note about setting up a simple local MLflow Tracking Server. For this step of the project, you'll need to point MLflow to the server\u2019s URI that has been configured and is currently running. Don't forget to set the server URI variable ",(0,i.jsx)(n.code,{children:"remote_server_uri"}),". You can refer to the official MLflow documentation for more details on ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/tracking/server.html#logging-to-a-tracking-server",children:"Logging to a Tracking Server"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import datetime\nimport os\n\nimport joblib\nimport mlflow\nimport pandas as pd\nfrom mlflow.models.signature import infer_signature\nfrom sklearn.model_selection import train_test_split\n\n# Initialize the MLflow client\nclient = mlflow.MlflowClient()\n\n# Set the URI of your MLflow Tracking Server\nremote_server_uri = "..."  # Replace with your server URI\n\n# Point MLflow to your MLflow Tracking Server\nmlflow.set_tracking_uri(remote_server_uri)\n\n# Set the experiment name for organizing runs in MLflow\nmlflow.set_experiment("Ensemble Model")\n\n# Load dataset from the provided URL\ndata = pd.read_csv(\n    "https://github.com/zobi123/Machine-Learning-project-with-Python/blob/master/Housing.csv?raw=true"\n)\n\n# Separate features and target variable\nX = data.drop("price", axis=1)\ny = data["price"]\n\n# Split dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create a directory to save the models and related files\nos.makedirs("models", exist_ok=True)\n\n# Initialize and train the EnsembleModel\nensemble_model = EnsembleModel()\n\n# Preprocess the training data using the defined feature engineering method\nX_train_processed = ensemble_model.feature_engineering(X_train)\n\n# Fit the models with the preprocessed training data and save them\nensemble_model.fit(X_train_processed, y_train, save_path="models")\n\n# Infer the model signature using a small example from the training data\nexample_input = X_train[:1]  # Use a single sample for signature inference\nexample_input["strategy"] = "average"\nexample_output = y_train[:1]\nsignature = infer_signature(example_input, example_output)\n\n# Save the preprocessing pipeline to disk\njoblib.dump(ensemble_model.preprocessor, "models/preprocessor.pkl")\n\n# Define strategies for the ensemble model\nstrategy_data = {\n    "strategy": [\n        "average_1",\n        "average_2",\n        "weighted_1",\n        "weighted_2",\n        "weighted_3",\n        "weighted_4",\n    ],\n    "model_list": [\n        "random_forest,xgboost,decision_tree,gradient_boosting,adaboost",\n        "decision_tree",\n        "random_forest,xgboost,decision_tree,gradient_boosting,adaboost",\n        "random_forest,xgboost,gradient_boosting",\n        "decision_tree,adaboost",\n        "xgboost,gradient_boosting",\n    ],\n    "weights": ["1", "1", "0.2,0.3,0.1,0.2,0.2", "0.4,0.4,0.2", "0.5,0.5", "0.7,0.3"],\n}\n\n# Create a DataFrame to hold the strategy information\nstrategies_df = pd.DataFrame(strategy_data)\n\n# Add strategies to the database\nensemble_model.add_strategy_and_save_to_db(strategies_df, "models/strategies.db")\n\n# Define the Conda environment configuration for the MLflow model\nconda_env = {\n    "name": "mlflow-env",\n    "channels": ["conda-forge"],\n    "dependencies": [\n        "python=3.8",\n        "scikit-learn=1.3.0",\n        "xgboost=2.0.3",\n        "joblib=1.2.0",\n        "pandas=1.5.3",\n        "numpy=1.23.5",\n        "duckdb=1.0.0",\n        {\n            "pip": [\n                "mlflow==2.14.1",\n            ]\n        },\n    ],\n}\n\n# Get current timestamp\ntimestamp = datetime.datetime.now().isoformat()\n\n# Log the model using MLflow\nwith mlflow.start_run(run_name=timestamp) as run:\n    # Log parameters, artifacts, and model signature\n    mlflow.log_param("model_type", "EnsembleModel")\n\n    artifacts = {\n        model_name: os.path.join("models", f"{model_name}.pkl")\n        for model_name in ensemble_model.models.keys()\n    }\n    artifacts["preprocessor"] = os.path.join("models", "preprocessor.pkl")\n    artifacts["strategies_db"] = os.path.join("models", "strategies.db")\n\n    mlflow.pyfunc.log_model(\n        artifact_path="ensemble_model",\n        python_model=ensemble_model,\n        artifacts=artifacts,\n        conda_env=conda_env,\n        signature=signature,\n    )\n\n    print(f"Model logged in run {run.info.run_id}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"registering-the-model-with-mlflow",children:"Registering the Model with MLflow"}),"\n",(0,i.jsx)(n.p,{children:"Following the completion of model training, the subsequent step involves registering the ensemble model with MLflow. This process entails logging the trained models, preprocessing pipelines, and associated strategies into the MLflow Tracking Server. This ensures that all components of the ensemble model are systematically saved and versioned, facilitating reproducibility and traceability."}),"\n",(0,i.jsxs)(n.p,{children:["Moreover, we will assign to this initial version of the model a production alias. This designation establishes a baseline model against which future iterations can be assessed. By marking this version as the ",(0,i.jsx)(n.code,{children:"production"})," model, we can effectively benchmark improvements and confirm that subsequent versions yield measurable advancements over this established baseline."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Register the model in MLflow and assign a production alias\nmodel_uri = f"runs:/{run.info.run_id}/ensemble_model"\nmodel_details = mlflow.register_model(model_uri=model_uri, name="ensemble_model")\n\nclient.set_registered_model_alias(\n\tname="ensemble_model", alias="production", version=model_details.version\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"The following illustration demonstrates the complete lifecycle of our ensemble model within the MLflow UI up until this step."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Ensemble Model within MLflow UI",src:t(4642).Z+"",width:"2918",height:"1508"})}),"\n",(0,i.jsxs)(n.h3,{id:"using-the-predict-method-to-perform-inference",children:["Using the ",(0,i.jsx)(n.code,{children:"predict"})," Method to Perform Inference"]}),"\n",(0,i.jsx)(n.p,{children:"With the ensemble model registered in the MLflow Model Registry, it can now be utilized to predict house prices by aggregating the predictions from the various sub-models within the ensemble."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pandas as pd\n\nimport mlflow\nfrom sklearn.metrics import r2_score\n\n# Load the registered model using its alias\nloaded_model = mlflow.pyfunc.load_model(\n\tmodel_uri=f"models:/ensemble_model@production"\n)\n\n# Define the different strategies for evaluation\nstrategies = [\n    "average_1",\n    "average_2",\n    "weighted_1",\n    "weighted_2",\n    "weighted_3",\n    "weighted_4",\n]\n\n# Initialize a DataFrame to store the results of predictions\nresults_df = pd.DataFrame()\n\n# Iterate over each strategy, make predictions, and calculate R^2 scores\nfor strategy in strategies:\n    # Create a test DataFrame with the current strategy\n    X_test_with_params = X_test.copy()\n    X_test_with_params["strategy"] = strategy\n\n    # Use the loaded model to make predictions\n    y_pred = loaded_model.predict(X_test_with_params)\n\n    # Calculate R^2 score for the predictions\n    r2 = r2_score(y_test, y_pred)\n\n    # Store the results and R^2 score in the results DataFrame\n    results_df[strategy] = y_pred\n    results_df[f"r2_{strategy}"] = r2\n\n# Add the actual target values to the results DataFrame\nresults_df["y_test"] = y_test.values\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Similar to out-of-the-box MLflow models, you begin by loading the ensemble model using ",(0,i.jsx)(n.code,{children:"mlflow.pyfunc.load_model"})," to generate the house price predictions. After defining the different strategies for aggregating sub-model predictions and creating the model input containing both the housing data features and aggregation strategy, simply call the ensemble model's ",(0,i.jsx)(n.code,{children:"predict"})," method to get the aggregated house price prediction."]}),"\n",(0,i.jsx)(n.h3,{id:"evaluating-model-performance-with-different-strategies",children:"Evaluating Model Performance with Different Strategies"}),"\n",(0,i.jsx)(n.p,{children:"To evaluate the performance of our ensemble model, we calculated the average R\xb2 scores for different aggregation strategies. These strategies include both simple averaging and weighted combinations of sub-models, with varying configurations of models and their respective weights. By comparing the R\xb2 scores, we can assess which strategies provide the most accurate predictions."}),"\n",(0,i.jsxs)(n.p,{children:["The bar graph below illustrates the average R\xb2 scores for each strategy. Higher values indicate better predictive performance. As shown in the graph, the ensemble strategies generally outperform individual models as depicted in our second strategy that is relying on a single ",(0,i.jsx)(n.code,{children:"DecisionTree"})," (average_2), demonstrating the effectiveness of aggregating predictions from multiple sub-models. This visual comparison highlights the benefits of using an ensemble approach, particularly with weighted strategies that optimize the contribution of each sub-model."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Ensemble Model Evaluation",src:t(4352).Z+"",width:"1585",height:"1087"})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This blog post highlights the capabilities of mlflow.pyfunc and its application in a Machine Learning project. This powerful feature of MLflow provides creative freedom and flexibility, enabling teams to build complex systems encapsulated as models within MLflow, following the same lifecycle as traditional models. The post showcases the creation of ensemble model setups, seamless integration with DuckDB, and the implementation of custom methods using this versatile module."}),"\n",(0,i.jsx)(n.p,{children:"Beyond offering a structured approach to achieving desired outcomes, this blog demonstrates practical possibilities based on hands-on experience, discussing potential challenges and their solutions."}),"\n",(0,i.jsx)(n.h2,{id:"additional-resources",children:"Additional resources"}),"\n",(0,i.jsx)(n.p,{children:"Explore the following resources for a deeper understanding of MLflow PyFunc models:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://mlflow.org/blog/custom-pyfunc",children:"Custom MLflow Models with mlflow.pyfunc"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part2-pyfunc-components.html",children:"Understanding PyFunc in MLflow"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html",children:"Building Custom Python Function Models with MLflow"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.html",children:"Deploy an MLflow PyFunc model with Model Serving"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},5707:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/ensemble-model-architecture-2b00df62594a4030a01d626d8d06d2be.png"},4352:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/ensemble-model-evaluation-19d41715d23f6288ea9d3074701fcd05.png"},9290:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/ensemble-model-lifecycle-f9bf951e52294104ffaaf1e91f473226.png"},4642:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/ensemble-model-mlflow-ui-241073e6b40b8f129f49f24f6e561102.gif"},1151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>a});var i=t(7294);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);