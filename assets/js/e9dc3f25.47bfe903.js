"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1069],{7974:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"llm-as-judge","metadata":{"permalink":"/mlflow-website/blog/llm-as-judge","source":"@site/blog/2024-10-03-llm-as-judge/index.md","title":"LLM as judge","description":"Perform LLM Evaluations with custom metrics","date":"2024-10-03T00:00:00.000Z","formattedDate":"October 3, 2024","tags":[{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"mlflow-evalaute","permalink":"/mlflow-website/blog/tags/mlflow-evalaute"}],"readingTime":16.54,"hasTruncateMarker":false,"authors":[{"name":"Pedro Azevedo","title":"Machine Learning Analyst at Adidas","url":"https://www.linkedin.com/in/pedro-azevedo-/","imageURL":"/img/authors/pedro.png","key":"pedro-azevedo"},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey"}],"frontMatter":{"title":"LLM as judge","description":"Perform LLM Evaluations with custom metrics","slug":"llm-as-judge","authors":["pedro-azevedo","rahul-pandey"],"tags":["genai","mlflow-evalaute"],"thumbnail":"/img/blog/llm-as-judge.png"},"unlisted":false,"nextItem":{"title":"Models from Code Logging in MLflow - What, Why, and How","permalink":"/mlflow-website/blog/models_from_code"}},"content":"In this blog post, we\'ll dive on a journey to revolutionize how we evaluate language models. We\'ll explore the power of MLflow Evaluate and harness the capabilities of Large Language Models (LLMs) as judges. By the end, you\'ll learn how to create custom metrics, implement LLM-based evaluation, and apply these techniques to real-world scenarios. Get ready to transform your model assessment process and gain deeper insights into your AI\'s performance!\\n\\n## The Challenge of Evaluating Language Models\\n\\nEvaluating large language models (LLMs) and natural language processing (NLP) systems presents several challenges, primarily due to their complexity and the diversity of tasks they can perform.\\n\\nOne major difficulty is creating metrics that comprehensively measure performance across varied applications, from generating coherent text to understanding nuanced human emotions. Traditional benchmarks often fail to capture these subtleties, leading to incomplete assessments.\\n\\nAn LLM acting as a judge can address these issues by leveraging its extensive training data to provide a more nuanced evaluation, offering insights into model behavior and areas needing improvement. For instance, an LLM can analyze whether a model generates text that is not only grammatically correct but also contextually appropriate and engaging, something more static metrics might miss.\\n\\nHowever, to move forward effectively, we need more than just better evaluation methods. Standardized experimentation setups are essential to ensure that comparisons between models are both fair and replicable. A uniform framework for testing and evaluation would enable researchers to build on each other\'s work, leading to more consistent progress and the development of more robust models.\\n\\n## Introducing MLflow LLM Evaluate\\n\\n[MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) is a powerful function within the MLflow ecosystem that allows for comprehensive model assessment by providing a standardized experiment setup. It supports both built-in metrics and custom (LLM) metrics, making it an ideal tool for evaluating complex language tasks. With [MLflow LLM Evaluate](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate), you can:\\n\\n- Evaluate models against multiple metrics simultaneously\\n- Use pre-defined metrics for specific model types (e.g., question-answering, text-summarization and pure text)\\n- Create custom metrics, including those that use LLMs as judges using [mlflow.metrics.genai.make_genai_metric()](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric)\\n  and\\n  [mlflow.metrics.genai.make_genai_metric_from_prompt()](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric_from_prompt)\\n\\n![MLflow Evaluate](mlflow_evaluate.drawio.svg)\\n\\n## Conquering new markets with an LLM as a judge\\n\\nImagine you\'re part of a global travel agency, \\"WorldWide Wandercorp,\\" that\'s expanding its reach to Spanish-speaking countries.\\n\\nYour team has developed an AI-powered translation system to help create culturally appropriate marketing materials and customer communications. However, as you begin to use this system, you realize that traditional evaluation metrics, such as BLEU (Bilingual Evaluation Understudy), fall short in capturing the nuances of language translation, especially when it comes to preserving cultural context and idiomatic expressions.\\n\\nFor instance, consider the phrase \\"kick the bucket.\\" A direct translation might focus on the literal words, but the idiom actually means \\"to die.\\" A traditional metric like BLEU may incorrectly evaluate the translation as adequate if the translated words match a reference translation, even if the cultural meaning is lost. In such cases, the metric might score the translation highly despite it being completely inappropriate in context. This could lead to embarrassing or culturally insensitive marketing content, which is something your team wants to avoid.\\n\\nYou need a way to evaluate whether the translation not only is accurate but also preserves the intended meaning, tone, and cultural context. This is where [MLflow Evaluate](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) and LLMs (Large Language Models) as judges come into play. These tools can assess translations more holistically by considering context, idiomatic expressions, and cultural relevance, providing a more reliable evaluation of the AI\u2019s output.\\n\\n## Custom Metrics: Tailoring Evaluation to Your Needs\\n\\nIn the following section, we\u2019ll implement three metrics:\\n\\n- The `\\"cultural_sensitivity\\"` metric ensures translations maintain cultural context and appropriateness.\\n- The `\\"faithfulness\\"` metric checks that chatbot responses align accurately with company policies and retrieved content.\\n- The `\\"toxicity\\"` metric evaluates responses for harmful or inappropriate content, ensuring respectful customer interactions.\\n\\nThese metrics will help Worldwide WanderAgency ensure their AI-driven translations and interactions meet their specific needs.\\n\\n## Evaluating Worldwide WanderAgency\'s AI Systems\\n\\nNow that we understand WanderAgency\'s challenges, let\'s dive into a code walkthrough to address them. We\'ll implement custom metrics to measure AI performance and build a gauge visualization chart for sharing results with stakeholders.\\n\\nWe\'ll start by evaluating a language translation model, focusing on the \\"cultural_sensitivity\\" metric to ensure it preserves cultural nuances. This will help WanderAgency maintain high standards in global communication.\\n\\n### Cultural Sensitivity Metric\\n\\nThe travel agency wants to ensure their translations are not only accurate but also culturally appropriate.\\nTo achieve this they are considering creating a custom metric that allows Worldwide WanderAgency to quantify how well their translations maintain cultural context and idiomatic expressions.\\n\\nFor instance, a phrase that is polite in one culture might be inappropriate in another.\\nIn English, addressing someone as \\"Dear\\" in a professional email might be seen as polite. However, in Spanish, using \\"Querido\\" in a professional context can be too personal and inappropriate.\\n\\nHow can we evaluate such an abstract concept in a systematic way? Traditional Metrics would fall short so we need a better way of doing it. In this case LLM as a judge would be a great fit!\\nFor this use case let\'s create a \\"cultural_sensitivity\\" metric.\\n\\nHere\'s a brief overview of the process:\\nStart by installing all the necessary libraries for this demo to work.\\n\\n```bash\\npip install mlflow>=2.14.1 openai  transformers torch torchvision evaluate datasets tiktoken fastapi rouge_score textstat tenacity plotly ipykernel nbformat>=5.10.4\\n```\\n\\nWe will be using gpt3.5 and gpt4 during this example for that let\'s start by making sure our [OpenAI key is setup](https://mlflow.org/docs/latest/llms/openai/notebooks/openai-quickstart.html#API-Key-Security-Overview).\\n\\nImport the necessary libraries.\\n\\n```python\\nimport mlflow\\nimport os\\n\\n# Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables\\n\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"OPENAI_API_KEY environment variable must be set\\"\\n\\nimport openai\\nimport pandas as pd\\n```\\n\\nWhen using the [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) function, your large language model (LLM) can take one of the following forms:\\n\\n1. A `mlflow.pyfunc.PyFuncModel()` \u2014 typically an MLflow model.\\n2. A Python function that accepts strings as inputs and returns a single string as output.\\n3. An `MLflow Deployments` endpoint URI.\\n4. `model=None` if the data you are providing has already been scored by a model, and you do not need to specify one.\\n\\nFor this example, we will use an MLflow model.\\n\\nWe\u2019ll begin by logging a translation model in MLflow. For this tutorial, we\'ll use GPT-3.5 with a defined system prompt.\\n\\nIn a production environment, you would typically experiment with different prompts and models to determine the most suitable configuration for your use case. For more details, refer to MLflow\u2019s [Prompt Engineering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html).\\n\\n```python\\n\\nsystem_prompt = \\"Translate the following sentences into Spanish\\"\\n# Let\'s set up an experiment to make it easier to track our results\\nmlflow.set_experiment(\\"/Path/to/your/experiment\\")\\n\\nbasic_translation_model = mlflow.openai.log_model(\\n    model=\\"gpt-3.5-turbo\\",\\n    task=openai.chat.completions,\\n    artifact_path=\\"model\\",\\n    messages=[\\n        {\\"role\\": \\"system\\", \\"content\\": system_prompt},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"{user_input}\\"},\\n    ],\\n)\\n```\\n\\nLet\'s test the model to make sure it works.\\n\\n```python\\nmodel = mlflow.pyfunc.load_model(basic_translation_model.model_uri)\\n\\nmodel.predict(\\"Hello, how are you?\\")\\n\\n# Output = [\'\xa1Hola, \xbfc\xf3mo est\xe1s?\']\\n```\\n\\nTo use [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate), we first need to prepare sample data that will serve as input to our LLM. In this scenario, the input would consist of the content the company is aiming to translate.\\n\\nFor demonstration purposes, we will define a set of common English expressions that we want the model to translate.\\n\\n```python\\n# Prepare evaluation data\\neval_data = pd.DataFrame(\\n    {\\n        \\"llm_inputs\\": [\\n            \\"I\'m over the moon about the news!\\",\\n            \\"Spill the beans.\\",\\n            \\"Bite the bullet.\\",\\n            \\"Better late than never.\\",\\n\\n        ]\\n    }\\n)\\n```\\n\\nTo meet the objectives of the travel agency, we will define custom metrics that evaluate the quality of translations. In particular, we need to assess how faithfully the translations capture not only the literal meaning but also cultural nuances.\\n\\nBy default, [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) uses `openai:/gpt-4` as the evaluation model. However, you also have the option to use a [local model for evaluation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#selecting-the-llm-as-judge-model), such as a model wrapped in a PyFunc (e.g., Ollama).\\n\\nFor this example, we will use GPT-4 as the evaluation model.\\n\\nTo begin, provide a few examples that illustrate good and poor translation scores.\\n\\n```python\\n# Define the custom metric\\ncultural_sensitivity = mlflow.metrics.genai.make_genai_metric(\\n    name=\\"cultural_sensitivity\\",\\n    definition=\\"Assesses how well the translation preserves cultural nuances and idioms.\\",\\n    grading_prompt=\\"Score from 1-5, where 1 is culturally insensitive and 5 is highly culturally aware.\\",\\n    examples=[\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Break a leg!\\",\\n            output=\\"\xa1R\xf3mpete una pierna!\\",\\n            score=2,\\n            justification=\\"This is a literal translation that doesn\'t capture the idiomatic meaning.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Break a leg!\\",\\n            output=\\"\xa1Mucha mierda!\\",\\n            score=5,\\n            justification=\\"This translation uses the equivalent Spanish theater idiom, showing high cultural awareness.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"It\'s raining cats and dogs.\\",\\n            output=\\"Est\xe1 lloviendo gatos y perros.\\",\\n            score=1,\\n            justification=\\"This literal translation does not convey the idiomatic meaning of heavy rain.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"It\'s raining cats and dogs.\\",\\n            output=\\"Est\xe1 lloviendo a c\xe1ntaros.\\",\\n            score=5,\\n            justification=\\"This translation uses a Spanish idiom that accurately conveys the meaning of heavy rain.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Kick the bucket.\\",\\n            output=\\"Patear el balde.\\",\\n            score=1,\\n            justification=\\"This literal translation fails to convey the idiomatic meaning of dying.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Kick the bucket.\\",\\n            output=\\"Estirar la pata.\\",\\n            score=5,\\n            justification=\\"This translation uses the equivalent Spanish idiom for dying, showing high cultural awareness.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Once in a blue moon.\\",\\n            output=\\"Una vez en una luna azul.\\",\\n            score=2,\\n            justification=\\"This literal translation does not capture the rarity implied by the idiom.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Once in a blue moon.\\",\\n            output=\\"De vez en cuando.\\",\\n            score=4,\\n            justification=\\"This translation captures the infrequency but lacks the idiomatic color of the original.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"The ball is in your court.\\",\\n            output=\\"La pelota est\xe1 en tu cancha.\\",\\n            score=3,\\n            justification=\\"This translation is understandable but somewhat lacks the idiomatic nuance of making a decision.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"The ball is in your court.\\",\\n            output=\\"Te toca a ti.\\",\\n            score=5,\\n            justification=\\"This translation accurately conveys the idiomatic meaning of it being someone else\'s turn to act.\\"\\n        )\\n    ],\\n    model=\\"openai:/gpt-4\\",\\n    parameters={\\"temperature\\": 0.0},\\n)\\n```\\n\\n### The Toxicity Metric\\n\\nIn addition to this custom metric let\'s use MLflow built-in metrics for the evaluators. In this case MLflow wll use roberta-hate-speech model to detect the [toxicity](https://huggingface.co/spaces/evaluate-measurement/toxicity). This metric evaluates responses for any harmful or inappropriate content, reinforcing the company\'s commitment to a positive customer experience.\\n\\n```python\\n# Log and evaluate the model\\nwith mlflow.start_run() as run:\\n    results = mlflow.evaluate(\\n        basic_translation_model.model_uri,\\n        data=eval_data,\\n        model_type=\\"text\\",\\n        evaluators=\\"default\\",\\n        extra_metrics=[cultural_sensitivity],\\n        evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"llm_inputs\\",\\n           }}\\n   )\\n\\nmlflow.end_run()\\n```\\n\\nYou can retrieve the final results as such:\\n\\n```python\\nresults.tables[\\"eval_results_table\\"]\\n```\\n\\n|     | llm_inputs                        | outputs                      | token_count | toxicity/v1/score | flesch_kincaid_grade_level/v1/score | ari_grade_level/v1/score | cultural_sensitivity/v1/score | cultural_sensitivity/v1/justification             |\\n| --- | --------------------------------- | ---------------------------- | ----------- | ----------------- | ----------------------------------- | ------------------------ | ----------------------------- | ------------------------------------------------- |\\n| 0   | I\'m over the moon about the news! | \xa1Estoy feliz por la noticia! | 9           | 0.000258          | 5.2                                 | 3.7                      | 4                             | The translation captures the general sentiment... |\\n| 1   | Spill the beans.                  | Revela el secreto.           | 7           | 0.001017          | 9.2                                 | 5.2                      | 5                             | The translation accurately captures the idioma... |\\n| 2   | Bite the bullet.                  | Morder la bala.              | 7           | 0.001586          | 0.9                                 | 3.6                      | 2                             | The translation \\"Morder la bala\\" is a litera...   |\\n| 3   | Better late than never.           | M\xe1s vale tarde que nunca.    | 7           | 0.004947          | 0.5                                 | 0.9                      | 5                             | The translation accurately captures the idioma... |\\n\\nLet\'s analyze the final metrics...\\n\\n```python\\ncultural_sensitivity_score = results.metrics[\'cultural_sensitivity/v1/mean\']\\nprint(f\\"Cultural Sensitivity Score: {cultural_sensitivity_score}\\")\\n\\ntoxicity_score = results.metrics[\'toxicity/v1/mean\']\\n# Calculate non-toxicity score\\nnon_toxicity_score = \\"{:.2f}\\".format((1 - toxicity_score) * 100)\\nprint(f\\"Non-Toxicity Score: {non_toxicity_score}%\\")\\n\\n```\\n\\nOutput:\\n\\n```bash\\nCultural Sensitivity Score: 3.75\\nPureness Score: 99.80\\n```\\n\\nIt is often the case we want to monitor and track these metrics on a dashboard so both data scientists and stakeholders have an understanding of the performance and reliability of these solutions.\\n\\nFor this example let\'s create a gauge to display the final metric.\\n\\n```python\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\n\\ndef create_gauge_chart(value1, title1, value2, title2):\\n    # Create a subplot figure with two columns\\n    fig = make_subplots(rows=1, cols=2, specs=[[{\'type\': \'indicator\'}, {\'type\': \'indicator\'}]])\\n\\n    # Add the first gauge chart\\n    fig.add_trace(go.Indicator(\\n        mode = \\"gauge+number\\",\\n        value = value1,\\n        title = {\'text\': title1},\\n        gauge = {\'axis\': {\'range\': [None, 5]}}\\n    ), row=1, col=1)\\n\\n    # Add the second gauge chart\\n    fig.add_trace(go.Indicator(\\n        mode = \\"gauge+number\\",\\n        value = value2,\\n        title = {\'text\': title2},\\n        gauge = {\'axis\': {\'range\': [None, 100]}}\\n    ), row=1, col=2)\\n\\n    # Update layout\\n    fig.update_layout(height=400, width=800)\\n\\n    # Show figure\\n    fig.show()\\n```\\n\\n```python\\ncreate_gauge_chart(cultural_sensitive_score, \\"Cultural Sensitivity Score\\", float(non_toxicity_score), \\"Non Toxicity Score\\")\\n```\\n\\n![Gauge Chart](gauge.png)\\n\\n### The Faithfulness Metric\\n\\nAs Worldwide WanderAgency\'s AI grows, they add a customer service chatbot that handles questions in multiple languages. This chatbot uses a RAG (Retrieval-Augmented Generation) system, which means it retrieves information from a database or documents and then generates an answer based on that information.\\n\\nIt\'s important that the answers provided by the chatbot stay true to the information it retrieves. To make sure of this, we create a \\"faithfulness\\" metric. This metric checks how well the chatbot\'s responses match the materials it\u2019s supposed to be based on, ensuring the information given to customers is accurate.\\n\\nFor example, If the retrieved document says \\"Returns are accepted within 30 days,\\" and the chatbot replies with \\"Our return policy is flexible and varies by region,\\" it is not aligning well with the retrieved material. This inaccurate response (bad faithfulness) could mislead customers and create confusion.\\n\\n### Using MLflow to Evaluate RAG - Faithfulness\\n\\nLet\'s evaluate how well our chatbot is doing in sticking to the retrieved information. Instead of using an MLflow model this time, we\u2019ll use a custom function to define the faithfulness metric and see how aligned the chatbot\'s answers are with the data it pulls from.\\n\\n```python\\n# Prepare evaluation data\\neval_data = pd.DataFrame(\\n    {\\n        \\"llm_inputs\\": [\\n            \\"\\"\\"Question: What is the company\'s policy on employee training?\\ncontext: \\"Our company offers various training programs to support employee development. Employees are required to complete at least one training course per year related to their role. Additional training opportunities are available based on performance reviews.\\" \\"\\"\\",\\n            \\"\\"\\"Question: What is the company\'s policy on sick leave?\\ncontext: \\"Employees are entitled to 10 days of paid sick leave per year. Sick leave can be used for personal illness or to care for an immediate family member. A doctor\'s note is required for sick leave exceeding three consecutive days.\\" \\"\\"\\",\\n            \\"\\"\\"Question: How does the company handle performance reviews?\\ncontext: \\"Performance reviews are conducted annually. Employees are evaluated based on their job performance, goal achievement, and overall contribution to the team. Feedback is provided, and development plans are created to support employee growth.\\" \\"\\"\\",\\n        ]\\n    }\\n)\\n\\n```\\n\\nNow let\'s define some examples for this faithfulness metric.\\n\\n```python\\nexamples = [\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s policy on remote work?\\ncontext: \\"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\\" \\"\\"\\",\\n            output=\\"Employees can work remotely up to three days a week if they maintain productivity and attend mandatory meetings.\\",\\n            score=5,\\n            justification=\\"The answer is accurate and directly related to the question and context provided.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s policy on remote work?\\ncontext: \\"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\\" \\"\\"\\",\\n            output=\\"Employees are allowed to work remotely as long as they want.\\",\\n            score=2,\\n            justification=\\"The answer is somewhat related but incorrect because it does not mention the three-day limit.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s policy on remote work?\\ncontext: \\"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\\" \\"\\"\\",\\n            output=\\"Our company supports flexible work arrangements.\\",\\n            score=3,\\n            justification=\\"The answer is related to the context but does not specifically answer the question about the remote work policy.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s annual leave policy?\\ncontext: \\"Employees are entitled to 20 days of paid annual leave per year. Leave must be approved by the employee\'s direct supervisor and should be planned in advance to ensure minimal disruption to work.\\" \\"\\"\\",\\n            output=\\"Employees are entitled to 20 days of paid annual leave per year, which must be approved by their supervisor.\\",\\n            score=5,\\n            justification=\\"The answer is accurate and directly related to the question and context provided.\\"\\n        )]\\n\\n#  Define the custom metric\\nfaithfulness = mlflow.metrics.genai.make_genai_metric(\\n    name=\\"faithfulness\\",\\n    definition=\\"Assesses how well the answer relates to the question and provided context.\\",\\n    grading_prompt=\\"Score from 1-5, where 1 is not related at all and 5 is highly relevant and accurate.\\",\\n    examples=examples)\\n\\n```\\n\\nDefine out LLM function (in this case it can be any function that follows certain input/output formats that [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate)).\\n\\n```python\\n# Using custom function\\ndef my_llm(inputs):\\n    answers = []\\n    system_prompt = \\"Please answer the following question in formal language based on the context provided.\\"\\n    for index, row in inputs.iterrows():\\n        print(\'INPUTS:\', row)\\n        completion = openai.chat.completions.create(\\n            model=\\"gpt-3.5-turbo\\",\\n            messages=[\\n                {\\"role\\": \\"system\\", \\"content\\": system_prompt},\\n                {\\"role\\": \\"user\\", \\"content\\": f\\"{row}\\"},\\n            ],\\n        )\\n        answers.append(completion.choices[0].message.content)\\n\\n    return answers\\n```\\n\\nResulting in a code that is similar to what we did before...\\n\\n```python\\nwith mlflow.start_run() as run:\\n    results = mlflow.evaluate(\\n        my_llm,\\n        eval_data,\\n        model_type=\\"text\\",\\n        evaluators=\\"default\\",\\n        extra_metrics=[faithfulness],\\n        evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"llm_inputs\\",\\n           }}\\n    )\\nmlflow.end_run()\\n```\\n\\n### GenAI Metrics\\n\\nAlternatively, we can leverage MLflow\'s built-in metrics for generative AI, using the same examples.\\n\\nMLflow provides several [built-in metrics](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html?highlight=genai%20answer#generative-ai-metrics) that use an LLM as a judge. Despite differences in implementation, these metrics are used in the same way. Simply include them in the `extra_metrics` argument of the [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) function.\\n\\nIn this case, we will use MLflow\u2019s built-in [faithfulness metric](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html?highlight=genai%20answer#mlflow.metrics.genai.faithfulness).\\n\\n```python\\nfrom mlflow.metrics.genai import EvaluationExample, faithfulness\\nfaithfulness_metric = faithfulness(model=\\"openai:/gpt-4\\")\\nprint(faithfulness_metric)\\n```\\n\\n[`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) simplifies the process of providing grading context, such as the documents retrieved by our system, directly into the evaluation. This feature integrates seamlessly with [LangChain\'s retrievers](https://python.langchain.com/docs/concepts/#retrievers), allowing you to supply the context for evaluation as a dedicated column. For more details, refer to [this example](https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/rag-evaluation-llama2.html).\\n\\nIn this case, since our retrieved documents are already included within the final prompt and we are not leveraging LangChain for this tutorial, we will simply map the `llm_input` column as our grading context.\\n\\n```python\\nwith mlflow.start_run() as run:\\n    results = mlflow.evaluate(\\n        my_llm,\\n        eval_data,\\n        model_type=\\"text\\",\\n        evaluators=\\"default\\",\\n        extra_metrics=[faithfulness_metric],\\n        evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"llm_inputs\\",\\n            \\"context\\": \\"llm_inputs\\",\\n           }}\\n    )\\nmlflow.end_run()\\n```\\n\\nAfter the evaluation we get the following results:\\n![Gauge faithfulness Chart](faithfulness.png)\\n\\n## Conclusion\\n\\nBy combining the Cultural Sensitivity score with our other calculated metrics, our travel agency can further refine its model to ensure the delivery of high-quality content across all languages. Moving forward, we can revisit and adjust the prompts used to boost our Cultural Sensitivity score. Alternatively, we could fine-tune a smaller model to maintain the same high level of cultural sensitivity while reducing costs. These steps will help us provide even better service to the agency\'s diverse customer base.\\n\\n[`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate), combined with LLMs as judges, opens up new possibilities for nuanced and context-aware model evaluation. By creating custom metrics tailored to specific aspects of model performance, data scientists can gain deeper insights into their models\' strengths and weaknesses.\\n\\nThe flexibility offered by `make_genai_metric()` allows you to create evaluation criteria that are perfectly suited to your specific use case. Whether you need structured guidance for your LLM judge or want full control over the prompting process, MLflow provides the tools you need.\\n\\nAs you explore MLflow evaluate and LLM-based metrics, remember that the key lies in designing thoughtful evaluation criteria and providing clear instructions to your LLM judge. With these tools at your disposal, you\'re well-equipped to take your model evaluation to the next level, ensuring that your language models not only perform well on traditional metrics but also meet the nuanced requirements of real-world applications.\\n\\nThe built-in metrics, such as toxicity, offer standardized assessments that are crucial for ensuring the safety and accessibility of model outputs.\\n\\nAs a final challenge, re-run all the tests performed but this time with \\"gpt-4o-mini\\" and see how the performance is affected."},{"id":"models_from_code","metadata":{"permalink":"/mlflow-website/blog/models_from_code","source":"@site/blog/2024-09-13-models-from-code-logging/index.md","title":"Models from Code Logging in MLflow - What, Why, and How","description":"We all (well, most of us) remember November 2022 when the public release of ChatGPT by OpenAI marked a significant turning point in the world of AI. While generative artificial intelligence (GenAI) had been evolving for some time, ChatGPT, built on OpenAI\'s GPT-3.5 architecture, quickly captured the public\u2019s imagination. This led to an explosion of interest in GenAI, both within the tech industry and among the general public.","date":"2024-09-13T00:00:00.000Z","formattedDate":"September 13, 2024","tags":[{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":11.53,"hasTruncateMarker":false,"authors":[{"name":"Awadelrahman M. A. Ahmed","title":"MLflow Ambassador | Cloud Data & Analytics Architect at REMA 1000","url":"https://www.linkedin.com/in/awadelrahman/","imageURL":"/img/authors/awadelrahman_ahmed.png","key":"awadelrahman-ahmed"}],"frontMatter":{"title":"Models from Code Logging in MLflow - What, Why, and How","tags":["genai","pyfunc","mlops"],"slug":"models_from_code","authors":["awadelrahman-ahmed"],"thumbnail":"/img/blog/thumbnail-models-from-code.gif"},"unlisted":false,"prevItem":{"title":"LLM as judge","permalink":"/mlflow-website/blog/llm-as-judge"},"nextItem":{"title":"AutoGen with Custom PyFunc","permalink":"/mlflow-website/blog/autogen-image-agent"}},"content":"We all (well, most of us) remember November 2022 when the public release of ChatGPT by OpenAI marked a significant turning point in the world of AI. While generative artificial intelligence (GenAI) had been evolving for some time, ChatGPT, built on OpenAI\'s GPT-3.5 architecture, quickly captured the public\u2019s imagination. This led to an explosion of interest in GenAI, both within the tech industry and among the general public.\\n\\nOn the tools side, MLflow continues to solidify its position as the favorite tool for (machine learning operations) MLOps among the ML community. However, the rise of GenAI has introduced new needs in how we use MLflow. One of these new challenges is how we log models in MLflow. If you\u2019ve used MLflow before (and I bet you have), you\u2019re probably familiar with the `mlflow.log_model()` function and how it efficiently [pickles](https://github.com/cloudpipe/cloudpickle) model artifacts.\\n\\nParticularly with GenAI, there\u2019s a new requirement: logging the models \\"from code\\", instead of serializing it into a pickle file! And guess what? This need isn\u2019t limited to GenAI models! So, in this post I will explore this concept and how MLflow has adapted to meet this new requirement.\\n\\nYou will notice that this feature is implemented at a very abstract level, allowing you to log any model \\"as code\\", whether it\u2019s GenAI or not! I like to think of it as a generic approach, with GenAI models being just one of its use cases. So, in this post, I\u2019ll explore this new feature, [\\"Models from Code logging\\"](https://mlflow.org/docs/latest/models.html#models-from-code).\\n\\nBy the end of this post, you should be able to answer the three main questions: \'What,\' \'Why,\' and \'How\' to use Models from Code logging.\\n\\n## What Is Models from Code Logging?\\n\\nIn fact, when MLflow announced this feature, it got me thinking in a more abstract way about the concept of a \\"model\\"! You might find it interesting as well, if you zoom out and consider a model as a mathematical representation or function that describes the relationship between input and output variables. At this level of abstraction, a model can be many things!\\n\\nOne might even recognize that a model, as an object or artifact, represents just one form of what a model can be, even if it\u2019s the most popular in the ML community. If you think about it, a model can also be as simple as a piece of code for a mapping function or a code that sends API requests to external services such as OpenAI\'s APIs.\\n\\nI\'ll explain the detailed workflow of how to log models from code later in the post, but for now, let\'s consider it at a high level with two main steps: first, writing your model code, and second, logging your model from code. This will look like the following figure:\\n\\n#### _High Level Models from Code Logging Workflow_:\\n\\n![High Level Models-from-Code Logging Workflow](models-from-code1.png)\\n\\n\ud83d\udd34 It\'s important to note that when we refer to \\"model code,\\" we\'re talking about code that can be treated as a model itself. This means it\'s **not** your training code that generates a trained model object, but rather the step-by-step code that is executed as a model itself.\\n\\n## How Models from Code Differs from Object-Based Logging?\\n\\nIn the previous section, we discussed the concept of Models from Code logging. However, concepts often become clearer when contrasted with their alternatives; a technique known as _contrast learning_. In our case, the alternative is Object-Based logging, which is the commonly used approach for logging models in MLflow.\\n\\nObject-Based logging treats a trained model as an _object_ that can be stored and reused. After training, the model is saved as an object and can be easily loaded for deployment. For example, this process can be initiated by calling `mlflow.log_model()`, where MLflow handles the serialization, often using [Pickle](https://github.com/cloudpipe/cloudpickle) or similar methods.\\n\\nObject-Based logging can be broken down into three high-level steps as in the following figure: first, creating the model object (whether by training it or acquiring it), second, serializing it (usually with Pickle or a similar tool), and third, logging it as an object.\\n\\n#### _High Level Object-Based Logging Workflow_:\\n\\n![High Level Object-Based Logging Workflow](models-from-code2.png)\\n\\n\ud83d\udca1The main distinction between the popular Object-Based logging and Models from Code logging is that in the former, we log the model object itself, whether it\'s a model you\'ve trained or a pre-trained model you\'ve acquired. In the latter, however, we log the code that _represents_ your model.\\n\\n## When Do You Need Models from Code Logging?\\n\\nBy now, I hope you have a clear understanding of _what_ Models from Code logging is! You might still be wondering, though, about the specific use cases where this feature can be applied. This section will cover exactly that\u2014the why!\\n\\nWhile we mentioned GenAI as a motivational use case in the introduction, we also highlighted that MLflow has approached Models from Code logging in a more generic way and we will see that in the next section. This means you can leverage the generalizability of the Models from Code feature for a wide range of scenarios. I\u2019ve identified three key usage patterns that I believe are particularly relevant:\\n\\n### 1\ufe0f\u20e3 When Your Model Relies on External Services:\\n\\nThis is one of the obvious and common use cases, especially with the rise of modern AI applications. It\u2019s becoming increasingly clear that we are shifting from building AI at the \\"model\\" granularity to the \\"system\\" granularity.\\n\\nIn other words, AI is no longer just about individual models; it\u2019s about how those models interact within a broader ecosystem. As we become more dependent on external AI services and APIs, the need for Models from Code logging becomes more pronounced.\\n\\nFor instance, frameworks like [LangChain](https://github.com/langchain-ai/langchain/) allow developers to build applications that chain together various AI models and services to perform complex tasks, such as language understanding and information retrieval. In such scenarios, the \\"model\\" is not just a set of trained parameters that can be _pickled_ but a \\"system\\" of interconnected services, often orchestrated by code that makes API calls to external platforms.\\n\\nModels from Code logging in these situations ensures that the entire workflow, including the logic and dependencies, is preserved. It offers is the ability to maintain the same model-like experience by capturing the code making it possible to faithfully recreate the model\u2019s behavior, even when the actual computational work is performed outside your domain.\\n\\n### 2\ufe0f\u20e3 When You\u2019re Combining Multiple Models to Calculate a Complex Metric:\\n\\nApart from GenAI, you can still benefit from the Models from Code feature in various other domains. There are many situations where multiple specialized models are combined to produce a comprehensive output. Note that we are not just referring to traditional ensemble modeling (predicting the same variable); often, you need to combine multiple models to predict different components of a complex inferential task.\\n\\nOne concrete example could be [Customer Lifetime Value (CLV)](https://en.wikipedia.org/wiki/Customer_lifetime_value) in customer analytics. In the context of CLV, you might have separate models for:\\n\\n- Customer Retention: Forecasting how long a customer will continue to engage with the business.\\n- Purchase Frequency: Predicting how often a customer will make a purchase.\\n- Average Order Value: Estimating the typical value of each transaction.\\n\\nEach of these models might already be logged and tracked properly using MLflow. Now, you need to \\"combine\\" these models into a single \\"system\\" that calculates CLV. We refer to it as a \\"system\\" because it contains multiple components.\\n\\nThe beauty of MLflow\'s Models from Code logging is that it allows you to treat this \\"CLV system\\" as a \\"CLV model\\". It enables you to leverage MLflow\'s capabilities, maintaining the MLflow-like model structure with all the advantages of tracking, versioning, and deploying your CLV model as a cohesive unit, even though it\'s built on top of other models. While such a complex model system is able to be built using a custom MLflow PythonModel, utilizing the Models from Code feature dramatically simplifies the serialization process, reducing the friction to building your solution.\\n\\n### 3\ufe0f\u20e3 When You Don\u2019t Have Serialization at All:\\n\\nDespite the rise of deep learning, industries still rely on rule-based algorithms that don\u2019t produce serialized models. In these cases, Models from Code logging can be beneficial for integrating these processes into the MLflow ecosystem.\\n\\nOne example is in industrial quality control, where the [Canny edge detection algorithm](https://en.wikipedia.org/wiki/Canny_edge_detector) is often used to identify defects. This rule-based algorithm doesn\u2019t involve serialization but is defined by specific steps.\\n\\nAnother example, which is gaining attention nowadays, is [Causal AI](https://en.wikipedia.org/wiki/Causal_AI). Constraint-based causal discovery algorithms like the [PC (Peter-Clark)](https://causal-learn.readthedocs.io/en/latest/search_methods_index/Constraint-based%20causal%20discovery%20methods/PC.html) algorithm that discover causal relationships in data but are implemented as code rather than as model objects.\\n\\nIn either case, with the Models from Code feature, you can log the entire process as a \\"model\\" in MLflow, preserving the logic and parameters while benefiting from MLflow\u2019s tracking and versioning features.\\n\\n## How To Implement Models from Code Logging?\\n\\nI hope that by this point, you have a clear understanding of the \\"What\\" and \\"Why\\" of Models from Code, and now you might be eager to get hands-on and focus on the _How_!\\n\\nIn this section, I\'ll provide a generic workflow for implementing MLflow\'s Models from Code logging, followed by a basic yet broadly applicable example. I hope the workflow provides a broad understanding that allows you to address a wide range of scenarios. I will also include links at the end to resources that cover more specific use cases (e.g., AI models).\\n\\n### Models from Code Workflow:\\n\\nA key \\"ingredient\\" of the implementation is MLflow\'s component [`pyfunc`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html). If you\'re not familiar with it, think of `pyfunc` as a universal interface in MLflow that lets you turn any model, from any framework, into an MLflow model by defining a _custom_ Python function. You can also refer to [this earlier post](https://mlflow.org/blog/custom-pyfunc) if you wish to gain a deeper understanding.\\n\\nFor our Models from Code logging, we\u2019ll particularly use the [`PythonModel`](https://mlflow.org/docs/latest/_modules/mlflow/pyfunc/model.html#PythonModel) class within `pyfunc`. This class in the MLflow Python client library allows us to create and manage Python functions as MLflow models. It enables us to define a custom function that processes input data and returns predictions or results. This model can then be deployed, tracked, and shared using MLflow\'s features.\\n\\nIt seems to be exactly what we\'re looking for\u2014we have some code that serves as our model, and we want to log it! That\'s why you\'ll soon see `mlflow.pyfunc.PythonModel` in our code example!\\n\\nNow, each time we need to implement Models from Code, we create _two_ separate Python files:\\n\\n1.  The first contains our model code (let\'s call it `model_code.py`). This file contains a class that inherits from the `mlflow.pyfunc.PythonModel` class.\\n    The class we\'re defining contains our model logic. It could be our calls to OpenAI APIs, CLV (Customer Lifetime Value) model, or our causal discovery code. We\'ll see a very simple 101 example soon.\\n\\n    \ud83d\udccc But wait! IMPORTANT:\\n\\n        - Our `model_code.py` script needs to call (i,e; include) [`mlflow.models.set_model()`](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.set_model) to set the model, which is crucial for loading the model back using `load_model()` for inference. You will notice this in the example.\\n\\n2.  The second file logs our class (that we defined in `model_code.py`). Think of it as the driver code; it can be either a notebook or a Python script (let\'s call it `driver.py`).\\n    In this file, we\'ll include the code that is responsible for logging our model code (essentially, providing the path to `model_code.py`) .\\n\\nThen we can deploy our model. Later, when the serving environment is loaded, `model_code.py` is executed, and when a serving request comes in, `PyFuncClass.predict()` is called.\\n\\nThis figure gives a generic template of these two files.\\n\\n![Models from Code files](models-from-code3.png)\\n\\n### A 101 Example of Model from Code Logging :\\n\\nLet\u2019s consider a straightforward example: a simple function to calculate the area of a circle based on its diameter. With Models from Code, we can log this calculation as a model! I like to think of it as framing the calculation as a prediction problem, allowing us to write our model code with a `predict` method.\\n\\n#### 1. Our `model_code.py` file :\\n\\n```python\\nimport mlflow\\nimport math\\n\\nclass CircleAreaModel(mlflow.pyfunc.PythonModel):\\n    def predict(self, context, model_input, params=None):\\n        return [math.pi * (r ** 2) for r in model_input]\\n\\n# It\'s important to call set_model() so it can be loaded for inference\\n# Also, note that it is set to an instance of the class, not the class itself.\\nmlflow.models.set_model(model=CircleAreaModel())\\n```\\n\\n#### 2. Our `driver.py` file :\\n\\nThis can be defined within a notebook as well. Here are its essential contents:\\n\\n```python\\nimport mlflow\\n\\ncode_path = \\"model_code.py\\" # make sure that you put the correct path\\n\\nwith mlflow.start_run():\\n  logged_model_info = mlflow.pyfunc.log_model(\\n                                            python_model=code_path,\\n                                            artifact_path=\\"test_code_logging\\"\\n                                            )\\n\\n#We can proint some info about the logged model\\nprint(f\\"MLflow Run: {logged_model_info.run_id}\\")\\nprint(f\\"Model URI: {logged_model_info.model_uri}\\")\\n```\\n\\n#### How that looks like on MLflow:\\n\\nExecuting the `driver.py` will start an MLflow run and log our model as code. The files can been as demonstrated below:\\n\\n![Models from Code files](models-from-code4.png)\\n\\n## Conclusion and Further Learning\\n\\nI hope that by this point, I have fulfilled the promises I made earlier! You should now have a clearer understanding of _What_ Models from Code is and how it differs from the popular Object-Based approach which logs models as serialized objects. You should also have a solid foundation of _Why_ and when to use it, as well as an understanding of _How_ to implement it through our general example.\\n\\nAs we mentioned in the introduction and throughout the post, there are various use cases where Models from Code can be beneficial. Our 101 example is just the beginning\u2014there is much more to explore. Below is a list of code examples that you may find helpful:\\n\\n1. Logging models from code using **Pyfunc** log model API ( [model code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/pyfunc/model_as_code.py) | [driver code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/pyfunc/model_as_code_driver.py) )\\n2. Logging model from code using **Langchain** log model API ( [model code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/langchain/chain_as_code.py) | [driver code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/langchain/chain_as_code_driver.py) )"},{"id":"autogen-image-agent","metadata":{"permalink":"/mlflow-website/blog/autogen-image-agent","source":"@site/blog/2024-08-29-autogen-pyfunc/index.md","title":"AutoGen with Custom PyFunc","description":"A guide for building an autonomous image generation agent","date":"2024-08-29T00:00:00.000Z","formattedDate":"August 29, 2024","tags":[{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":21.295,"hasTruncateMarker":true,"authors":[{"name":"Michael Berk","title":"Sr. Resident Solutions Architect at Databricks","url":"https://www.linkedin.com/in/-michael-berk/","imageURL":"/img/authors/michael_berk.png","key":"michael-berk"},{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"AutoGen with Custom PyFunc","description":"A guide for building an autonomous image generation agent","tags":["genai","mlops"],"slug":"autogen-image-agent","authors":["michael-berk","mlflow-maintainers"],"thumbnail":"/img/blog/autogen-blog.png"},"unlisted":false,"prevItem":{"title":"Models from Code Logging in MLflow - What, Why, and How","permalink":"/mlflow-website/blog/models_from_code"},"nextItem":{"title":"LangGraph with Model From Code","permalink":"/mlflow-website/blog/langgraph-model-from-code"}},"content":"In this blog, we\'ll guide you through creating an [AutoGen](https://microsoft.github.io/autogen/) agent framework within an MLflow custom PyFunc. By combining MLflow with AutoGen\'s ability to create multi-agent frameworks, we are able to create scalable and stable GenAI applications.\\n\\n## Agent Frameworks\\n\\nAgent frameworks enable autonomous agents to handle complex, multi-turn tasks by integrating discrete logic at each step. These frameworks are crucial for LLM-driven workflows, where agents manage dynamic interactions across multiple stages. Each agent operates based on specific logic, enabling precise task automation, decision-making, and coordination. This is ideal for applications like workflow orchestration, customer support, and multi-agent systems, where LLMs must interpret evolving context and respond accordingly.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Agent Frameworks with AutoGen\\n\\nAutoGen is an open-source programming framework designed for building agent-based AI systems. It offers a multi-agent conversation framework, allowing users to build [complex LLM workflows](https://microsoft.github.io/autogen/docs/Examples/) using high-level abstractions. AutoGen simplifies the creation of diverse applications across various domains by providing pre-built systems. Additionally, it enhances LLM inference and optimization through specialized APIs, improving performance and reducing operational costs. The framework is tailored to streamline the development and deployment of agentic AI solutions.\\n\\n## Setup\\n\\nFirst, let\'s install the required dependencies. Note that pyautogen requires `python>=3.9`.\\n\\n### Environment Setup\\n\\n```shell\\n%pip install pyautogen mlflow -U -q\\n```\\n\\nWe must also get API credentials to use an LLM. For this tutorial, we\'ll be using OpenAI. Note that a great way to securely pass tokens to your interactive python environment is via the [getpass](https://docs.python.org/3/library/getpass.html) package.\\n\\n```python\\nimport os\\nfrom getpass import getpass\\n\\nos.environ[\\"OPENAI_API_KEY\\"] = getpass(\\"OPENAI_API_KEY:\\")\\n\\nassert os.getenv(\\"OPENAI_API_KEY\\"), \\"Please set an OPENAI_API_KEY environment variable.\\"\\n```\\n\\nGreat! We\'ve setup our authentication configuration and are ready to start building an agent framework.\\n\\n## Create Our Agent Framework with AutoGen and MLflow\\n\\nIn this tutorial we will be creating an image generation agent framework. There is a lot of code copied and modified from the [autogen tutorial](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_dalle_and_gpt4v.ipynb), but the core agent functionality remains the same.\\n\\n### Agent Code\\n\\nYou don\'t have to worry about the specifics of the implementation. At a high level, we are creating an agent framework that...\\n\\n1. Takes a prompt.\\n2. Leverages [OpenAI\'s DALLE](https://openai.com/index/dall-e-3/) to create an image based on that prompt.\\n3. Iteratively \\"catifies\\" e.g. adds fluffy cats to the image.\\n\\nStep 3 is where AutoGen shines. We\'re able to leverage AutoGen\'s [MultimodalConversableAgent](https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent#multimodalconversableagent) to create a critic agent that observes the images and, based on a system prompt provided by the user to \\"add fluffy cats\\", gives feedback on how the prompt should be improved.\\n\\n```python\\nimport os\\nimport re\\nfrom typing import Dict, List, Optional, Union\\n\\nimport matplotlib.pyplot as plt\\nimport PIL\\nfrom diskcache import Cache\\nfrom openai import OpenAI\\nfrom PIL import Image\\n\\nfrom autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\\nfrom autogen.agentchat.contrib.img_utils import _to_pil, get_image_data, get_pil_image\\nfrom autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\\n\\n# Define our prompt of interest\\nCRITIC_PROMPT = \\"\\"\\"Add fluffy cats. Like a lot of cats. If there\'s less than 100 cats I\'ll be mad.\\"\\"\\"\\n\\n# Define our LLM configurations\\n\\ndef dalle_call(client: OpenAI, model: str, prompt: str, size: str, quality: str, n: int) -> str:\\n    \\"\\"\\"\\n    Generate an image using OpenAI\'s DALL-E model and cache the result.\\n\\n    This function takes a prompt and other parameters to generate an image using OpenAI\'s DALL-E model.\\n    It checks if the result is already cached; if so, it returns the cached image data. Otherwise,\\n    it calls the DALL-E API to generate the image, stores the result in the cache, and then returns it.\\n\\n    Args:\\n        client (OpenAI): The OpenAI client instance for making API calls.\\n        model (str): The specific DALL-E model to use for image generation.\\n        prompt (str): The text prompt based on which the image is generated.\\n        size (str): The size specification of the image.\\n        quality (str): The quality setting for the image generation.\\n        n (int): The number of images to generate.\\n\\n    Returns:\\n    str: The image data as a string, either retrieved from the cache or newly generated.\\n\\n    Note:\\n    - The cache is stored in a directory named \'.cache/\'.\\n    - The function uses a tuple of (model, prompt, size, quality, n) as the key for caching.\\n    - The image data is obtained by making a secondary request to the URL provided by the DALL-E API response.\\n    \\"\\"\\"\\n    # Function implementation...\\n    cache = Cache(\\".cache/\\")  # Create a cache directory\\n    key = (model, prompt, size, quality, n)\\n    if key in cache:\\n        return cache[key]\\n\\n    # If not in cache, compute and store the result\\n    response = client.images.generate(\\n        model=model,\\n        prompt=prompt,\\n        size=size,\\n        quality=quality,\\n        n=n,\\n    )\\n    image_url = response.data[0].url\\n    img_data = get_image_data(image_url)\\n    cache[key] = img_data\\n\\n    return img_data\\n\\ndef extract_img(agent: Agent) -> PIL.Image:\\n    \\"\\"\\"\\n    Extracts an image from the last message of an agent and converts it to a PIL image.\\n\\n    This function searches the last message sent by the given agent for an image tag,\\n    extracts the image data, and then converts this data into a PIL (Python Imaging Library) image object.\\n\\n    Parameters:\\n        agent (Agent): An instance of an agent from which the last message will be retrieved.\\n\\n    Returns:\\n        PIL.Image: A PIL image object created from the extracted image data.\\n\\n    Note:\\n    - The function assumes that the last message contains an <img> tag with image data.\\n    - The image data is extracted using a regular expression that searches for <img> tags.\\n    - It\'s important that the agent\'s last message contains properly formatted image data for successful extraction.\\n    - The `_to_pil` function is used to convert the extracted image data into a PIL image.\\n    - If no <img> tag is found, or if the image data is not correctly formatted, the function may raise an error.\\n    \\"\\"\\"\\n    last_message = agent.last_message()[\\"content\\"]\\n\\n    if isinstance(last_message, str):\\n        img_data = re.findall(\\"<img (.*)>\\", last_message)[0]\\n    elif isinstance(last_message, list):\\n        # The GPT-4V format, where the content is an array of data\\n        assert isinstance(last_message[0], dict)\\n        img_data = last_message[0][\\"image_url\\"][\\"url\\"]\\n\\n    pil_img = get_pil_image(img_data)\\n    return pil_img\\n\\nclass DALLEAgent(ConversableAgent):\\n    def __init__(self, name, llm_config: dict, **kwargs):\\n        super().__init__(name, llm_config=llm_config, **kwargs)\\n\\n        api_key = os.getenv(\\"OPENAI_API_KEY\\")\\n        self._dalle_client = OpenAI(api_key=api_key)\\n        self.register_reply([Agent, None], DALLEAgent.generate_dalle_reply)\\n\\n    def send(\\n        self,\\n        message: Union[Dict, str],\\n        recipient: Agent,\\n        request_reply: Optional[bool] = None,\\n        silent: Optional[bool] = False,\\n    ):\\n        # override and always \\"silent\\" the send out message;\\n        # otherwise, the print log would be super long!\\n        super().send(message, recipient, request_reply, silent=True)\\n\\n    def generate_dalle_reply(self, messages: Optional[List[Dict]], sender: \\"Agent\\", config):\\n        \\"\\"\\"Generate a reply using OpenAI DALLE call.\\"\\"\\"\\n        client = self._dalle_client if config is None else config\\n        if client is None:\\n            return False, None\\n        if messages is None:\\n            messages = self._oai_messages[sender]\\n\\n        prompt = messages[-1][\\"content\\"]\\n        img_data = dalle_call(\\n            client=client,\\n            model=\\"dall-e-3\\",\\n            prompt=prompt,\\n            size=\\"1024x1024\\",\\n            quality=\\"standard\\",\\n            n=1,\\n        )\\n\\n        img_data = _to_pil(img_data)  # Convert to PIL image\\n\\n        # Return the OpenAI message format\\n        return True, {\\"content\\": [{\\"type\\": \\"image_url\\", \\"image_url\\": {\\"url\\": img_data}}]}\\n\\nclass CatifyWithDalle(AssistantAgent):\\n    def __init__(self, n_iters=2, **kwargs):\\n        \\"\\"\\"\\n        Initializes a CatifyWithDalle instance.\\n\\n        This agent facilitates the creation of visualizations through a collaborative effort among\\n        its child agents: dalle and critics.\\n\\n        Parameters:\\n            - n_iters (int, optional): The number of \\"improvement\\" iterations to run. Defaults to 2.\\n            - **kwargs: keyword arguments for the parent AssistantAgent.\\n        \\"\\"\\"\\n        super().__init__(**kwargs)\\n        self.register_reply([Agent, None], reply_func=CatifyWithDalle._reply_user, position=0)\\n        self._n_iters = n_iters\\n\\n    def _reply_user(self, messages=None, sender=None, config=None):\\n        if all((messages is None, sender is None)):\\n            error_msg = f\\"Either {messages=} or {sender=} must be provided.\\"\\n            raise AssertionError(error_msg)\\n\\n        if messages is None:\\n            messages = self._oai_messages[sender]\\n\\n        img_prompt = messages[-1][\\"content\\"]\\n\\n        ## Define the agents\\n        self.critics = MultimodalConversableAgent(\\n            name=\\"Critics\\",\\n            system_message=f\\"\\"\\"You need to improve the prompt of the figures you saw.\\n{CRITIC_PROMPT}\\nReply with the following format:\\n\\nCRITICS: the image needs to improve...\\nPROMPT: here is the updated prompt!\\n\\n\\"\\"\\",\\n            llm_config={\\"max_tokens\\": 1000, \\"model\\": \\"gpt-4o\\"},\\n            human_input_mode=\\"NEVER\\",\\n            max_consecutive_auto_reply=3,\\n        )\\n\\n        self.dalle = DALLEAgent(\\n            name=\\"Dalle\\", llm_config={\\"model\\": \\"dalle\\"}, max_consecutive_auto_reply=0\\n        )\\n\\n        # Data flow begins\\n        self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n        img = extract_img(self.dalle)\\n        plt.imshow(img)\\n        plt.axis(\\"off\\")  # Turn off axis numbers\\n        plt.show()\\n        print(\\"Image PLOTTED\\")\\n\\n        for i in range(self._n_iters):\\n            # Downsample the image s.t. GPT-4V can take\\n            img = extract_img(self.dalle)\\n            smaller_image = img.resize((128, 128), Image.Resampling.LANCZOS)\\n            smaller_image.save(\\"result.png\\")\\n\\n            self.msg_to_critics = f\\"\\"\\"Here is the prompt: {img_prompt}.\\n            Here is the figure <img result.png>.\\n            Now, critique and create a prompt so that DALLE can give me a better image.\\n            Show me both \\"CRITICS\\" and \\"PROMPT\\"!\\n            \\"\\"\\"\\n            self.send(message=self.msg_to_critics, recipient=self.critics, request_reply=True)\\n            feedback = self._oai_messages[self.critics][-1][\\"content\\"]\\n            img_prompt = re.findall(\\"PROMPT: (.*)\\", feedback)[0]\\n\\n            self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n            img = extract_img(self.dalle)\\n            plt.imshow(img)\\n            plt.axis(\\"off\\")  # Turn off axis numbers\\n            plt.show()\\n            print(f\\"Image {i} PLOTTED\\")\\n\\n        return True, \\"result.jpg\\"\\n\\n```\\n\\nGreat! We have an agent framework. To quickly show how it works, let\'s instantiate our agent and give it a prompt.\\n\\n```python\\ncreator = CatifyWithDalle(\\n    name=\\"creator\\",\\n    max_consecutive_auto_reply=0,\\n    system_message=\\"Help me coordinate generating image\\",\\n    llm_config={\\"model\\": \\"gpt-4\\"},\\n)\\n\\nuser_proxy = UserProxyAgent(\\n    name=\\"User\\",\\n    human_input_mode=\\"NEVER\\",\\n    max_consecutive_auto_reply=0,\\n    code_execution_config={\\n        \\"work_dir\\": \\"output\\", # Location where code will be written\\n        \\"use_docker\\": False # Use local jupyter execution environment instead of docker\\n    }\\n)\\n\\n_ = user_proxy.initiate_chat(\\n    creator, message=\\"Show me something boring\\"\\n)\\n```\\n\\nThe initial result from the first iteration from the user prompt:\\n\\n```text\\nUser (to creator):\\n\\nShow me something boring\\n\\ncreator (to Dalle):\\n\\nShow me something boring\\n\\n```\\n\\n![An uninspired image](_img/boring_0.png)\\n\\nThis is definitely a boring room. Notice the responses of the critics and how the critics enhance the submission prompt in the following iterations.\\n\\n```text\\nImage PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: Show me something boring.\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image is simple and mundane, with a plain room and basic furniture, which accomplishes the task of showing something boring. However, it can be improved by adding an element of whimsy or interest, juxtaposing the boring scene with something unexpected. Let\'s add a lot of cats to make it more engaging.\\n\\nPROMPT: Show me a boring living room with plain furniture, but add 100 cats in various places around the room.\\n\\ncreator (to Dalle):\\n\\nShow me a boring living room with plain furniture, but add 100 cats in various places around the room.\\n```\\n\\n![A mild improvement](_img/boring_1.png)\\n\\nOn the final iteration, we can see a more refined instruction set to add additional details.\\n\\n```text\\n\\nImage 0 PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: Show me a boring living room with plain furniture, but add 100 cats in various places around the room..\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image has successfully incorporated cats into a boring living room, bringing in an element of surprise and quirkiness. However, it is in black and white, which can make the image feel duller and less lively. Additionally, while there are many cats, they could be positioned in more playful and unexpected ways to create more interest.\\n\\nPROMPT: Show me a colorful, boring living room with plain furniture, but add 100 cats in various imaginative and playful positions around the room.\\n\\ncreator (to Dalle):\\n\\nShow me a colorful, boring living room with plain furniture, but add 100 cats in various imaginative and playful positions around the room.\\n\\n```\\n\\n![Final cat room](_img/boring_2.png)\\n\\nWithout any direct intervention, we now have an image that is remarkably different in style than the original user instruction. The agent has successfully\\nintroduced elements of whimsy into the original instruction set.\\n\\n### MLflow Model From Code\\n\\nNow that we\'ve proven the concept, it\'s time to leverage MLflow to manage our ML modeling lifecycle. For instance, it\'s highly likely that we\'d want to take this model to production, so strong dependency management, model versioning, and support for tracking between development cycles would all be useful.\\n\\nIn this blog we will leverage the [Model from Code](https://mlflow.org/docs/latest/models.html#models-from-code) feature to achieve the above functionality. MLflow Model from Code allows you to define and log models directly from a stand-alone python script. This feature is particularly useful when you want to log models that can be effectively stored as a code representation (models that do not need optimized weights through training) or applications that rely on external services (e.g., LangChain chains). Another benefit is that this approach entirely bypasses the use of the `pickle` or `cloudpickle` modules within Python, which can carry security\\n\\nTo leverage Model from Code, we must perform the following steps:\\n\\n1. Declare a [custom PyFunc](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n2. Leverage [mlflow.models.set_model](https://mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=set_model#mlflow.models.set_model) to indicate which python object is our model.\\n\\nTo achieve these steps, we simply copy the above and below code to a python file. For simplicity, you can just create a single Python file with both code snippets, but MLflow also supports specifying local dependencies when logging our model via the `code_paths` parameter in [mlflow.pyfunc.lod_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc%20log_model#mlflow.pyfunc.log_model)\\n\\n**This step was omitted for brevity and must be done manually.**\\n\\n```python\\nimport mlflow\\n\\nclass CatifyPyfunc(mlflow.pyfunc.PythonModel):\\n  def predict(self, context, model_input, params):\\n    import mlflow\\n    mlflow.autogen.autolog()\\n\\n    creator = CatifyWithDalle(\\n            name=\\"creator\\",\\n            max_consecutive_auto_reply=0,\\n            system_message=\\"Help me coordinate generating image\\",\\n            llm_config={\\"model\\":\\"gpt-4\\"},\\n        )\\n\\n    user_proxy = UserProxyAgent(name=\\"User\\", human_input_mode=\\"NEVER\\", max_consecutive_auto_reply=0, code_execution_config={\\n                \\"work_dir\\": \\"output\\", # Location where code will be written\\n                \\"use_docker\\": False # Use local jupyter execution environment instead of docker\\n            })\\n\\n    return user_proxy.initiate_chat(\\n        creator, message=model_input\\n    )\\nmlflow.models.set_model(CatifyPyfunc())\\n```\\n\\nAt the end of this step, you should have a Python file that has both code snippets. The name is up to the user, but for this blog we will use \\"catify_model.py\\".\\n\\n## Use Our Agent Framework\\n\\nWe are now positioned to leverage MLflow to interact with our powerful \\"catify\\" agent.\\n\\n### Log and Load\\n\\nFirst, let\'s demonstrate the standard user journey of logging model to MLflow\'s tracking server. We will then load it back and perform inference.\\n\\n```python\\nimport mlflow\\nmlflow.autogen.autolog() # Enable logging of traces\\n\\nwith mlflow.start_run() as run:\\n    mlflow.pyfunc.log_model(\\n        artifact_path=\\"autogen_pyfunc\\",\\n        python_model=\\"catify_model.py\\", # Our model from code python file\\n\\n    )\\n\\n    run_id = run.info.run_id\\n```\\n\\nWith our model logged, let\'s reload it and perform inference, this time with a more cool prompt.\\n\\n```python\\nloaded = mlflow.pyfunc.load_model(f\\"runs:/{run_id}/autogen_pyfunc\\")\\nout = loaded.predict(\\"The matrix with a cat\\")\\n```\\n\\nThe initial stage\'s results:\\n\\n```text\\nUser (to creator):\\n\\nThe matrix with a cat\\n\\ncreator (to Dalle):\\n\\nThe matrix with a cat\\n```\\n\\n![Initial Matrix Cat](_img/cool_0.png)\\n\\nOn the next stage, the generation prompt is greatly enhanced by the critic agent.\\n\\n```text\\nImage PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: The matrix with a cat.\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image effectively captures the Matrix-themed aesthetic with a cat, combining a cyberpunk atmosphere with digital elements. However, to improve the image:\\n\\n- Increase the number of cats to align with the requirement of having lots of cats (aim for around 100).\\n- Enhance the digital and neon elements to make the Matrix theme more pronounced.\\n- Add more movement or dynamic elements to the scene for a more immersive feel.\\n- Ensure diversity in cat appearances, sizes, and positions to make the scene more complex and interesting.\\n\\nPROMPT: \\"Create a Matrix-themed scene set in a cyberpunk alleyway, with digital and neon elements filling the atmosphere. The scene should feature around 100 cats of various sizes, colors, and positions\u2014some sitting, some walking, and some interacting with the digital elements. Make the digital grid and floating code more prominent, and add dynamic elements such as digital rain or floating holograms to create a more immersive and lively environment.\\"\\n\\ncreator (to Dalle):\\n\\n\\"Create a Matrix-themed scene set in a cyberpunk alleyway, with digital and neon elements filling the atmosphere. The scene should feature around 100 cats of various sizes, colors, and positions\u2014some sitting, some walking, and some interacting with the digital elements. Make the digital grid and floating code more prominent, and add dynamic elements such as digital rain or floating holograms to create a more immersive and lively environment.\\"\\n```\\n\\n![First Matrix Iteration](_img/cool_1.png)\\n\\nThis is definitely an improvement, show casing the power of multi-turn agents.\\n\\nThe final stage enhances the instruction set even further.\\n\\n```text\\nImage 0 PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: \\"Create a Matrix-themed scene set in a cyberpunk alleyway, with digital and neon elements filling the atmosphere. The scene should feature around 100 cats of various sizes, colors, and positions\u2014some sitting, some walking, and some interacting with the digital elements. Make the digital grid and floating code more prominent, and add dynamic elements such as digital rain or floating holograms to create a more immersive and lively environment.\\".\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image significantly improves the Matrix-themed atmosphere with a cyberpunk alley and an abundance of cats. However, there are a few areas for improvement:\\n\\n- Increase the variety of the digital elements (e.g., different shapes of holograms, varied colors and intensities of neon signs).\\n- Make the cats more dynamic by showing more interactions such as jumping, playing, or chasing digital elements.\\n- Enhance the depth and perspective of the scene to create a more three-dimensional and immersive look.\\n- Add more detail to the surrounding environment, like futuristic posters or graffiti to intensify the cyberpunk feel.\\n\\nPROMPT: \\"Craft a highly detailed, Matrix-themed scene set in a cyberpunk alleyway. The atmosphere should be rich with diverse digital and neon elements, including various shapes of holograms and a range of vivid colors. Populate the scene with around 100 dynamic cats of different sizes, colors, and actions\u2014some sitting, some walking, some jumping, playing, or chasing digital elements. Enhance the depth and perspective of the scene to create a more immersive three-dimensional experience. Include detailed futuristic environment elements like posters, graffiti, and neon signs to intensify the cyberpunk feel.\\"\\n\\ncreator (to Dalle):\\n\\n\\"Craft a highly detailed, Matrix-themed scene set in a cyberpunk alleyway. The atmosphere should be rich with diverse digital and neon elements, including various shapes of holograms and a range of vivid colors. Populate the scene with around 100 dynamic cats of different sizes, colors, and actions\u2014some sitting, some walking, some jumping, playing, or chasing digital elements. Enhance the depth and perspective of the scene to create a more immersive three-dimensional experience. Include detailed futuristic environment elements like posters, graffiti, and neon signs to intensify the cyberpunk feel.\\"\\n```\\n\\n![2nd cool image](_img/cool_2.png)\\n\\nA little dystopian, but we\'ll take it!\\n\\nWe have successfully demonstrated that we can log and load our model, then perform inference from the loaded model.\\n\\n### Show MLflow Traces\\n\\n[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) provides a thread-safe API to track the execution of complex applications. The MLflow AutoGen flavor has tracing built in as an autologging feature. So, simply by running `mlflow.autogen.autolog()` prior to doing inference, we will get traces logged automatically.\\n\\nTraces can be accessed via the fluent APIs, MLflow client, and manually via the MLflow UI. For more, please visit the documentation linked above.\\n\\n```python\\n# Example with fluent APIs\\nlast_active_trace = mlflow.get_last_active_trace()\\nprint(last_active_trace)\\n\\n# Output: Trace(request_id=71ffcf92785b4dfc965760a43193095c)\\n```\\n\\nIn the meantime, we will display the MLFlow UI here. If you are running in an interactive context, such as jupyter, run the following command.\\n\\n```python\\nimport subprocess\\nfrom IPython.display import IFrame\\n\\n# Start MLflow server in the background\\nmlflow_ui_server = subprocess.Popen([\\"mlflow\\", \\"ui\\", \\"--host\\", \\"127.0.0.1\\", \\"--port\\", \\"5000\\"])\\nIFrame(src=\\"http://127.0.0.1:5000\\", width=\\"100%\\", height=\\"600\\")\\n\\n# Run the below command to stop the server\\n# mlflow_ui_server.terminate()\\n```\\n\\nIf you\'re not running interactively, you can simply run the follow shell command and navigate to the associated host and port in your web browser.\\n\\n```bash\\nmlflow ui\\n```\\n\\nIf we navigate to the tracing tab, as shown in the image below, we can see our logged trace.\\n\\n![The MLflow Tracing UI](./_img/tracing_main_page.png)\\n\\nBy clicking on that trace ID, we can see a detailed execution plan. At the bottom, we can see our prompt `\\"The matrix with a cat\\"` which kicked off the chat session. From there, many agents interacted to create images and provide feedback to \\"catify\\" them. Also, note that the trace ID is the same as the one returned by `mlflow.get_last_active_trace()` above.\\n\\n![The MLflow Tracing UI](./_img/tracing_detail.png)\\n\\nFinally, let\'s dig a bit deeper on the tracing LLM call. As you can see, we have lots of valuable information about the execution, such as the model and usage statistics. Tracing helps you monitor not just performance, but cost, usage patterns, and much more! You can also leverage custom metadata to get even more granular insights.\\n\\n![The MLflow Tracing UI](./_img/tracing_chat_completion_1.png)\\n\\n### Logging Artifacts with MLflow\\n\\nTracing\'s primary purpose is to provide robust lightweight summaries of complex agent executions. For larger or custom payloads, MLflow exposes a variety of artifact-logging APIs that can store images, text, tables, and more in the MLflow tracking server. Let\'s quickly demonstrate this functionality by logging the prompts and their associated images.\\n\\nWithin our `CatifyWithDalle` class, we will make 4 modifications...\\n\\n1. Create an instance variable in the class `__init__` to save metadata about our objects.\\n2. Create a private utility to increment our metadata and log and images with [mlflow.log_image](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=log_image#mlflow.log_image).\\n3. Call the above utility after new images have been generated.\\n4. Finally, log our metadata object as JSON with [mlflow.log_dict](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=log_image#mlflow.log_dict).\\n\\n```python\\nimport uuid  # Add to generate artifact file names and indeces for prompt mapping to generated images\\n\\nclass CatifyWithDalle(AssistantAgent):\\n    def __init__(self, n_iters=2, **kwargs):\\n        \\"\\"\\"\\n        Initializes a CatifyWithDalle instance.\\n\\n        This agent facilitates the creation of visualizations through a collaborative effort among\\n        its child agents: dalle and critics.\\n\\n        Parameters:\\n            - n_iters (int, optional): The number of \\"improvement\\" iterations to run. Defaults to 2.\\n            - **kwargs: keyword arguments for the parent AssistantAgent.\\n        \\"\\"\\"\\n        super().__init__(**kwargs)\\n        self.register_reply([Agent, None], reply_func=CatifyWithDalle._reply_user, position=0)\\n        self._n_iters = n_iters\\n        self.dict_to_log = {}  # Add a buffer for storing mapping information\\n\\n    # Adding this method to log the generated images and the prompt-to-image mapping file\\n    def _log_image_and_append_to_dict(self, img: Image, img_prompt: str, image_index: int)-> None:\\n        \\"\\"\\" Method for logging generated images to MLflow and building a prompt mapping file \\"\\"\\"\\n        # Generate a unique ID\\n        _id = str(uuid.uuid1())\\n\\n        # Append to class variable to log once at the end of all inference\\n        self.dict_to_log[_id] = {\\"prompt\\": img_prompt, \\"index\\": image_index}\\n\\n        # Log image to MLflow tracking server\\n        mlflow.log_image(img, f\\"{_id}.png\\")\\n\\n    def _reply_user(self, messages=None, sender=None, config=None):\\n        if all((messages is None, sender is None)):\\n            error_msg = f\\"Either {messages=} or {sender=} must be provided.\\"\\n            raise AssertionError(error_msg)\\n\\n        if messages is None:\\n            messages = self._oai_messages[sender]\\n\\n        img_prompt = messages[-1][\\"content\\"]\\n\\n        ## Define the agents\\n        self.critics = MultimodalConversableAgent(\\n            name=\\"Critics\\",\\n            system_message=f\\"\\"\\"You need to improve the prompt of the figures you saw.\\n{CRITIC_PROMPT}\\nReply with the following format:\\n\\nCRITICS: the image needs to improve...\\nPROMPT: here is the updated prompt!\\n\\n\\"\\"\\",\\n            llm_config={\\"max_tokens\\": 1000, \\"model\\": \\"gpt-4o\\"},\\n            human_input_mode=\\"NEVER\\",\\n            max_consecutive_auto_reply=3,\\n        )\\n\\n        self.dalle = DALLEAgent(\\n            name=\\"Dalle\\", llm_config={\\"model\\": \\"dalle\\"}, max_consecutive_auto_reply=0\\n        )\\n\\n        # Data flow begins\\n        self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n        img = extract_img(self.dalle)\\n        plt.imshow(img)\\n        plt.axis(\\"off\\")  # Turn off axis numbers\\n        plt.show()\\n        print(\\"Image PLOTTED\\")\\n\\n        self._log_image_and_append_to_dict(img, img_prompt, -1)  # Add image logging and buffer updates\\n\\n        for i in range(self._n_iters):\\n            # Downsample the image s.t. GPT-4V can take\\n            img = extract_img(self.dalle)\\n            smaller_image = img.resize((128, 128), Image.Resampling.LANCZOS)\\n            smaller_image.save(\\"result.png\\")\\n\\n            self.msg_to_critics = f\\"\\"\\"Here is the prompt: {img_prompt}.\\n            Here is the figure <img result.png>.\\n            Now, critic and create a prompt so that DALLE can give me a better image.\\n            Show me both \\"CRITICS\\" and \\"PROMPT\\"!\\n            \\"\\"\\"\\n            self.send(message=self.msg_to_critics, recipient=self.critics, request_reply=True)\\n            feedback = self._oai_messages[self.critics][-1][\\"content\\"]\\n            img_prompt = re.findall(\\"PROMPT: (.*)\\", feedback)[0]\\n\\n            self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n            img = extract_img(self.dalle)\\n            plt.imshow(img)\\n            plt.axis(\\"off\\")  # Turn off axis numbers\\n            plt.show()\\n            print(f\\"Image {i} PLOTTED\\")\\n            self._log_image_and_append_to_dict(img, img_prompt, i)  # Log the image in the iteration\\n\\n\\n\\n        mlflow.log_dict(self.dict_to_log, \\"image_lookup.json\\")  # Log the prompt-to-image mapping buffer\\n        return True, \\"result.jpg\\"\\n```\\n\\nNow, if we rerun the above model logging code, every time we load the newest version of our model, images generated by our agent will be logged and a JSON object with all prompts, indexes of the prompts, and image names (for lookup purposes) will be logged.\\n\\nLet\'s demonstrate this and wrap infernce in a single MLflow run for easy aggregation. Also note that we will be leveraging Autogen\'s [caching](https://microsoft.github.io/autogen/docs/reference/cache/) functionality, so given we\'ve already done inference with this prompt, we won\'t actually be making LLM calls again; we\'re just reading from cache and logging with our new MLflow code.\\n\\n```python\\n# Be sure to re-log the model by rerunning the above code\\nwith mlflow.start_run(run_name=\\"log_image_during_inferfence\\"):\\n    loaded = mlflow.pyfunc.load_model(f\\"runs:/{run_id}/autogen_pyfunc\\")\\n    loaded.predict(\\"The matrix with a cat\\")\\n```\\n\\n![Logged Images and JSON Artifacts](./_img/logged_images.png)\\n\\nAs you can see, we have logged three images of interest and a lookup dict. The keys of the dict correspond to the image names and the values correspond to additional information for how the image was generated. With these artifacts we can perform detailed analyses on prompt quality and make iterative improvements to our \\"catify\\" agent!\\n\\n### Additional Benefits of MLflow\\n\\nThere is a lot more happening behind the scenes that is out of the scope of this tutorial, but here\'s a quick list of additional MLflow features that are useful when building agentic frameworks.\\n\\n- **Dependency management**: when you log a model, MLflow will automatically try to infer your pip requirements. These requirements are written in several formats that makes remote serving of your model much simpler. If you have local dependencies, as noted above, you can specify additional paths for MLflow to serialize via the `code_paths` argument when logging your model.\\n- **Model aliasing**: when iteratively building your agentic framework, you want an easy way to compare models. MLflow model [aliases and tags](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags) facilitate lookups to the MLflow model registry and allow you to easily load and deploy an specific model version.\\n- **Nested Runs**: with agentic frameworks, especially when training underlying LLM components, you will often have complex nested structures. MLflow supports [nested runs](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs.html) to facilitate aggregating your run information. This can be especially useful with LLM training and fine tuning.\\n\\n## Summary\\n\\nIn this blog we outlined how to create a complex agent with AutoGen. We also showed how to leverage the MLflow [Model from Code](https://mlflow.org/docs/latest/models.html#models-from-code) feature to log and load our model. Finally, we leveraged the MLflow AutoGen\'s autologging capabilities to automatically leverage MLflow tracing to get fine-grained and thread-safe agent execution information.\\n\\nHappy coding!"},{"id":"langgraph-model-from-code","metadata":{"permalink":"/mlflow-website/blog/langgraph-model-from-code","source":"@site/blog/2024-08-06-langgraph-model-from-code/index.md","title":"LangGraph with Model From Code","description":"In this blog, we\'ll guide you through creating a LangGraph chatbot using MLflow. By combining MLflow with LangGraph\'s ability to create and manage cyclical graphs, you can create powerful stateful, multi-actor applications in a scalable fashion.","date":"2024-08-06T00:00:00.000Z","formattedDate":"August 6, 2024","tags":[{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":7.255,"hasTruncateMarker":false,"authors":[{"name":"Michael Berk","title":"Sr. Resident Solutions Architect at Databricks","url":"https://www.linkedin.com/in/-michael-berk/","imageURL":"/img/authors/michael_berk.png","key":"michael-berk"},{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"LangGraph with Model From Code","tags":["genai","mlops"],"slug":"langgraph-model-from-code","authors":["michael-berk","mlflow-maintainers"],"thumbnail":"/img/blog/release-candidates.png"},"unlisted":false,"prevItem":{"title":"AutoGen with Custom PyFunc","permalink":"/mlflow-website/blog/autogen-image-agent"},"nextItem":{"title":"PyFunc in Practice","permalink":"/mlflow-website/blog/pyfunc-in-practice"}},"content":"In this blog, we\'ll guide you through creating a LangGraph chatbot using MLflow. By combining MLflow with LangGraph\'s ability to create and manage cyclical graphs, you can create powerful stateful, multi-actor applications in a scalable fashion.\\n\\nThroughout this post we will demonstrate how to leverage MLflow\'s capabilities to create a serializable and servable MLflow model which can easily be tracked, versioned, and deployed on a variety of servers. We\'ll be using the [langchain flavor](https://mlflow.org/docs/latest/llms/langchain/index.html) combined with MLflow\'s [model from code](https://mlflow.org/docs/latest/models.html#models-from-code) feature.\\n\\n### What is LangGraph?\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits:\\n\\n- **Cycles and Branching**: Implement loops and conditionals in your apps.\\n- **Persistence**: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\\n- **Human-in-the-Loop**: Interrupt graph execution to approve or edit next action planned by the agent.\\n- **Streaming Support**: Stream outputs as they are produced by each node (including token streaming).\\n- **Integration with LangChain**: LangGraph integrates seamlessly with LangChain.\\n\\nLangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\\n\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nFor a full walkthrough, check out the [LangGraph Quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/) and for more on the fundamentals of design with LangGraph, check out the [conceptual guides](https://langchain-ai.github.io/langgraph/concepts/#human-in-the-loop).\\n\\n## 1 - Setup\\n\\nFirst, we must install the required dependencies. We will use OpenAI for our LLM in this example, but using LangChain with LangGraph makes it easy to substitute any alternative supported LLM or LLM provider.\\n\\n```python\\n%%capture\\n%pip install langchain_openai==0.2.0 langchain==0.3.0 langgraph==0.2.27\\n%pip install -U mlflow\\n```\\n\\nNext, let\'s get our relevant secrets. `getpass`, as demonstrated in the [LangGraph quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/#setup) is a great way to insert your keys into an interactive jupyter environment.\\n\\n```python\\nimport os\\n\\n# Set required environment variables for authenticating to OpenAI\\n# Check additional MLflow tutorials for examples of authentication if needed\\n# https://mlflow.org/docs/latest/llms/openai/guide/index.html#direct-openai-service-usage\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set the OPENAI_API_KEY environment variable.\\"\\n```\\n\\n## 2 - Custom Utilities\\n\\nWhile this is a demo, it\'s good practice to separate reusable utilities into a separate file/directory. Below we create three general utilities that theoretically would valuable when building additional MLflow + LangGraph implementations.\\n\\nNote that we use the magic `%%writefile` command to create a new file in a jupyter notebook context. If you\'re running this outside of an interactive notebook, simply create the file below, omitting the `%%writefile {FILE_NAME}.py` line.\\n\\n```python\\n%%writefile langgraph_utils.py\\n# omit this line if directly creating this file; this command is purely for running within Jupyter\\n\\nimport os\\nfrom typing import Union\\nfrom langgraph.pregel.io import AddableValuesDict\\n\\ndef _langgraph_message_to_mlflow_message(\\n    langgraph_message: AddableValuesDict,\\n) -> dict:\\n    langgraph_type_to_mlflow_role = {\\n        \\"human\\": \\"user\\",\\n        \\"ai\\": \\"assistant\\",\\n        \\"system\\": \\"system\\",\\n    }\\n\\n    if type_clean := langgraph_type_to_mlflow_role.get(langgraph_message.type):\\n        return {\\"role\\": type_clean, \\"content\\": langgraph_message.content}\\n    else:\\n        raise ValueError(f\\"Incorrect role specified: {langgraph_message.type}\\")\\n\\n\\ndef get_most_recent_message(response: AddableValuesDict) -> dict:\\n    most_recent_message = response.get(\\"messages\\")[-1]\\n    return _langgraph_message_to_mlflow_message(most_recent_message)[\\"content\\"]\\n\\n\\ndef increment_message_history(\\n    response: AddableValuesDict, new_message: Union[dict, AddableValuesDict]\\n) -> list[dict]:\\n    if isinstance(new_message, AddableValuesDict):\\n        new_message = _langgraph_message_to_mlflow_message(new_message)\\n\\n    message_history = [\\n        _langgraph_message_to_mlflow_message(message)\\n        for message in response.get(\\"messages\\")\\n    ]\\n\\n    return message_history + [new_message]\\n```\\n\\nBy the end of this step, you should see a new file in your current directory with the name `langgraph_utils.py`.\\n\\nNote that it\'s best practice to add unit tests and properly organize your project into logically structured directories.\\n\\n## 3 - Log the LangGraph Model\\n\\nGreat! Now that we have some reusable utilities located in `./langgraph_utils.py`, we are ready to log the model with MLflow\'s official LangGraph flavor.\\n\\n### 3.1 - Create our Model-From-Code File\\n\\nQuickly, some background. MLflow looks to serialize model artifacts to the MLflow tracking server. Many popular ML packages don\'t have robust serialization and deserialization support, so MLflow looks to augment this functionality via the [models from code](https://mlflow.org/docs/latest/models.html#models-from-code) feature. With models from code, we\'re able to leverage Python as the serialization format, instead of popular alternatives such as JSON or pkl. This opens up tons of flexibility and stability.\\n\\nTo create a Python file with models from code, we must perform the following steps:\\n\\n1. Create a new python file. Let\'s call it `graph.py`.\\n2. Define our langgraph graph.\\n3. Leverage [mlflow.models.set_model](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.set_model) to indicate to MLflow which object in the Python script is our model of interest.\\n\\nThat\'s it!\\n\\n```python\\n%%writefile graph.py\\n# omit this line if directly creating this file; this command is purely for running within Jupyter\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.graph.state import CompiledStateGraph\\n\\nimport mlflow\\n\\nimport os\\nfrom typing import TypedDict, Annotated\\n\\ndef load_graph() -> CompiledStateGraph:\\n    \\"\\"\\"Create example chatbot from LangGraph Quickstart.\\"\\"\\"\\n\\n    assert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set the OPENAI_API_KEY environment variable.\\"\\n\\n    class State(TypedDict):\\n        messages: Annotated[list, add_messages]\\n\\n    graph_builder = StateGraph(State)\\n    llm = ChatOpenAI()\\n\\n    def chatbot(state: State):\\n        return {\\"messages\\": [llm.invoke(state[\\"messages\\"])]}\\n\\n    graph_builder.add_node(\\"chatbot\\", chatbot)\\n    graph_builder.add_edge(START, \\"chatbot\\")\\n    graph_builder.add_edge(\\"chatbot\\", END)\\n    graph = graph_builder.compile()\\n    return graph\\n\\n# Set are model to be leveraged via model from code\\nmlflow.models.set_model(load_graph())\\n```\\n\\n### 3.2 - Log with \\"Model from Code\\"\\n\\nAfter creating this implementation, we can leverage the standard MLflow APIs to log the model.\\n\\n```python\\nimport mlflow\\n\\nwith mlflow.start_run() as run_id:\\n    model_info = mlflow.langchain.log_model(\\n        lc_model=\\"graph.py\\", # Path to our model Python file\\n        artifact_path=\\"langgraph\\",\\n    )\\n\\n    model_uri = model_info.model_uri\\n```\\n\\n## 4 - Use the Logged Model\\n\\nNow that we have successfully logged a model, we can load it and leverage it for inference.\\n\\nIn the code below, we demonstrate that our chain has chatbot functionality!\\n\\n```python\\nimport mlflow\\n\\n# Custom utilities for handling chat history\\nfrom langgraph_utils import (\\n    increment_message_history,\\n    get_most_recent_message,\\n)\\n\\n# Enable tracing\\nmlflow.set_experiment(\\"Tracing example\\") # In Databricks, use an absolute path. Visit Databricks docs for more.\\nmlflow.langchain.autolog()\\n\\n# Load the model\\nloaded_model = mlflow.langchain.load_model(model_uri)\\n\\n# Show inference and message history functionality\\nprint(\\"-------- Message 1 -----------\\")\\nmessage = \\"What\'s my name?\\"\\npayload = {\\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": message}]}\\nresponse = loaded_model.invoke(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {get_most_recent_message(response)}\\")\\n\\nprint(\\"\\\\n-------- Message 2 -----------\\")\\nmessage = \\"My name is Morpheus.\\"\\nnew_messages = increment_message_history(response, {\\"role\\": \\"user\\", \\"content\\": message})\\npayload = {\\"messages\\": new_messages}\\nresponse = loaded_model.invoke(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {get_most_recent_message(response)}\\")\\n\\nprint(\\"\\\\n-------- Message 3 -----------\\")\\nmessage = \\"What is my name?\\"\\nnew_messages = increment_message_history(response, {\\"role\\": \\"user\\", \\"content\\": message})\\npayload = {\\"messages\\": new_messages}\\nresponse = loaded_model.invoke(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {get_most_recent_message(response)}\\")\\n```\\n\\nOuput:\\n\\n```text\\n-------- Message 1 -----------\\nUser: What\'s my name?\\nAgent: I\'m sorry, I cannot guess your name as I do not have access to that information. If you would like to share your name with me, feel free to do so.\\n\\n-------- Message 2 -----------\\nUser: My name is Morpheus.\\nAgent: Nice to meet you, Morpheus! How can I assist you today?\\n\\n-------- Message 3 -----------\\nUser: What is my name?\\nAgent: Your name is Morpheus.\\n```\\n\\n### 4.1 - MLflow Tracing\\n\\nBefore concluding, let\'s demonstrate [MLflow tracing](https://mlflow.org/docs/latest/llms/tracing/index.html).\\n\\nMLflow Tracing is a feature that enhances LLM observability in your Generative AI (GenAI) applications by capturing detailed information about the execution of your application\u2019s services. Tracing provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors.\\n\\nStart the MLflow server as outlined in the [tracking server docs](https://mlflow.org/docs/latest/tracking/server.html). After entering the MLflow UI, we can see our experiment and corresponding traces.\\n\\n![MLflow UI Experiment Traces](_img/mlflow_ui_experiment_traces.png)\\n\\nAs you can see, we\'ve logged our traces and can easily see them by clicking our experiment of interest and the then the \\"Tracing\\" tab.\\n\\n![MLflow UI Trace](_img/mlflow_ui_trace.png)\\n\\nAfter clicking on one of the traces, we can now see run execution for a single query. Notice that we log inputs, outputs, and lots of great metadata such as usage and invocation parameters. As we scale our application both from a usage and complexity perspective, this thread-safe and highly-performant tracking system will ensure robust monitoring of the app.\\n\\n## 5 - Summary\\n\\nThere are many logical extensions of the this tutorial, however the MLflow components can remain largely unchanged. Some examples include persisting chat history to a database, implementing a more complex langgraph object, productionizing this solution, and much more!\\n\\nTo summarize, here\'s what was covered in this tutorial:\\n\\n- Creating a simple LangGraph chain.\\n- Leveraging MLflow [model from code](https://mlflow.org/docs/latest/models.html#models-from-code) functionality to log our graph.\\n- Loading the model via the standard MLflow APIs.\\n- Leveraging [MLflow tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) to view graph execution.\\n\\nHappy coding!"},{"id":"pyfunc-in-practice","metadata":{"permalink":"/mlflow-website/blog/pyfunc-in-practice","source":"@site/blog/2024-07-26-pyfunc-in-practice/index.md","title":"PyFunc in Practice","description":"Creative Applications of MLflow Pyfunc in Machine Learning Projects","date":"2024-07-26T00:00:00.000Z","formattedDate":"July 26, 2024","tags":[{"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"label":"ensemble-models","permalink":"/mlflow-website/blog/tags/ensemble-models"}],"readingTime":22.055,"hasTruncateMarker":true,"authors":[{"name":"Hugo Carvalho","title":"Machine Learning Analyst at adidas","url":"https://www.linkedin.com/in/hugodscarvalho/","imageURL":"/img/authors/hugo_carvalho.png","key":"hugo-carvalho"},{"name":"Joana Ferreira","title":"Machine Learning Engineer at adidas","url":"https://www.linkedin.com/in/joanaferreira96/","imageURL":"/img/authors/joana_ferreira.png","key":"joana-ferreira"},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey"},{"name":"Filipe Miranda","title":"Sr. Data Engineer at adidas","url":"https://www.linkedin.com/in/filipe-miranda-b576b186/","imageURL":"/img/authors/filipe_miranda.png","key":"filipe-miranda"}],"frontMatter":{"title":"PyFunc in Practice","description":"Creative Applications of MLflow Pyfunc in Machine Learning Projects","tags":["pyfunc","mlflow","ensemble-models"],"slug":"pyfunc-in-practice","authors":["hugo-carvalho","joana-ferreira","rahul-pandey","filipe-miranda"],"thumbnail":"/img/blog/pyfunc-in-practice.png"},"unlisted":false,"prevItem":{"title":"LangGraph with Model From Code","permalink":"/mlflow-website/blog/langgraph-model-from-code"},"nextItem":{"title":"Introducing MLflow Tracing","permalink":"/mlflow-website/blog/mlflow-tracing"}},"content":"If you\'re looking to fully leverage the capabilities of `mlflow.pyfunc` and understand how it can be utilized in a Machine Learning project, this blog post will guide you through the process. MLflow PyFunc offers creative freedom and flexibility, allowing the development of complex systems encapsulated as models in MLflow that follow the same lifecycle as traditional ones. This blog will showcase how to create multi-model setups, seamlessly connect to databases, and implement your own custom fit method in your MLflow PyFunc model.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nThis blog post demonstrates the capabilities of [MLflow PyFunc](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and how it can be utilized to build a multi-model setup encapsulated as a PyFunc flavor model in MLflow. This approach allows ensemble models to follow the same lifecycle as traditional [Built-In Model Flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors) in MLflow.\\n\\nBut first, let\'s use an analogy to get you familiarized with the concept of ensemble models and why you should consider this solution in your next Machine Learning project.\\n\\nImagine you are in the market to buy a house. Would you make a decision based solely on the first house you visit and the advice of a single real estate agent? Of course not! The process of buying a house involves considering multiple factors and gathering information from various sources to make an informed decision.\\n\\nThe house buying process explained:\\n\\n- **Identify Your Needs**: Determine whether you want a new or used house, the type of house, the model, and the year of construction.\\n- **Research**: Look for a list of available houses, check for discounts and offers, read customer reviews, and seek opinions from friends and family.\\n- **Evaluate**: Consider the performance, location, neighborhood amenities, and price range.\\n- **Compare**: Compare multiple houses to find the best fit for your needs and budget.\\n\\nIn short, you wouldn\u2019t directly reach a conclusion but would instead make a decision considering all the aforementioned factors before deciding on the best choice.\\n\\nEnsemble models in Machine Learning operate on a similar idea. Ensemble learning helps improve Machine Learning results by combining several models to improve predictive performance compared to a single model. The performance increase can be due to several factors such as the reduction in variance by averaging multiple models or reducing bias by focusing on errors of previous models. There are several types of ensemble learning techniques exists such as:\\n\\n- **Averaging**\\n- **Weighted Averaging**\\n- **Bagging**\\n- **Boosting**\\n- **Stacking**\\n\\nHowever, developing such systems requires careful management of the lifecycle of ensemble models, as integrating diverse models can be highly complex. This is where MLflow PyFunc becomes invaluable. It offers the flexibility to build complex systems, treating the entire ensemble as a model that adheres to the same lifecycle processes as traditional models. Essentially, MLflow PyFunc allows the creation of custom methods tailored to ensemble models, serving as an alternative to the built-in MLflow flavors available for popular frameworks such as scikit-learn, PyTorch, and LangChain.\\n\\nThis blog utilizes the house price dataset from [Kaggle](https://www.kaggle.com/) to demonstrate the development and management of ensemble models through MLflow.\\n\\nWe will leverage various tools and technologies to highlight the capabilities of MLflow PyFunc models. Before delving into the ensemble model itself, we will explore how these components integrate to create a robust and efficient Machine Learning pipeline.\\n\\n### Components of the Project\\n\\n**DuckDB**  \\nDuckDB is a high-performance analytical database system designed to be fast, reliable, portable, and easy to use. In this project, it showcases the integration of a database connection within the model context, facilitating efficient data handling directly within the model. [Learn more about DuckDB](https://duckdb.org/).\\n\\n**scikit-learn (sklearn)**  \\nscikit-learn is a Machine Learning library for Python that provides efficient tools for data analysis and modelling. In this project, it is used to develop and evaluate various Machine Learning models that are integrated into our ensemble model. [Learn more about scikit-learn](https://scikit-learn.org/).\\n\\n**MLflow**  \\nMLflow is an open-source platform for managing the end-to-end Machine Learning lifecycle, including experimentation, reproducibility, and deployment. In this project, it tracks experiments, manages model versions, and facilitates the deployment of MLflow PyFunc models in a similar manner to how we are familiar with individual flavors. [Learn more about MLflow](https://mlflow.org/).\\n\\n> **Note:** To reproduce this project, please refer to the official MLflow documentation for more details on setting up a simple local [MLflow Tracking Server](https://mlflow.org/docs/latest/tracking/server.html).\\n\\n## Creating the Ensemble Model\\n\\nCreating a MLflow PyFunc ensemble model requires additional steps compared to using the built-in flavors for logging and working with popular Machine Learning frameworks.\\n\\nTo implement an ensemble model, you need to define an `mlflow.pyfunc` model, which involves creating a Python class that inherits from the `PythonModel` class and implementing its constructor and class methods. While the basic creation of a PyFunc model only requires implementing the `predict` method, an ensemble model requires additional methods to manage the models and obtain multi-model predictions. After instantiating the ensemble model, you must use the custom `fit` method to train the ensemble model\'s sub-models. Similar to an out-of-the-box MLflow model, you need to log the model along with its artifacts during the training run and then register the model in the MLflow Model Registry. A model alias `production` will also be added to the model to streamline both model updates and inference. Model aliases allow you to assign a mutable, named reference to a specific version of a registered model. By assigning the alias to a particular model version, it can be easily referenced via a model URI or the model registry API. This setup allows for seamless updates to the model version used for inference without changing the serving workload code. For more details, refer to [Deploy and Organize Models with Aliases and Tags](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags).\\n\\nThe following sections, as depicted in the diagram, detail the implementation of each method for the ensemble model, providing a comprehensive understanding of defining, managing, and utilizing an ensemble model with MLflow PyFunc.\\n\\n![Ensemble Model Architecture](ensemble-model-architecture.png)\\n\\nBefore delving into the detailed implementation of each method, let\'s first review the skeleton of our `EnsembleModel` class. This skeleton serves as a blueprint for understanding the structure of the ensemble model. The subsequent sections will provide an overview and code for both the default methods provided by MLflow PyFunc and the custom methods implemented for the ensemble model.\\n\\nHere is the skeleton of the `EnsembleModel` class:\\n\\n```python\\nimport mlflow\\n\\nclass EnsembleModel(mlflow.pyfunc.PythonModel):\\n    \\"\\"\\"Ensemble model class leveraging Pyfunc for multi-model integration in MLflow.\\"\\"\\"\\n\\n    def __init__(self):\\n        \\"\\"\\"Initialize the EnsembleModel instance.\\"\\"\\"\\n        ...\\n\\n    def add_strategy_and_save_to_db(self):\\n        \\"\\"\\"Add strategies to the DuckDB database.\\"\\"\\"\\n        ...\\n\\n    def feature_engineering(self):\\n        \\"\\"\\"Perform feature engineering on input data.\\"\\"\\"\\n        ...\\n\\n    def initialize_models(self):\\n        \\"\\"\\"Initialize models and their hyperparameter grids.\\"\\"\\"\\n        ...\\n\\n    def fit(self):\\n        \\"\\"\\"Train the ensemble of models.\\"\\"\\"\\n        ...\\n\\n    def predict(self):\\n        \\"\\"\\"Predict using the ensemble of models.\\"\\"\\"\\n        ...\\n\\n    def load_context(self):\\n        \\"\\"\\"Load the preprocessor and models from the MLflow context.\\"\\"\\"\\n        ...\\n```\\n\\n### Initializing the EnsembleModel\\n\\nThe constructor method in the ensemble model is crucial for setting up its essential elements. It establishes key attributes such as the preprocessor, a dictionary to store trained models, the path to a DuckDB database, and a pandas DataFrame for managing different ensemble strategies. Additionally, it takes advantage of the `initialize_models` method to define the sub-models integrated into the ensemble.\\n\\n```python\\nimport pandas as pd\\n\\ndef __init__(self):\\n    \\"\\"\\"\\n    Initializes the EnsembleModel instance.\\n\\n    Sets up an empty preprocessing pipeline, a dictionary for fitted models,\\n    and a DataFrame to store strategies. Also calls the method to initialize sub-models.\\n    \\"\\"\\"\\n    self.preprocessor = None\\n    self.fitted_models = {}\\n    self.db_path = None\\n    self.strategies = pd.DataFrame(columns=[\\"strategy\\", \\"model_list\\", \\"weights\\"])\\n    self.initialize_models()\\n```\\n\\n### Adding Strategies and Saving to the Database\\n\\nThe custom-defined `add_strategy_and_save_to_db` method enables the addition of new ensemble strategies to the model and their storage in a DuckDB database. This method accepts a pandas DataFrame containing the strategies and the database path as inputs. It appends the new strategies to the existing ones and saves them in the database specified during the initialization of the ensemble model. This method facilitates the management of various ensemble strategies and ensures their persistent storage for future use.\\n\\n```python\\nimport duckdb\\nimport pandas as pd\\n\\ndef add_strategy_and_save_to_db(self, strategy_df: pd.DataFrame, db_path: str) -> None:\\n    \\"\\"\\"Add strategies from a DataFrame and save them to the DuckDB database.\\n\\n    Args:\\n        strategy_df (pd.DataFrame): DataFrame containing strategies.\\n        db_path (str): Path to the DuckDB database.\\n    \\"\\"\\"\\n    # Update the instance-level database path for the current object\\n    self.db_path = db_path\\n\\n    # Attempt to concatenate new strategies with the existing DataFrame\\n    try:\\n        self.strategies = pd.concat([self.strategies, strategy_df], ignore_index=True)\\n    except Exception as e:\\n        # Print an error message if any exceptions occur during concatenation\\n        print(f\\"Error concatenating DataFrames: {e}\\")\\n        return  # Exit early to prevent further errors\\n\\n    # Use context manager for the database connection\\n    try:\\n        with duckdb.connect(self.db_path) as con:\\n            # Register the strategies DataFrame as a temporary table in DuckDB\\n            con.register(\\"strategy_df\\", self.strategies)\\n\\n            # Drop any existing strategies table and create a new one with updated strategies\\n            con.execute(\\"DROP TABLE IF EXISTS strategies\\")\\n            con.execute(\\"CREATE TABLE strategies AS SELECT * FROM strategy_df\\")\\n    except Exception as e:\\n        # Print an error message if any exceptions occur during database operations\\n        print(f\\"Error executing database operations: {e}\\")\\n```\\n\\nThe following example demonstrates how to use this method to add strategies to the database.\\n\\n```python\\nimport pandas as pd\\n\\n# Initialize ensemble model\\nensemble_model = EnsembleModel()\\n\\n# Define strategies for the ensemble model\\nstrategy_data = {\\n    \\"strategy\\": [\\"average_1\\"],\\n    \\"model_list\\": [\\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\"],\\n    \\"weights\\": [\\"1\\"],\\n}\\n\\n# Create a DataFrame to hold the strategy information\\nstrategies_df = pd.DataFrame(strategy_data)\\n\\n# Add strategies to the database\\nensemble_model.add_strategy_and_save_to_db(strategies_df, \\"models/strategies.db\\")\\n```\\n\\nThe DataFrame `strategy_data` includes:\\n\\n- **strategy**: The name of the strategy for model predictions.\\n- **model_list**: A comma-separated list of model names included in the strategy.\\n- **weights**: A comma-separated list of weights assigned to each model in the `model_list`. If not provided, implies equal weights or default values.\\n\\n| strategy  | model_list                                                     | weights |\\n| --------- | -------------------------------------------------------------- | ------- |\\n| average_1 | random_forest,xgboost,decision_tree,gradient_boosting,adaboost | 1       |\\n\\n### Feature Engineering\\n\\nThe `feature_engineering` method preprocesses input data by handling missing values, scaling numerical features, and encoding categorical features. It applies different transformations to both numerical and categorical features, and returns the processed features as a NumPy array. This method is crucial for preparing data in a suitable format for model training, ensuring consistency and enhancing model performance.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ndef feature_engineering(self, X: pd.DataFrame) -> np.ndarray:\\n    \\"\\"\\"\\n    Applies feature engineering to the input data X, including imputation, scaling, and encoding.\\n\\n    Args:\\n        X (pd.DataFrame): Input features with potential categorical and numerical columns.\\n\\n    Returns:\\n        np.ndarray: Processed feature array after transformations.\\n    \\"\\"\\"\\n    # Convert columns with \'object\' dtype to \'category\' dtype for proper handling of categorical features\\n    X = X.apply(\\n        lambda col: col.astype(\\"category\\") if col.dtypes == \\"object\\" else col\\n    )\\n\\n    # Identify categorical and numerical features from the DataFrame\\n    categorical_features = X.select_dtypes(include=[\\"category\\"]).columns\\n    numerical_features = X.select_dtypes(include=[\\"number\\"]).columns\\n\\n    # Define the pipeline for numerical features: imputation followed by scaling\\n    numeric_transformer = Pipeline(\\n        steps=[\\n            (\\n                \\"imputer\\",\\n                SimpleImputer(strategy=\\"median\\"),\\n            ),  # Replace missing values with the median\\n            (\\n                \\"scaler\\",\\n                StandardScaler(),\\n            ),  # Standardize features by removing the mean and scaling to unit variance\\n        ]\\n    )\\n\\n    # Define the pipeline for categorical features: imputation followed by one-hot encoding\\n    categorical_transformer = Pipeline(\\n        steps=[\\n            (\\n                \\"imputer\\",\\n                SimpleImputer(strategy=\\"most_frequent\\"),\\n            ),  # Replace missing values with the most frequent value\\n            (\\n                \\"onehot\\",\\n                OneHotEncoder(handle_unknown=\\"ignore\\"),\\n            ),  # Encode categorical features as a one-hot numeric array\\n        ]\\n    )\\n\\n    # Create a ColumnTransformer to apply the appropriate pipelines to the respective feature types\\n    preprocessor = ColumnTransformer(\\n        transformers=[\\n            (\\n                \\"num\\",\\n                numeric_transformer,\\n                numerical_features,\\n            ),  # Apply the numeric pipeline to numerical features\\n            (\\n                \\"cat\\",\\n                categorical_transformer,\\n                categorical_features,\\n            ),  # Apply the categorical pipeline to categorical features\\n        ]\\n    )\\n\\n    # Fit and transform the input data using the preprocessor\\n    X_processed = preprocessor.fit_transform(X)\\n\\n    # Store the preprocessor for future use in the predict method\\n    self.preprocessor = preprocessor\\n    return X_processed\\n```\\n\\n### Initializing Models\\n\\nThe `initialize_models` method sets up a dictionary of various Machine Learning models along with their hyperparameter grids. This includes models such as `RandomForest`, `XGBoost`, `DecisionTree`, `GradientBoosting`, and `AdaBoost`. This step is crucial for preparing the ensemble\u2019s sub-models and specifying the hyperparameters to adjust during training, ensuring that each model is configured correctly and ready for training.\\n\\n```python\\nfrom sklearn.ensemble import (\\n    AdaBoostRegressor,\\n    GradientBoostingRegressor,\\n    RandomForestRegressor,\\n)\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom xgboost import XGBRegressor\\n\\ndef initialize_models(self) -> None:\\n    \\"\\"\\"\\n    Initializes a dictionary of models along with their hyperparameter grids for grid search.\\n    \\"\\"\\"\\n    # Define various regression models with their respective hyperparameter grids for tuning\\n    self.models = {\\n        \\"random_forest\\": (\\n            RandomForestRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [None, 10, 20]},\\n        ),\\n        \\"xgboost\\": (\\n            XGBRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [3, 6, 10]},\\n        ),\\n        \\"decision_tree\\": (\\n            DecisionTreeRegressor(random_state=42),\\n            {\\"max_depth\\": [None, 10, 20]},\\n        ),\\n        \\"gradient_boosting\\": (\\n            GradientBoostingRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [3, 5, 7]},\\n        ),\\n        \\"adaboost\\": (\\n            AdaBoostRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"learning_rate\\": [0.01, 0.1, 1.0]},\\n        ),\\n    }\\n```\\n\\n### Defining a Custom `fit` Method to Train and Save Multi-Models\\n\\nAs already highlighted in the previous method, a key feature of MLflow PyFunc models is the ability to define custom methods, providing significant flexibility and customization for various tasks. In the multi-model PyFunc setup, the `fit` method is essential for customizing and optimizing multiple sub-models. It manages the training and fine-tuning of algorithms such as `RandomForestRegressor`, `XGBRegressor`, `DecisionTreeRegressor`, `GradientBoostingRegressor`, and `AdaBoostRegressor`. For demonstration purposes, Grid Search is used, which, while straightforward, can be computationally intensive and time-consuming, especially for ensemble models. To enhance efficiency, advanced optimization methods such as Bayesian optimization are recommended. Tools like [Optuna](https://optuna.org/) and [Hyperopt](https://hyperopt.github.io/hyperopt/) leverage probabilistic models to intelligently navigate the search space, significantly reducing the number of evaluations needed to identify optimal configurations.\\n\\n```python\\nimport os\\n\\nimport joblib\\nimport pandas as pd\\nfrom sklearn.model_selection import GridSearchCV\\n\\ndef fit(\\n        self, X_train_processed: pd.DataFrame, y_train: pd.Series, save_path: str\\n    ) -> None:\\n    \\"\\"\\"\\n    Trains the ensemble of models using the provided preprocessed training data.\\n\\n    Args:\\n        X_train_processed (pd.DataFrame): Preprocessed feature matrix for training.\\n        y_train (pd.Series): Target variable for training.\\n        save_path (str): Directory path where trained models will be saved.\\n    \\"\\"\\"\\n    # Create the directory for saving models if it does not exist\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    # Iterate over each model and its parameter grid\\n    for model_name, (model, param_grid) in self.models.items():\\n        # Perform GridSearchCV to find the best hyperparameters for the current model\\n        grid_search = GridSearchCV(\\n            model, param_grid, cv=5, n_jobs=-1, scoring=\\"neg_mean_squared_error\\"\\n        )\\n        grid_search.fit(\\n            X_train_processed, y_train\\n        )  # Fit the model with the training data\\n\\n        # Save the best estimator from GridSearchCV\\n        best_model = grid_search.best_estimator_\\n        self.fitted_models[model_name] = best_model\\n\\n        # Save the trained model to disk\\n        joblib.dump(best_model, os.path.join(save_path, f\\"{model_name}.pkl\\"))\\n```\\n\\n### Defining a Custom `predict` Method to Aggregate Multi-model Predictions\\n\\nTo streamline the inference process, every PyFunc model should define a custom `predict` method as the single entry point for inference. This approach abstracts the model\'s internal workings at inference time, whether dealing with a custom PyFunc model or an out-of-the-box MLflow built-in flavor for popular ML frameworks.\\n\\nThe custom `predict` method for the ensemble model is designed to collect and combine predictions from the sub-models, supporting various aggregation strategies (e.g., average, weighted). The process involves the following steps:\\n\\n1. Load the sub-model predictions aggregation strategy based on the user-defined approach.\\n2. Load the models to be used for inference.\\n3. Preprocess the input data.\\n4. Collect predictions from individual models.\\n5. Aggregate the model predictions according to the specified strategy.\\n\\n```python\\nimport duckdb\\nimport joblib\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ndef predict(self, context, model_input: pd.DataFrame) -> np.ndarray:\\n    \\"\\"\\"\\n    Predicts the target variable using the ensemble of models based on the selected strategy.\\n\\n    Args:\\n        context: MLflow context object.\\n        model_input (pd.DataFrame): Input features for prediction.\\n\\n    Returns:\\n        np.ndarray: Array of predicted values.\\n\\n    Raises:\\n        ValueError: If the strategy is unknown or no models are fitted.\\n    \\"\\"\\"\\n    # Check if the \'strategy\' column is present in the input DataFrame\\n    if \\"strategy\\" in model_input.columns:\\n        # Extract the strategy and drop it from the input features\\n        print(f\\"Strategy: {model_input[\'strategy\'].iloc[0]}\\")\\n        strategy = model_input[\\"strategy\\"].iloc[0]\\n        model_input.drop(columns=[\\"strategy\\"], inplace=True)\\n    else:\\n        # Default to \'average\' strategy if none is provided\\n        strategy = \\"average\\"\\n\\n    # Load the strategy details from the pre-loaded strategies DataFrame\\n    loaded_strategy = self.strategies[self.strategies[\\"strategy\\"] == strategy]\\n\\n    if loaded_strategy.empty:\\n        # Raise an error if the specified strategy is not found\\n        raise ValueError(\\n            f\\"Strategy \'{strategy}\' not found in the pre-loaded strategies.\\"\\n        )\\n\\n    # Parse the list of models to be used for prediction\\n    model_list = loaded_strategy[\\"model_list\\"].iloc[0].split(\\",\\")\\n\\n    # Transform input features using the preprocessor, if available\\n    if self.preprocessor is None:\\n        # Feature engineering is required if the preprocessor is not set\\n        X_processed = self.feature_engineering(model_input)\\n    else:\\n        # Use the existing preprocessor to transform the features\\n        X_processed = self.preprocessor.transform(model_input)\\n\\n    if not self.fitted_models:\\n        # Raise an error if no models are fitted\\n        raise ValueError(\\"No fitted models found. Please fit the models first.\\")\\n\\n    # Collect predictions from all models specified in the strategy\\n    predictions = np.array(\\n        [self.fitted_models[model].predict(X_processed) for model in model_list]\\n    )\\n\\n    # Apply the specified strategy to combine the model predictions\\n    if \\"average\\" in strategy:\\n        # Calculate the average of predictions from all models\\n        return np.mean(predictions, axis=0)\\n    elif \\"weighted\\" in strategy:\\n        # Extract weights from the strategy and normalize them\\n        weights = [float(w) for w in loaded_strategy[\\"weights\\"].iloc[0].split(\\",\\")]\\n        weights = np.array(weights)\\n        weights /= np.sum(weights)  # Ensure weights sum to 1\\n\\n        # Compute the weighted average of predictions\\n        return np.average(predictions, axis=0, weights=weights)\\n    else:\\n        # Raise an error if an unknown strategy is encountered\\n        raise ValueError(f\\"Unknown strategy: {strategy}\\")\\n```\\n\\n### Defining a `load context` custom method to initialize the Ensemble Model\\n\\nWhen loading the ensemble model using `mlflow.pyfunc.load_model`, the custom `load_context` method is executed to handle the required model initialization steps before inference.\\n\\nThis initialization process includes:\\n\\n1. Loading model artifacts, including both the pre-trained models and the preprocessor, using the context object that contains the artifacts references.\\n2. Fetching strategies definitions from DuckDB Database.\\n\\n```python\\nimport duckdb\\nimport joblib\\nimport pandas as pd\\n\\ndef load_context(self, context) -> None:\\n    \\"\\"\\"\\n    Loads the preprocessor and models from the MLflow context.\\n\\n    Args:\\n        context: MLflow context object which provides access to saved artifacts.\\n    \\"\\"\\"\\n    # Load the preprocessor if its path is specified in the context artifacts\\n    preprocessor_path = context.artifacts.get(\\"preprocessor\\", None)\\n    if preprocessor_path:\\n        self.preprocessor = joblib.load(preprocessor_path)\\n\\n    # Load each model from the context artifacts and store it in the fitted_models dictionary\\n    for model_name in self.models.keys():\\n        model_path = context.artifacts.get(model_name, None)\\n        if model_path:\\n            self.fitted_models[model_name] = joblib.load(model_path)\\n        else:\\n            # Print a warning if a model is not found in the context artifacts\\n            print(\\n                f\\"Warning: {model_name} model not found in artifacts. Initialized but not fitted.\\"\\n            )\\n\\n    # Reconnect to the DuckDB database to load the strategies\\n    conn = duckdb.connect(self.db_path)\\n    # Fetch strategies from the DuckDB database into the strategies DataFrame\\n    self.strategies = conn.execute(\\"SELECT * FROM strategies\\").fetchdf()\\n    # Close the database connection\\n    conn.close()\\n```\\n\\n### Bringing It All Together\\n\\nHaving explored each method in detail, the next step is to integrate them to observe the complete implementation in action. This will offer a comprehensive view of how the components interact to achieve the project\'s objectives.\\n\\nYou can use the skeleton provided in the [Creating the Ensemble Model](#creating-the-ensemble-model) section to assemble the entire `EnsembleModel` class. Each method was demonstrated with its specific dependencies included. Now, you just need to combine these methods into the class definition, following the outline given. Feel free to add any custom logic that fits your specific use case or enhances the functionality of the ensemble model.\\n\\nAfter everything has been encapsulated in a PyFunc model, the lifecycle of the ensemble model closely mirrors that of a traditional MLflow model. The following diagram depicts the lifecycle of the model.\\n\\n![Ensemble Model Lifecycle](ensemble-model-lifecycle.png)\\n\\n## MLflow Tracking\\n\\n### Using the `fit` Method to Train Sub-Models\\n\\nOnce the data is preprocessed, we use the custom `fit` method to train all the sub-models in our Ensemble Model. This method applies grid search to find the best hyperparameters for each sub-model, fits them to the training data, and saves the trained models for future use.\\n\\n> **Note:** For the following block of code, you might need to set the MLflow Tracking Server if you\'re not using Managed MLflow. In the [Components of the Project](#components-of-the-project), there\'s a note about setting up a simple local MLflow Tracking Server. For this step of the project, you\'ll need to point MLflow to the server\u2019s URI that has been configured and is currently running. Don\'t forget to set the server URI variable `remote_server_uri`. You can refer to the official MLflow documentation for more details on [Logging to a Tracking Server](https://mlflow.org/docs/latest/tracking/server.html#logging-to-a-tracking-server).\\n\\n```python\\nimport datetime\\nimport os\\n\\nimport joblib\\nimport mlflow\\nimport pandas as pd\\nfrom mlflow.models.signature import infer_signature\\nfrom sklearn.model_selection import train_test_split\\n\\n# Initialize the MLflow client\\nclient = mlflow.MlflowClient()\\n\\n# Set the URI of your MLflow Tracking Server\\nremote_server_uri = \\"...\\"  # Replace with your server URI\\n\\n# Point MLflow to your MLflow Tracking Server\\nmlflow.set_tracking_uri(remote_server_uri)\\n\\n# Set the experiment name for organizing runs in MLflow\\nmlflow.set_experiment(\\"Ensemble Model\\")\\n\\n# Load dataset from the provided URL\\ndata = pd.read_csv(\\n    \\"https://github.com/zobi123/Machine-Learning-project-with-Python/blob/master/Housing.csv?raw=true\\"\\n)\\n\\n# Separate features and target variable\\nX = data.drop(\\"price\\", axis=1)\\ny = data[\\"price\\"]\\n\\n# Split dataset into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)\\n\\n# Create a directory to save the models and related files\\nos.makedirs(\\"models\\", exist_ok=True)\\n\\n# Initialize and train the EnsembleModel\\nensemble_model = EnsembleModel()\\n\\n# Preprocess the training data using the defined feature engineering method\\nX_train_processed = ensemble_model.feature_engineering(X_train)\\n\\n# Fit the models with the preprocessed training data and save them\\nensemble_model.fit(X_train_processed, y_train, save_path=\\"models\\")\\n\\n# Infer the model signature using a small example from the training data\\nexample_input = X_train[:1]  # Use a single sample for signature inference\\nexample_input[\\"strategy\\"] = \\"average\\"\\nexample_output = y_train[:1]\\nsignature = infer_signature(example_input, example_output)\\n\\n# Save the preprocessing pipeline to disk\\njoblib.dump(ensemble_model.preprocessor, \\"models/preprocessor.pkl\\")\\n\\n# Define strategies for the ensemble model\\nstrategy_data = {\\n    \\"strategy\\": [\\n        \\"average_1\\",\\n        \\"average_2\\",\\n        \\"weighted_1\\",\\n        \\"weighted_2\\",\\n        \\"weighted_3\\",\\n        \\"weighted_4\\",\\n    ],\\n    \\"model_list\\": [\\n        \\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\",\\n        \\"decision_tree\\",\\n        \\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\",\\n        \\"random_forest,xgboost,gradient_boosting\\",\\n        \\"decision_tree,adaboost\\",\\n        \\"xgboost,gradient_boosting\\",\\n    ],\\n    \\"weights\\": [\\"1\\", \\"1\\", \\"0.2,0.3,0.1,0.2,0.2\\", \\"0.4,0.4,0.2\\", \\"0.5,0.5\\", \\"0.7,0.3\\"],\\n}\\n\\n# Create a DataFrame to hold the strategy information\\nstrategies_df = pd.DataFrame(strategy_data)\\n\\n# Add strategies to the database\\nensemble_model.add_strategy_and_save_to_db(strategies_df, \\"models/strategies.db\\")\\n\\n# Define the Conda environment configuration for the MLflow model\\nconda_env = {\\n    \\"name\\": \\"mlflow-env\\",\\n    \\"channels\\": [\\"conda-forge\\"],\\n    \\"dependencies\\": [\\n        \\"python=3.8\\",\\n        \\"scikit-learn=1.3.0\\",\\n        \\"xgboost=2.0.3\\",\\n        \\"joblib=1.2.0\\",\\n        \\"pandas=1.5.3\\",\\n        \\"numpy=1.23.5\\",\\n        \\"duckdb=1.0.0\\",\\n        {\\n            \\"pip\\": [\\n                \\"mlflow==2.14.1\\",\\n            ]\\n        },\\n    ],\\n}\\n\\n# Get current timestamp\\ntimestamp = datetime.datetime.now().isoformat()\\n\\n# Log the model using MLflow\\nwith mlflow.start_run(run_name=timestamp) as run:\\n    # Log parameters, artifacts, and model signature\\n    mlflow.log_param(\\"model_type\\", \\"EnsembleModel\\")\\n\\n    artifacts = {\\n        model_name: os.path.join(\\"models\\", f\\"{model_name}.pkl\\")\\n        for model_name in ensemble_model.models.keys()\\n    }\\n    artifacts[\\"preprocessor\\"] = os.path.join(\\"models\\", \\"preprocessor.pkl\\")\\n    artifacts[\\"strategies_db\\"] = os.path.join(\\"models\\", \\"strategies.db\\")\\n\\n    mlflow.pyfunc.log_model(\\n        artifact_path=\\"ensemble_model\\",\\n        python_model=ensemble_model,\\n        artifacts=artifacts,\\n        conda_env=conda_env,\\n        signature=signature,\\n    )\\n\\n    print(f\\"Model logged in run {run.info.run_id}\\")\\n```\\n\\n### Registering the Model with MLflow\\n\\nFollowing the completion of model training, the subsequent step involves registering the ensemble model with MLflow. This process entails logging the trained models, preprocessing pipelines, and associated strategies into the MLflow Tracking Server. This ensures that all components of the ensemble model are systematically saved and versioned, facilitating reproducibility and traceability.\\n\\nMoreover, we will assign to this initial version of the model a production alias. This designation establishes a baseline model against which future iterations can be assessed. By marking this version as the `production` model, we can effectively benchmark improvements and confirm that subsequent versions yield measurable advancements over this established baseline.\\n\\n```python\\n# Register the model in MLflow and assign a production alias\\nmodel_uri = f\\"runs:/{run.info.run_id}/ensemble_model\\"\\nmodel_details = mlflow.register_model(model_uri=model_uri, name=\\"ensemble_model\\")\\n\\nclient.set_registered_model_alias(\\n\\tname=\\"ensemble_model\\", alias=\\"production\\", version=model_details.version\\n)\\n```\\n\\nThe following illustration demonstrates the complete lifecycle of our ensemble model within the MLflow UI up until this step.\\n\\n![Ensemble Model within MLflow UI](ensemble-model-mlflow-ui.gif)\\n\\n### Using the `predict` Method to Perform Inference\\n\\nWith the ensemble model registered in the MLflow Model Registry, it can now be utilized to predict house prices by aggregating the predictions from the various sub-models within the ensemble.\\n\\n```python\\nimport pandas as pd\\n\\nimport mlflow\\nfrom sklearn.metrics import r2_score\\n\\n# Load the registered model using its alias\\nloaded_model = mlflow.pyfunc.load_model(\\n\\tmodel_uri=f\\"models:/ensemble_model@production\\"\\n)\\n\\n# Define the different strategies for evaluation\\nstrategies = [\\n    \\"average_1\\",\\n    \\"average_2\\",\\n    \\"weighted_1\\",\\n    \\"weighted_2\\",\\n    \\"weighted_3\\",\\n    \\"weighted_4\\",\\n]\\n\\n# Initialize a DataFrame to store the results of predictions\\nresults_df = pd.DataFrame()\\n\\n# Iterate over each strategy, make predictions, and calculate R^2 scores\\nfor strategy in strategies:\\n    # Create a test DataFrame with the current strategy\\n    X_test_with_params = X_test.copy()\\n    X_test_with_params[\\"strategy\\"] = strategy\\n\\n    # Use the loaded model to make predictions\\n    y_pred = loaded_model.predict(X_test_with_params)\\n\\n    # Calculate R^2 score for the predictions\\n    r2 = r2_score(y_test, y_pred)\\n\\n    # Store the results and R^2 score in the results DataFrame\\n    results_df[strategy] = y_pred\\n    results_df[f\\"r2_{strategy}\\"] = r2\\n\\n# Add the actual target values to the results DataFrame\\nresults_df[\\"y_test\\"] = y_test.values\\n```\\n\\nSimilar to out-of-the-box MLflow models, you begin by loading the ensemble model using `mlflow.pyfunc.load_model` to generate the house price predictions. After defining the different strategies for aggregating sub-model predictions and creating the model input containing both the housing data features and aggregation strategy, simply call the ensemble model\'s `predict` method to get the aggregated house price prediction.\\n\\n### Evaluating Model Performance with Different Strategies\\n\\nTo evaluate the performance of our ensemble model, we calculated the average R\xb2 scores for different aggregation strategies. These strategies include both simple averaging and weighted combinations of sub-models, with varying configurations of models and their respective weights. By comparing the R\xb2 scores, we can assess which strategies provide the most accurate predictions.\\n\\nThe bar graph below illustrates the average R\xb2 scores for each strategy. Higher values indicate better predictive performance. As shown in the graph, the ensemble strategies generally outperform individual models as depicted in our second strategy that is relying on a single `DecisionTree` (average_2), demonstrating the effectiveness of aggregating predictions from multiple sub-models. This visual comparison highlights the benefits of using an ensemble approach, particularly with weighted strategies that optimize the contribution of each sub-model.\\n\\n![Ensemble Model Evaluation](ensemble-model-evaluation.png)\\n\\n## Summary\\n\\nThis blog post highlights the capabilities of mlflow.pyfunc and its application in a Machine Learning project. This powerful feature of MLflow provides creative freedom and flexibility, enabling teams to build complex systems encapsulated as models within MLflow, following the same lifecycle as traditional models. The post showcases the creation of ensemble model setups, seamless integration with DuckDB, and the implementation of custom methods using this versatile module.\\n\\nBeyond offering a structured approach to achieving desired outcomes, this blog demonstrates practical possibilities based on hands-on experience, discussing potential challenges and their solutions.\\n\\n## Additional resources\\n\\nExplore the following resources for a deeper understanding of MLflow PyFunc models:\\n\\n- [Custom MLflow Models with mlflow.pyfunc](https://mlflow.org/blog/custom-pyfunc)\\n- [Understanding PyFunc in MLflow](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part2-pyfunc-components.html)\\n- [Building Custom Python Function Models with MLflow](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n- [Deploy an MLflow PyFunc model with Model Serving](https://mlflow.org/docs/latest/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.html)"},{"id":"mlflow-tracing","metadata":{"permalink":"/mlflow-website/blog/mlflow-tracing","source":"@site/blog/2024-06-10-mlflow-tracing/index.md","title":"Introducing MLflow Tracing","description":"We\'re excited to announce the release of a powerful new feature in MLflow: MLflow Tracing.","date":"2024-06-10T00:00:00.000Z","formattedDate":"June 10, 2024","tags":[{"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"},{"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":3.825,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"Introducing MLflow Tracing","tags":["tracing","genai","mlops"],"slug":"mlflow-tracing","authors":["mlflow-maintainers"],"thumbnail":"/img/blog/trace-intro.gif"},"unlisted":false,"prevItem":{"title":"PyFunc in Practice","permalink":"/mlflow-website/blog/pyfunc-in-practice"},"nextItem":{"title":"Deep Learning with MLflow (Part 2)","permalink":"/mlflow-website/blog/deep-learning-part-2"}},"content":"We\'re excited to announce the release of a powerful new feature in MLflow: [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html).\\nThis feature brings comprehensive instrumentation capabilities to your GenAI applications, enabling you to gain deep insights into the execution of your\\nmodels and workflows, from simple chat interfaces to complex multi-stage Retrieval Augmented Generation (RAG) applications.\\n\\n> NOTE: MLflow Tracing has been released in MLflow 2.14.0 and is not available in previous versions.\\n\\n## Introducing MLflow Tracing\\n\\nTracing is a critical aspect of understanding and optimizing complex applications, especially in the realm of machine learning and artificial intelligence.\\nWith the release of MLflow Tracing, you can now easily capture, visualize, and analyze detailed execution traces of your GenAI applications.\\nThis new feature aims to provide greater visibility and control over your applications\' performance and behavior, aiding in everything from fine-tuning to debugging.\\n\\n## What is MLflow Tracing?\\n\\nMLflow Tracing offers a variety of methods to enable [tracing](https://mlflow.org/docs/latest/llms/tracing/overview.html) in your applications:\\n\\n- **Automated Tracing with LangChain**: A fully automated integration with [LangChain](https://www.langchain.com/) allows you to activate tracing simply by enabling `mlflow.langchain.autolog()`.\\n- **Manual Trace Instrumentation with High-Level Fluent APIs**: Use decorators, function wrappers, and context managers via the fluent API to add tracing functionality with minimal code modifications.\\n- **Low-Level Client APIs for Tracing**: The MLflow client API provides a thread-safe way to handle trace implementations for fine-grained control of what and when data is recorded.\\n\\n## Getting Started with MLflow Tracing\\n\\n### LangChain Automatic Tracing\\n\\nThe easiest way to get started with MLflow Tracing is through the built-in integration with LangChain. By enabling autologging, traces are automatically logged to the active MLflow experiment when calling invocation APIs on chains. Here\u2019s a quick example:\\n\\n```python\\nimport os\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI\\nimport mlflow\\n\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set your OPENAI_API_KEY environment variable.\\"\\n\\nmlflow.set_experiment(\\"LangChain Tracing\\")\\nmlflow.langchain.autolog(log_models=True, log_input_examples=True)\\n\\nllm = OpenAI(temperature=0.7, max_tokens=1000)\\nprompt_template = \\"Imagine you are {person}, and you are answering a question: {question}\\"\\nchain = prompt_template | llm\\n\\nchain.invoke({\\"person\\": \\"Richard Feynman\\", \\"question\\": \\"Why should we colonize Mars?\\"})\\nchain.invoke({\\"person\\": \\"Linus Torvalds\\", \\"question\\": \\"Can I set everyone\'s access to sudo?\\"})\\n\\n```\\n\\nAnd this is what you will see after invoking the chains when navigating to the **LangChain Tracing** experiment in the MLflow UI:\\n\\n![Traces in UI](tracing-ui.gif)\\n\\n### Fluent APIs for Manual Tracing\\n\\nFor more control, you can use MLflow\u2019s fluent APIs to manually instrument your code. This approach allows you to capture detailed trace data with minimal changes to your existing code.\\n\\n#### Trace Decorator\\n\\nThe trace decorator captures the inputs and outputs of a function:\\n\\n```python\\nimport mlflow\\n\\nmlflow.set_experiment(\\"Tracing Demo\\")\\n\\n@mlflow.trace\\ndef some_function(x, y, z=2):\\n    return x + (y - z)\\n\\nsome_function(2, 4)\\n```\\n\\n#### Context Handler\\n\\nThe context handler is ideal for supplementing span information with additional data at the point of information generation:\\n\\n```python\\nimport mlflow\\n\\n@mlflow.trace\\ndef first_func(x, y=2):\\n    return x + y\\n\\n@mlflow.trace\\ndef second_func(a, b=3):\\n    return a * b\\n\\ndef do_math(a, x, operation=\\"add\\"):\\n    with mlflow.start_span(name=\\"Math\\") as span:\\n        span.set_inputs({\\"a\\": a, \\"x\\": x})\\n        span.set_attributes({\\"mode\\": operation})\\n        first = first_func(x)\\n        second = second_func(a)\\n        result = first + second if operation == \\"add\\" else first - second\\n        span.set_outputs({\\"result\\": result})\\n        return result\\n\\ndo_math(8, 3, \\"add\\")\\n```\\n\\n### Comprehensive Tracing with Client APIs\\n\\nFor advanced use cases, the MLflow client API offers fine-grained control over trace management. These APIs allows you to create, manipulate, and retrieve traces programmatically, albeit with additional complexity throughout the implementation.\\n\\n#### Starting and Managing Traces with the Client APIs\\n\\n```python\\nfrom mlflow import MlflowClient\\n\\nclient = MlflowClient()\\n\\n# Start a new trace\\nroot_span = client.start_trace(\\"my_trace\\")\\nrequest_id = root_span.request_id\\n\\n# Create a child span\\nchild_span = client.start_span(\\n    name=\\"child_span\\",\\n    request_id=request_id,\\n    parent_id=root_span.span_id,\\n    inputs={\\"input_key\\": \\"input_value\\"},\\n    attributes={\\"attribute_key\\": \\"attribute_value\\"},\\n)\\n\\n# End the child span\\nclient.end_span(\\n    request_id=child_span.request_id,\\n    span_id=child_span.span_id,\\n    outputs={\\"output_key\\": \\"output_value\\"},\\n    attributes={\\"custom_attribute\\": \\"value\\"},\\n)\\n\\n# End the root span (trace)\\nclient.end_trace(\\n    request_id=request_id,\\n    outputs={\\"final_output_key\\": \\"final_output_value\\"},\\n    attributes={\\"token_usage\\": \\"1174\\"},\\n)\\n```\\n\\n## Diving Deeper into Tracing\\n\\nMLflow Tracing is designed to be flexible and powerful, supporting various use cases from simple function tracing to complex, asynchronous workflows.\\n\\nTo learn more about this feature, [read the guide](https://mlflow.org/docs/latest/llms/tracing/index.html), [review the API Docs](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-fluent-python-apis) and [get started with the LangChain integration](https://mlflow.org/docs/latest/llms/tracing/index.html#langchain-automatic-tracing) today!\\n\\n## Join Us on This Journey\\n\\nThe introduction of MLflow Tracing marks a significant milestone in our mission to provide comprehensive tools for managing machine learning workflows. We\u2019re excited about the possibilities this new feature opens up and look forward to your [feedback](https://github.com/mlflow/mlflow/issues) and [contributions](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).\\n\\nFor those in our community with a passion for sharing knowledge, we invite you to [collaborate](https://github.com/mlflow/mlflow-website/blob/main/CONTRIBUTING.md). Whether it\u2019s writing tutorials, sharing use-cases, or providing feedback, every contribution enriches the MLflow community.\\n\\nStay tuned for more updates, and as always, happy coding!"},{"id":"deep-learning-part-2","metadata":{"permalink":"/mlflow-website/blog/deep-learning-part-2","source":"@site/blog/2024-04-26-deep-learning-part-2/index.md","title":"Deep Learning with MLflow (Part 2)","description":"Using MLflow\'s Deep Learning tracking features for fine tuning an LLM","date":"2024-04-26T00:00:00.000Z","formattedDate":"April 26, 2024","tags":[{"label":"Deep Learning","permalink":"/mlflow-website/blog/tags/deep-learning"}],"readingTime":11.64,"hasTruncateMarker":true,"authors":[{"name":"Puneet Jain","title":"Sr. Specialist Solutions Architect at Databricks","url":"https://www.linkedin.com/in/puneetjain159/","imageURL":"/img/authors/puneet.png","key":"puneet-jain"},{"name":"Avinash Sooriyarachchi","title":"Sr. Solutions Architect at Databricks","url":"https://www.linkedin.com/in/avi-data-ml/","imageURL":"/img/authors/avinash.png","key":"avinash-sooriyarachchi"},{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/img/authors/abe.png","key":"abe-omorogbe"},{"name":"Ben Wilson","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/benjamin-wilson-arch/","imageURL":"/img/authors/ben.png","key":"ben"}],"frontMatter":{"title":"Deep Learning with MLflow (Part 2)","description":"Using MLflow\'s Deep Learning tracking features for fine tuning an LLM","slug":"deep-learning-part-2","authors":["puneet-jain","avinash-sooriyarachchi","abe-omorogbe","ben"],"tags":["Deep Learning"],"thumbnail":"/img/blog/dl-blog-2.png"},"unlisted":false,"prevItem":{"title":"Introducing MLflow Tracing","permalink":"/mlflow-website/blog/mlflow-tracing"},"nextItem":{"title":"MLflow Release Candidates","permalink":"/mlflow-website/blog/release-candidates"}},"content":"In the realm of deep learning, finetuning of pre-trained Large Language Models (LLMs) on private datasets is an excellent customization\\noption to increase a model\u2019s relevancy for a specific task. This practice is not only common, but also essential for developing specialized\\nmodels, particularly for tasks like text classification and summarization.\\n\\nIn such scenarios, tools like MLflow are invaluable. Tracking tools like MLflow help to ensure that every aspect of the training\\nprocess - metrics, parameters, and artifacts - are reproducibly tracked and logged, allowing for the analysis, comparison, and sharing of tuning iterations.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this blog post, we are going to be using [MLflow 2.12](https://mlflow.org/releases/2.12.1) and the\\n[recently introduced MLflow Deep Learning features](https://mlflow.org/blog/deep-learning-part-1) to track all the important aspects of fine\\ntuning a large language model for text classification, including the use of automated logging of training checkpoints in order to simplify\\nthe process of resumption of training.\\n\\n## Use Case: Fine Tuning a Transformer Model for Text Classification\\n\\nThe example scenario that we\'re using within this blog utilizes the [unfair-TOS](https://huggingface.co/datasets/coastalcph/lex_glue/viewer/unfair_tos) dataset.\\n\\nIn today\u2019s world, it\u2019s hard to find a service, platform, or even a consumer good that doesn\u2019t have a legally-binding terms of service connected\\nwith it. These encyclopedic size agreements, filled with dense legal jargon and sometimes baffling levels of specificity, are so large that\\nmost people simply accept them without reading them. However, reports have indicated over time that occasionally, some suspiciously unfair\\nterms are embedded within them.\\n\\nAddressing unfair clauses in Terms of Service (TOS) agreements through machine learning (ML) is particularly relevant due to the pressing\\nneed for transparency and fairness in legal agreements that affect consumers. Consider the following clause from an example TOS\\nagreement: **\\"We may revise these Terms from time to time. The changes will not be retroactive, and the most current version of the Terms, which will always...\\"**\\nThis clause stipulates that the service provider may suspend or terminate the service at any time for any reason,\\nwith or without notice. Most people would consider this to be quite unfair.\\n\\nWhile this sentence is buried quite deeply within a fairly dense document, an ML algorithm is not burdened by the exhaustion that a human\\nwould have for combing through the text and identifying clauses that might seem a bit unfair. By automatically identifying potentially\\nunfair clauses, a transformers-based Deep Learning (DL) model can help protect consumers from exploitative practices, ensure greater compliance with legal standards,\\nand foster trust between service providers and users.\\n\\nA base pre-trained transformer model, without specialized fine-tuning, faces several challenges in accurately identifying unfair Terms of Service clauses.\\nFirstly, it lacks the domain-specific knowledge essential for understanding complex legal language. Secondly, its training objectives are\\ntoo general to capture the nuanced interpretation required for legal analysis. Lastly, it may not effectively recognize the subtle\\ncontextual meanings that determine the fairness of contractual terms, making it less effective for this specialized task.\\n\\nUsing prompt engineering to address the identification of unfair Terms of Service clauses with a closed-source Large language model\\ncan be prohibitively expensive. This approach requires extensive trial and error to refine prompts without the ability to tweak\\nthe underlying model mechanics. Each iteration can consume significant computational resources , especially when using\\n[few-shot prompting](https://www.promptingguide.ai/techniques/fewshot), leading to escalating costs without guaranteeing a corresponding\\nincrease in accuracy or effectiveness.\\n\\nIn this context, the use of the **RoBERTa-base** model is particularly effective, provided that it is fine-tuned. This model is robust\\nenough to handle complex tasks like discerning embedded instructions within texts, yet it is sufficiently compact to be fine-tuned\\non modest hardware, such as an Nvidia T4 GPU.\\n\\n### What is PEFT?\\n\\n[Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/en/index) approaches are advantageous as they involve\\nkeeping the bulk of the pre-trained model parameters fixed while either only training a few additional layers or modifying the parameters used\\nwhen interacting with the model\'s weights. This methodology not only conserves memory during training, but also significantly reduces the overall training time. When\\ncompared with the alternative of fine-tuning a base model\'s weights in order to customize its performance for a specific targeted task, the PEFT\\napproach can save significant cost in both time and money, while providing an equivalent or better performance results with less data than is required\\nfor a comprehensive fine-tuning training task.\\n\\n## Integrating Hugging-Face models and the PyTorch Lightning framework\\n\\n[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) integrates seamlessly with\\n[Hugging Face\'s Transformers library](https://huggingface.co/docs/transformers/en/index), enabling streamlined model training workflows\\nthat capitalize on Lightning\'s easy-to-use Higher level API\u2019s and HF\'s state-of-the-art pre-trained models. The combination of Lightning with transformers\u2019\\n[PEFT module](https://huggingface.co/blog/peft) enhances productivity and scalability by reducing code complexity and enabling the use of\\nhigh-quality pre-optimized models for a range of diverse NLP tasks.\\n\\nBelow is an example of configuring the PEFT-based fine tuning of a base model using PyTorch Lightning and HuggingFace\'s `peft` module.\\n\\n```python\\nfrom typing import List\\nfrom lightning import LightningModule\\nfrom peft import get_peft_model, LoraConfig, TaskType\\nfrom transformers import AutoModelForSequenceClassification\\n\\n\\nclass TransformerModule(LightningModule):\\n    def __init__(\\n        self,\\n        pretrained_model: str,\\n        num_classes: int = 2,\\n        lora_alpha: int = 32,\\n        lora_dropout: float = 0.1,\\n        r: int = 8,\\n        lr: float = 2e-4\\n    ):\\n        super().__init__()\\n        self.model = self.create_model(pretrained_model, num_classes, lora_alpha, lora_dropout, r)\\n        self.lr = lr\\n        self.save_hyperparameters(\\"pretrained_model\\")\\n\\n    def create_model(self, pretrained_model, num_classes, lora_alpha, lora_dropout, r):\\n        \\"\\"\\"Create and return the PEFT model with the given configuration.\\n\\n        Args:\\n            pretrained_model: The path or identifier for the pretrained model.\\n            num_classes: The number of classes for the sequence classification.\\n            lora_alpha: The alpha parameter for LoRA.\\n            lora_dropout: The dropout rate for LoRA.\\n            r: The rank of LoRA adaptations.\\n\\n        Returns:\\n            Model: A model configured with PEFT.\\n        \\"\\"\\"\\n        model = AutoModelForSequenceClassification.from_pretrained(\\n            pretrained_model_name_or_path=pretrained_model,\\n            num_labels=num_classes\\n        )\\n        peft_config = LoraConfig(\\n            task_type=TaskType.SEQ_CLS,\\n            inference_mode=False,\\n            r=r,\\n            lora_alpha=lora_alpha,\\n            lora_dropout=lora_dropout\\n        )\\n        return get_peft_model(model, peft_config)\\n\\n    def forward(self, input_ids: List[int], attention_mask: List[int], label: List[int]):\\n        \\"\\"\\"Calculate the loss by passing inputs to the model and comparing against ground truth labels.\\n\\n        Args:\\n            input_ids: List of token indices to be fed to the model.\\n            attention_mask: List to indicate to the model which tokens should be attended to, and which should not.\\n            label: List of ground truth labels associated with the input data.\\n\\n        Returns:\\n            torch.Tensor: The computed loss from the model as a tensor.\\n        \\"\\"\\"\\n        return self.model(\\n            input_ids=input_ids,\\n            attention_mask=attention_mask,\\n            labels=label\\n        )\\n```\\n\\nAdditional references for the full implementation can be [seen within the companion repository here](https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/custom_module/fine_tune_clsify_head.py)\\n\\n## Configuring MLflow for PEFT-based fine-tuning\\n\\nBefore initiating the training process, it\'s crucial to configure MLFlow so that all system metrics, loss metrics, and parameters are logged for the training run.\\nAs of MLFlow 2.12, auto-logging for TensorFlow and PyTorch now includes support for checkpointing model weights during training, giving a snapshot of the model\\nweights at defined epoch frequencies in order to provide for training resumption in the case of an error or loss of the compute environment.\\nBelow is an example of how to enable this feature:\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.enable_system_metrics_logging()\\nmlflow.pytorch.autolog(checkpoint_save_best_only = False, checkpoint_save_freq=\'epoch\')\\n```\\n\\nIn the code above we are doing the following:\\n\\n- **Enabling System Metrics Logging**: The system resources will be logged to MLflow in order to understand where bottlenecks in memory, CPU, GPU, disk usage, and network traffic are throughout the training process.\\n\\n![MLflow UI System Metrics](sys_metrics.png)\\n\\n- **Configuring Auto Logging to log parameters, metrics and checkpoints for all epochs**: Deep learning involves experimenting with various model architectures and hyperparameter settings. Auto logging plays a crucial role in systematically recording these experiments, making it easier to compare different runs and determine which configurations yield the best results. Checkpoints are logged at every epoch, enabling detailed evaluations of all intermediate epochs during the initial exploration phase of the project. However, it is generally not advisable to log all epochs during late-stage development to avoid excessive data writes and latency in the final training stages.\\n\\n![System Metrics Logged](epoch_logging.png)\\n\\nThe auto-logged checkpoint metrics and model artifacts will be viewable in the MLflow UI as the model trains, as shown below:\\n\\n![Metrics logged with each epoch](checkpoint_metrics.png)\\n\\n## The Importance of Logging and Early-stopping\\n\\nThe integration of the Pytorch Lightning `Trainer` callback with MLflow is crucial within this training exercise. The integration allows for comprehensive\\ntracking and logging of metrics, parameters, and artifacts during model finetuning without having to explicitly call MLflow logging APIs. Additionally,\\nthe autologging API allows for modifying the default logging behavior, permitting changes to the logging frequency, allowing for logging to occur at each\\nepoch, after a specified number of epochs, or at explicitly defined steps.\\n\\n### Early stopping\\n\\nEarly stopping is a critical regularization technique in neural network training, designed to assist in preventing overfitting through the act of\\nhalting training when validation performance plateaus. Pytorch Lightning includes APIs that allow for an easy high-level control of training cessation,\\nas demonstrated below.\\n\\n### Configuring Pytorch Trainer Callback with Early stopping\\n\\nThe example below shows the configuration of the `Trainer` object within `Lightning` to leverage early stopping to prevent overfitting. Once configured, the\\ntraining is executed by calling `fit` on the `Trainer` object. By providing the `EarlyStopping` callback, in conjunction with MLflow\'s autologging, the\\nappropriate number of epochs will be used, logged, and tracked without any additional effort.\\n\\n```python\\nfrom dataclasses import dataclass, field\\nimport os\\n\\nfrom data import LexGlueDataModule\\nfrom lightning import Trainer\\nfrom lightning.pytorch.callbacks import EarlyStopping\\nimport mlflow\\n\\n\\n@dataclass\\nclass TrainConfig:\\n    pretrained_model: str = \\"bert-base-uncased\\"\\n    num_classes: int = 2\\n    lr: float = 2e-4\\n    max_length: int = 128\\n    batch_size: int = 256\\n    num_workers: int = os.cpu_count()\\n    max_epochs: int = 10\\n    debug_mode_sample: int | None = None\\n    max_time: dict[str, float] = field(default_factory=lambda: {\\"hours\\": 3})\\n    model_checkpoint_dir: str = \\"/local_disk0/tmp/model-checkpoints\\"\\n    min_delta: float = 0.005\\n    patience: int = 4\\n\\ntrain_config = TrainConfig()\\n\\n# Instantiate the custom Transformer class for PEFT training\\nnlp_model = TransformerModule(\\n        pretrained_model=train_config.pretrained_model,\\n        num_classes=train_config.num_classes,\\n        lr=train_config.lr,\\n    )\\n\\ndatamodule = LexGlueDataModule(\\n        pretrained_model=train_config.pretrained_model,\\n        max_length=train_config.max_length,\\n        batch_size=train_config.batch_size,\\n        num_workers=train_config.num_workers,\\n        debug_mode_sample=train_config.debug_mode_sample,\\n    )\\n\\n# Log system metrics while training loop is running\\nmlflow.enable_system_metrics_logging()\\n\\n# Automatically log per-epoch parameters, metrics, and checkpoint weights\\nmlflow.pytorch.autolog(checkpoint_save_best_only = False)\\n\\n# Define the Trainer configuration\\ntrainer = Trainer(\\n   callbacks=[\\n       EarlyStopping(\\n           monitor=\\"Val_F1_Score\\",\\n           min_delta=train_config.min_delta,\\n           patience=train_config.patience,\\n           verbose=True,\\n           mode=\\"max\\",\\n       )\\n   ],\\n   default_root_dir=train_config.model_checkpoint_dir,\\n   fast_dev_run=bool(train_config.debug_mode_sample),\\n   max_epochs=train_config.max_epochs,\\n   max_time=train_config.max_time,\\n   precision=\\"32-true\\"\\n)\\n\\n# Execute the training run\\ntrainer.fit(model=nlp_model, datamodule=datamodule)\\n```\\n\\n## Visualization and Sharing Capabilities within MLflow\\n\\nThe newly introduced DL-specific visualization capabilities introduced in MLflow 2.12 enable you to make comparisons between different runs and artifacts over epochs.\\nWhen comparing training runs, MLflow is capable of generating useful visualization that can be integrated into dashboards, facilitating\\neasy sharing. Additionally, the centralized storage of metrics, in conjunction with parameters, allows for effective analysis of the training\\nefficacy, as shown in the image below.\\n\\n![Epoch Run Compare](compare.png)\\n\\n## When to stop training?\\n\\nWhen training DL models, it is important to understand when to stop. Efficient training (for minimizing the overall cost incurred for\\nconducting training) and optimal model performance rely heavily on preventing a model from overfitting on the training data. A model\\nthat trains for too long will invariably become quite good at effectively \u2018memorizing\u2019 the training data, resulting in a reduction in\\nthe performance of the model when presented with novel data. A straightforward way to evaluate this behavior is to ensure that\\nvalidation data set metrics (scoring loss metrics on data that is not in the training data set) are captured during the training\\nloop. Integrating the MLflow callback into the PyTorch Lightning Trainer allows for iterative logging of loss metrics at\\nconfigurable iterations, enabling an easily debuggable evaluation of the training performance, ensuring that stopping criteria\\ncan be enforced at the appropriate time to prevent overfitting.\\n\\n### Evaluating epoch checkpoints of Fine Tuned Models with MLflow\\n\\nWith your training process meticulously tracked and logged by MLflow, you have the flexibility to retrieve and test your model at\\nany arbitrary checkpoint. To do this, you can use the mlflow.pytorch.load_model() API to load the model from a specific run\\nand use the `predict()` method for evaluation.\\n\\nIn the example below, we will load the model checkpoint from the 3rd epoch and use the `Lightning` train module to generate predictions based on the\\ncheckpoint state of the saved training epoch.\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.pytorch.autolog(disable = True)\\n\\nrun_id = \'<Add the run ID>\'\\n\\nmodel = mlflow.pytorch.load_checkpoint(TransformerModule, run_id, 3)\\n\\nexamples_to_test = [\\"We reserve the right to modify the service price at any time and retroactively apply the adjusted price to historical service usage.\\"]\\n\\ntrain_module = Trainer()\\ntokenizer = AutoTokenizer.from_pretrained(train_config.pretrained_model)\\ntokens = tokenizer(examples_to_test,\\n                  max_length=train_config.max_length,\\n                  padding=\\"max_length\\",\\n                  truncation=True)\\nds = Dataset.from_dict(dict(tokens))\\nds.set_format(\\n            type=\\"torch\\", columns=[\\"input_ids\\", \\"attention_mask\\"]\\n        )\\n\\ntrain_module.predict(model, dataloaders = DataLoader(ds))\\n```\\n\\n## Summary\\n\\nThe integration of MLflow into the finetuning process of pre-trained language models, particularly for applications like custom\\nnamed entity recognition, text classification and instruction-following represents a significant advancement in managing and\\noptimizing deep learning workflows. Leveraging the autologging and tracking capabilities of MLflow in these workstreams not only\\nenhances the reproducibility and efficiency of model development, but also fosters a collaborative environment where insights\\nand improvements can be easily shared and implemented.\\n\\nAs we continue to push the boundaries of what these models can achieve, tools like MLflow will be instrumental in harnessing their full potential.\\n\\nIf you\'re interested in seeing the full example in its entirety, feel free to [see the full example implementation](https://github.com/puneet-jain159/deeplearning_with_mlfow)\\n\\n### Check out the code\\n\\nThe code we provide will delve into additional aspects such as training from a checkpoint, integrating MLflow and TensorBoard, and utilizing Pyfunc for model wrapping, among others. These resources are specifically tailored for implementation on [Databricks Community Edition](https://mlflow.org/blog/databricks-ce). The main runner notebook\\nwithin the full example repository [can be found here](https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/train.ipynb).\\n\\n## Get Started with MLflow 2.12 Today\\n\\nDive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\\nMLflow is here to elevate your ML workflow to new heights. [Get started now with MLflow\'s cutting-edge tools and features](https://mlflow.org/releases/2.12.1).\\n\\n## Feedback\\n\\nWe value your input! Our feature prioritization is guided by feedback from the MLflow late 2023 survey. Please fill out our\\n[Spring 2024 survey](https://surveys.training.databricks.com/jfe/form/SV_3jGIliwGC0g5xTU), and by participating, you can help ensure that the features\\nyou want most are implemented in MLflow."},{"id":"release-candidates","metadata":{"permalink":"/mlflow-website/blog/release-candidates","source":"@site/blog/2024-04-17-release-candidates.md","title":"MLflow Release Candidates","description":"We are excited to announce the implementation of a release candidate process for MLflow!","date":"2024-04-17T00:00:00.000Z","formattedDate":"April 17, 2024","tags":[{"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"}],"readingTime":2.82,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"MLflow Release Candidates","tags":["mlflow"],"slug":"release-candidates","authors":["mlflow-maintainers"],"thumbnail":"/img/blog/release-candidates.png"},"unlisted":false,"prevItem":{"title":"Deep Learning with MLflow (Part 2)","permalink":"/mlflow-website/blog/deep-learning-part-2"},"nextItem":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","permalink":"/mlflow-website/blog/deep-learning-part-1"}},"content":"We are excited to announce the implementation of a release candidate process for MLflow!\\nThe pace of feature development in MLflow is faster now than ever before and the core maintainer team has even more exciting things planned in the near future! However, with an increased velocity on major feature development comes with a risk of breaking things. As the maintainers of such a widely used project, we are cognizant of the disruptive nature of regressions and we strive to avoid them as much as we can. Aside from new feature development work, our primary goal is in ensuring the stability of production systems. While we do have the aspirational goal of moving fast(er), we certainly don\'t want to move fast and break things. With that goal in mind, we\'ve decided to introduce a release candidate (RC) process. The RC process allows us to introduce new features and fixes in a controlled environment before they become part of the official release.\\n\\n## How It Works\\n\\nStarting from MLflow 2.13.0, new MLflow major and minor releases will be tagged as release candidates (e.g., `2.13.0rc0`) in PyPI two weeks before they are officially released.\\n\\nThe release candidate process involves several key stages:\\n\\n- Feature Development Freeze: Prior to cutting the RC branch and announcing its availability, we will freeze the RC branch from feature commits. Once the branch is cut, only bug fix and stability PRs will be permitted to be merged, ensuring that unexpected, late-arriving, potentially regression-causing merges are not permitted to destabilize the forthcoming release.\\n- Pre-Release Announcement: We will announce upcoming features and improvements, providing our community with a roadmap of what to expect.\\n- Release Candidate Rollout: A release candidate version will be made available for testing, accompanied by detailed release notes outlining the changes.\\n- Community Testing and Feedback: We encourage our users to test the release candidate in their environments and share their feedback with us by filing issue reports on the MLflow Github repository. This feedback is invaluable for identifying issues and ensuring the final release aligns with user needs (i.e., we didn\'t break your workflows).\\n- Final Release: After incorporating feedback and making necessary adjustments, we will proceed with the final release. This version will include all updates tested in the RC phase, offering a polished and stable experience for all users.\\n\\nThis approach provides several benefits:\\n\\n- Enhanced Stability: By rigorously testing release candidates, we can identify and address potential issues early, reducing the likelihood of disruptions in production environments.\\n- Community Feedback: The RC phase offers you, a member of the MLflow community, the opportunity to provide feedback on upcoming changes. This collaborative approach ensures that the final release aligns with the needs and expectations of our users.\\n- Gradual Adoption: Users can choose to experiment with new features in a release candidate without committing to a full upgrade. This flexibility supports cautious integration and thorough evaluation in various environments.\\n\\n## Get Involved\\n\\nYour participation is crucial to the success of this process. We invite you to join us in testing upcoming release candidates and sharing your insights. Together, we can ensure that MLflow continues to serve as a reliable foundation for your machine learning projects.\\nStay tuned for announcements regarding our first release candidate. We look forward to your contributions and feedback as we take this important step toward a more stable and dependable MLflow."},{"id":"deep-learning-part-1","metadata":{"permalink":"/mlflow-website/blog/deep-learning-part-1","source":"@site/blog/2024-03-05-deep-learning-part-1/index.md","title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","description":"Highlighting the recent improvements in MLflow for Deep Learning workflows","date":"2024-03-05T00:00:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Deep Learning","permalink":"/mlflow-website/blog/tags/deep-learning"}],"readingTime":5.175,"hasTruncateMarker":true,"authors":[{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/img/authors/abe.png","key":"abe-omorogbe"},{"name":"Hubert Zub","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/hubert-zub/","imageURL":"/img/authors/hubert.png","key":"hubert-zub"},{"name":"Yun Park","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/yunpark93/","imageURL":"/img/authors/yun.png","key":"yun-park"},{"name":"Chen Qian","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/thomas-chen-qian/","imageURL":"/img/authors/chen.png","key":"chen-qian"},{"name":"Jesse Chan","title":"Software Engineer at Databricks","key":"jesse-chan"}],"frontMatter":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","description":"Highlighting the recent improvements in MLflow for Deep Learning workflows","slug":"deep-learning-part-1","authors":["abe-omorogbe","hubert-zub","yun-park","chen-qian","jesse-chan"],"tags":["Deep Learning"],"thumbnail":"/img/blog/dl-chart-grouping.gif"},"unlisted":false,"prevItem":{"title":"MLflow Release Candidates","permalink":"/mlflow-website/blog/release-candidates"},"nextItem":{"title":"2023 Year in Review","permalink":"/mlflow-website/blog/mlflow-year-in-review"}},"content":"In the quickly evolving world of artificial intelligence, where generative AI has taken center stage, the landscape of machine learning is\\nevolving at an unprecedented pace. There has been a surge in the use of cutting-edge deep learning (DL) libraries like\\n[Transformers](https://huggingface.co/docs/transformers/index), [Tensorflow](https://www.tensorflow.org/),\\nand [PyTorch](https://pytorch.org/) to fine-tune these generative AI models for enhanced performance.\\nAs this trend accelerates, it\'s become clear that the tools used to build these models must rapidly evolve as well, particularly when it comes\\nto managing and optimizing these deep learning workloads. MLflow offers a practical solution for managing the complexities of these machine learning projects.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn collaboration with [MosaicML](https://www.mosaicml.com/) and the broader ML community, MLflow is thrilled to unveil a set of eagerly awaited enhancements.\\nThis latest release ([MLflow 2.11](https://www.mlflow.org/releases/2.11.0)) introduces updated tracking UI capabilities in direct response to\\n[the feedback](https://www.linkedin.com/posts/mlflow-org_qualtrics-survey-qualtrics-experience-management-activity-7128154257924513793-RCDG?utm_source=share&utm_medium=member_desktop)\\nand needs of MLflow enthusiasts. These updates are not just incremental; they represent a leap forward in addressing the needs of MLflow users doing Deep Learning.\\n\\nThe evolution of enhanced Deep Learning capabilities is a testament to MLflow\'s commitment to serving the open-source community, ensuring that its offerings\\nare not just keeping pace, but setting the pace in the rapidly evolving domain of machine learning.\\n\\n## Deep Learning API Improvements\\n\\nLeveraging valuable insights from our user community, we\'ve implemented critical enhancements to the effective scale of metrics logging and the inclusion of\\nsystem-related metric logging within our platform. These improvements encompass expanded scalability options, support for logging more iterations and the\\nlogging of system metrics.\\n\\n### System Metrics\\n\\nThis feature allows you to [monitor system metrics](https://mlflow.org/docs/latest/system-metrics/index.html?highlight=system) and identify any hardware issues that might be impacting performance.\\nMetrics such as CPU utilization, Memory usage, disk usage etc., from all nodes in your cluster can now be logged and visualized within the MLflow UI.\\n\\n![System Metrics](system-metrics.png)\\n\\n### Improved Logging Performance\\n\\nWe recently introduced both asynchronous and batch logging, making it easier to log both\\n[parallel and distributed](https://mlflow.org/docs/latest/tracking/tracking-api.html#parallel-runs) DL training sessions. Additionally, the MLflow Client\\nnow supports up to **1 million** steps (iterations) when logging metrics, allowing users to log more steps during long-running DL jobs.\\n\\n![Parallel Runs](parallel-runs.png)\\n\\n### Checkpointing for Deep Learning\\n\\n[TensorFlow](https://mlflow.org/releases/2.11.0#autologging-for-tensorflow-and-pytorch-now-supports-checkpointing-of-model-weights:~:text=both%20PyTorch%20and-,TensorFlow,-for%20automatic%20model)\\nand [PyTorch](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) now support model weight checkpointing when\\nusing autologging.\\n\\n![DL Checkpointing](dl-checkpointing.png)\\n\\n## User Experience and Productivity Enhancements\\n\\nWe have introduced substantial improvements to user experience and feature organization within our platform. These enhancements include more\\nsophisticated user interfaces and an intuitive redesign of the run details page, the addition of chart groups and metric aggregation, all\\naimed at simplifying navigation and enhancing productivity especially for Deep Learning use cases.\\n\\n### Metric Aggregation\\n\\nWe\'ve enhanced the UI with metric aggregation, enabling you to aggregate metrics across multiple runs based on\\ndatasets, tags, or parameters. These improvements significantly improve the time it takes to understand training results when working\\nwith large DL models, enabling more nuanced and comprehensive analysis of overarching trends in model performance across multiple dimensions.\\n\\n![DL Metric Aggregation](dl-metric-aggregation.gif)\\n\\n### Chart Grouping Functionality\\n\\nYou can now easily categorize and organize your metrics, such as training, testing, and system metrics into\\nnamed groups within the MLflow UI. This organization allows for a comprehensive overview of all metrics, enabling quicker access and\\nbetter management, particularly when handling experiments with many metrics.\\n\\n![DL Chart Grouping](dl-chart-grouping.gif)\\n\\n### Slash (\\"/\\") Logging Syntax\\n\\nTo further streamline metric organization, we\'ve implemented a new logging syntax that uses slashes\\n(\\"/\\") to group metrics. For example, using mlflow.log_metric(\\"x/y/score\\", 100) helps in structuring and segregating different types\\nof data or metrics into hierarchical groups, making it easier to navigate and interpret the logs, especially when dealing with complex\\nmodels and experiments.\\n\\n```python\\n\\nmlflow.log_metric(\'SVR/val_MAPE\', mean_absolute_percentage_error(test_y, pred_y))\\n\\n```\\n\\n![DL Slash Logging](dl-slash-logging.png)\\n\\n### Chart Searching\\n\\nWe\'ve significantly enhanced the search functionality within our platform, enabling more robust and intuitive searching\\nacross charts, parameters, and metrics. This upgrade allows for quicker and more precise retrieval of specific data points, streamlining the\\nprocess of analyzing and comparing different aspects of your experiments.\\n\\n![DL Chart Searching](dl-chart-searching.gif)\\n\\n### Run Details Redesign\\n\\nWe reorganized the Run Details UI to a modular tabbed layout, added new drag and drop UI functionality so that you can\\ncan now render logged tables. This enhancement will make it easier to organize your runs and experiments.\\n\\n![DL Run Details Redesign](dl-run-details.gif)\\n\\n## Getting Started Updates\\n\\nFollowing extensive feedback from our user community, we\'ve introduced significant updates to enhance the\\n[getting started](https://www.mlflow.org/docs/latest/getting-started/index.html) documentation within MLflow. These updates include a\\n[comprehensive overhaul](https://www.mlflow.org/docs/latest/deep-learning/index.html) of our documentation for easier navigation and\\n[enriched guidance](https://www.mlflow.org/docs/latest/deep-learning/pytorch/quickstart/pytorch_quickstart.html), along with a streamlined\\n[login API](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login). These enhancements, reflecting our commitment to improving the\\nuser experience and workflow, aim to empower our users to achieve more with greater speed and ease.\\n\\n### New Tutorials and Docs\\n\\nWe\'ve overhauled our documentation to offer a more comprehensive, user-friendly experience with practical examples\\nto support both newcomers and experienced practitioners with the information they need to start a Deep Learning project.\\n\\n![Deep Learning Docs](dl-docs.png)\\n\\n### Seamless login with mlflow.login()\\n\\nWe\'ve streamlined our authentication processes.\\n[This method](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#method-2-use-free-hosted-tracking-server-databricks-community-edition)\\nprovides a simple way to connect MLflow to your tracking server without having to leave your development environment.\\n[Try it out today](https://mlflow.org/blog/databricks-ce)\\n\\n![Login Update](login-update.png)\\n\\n## Get Started Today\\n\\nDive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\\nMLflow is here to elevate your data science workflow to new heights.\\n[Get started now with MLflow\'s cutting-edge tools and features](https://mlflow.org/releases/2.11.0).\\n\\n```bash\\npip install mlflow==2.11\\n\\nmlflow ui --port 8080\\n```\\n\\n```python\\nimport mlflow\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n# Set our tracking server uri for logging\\nmlflow.set_tracking_uri(uri=\\"http://127.0.0.1:8080\\")\\n\\nmlflow.autolog()\\n\\ndb = load_diabetes()\\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\\n\\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\\n# MLflow triggers logging automatically upon model fitting\\nrf.fit(X_train, y_train)\\n```\\n\\n## Feedback\\n\\nWe value your input! Our [feature roadmap](https://github.com/orgs/mlflow/projects/4) prioritization is guided by feedback from the [MLflow late 2023 survey](https://www.linkedin.com/feed/update/urn:li:activity:7128154257924513793), [GitHub Issues](https://github.com/mlflow/mlflow) and [Slack](https://mlflow.org/slack). Look out for our next survey later this year, by participating you can help ensure that the features you want are implemented in MLflow. You can also create an [issue on GitHub](https://github.com/mlflow/mlflow) or join our [Slack](https://mlflow.org/slack)."},{"id":"mlflow-year-in-review","metadata":{"permalink":"/mlflow-website/blog/mlflow-year-in-review","source":"@site/blog/2024-01-26-mlflow-year-in-review/index.md","title":"2023 Year in Review","description":"MLflow year-end recap","date":"2024-01-26T00:00:00.000Z","formattedDate":"January 26, 2024","tags":[{"label":"MLflow","permalink":"/mlflow-website/blog/tags/m-lflow"},{"label":"2023","permalink":"/mlflow-website/blog/tags/2023"},{"label":"Linux Foundation","permalink":"/mlflow-website/blog/tags/linux-foundation"}],"readingTime":6.625,"hasTruncateMarker":true,"authors":[{"name":"Carly Akerly","title":"OSS Marketing Consultant at The Linux Foundation","url":"https://www.linkedin.com/in/carlyakerly/","imageURL":"/img/authors/carly.png","key":"carly-akerly"}],"frontMatter":{"title":"2023 Year in Review","description":"MLflow year-end recap","slug":"mlflow-year-in-review","authors":["carly-akerly"],"tags":["MLflow","2023","Linux Foundation"],"thumbnail":"/img/blog/2023-year-in-review.png"},"unlisted":false,"prevItem":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","permalink":"/mlflow-website/blog/deep-learning-part-1"},"nextItem":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","permalink":"/mlflow-website/blog/databricks-ce"}},"content":"With more than **16 million** monthly downloads, MLflow has established itself as a leading open-source MLOps platform worldwide.\\nThis achievement underscores the robustness of MLflow and the active community that consistently refines and improves it.\\n\\nThe past year marked a significant milestone for MLflow, particularly in Generative AI. Its integration and support for Large Language Models\\n(LLMs) stood out. This strategic decision has propelled MLflow to the forefront of the AI revolution, establishing itself as the premier GenAI\\nplatform that enables users to create more intelligent, efficient, and adaptable AI models and applications.\\n\\n\x3c!-- truncate --\x3e\\n\\n![16 Million Downloads!](download-graph.png)\\n\\n## 2023: A Year of GenAI and Innovation\\n\\nLast year was remarkable for MLflow, particularly in integrating LLMs and other generative AI tools. MLflow has evolved significantly by offering\\na unified platform and workflow for traditional ML, deep learning, and GenAI applications. This integration ensures unparalleled efficiency and\\ninnovation. MLflow\'s dedication to improving LLM support has revolutionized how users create and oversee AI workflows, establishing it as an\\nindispensable tool for building advanced machine learning applications.\\n\\n### Integrations with Leading AI Tools\\n\\nMLflow has successfully incorporated support for popular AI services/frameworks such as [Hugging Face](https://huggingface.co/),\\n[LangChain](https://www.langchain.com/), and [OpenAI](https://openai.com/), while offering a unified and framework-agnostic interface for\\npackaging, evaluating, and deploying them. These integrations have opened new horizons for MLflow users, allowing them to leverage the capabilities\\nof these advanced AI tools seamlessly within their MLflow workflows.\\n\\n![GenAI Integrations](integrations.png)\\n\\n#### Model Packaging for LLMs\\n\\nRecognizing the surge in LLM popularity and utility, MLflow has focused on enhancing packaging support for these models. With MLflow\u2019s new built-in\\nmodel flavors for [Hugging Face](https://www.mlflow.org/docs/latest/llms/transformers/index.html), [LangChain](https://www.mlflow.org/docs/latest/llms/langchain/index.html)\\n, and [OpenAI](https://www.mlflow.org/docs/latest/llms/openai/index.html), users can log and deploy their LLMs and generative AI applications within minutes.\\n\\n#### Retrieval Augmented Generation (RAG) and MLflow Integration\\n\\n[Retrieval Augmented Generation (RAG)](https://mlflow.org/docs/latest/llms/rag/index.html) represents an impactful method in natural language processing.\\nIt combines pre-trained models with retrieval mechanisms to access a dataset of documents that fetch validated and curated content as opposed to relying\\non pure generation. This approach significantly improves generated responses\' contextual relevance and factual accuracy. With\\n[mlflow.evaluate()](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html), users can compare RAG systems across prompts, models, vector\\ndatabases, and more. See further details in the blog post:\\n[\\"Evaluating Retrieval Augmented Generation (RAG) Systems with MLflow\\"](https://medium.com/@dliden/evaluating-retrieval-augmented-generation-rag-systems-with-mlflow-cf09a74faadb).\\n\\n![RAG with MLflow](rag.webp)\\n\\n#### MLflow Deployment Server in MLflow 2.9.0\\n\\nThe [MLflow Deployment Server](https://www.mlflow.org/docs/latest/llms/deployments/index.html) simplifies LLM usage and management from various providers\\nlike OpenAI, MosaicML, Anthropic, Hugging Face, Cohere, MLflow models, and more. Besides supporting popular SaaS LLM providers, the MLflow Deployment Server\\nintegrates with MLflow model serving, enabling users to serve their own LLM or fine-tuned foundation models within their serving infrastructure.\\nThe MLflow Deployment Server also provides a unified inference API across different providers and services, making it much easier to query and compose\\nthem together. It uses securely stored keys from a centralized location, so users no longer need to share sensitive API keys with each member of their\\norganization. This simplifies how we interact with language models, adding an extra layer of security for managing API keys.\\n\\n#### Enhanced MLflow Evaluate API in MLflow 2.8.0\\n\\nThe [MLflow Evaluate API](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) underwent significant feature enhancements to support LLM\\nworkflows better and incorporate multiple new evaluation modes, including support for\\n[LLM-as-a-judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge). This upgraded API enables a more refined\\nand thorough analysis of LLM performance.\\n\\n#### Prompt Engineering UI in MLflow 2.7.0\\n\\nMLflow introduced the [Prompt Engineering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html), a tool specifically designed for efficient prompt\\ndevelopment, testing, and assessment in Large Language Models (LLMs). This user-friendly interface and comprehensive toolkit have notably improved the\\naccessibility and efficiency of prompt engineering within LLM workflows.\\n\\n![Prompt Engineering UI](prompt-engineering.png)\\n\\n## Community Growth and Engagement\\n\\nThe introduction of the MLflow blog in 2023 was a new addition to the MLflow website. This fresh section signifies a crucial stride toward boosting\\ncommunity involvement and fostering knowledge exchange within the MLflow ecosystem. The blog serves as a direct avenue for sharing updates about new\\nfeatures, improvements, and the future trajectory of the MLflow project.\\n\\nMLflow surpassed 45,000 followers in 2023! Not only this, across [X](https://twitter.com/MLflow?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\\nand [LinkedIn](https://www.linkedin.com/company/mlflow-org/), MLflow had over 1 million impressions, the number of times our\\ncontent was displayed to users. When it came to MLflow contributor growth, the MLflow contributor count grew from 530 to 690 in 2023.\\n\\n### MLflow Docs Overhaul\\n\\nWe have undertaken a massive initiative to reimagine how our users interact with our content. The primary goal is to enhance clarity, improve navigation,\\nand provide more in-depth resources for our community, in addition to refreshing the look and feel. The overhaul of the MLflow documentation is a significant\\nmilestone, but it\u2019s just the beginning. We have a roadmap full of exciting updates, new content, and features. Whether it\u2019s writing tutorials, sharing use cases,\\nor providing feedback, every contribution enriches the MLflow community.\\n\\n![Docs Overhaul](docs-overhaul.png)\\n\\n### 2023 Events\\n\\nMLflow made a substantial impact at two significant events: **NeurIPS 2023** and the **Data+AI Summit 2023**. These events underscored MLflow\'s commitment\\nto contributing to the evolving discourse in machine learning and AI, emphasizing its pivotal role in shaping the future of these dynamic fields.\\nThe Data+AI Summit occurred in June 2023 and featured various MLflow-related sessions. Notably, two sessions stood out:\\n\\n- [Advancements in Open Source LLM Tooling, Including MLflow](https://www.youtube.com/watch?v=WpudXKAZQNI): Explored MLflow\'s seamless integration\\n  with leading generative AI tools like Hugging Face, LangChain, and OpenAI. It highlighted how these integrations enable effortless construction of AI workflows.\\n- [How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse](https://www.youtube.com/watch?v=MYqXfMqEUq4): Offered a\\n  comprehensive insight into how the Texas Rangers baseball team leveraged MLflow and Databricks to revolutionize their approach to data analytics.\\n\\n![Big Data Baseball](baseball.png)\\n\\nIn December 2023, MLflow participated in the 37th Annual Conference of Neural Information Processing Systems (NeurIPS) held in New Orleans, LA. NeurIPS\\nstands as one of the most prestigious conferences in machine learning and computational neuroscience.\\nFor those seeking guidance on fine-tuning a Large Language Model for general-purpose instruction following, the session\\n[\\"LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms\\"](https://arxiv.org/abs/2311.13133) at NeurIPS presented valuable insights.\\n\\n### Stay Plugged In\\n\\nIf you are interested in joining the MLflow community, we\u2019d love to connect! Join us on\\n[Slack](https://mlflow-users.slack.com/ssb/redirect), [Google Groups](https://groups.google.com/g/mlflow-users), and [GitHub](https://github.com/mlflow/mlflow/).\\nWe have a roadmap full of exciting updates, new content, and features. Whether it\u2019s writing tutorials, developing code, sharing use-cases, or providing feedback, let\u2019s work together!\\nAre you already an MLflow contributor? The newly launched MLflow Ambassador Program is a great way to boost your involvement. As an MLflow Ambassador,\\nyou will serve as one of our esteemed global ambassadors, pivotal in propelling the adoption and amplifying awareness of MLflow within the global data\\ncommunity. We invite you to submit an application [here](https://forms.gle/adAPNvH6aVq4diPF9).\\n\\n![Ambassador Program](ambassador-program.png)\\n\\n### Looking Forward\\n\\n\u201cIn 2024, we\'re launching new initiatives to engage, support, and expand our community. MLflow is thrilled to broaden its horizons this year through strategic\\ncollaboration and partnership\u201d, says Ben Wilson, Software Engineer at Databricks. \u201cThis collaboration will unlock fresh opportunities for our users and\\nsignificantly contribute to MLflow\'s evolution. Stay tuned for an announcement about this exciting effort.\u201d\\n\\nThe year 2023 marked a transformative period for MLflow. By embracing the latest ML and GenAI advancements, MLflow improved its platform and made substantial\\ncontributions to the wider AI and machine learning community. To our MLflow community, we extend our deepest gratitude.\\n\\nYou have been instrumental in driving MLflow\'s success over the past year. Whether it\'s enhancing existing features, exploring new integrations, or sharing\\nyour expertise, your contributions are the lifeblood of the MLflow community. If you\'re interested in contributing to MLflow,\\n[this guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md) is an excellent starting point. Looking ahead, we\'re excited about the myriad\\npossibilities and new frontiers we can explore together.\\n\\nMLflow is poised to continue its path of growth and innovation, cementing its role as a leader in managing machine learning and GenAI workflows across the\\nentire lifecycle. We\'re eager to keep pushing the boundaries of what\'s achievable in AI and strive to create an innovative, inclusive, and open future."},{"id":"databricks-ce","metadata":{"permalink":"/mlflow-website/blog/databricks-ce","source":"@site/blog/2024-01-25-databricks-ce/index.md","title":"Streamline your MLflow Projects with Free Hosted MLflow","description":"A guide to using Databricks Community Edition with integrated managed MLflow","date":"2024-01-25T00:00:00.000Z","formattedDate":"January 25, 2024","tags":[{"label":"managed mlflow","permalink":"/mlflow-website/blog/tags/managed-mlflow"},{"label":"getting started","permalink":"/mlflow-website/blog/tags/getting-started"}],"readingTime":5.27,"hasTruncateMarker":true,"authors":[{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/img/authors/abe.png","key":"abe-omorogbe"}],"frontMatter":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","description":"A guide to using Databricks Community Edition with integrated managed MLflow","slug":"databricks-ce","thumbnail":"/img/blog/databricks-ce.png","authors":["abe-omorogbe"],"tags":["managed mlflow","getting started"]},"unlisted":false,"prevItem":{"title":"2023 Year in Review","permalink":"/mlflow-website/blog/mlflow-year-in-review"},"nextItem":{"title":"Custom MLflow Models with mlflow.pyfunc","permalink":"/mlflow-website/blog/custom-pyfunc"}},"content":"If you\'re new to MLflow and want to get started with a fully-managed and completely free deployment of MLflow, this blog will show you how to get started using MLflow in minutes.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Streamline Your ML Projects: Get Started with Hosted MLflow for Free\\n\\nExplore the world of big data and machine learning with [Databricks Community Edition (CE)](https://community.cloud.databricks.com/), a free, limited[^1] version of the Databricks platform.\\nIdeal for beginners and those new to Databricks and MLflow, this edition streamlines the learning curve by offering a managed environment. It eliminates the complexity of manually\\nsetting up a tracking server. Databricks CE includes hosted MLflow, enabling efficient management and visualization of your MLflow experiments. This makes it a prime choice for\\ndeveloping machine learning projects in a user-friendly interface, allowing you to connect from your favorite IDE, notebook environment, or even from within Databricks CE\'s notebooks.\\n\\n[^1]: The Model Registry and Model Deployment features are not available in the Databricks Community Edition.\\n\\n### Benefits of Using Databricks CE for MLflow\\n\\nMLflow is an open-source framework compatible with any platform, yet it offers distinct benefits when used on Databricks (including the Community Edition, CE) compared to other platforms. These advantages include:\\n\\n1. **Cost-Effective**: Free of charge, MLflow on Databricks CE is perfect for educational purposes and small-scale projects.\\n\\n2. **Simple Setup**: Gain access to a fully managed tracking server and user interface from any location. To connect to Databricks CE, just execute `mlflow.login()`.\\n\\n3. **Easy Sharing**: In the Databricks ecosystem, sharing your notebooks is straightforward and hassle-free.\\n\\n4. **Seamless Integration**: Databricks CE allows for direct storage and visualization of MLflow experiments, runs, and models.\\n\\n5. **Scalability**: MLflow on Databricks CE provides an easy path to scale your projects. It also integrates seamlessly with a wide range of data tools available on the Databricks platform.\\n\\n### Scenario\\n\\nIn this blog, we will walk through running ML experiments on your local device and tracking them on an [MLflow tracking server hosted on Databricks CE](https://mlflow.org/docs/latest/tracking.html#common-setups)\\n\\nTo give you an idea of the options available for running MLflow, the figure below shows what is possible for common setup configurations.\\n\\n![Remote Tracking Server](remote-tracking-server.png)\\n\\nFor this blog, we\'re showing #3, using a remote (fully managed) tracking server.\\n\\n### Step-by-Step Guide\\n\\n#### 1. Creating a Databricks CE Account\\n\\nIf you haven\'t already, you can [sign up for a free account](https://www.databricks.com/try-databricks#account). The process is quick, typically taking no more than 3 minutes.\\n\\nFill out the signup form and select \u201cGet started with Community Edition.\u201d\\n\\n![Databricks CE Signup Page](ce-signup.png)\\n\\nOnce signed up, you\'ll get information on how to set a password that you can use to login to CE with[^2].\\n\\n[^2]: Databricks CE only supports basic authorization signin (username / password). For more advanced and secure authorization setups, only the full Databricks product supports those.\\n\\n#### 2. Installing Dependencies\\n\\nBefore you start, ensure that you have the necessary packages installed. Run the following command in your favorite IDE or notebook from your device:\\n\\n```bash\\n%pip install -q mlflow databricks-sdk\\n```\\n\\n#### 3. Setting Up Databricks CE Authentication\\n\\nThe main advantage of Databricks Community Edition (CE) is its convenience: it offers an MLflow tracking server without requiring\\n[local infrastructure setup](https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server.html). You can easily access this server through the\\n[mlflow.login()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login) function after creating your CE account, streamlining the process for MLflow experiment tracking.\\n\\nTo authenticate with Databricks CE, use the [mlflow.login()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login) function. This will prompt you for:\\n\\n- **Databricks Host**: `https://community.cloud.databricks.com/`\\n\\n- **Username**: Your Databricks CE email address.\\n\\n- **Password**: Your Databricks CE password.\\n\\nUpon successful authentication, you will see a confirmation message.\\n\\n```python\\nimport mlflow\\n\\nmlflow.login()\\n\\n# Follow the prompts for authentication\\n```\\n\\n#### 4. Connect to Hosted MLflow and Track Experiments with Databricks CE\\n\\nAfter you login from your local machine, start an experiment with [mlflow.set_experiment()](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=mlflow%20set_experiment#mlflow.set_experiment) and log some metrics. For instance:\\n\\n```python\\nmlflow.set_experiment(\\"/Users/\\\\<email>/check-databricks-ce-connection\\")\\n\\nwith mlflow.start_run():\\n\\n\xa0\xa0\xa0\xa0mlflow.log_metric(\\"foo\\", 1)\\n\\n\xa0\xa0\xa0\xa0mlflow.log_metric(\\"bar\\", 2)\\n```\\n\\n> **Note**: The Databricks environment requires you to set experiments with the directory (from root)\\n\\n    `/Users/{your email address for your account}/{name of your experiment}`, which is different from the behavior in self-hosted MLflow (and when running MLFlow locally).\\n\\n#### 5. Viewing Your Experiment in Databricks CE\\n\\nNow let\u2019s navigate to Databricks CE to view the experiment result. Log in to your [Databricks CE](https://community.cloud.databricks.com/)\\naccount, and click on the top left to select machine learning in the drop down list. Finally, click on the experiment icon. See the screenshots below:\\n\\nNavigate to the Machine Learning Section\\n\\n![Navigate to ML Section of Databricks CE](navigate-to-experiments.png)\\n\\nNavigate to the MLflow UI\\n\\n![Navigate to the MLflow UI on Databricks CE](navigate-to-mlflow-ui.png)\\n\\nIn the \u201cExperiments\u201d view, you should be able to find the experiment `/Users/{your email}/check-databricks-ce-connection`, similar to:\\n\\n![Experiment view of Databricks MLflow server](view-experiment.png)\\n\\nClicking on the run name, which in this example is \'youthful-lamb-287\' (note that you will see a different, randomly generated name in your CE console),\\nwill take you to the run view that looks similar to the following:\\n\\n![Run view of Databricks MLflow server](view-run.png)\\n\\nIn the run view, you will see our dummy metrics `\u201cfoo\u201d` and `\u201cbar\u201d` have been logged successfully.\\n\\n#### 6. Run any MLflow tutorial in Databricks CE\\n\\nIf you want to try a tutorial from the MLflow website, you can use Databricks CE to quickly test (and modify, if you\'re inclined) the tutorial. For example, if you wanted to test\\nthe [Creating Custom Pyfunc tutorial](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.html):\\n\\n1. Click Workspace and\xa0 select \u201cImport notebook\u201d\\n\\n![Import a Notebook](import-notebook.png)\\n\\n2. Use the `URL` option to import the notebook directly from the MLflow documentation website. For this example, to import, replace the last element of the url\\n   from `html` to `ipynb`. This can be done with any of the tutorial or guide notebooks that are hosted on the MLflow website.\\n\\n   .../notebooks/basic-pyfunc.~~html~~ &rarr; .../notebooks/basic-pyfunc.**ipynb**[^3]\\n\\n[^3]: Or you can [download the notebook](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb) and manually load it in the UI by selecting `File` instead of `URL`.\\n\\n![Select the Notebook for Importing](import-notebook-2.png)\\n\\n### Conclusion\\n\\nDatabricks Community Edition (CE) offers an accessible and collaborative platform for MLflow experiment tracking, presenting several advantages. Its setup process is effortless\\nand quick, providing a user-friendly experience. Additionally, it\'s free to use, making it an ideal choice for beginners, learners, and small-scale projects.\\n\\n### Getting started\\n\\nTry out the notebook on [Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2830662238121329/3266358972198675/8538262732615206/latest.html)\\n\\n### Further Reading\\n\\n- Learn more about [different methods to setup your tracking server](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#minute-tracking-server-overv)\\n\\n- Learn more about running [Tutorial Notebooks ](https://mlflow.org/docs/latest/getting-started/running-notebooks/index.html)with Databricks CE"},{"id":"custom-pyfunc","metadata":{"permalink":"/mlflow-website/blog/custom-pyfunc","source":"@site/blog/2024-01-23-custom-pyfunc/index.md","title":"Custom MLflow Models with mlflow.pyfunc","description":"A guide for creating custom MLflow models","date":"2024-01-23T00:00:00.000Z","formattedDate":"January 23, 2024","tags":[{"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"label":"models","permalink":"/mlflow-website/blog/tags/models"}],"readingTime":15.29,"hasTruncateMarker":true,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/img/authors/daniel_liden.png","key":"daniel-liden"}],"frontMatter":{"title":"Custom MLflow Models with mlflow.pyfunc","description":"A guide for creating custom MLflow models","slug":"custom-pyfunc","authors":["daniel-liden"],"tags":["pyfunc","models"],"thumbnail":"/img/blog/custom-pyfunc.png"},"unlisted":false,"prevItem":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","permalink":"/mlflow-website/blog/databricks-ce"},"nextItem":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","permalink":"/mlflow-website/blog/mlflow-autolog"}},"content":"If you\'re looking to learn about all of the flexibility and customization that is possible within\\nMLflow\'s custom models, this blog will help you on your journey in understanding more about how to\\nleverage this powerful and highly customizable model storage format.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Welcome](./header.png)\\n\\nMLflow offers built-in methods for logging and working with models from many popular machine\\nlearning and generative AI frameworks and model providers, such as scikit-learn, PyTorch,\\nHuggingFace transformers, and LangChain. For example,\\n[mlflow.sklearn.log_model](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)\\nwill log a scikit-learn model as an MLflow artifact without requiring you to define custom methods for\\nprediction or for handling artifacts.\\n\\nIn some cases, however, you might be working in a framework for which MLflow does not have\\nbuilt-in methods, or you might want something different than the model\u2019s default prediction\\noutputs. In those cases, MLflow allows you to create custom models to work with essentially\\nany framework and to integrate custom logic to existing supported frameworks.\\n\\nIn its simplest form, all that\u2019s required is to define a custom predict method and log the model.\\nThe following example defines a simple pyfunc model that just returns the square of its input:\\n\\n```python\\nimport mlflow\\n\\n# Define a custom model\\nclass MyModel(mlflow.pyfunc.PythonModel):\\n    def predict(self, context, model_input):\\n        # Directly return the square of the input\\n        return model_input**2\\n\\n\\n# Save the model\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"model\\",\\n        python_model=MyModel()\\n    )\\n\\n# Load the model\\nloaded_model = mlflow.pyfunc.load_model(\\n    model_uri=model_info.model_uri\\n)\\n\\n# Predict\\nloaded_model.predict(2)\\n```\\n\\nLet\u2019s dig into how this works, starting with some basic concepts.\\n\\n## Models and Model Flavors\\n\\n![Models and Flavors](models-and-flavors.png)\\n\\nAn MLflow model is a directory that includes everything needed to reproduce a machine learning model\\nacross different environments. Aside from the stored model itself, the most important component\\nstored is an `MLmodel` YAML file that specifies the model\u2019s supported model flavors.\\nA [model flavor](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part1-named-flavors.html#components-of-a-model-in-mlflow)\\nis a set of rules specifying how MLflow can interact with the model (i.e., save it, load it, and\\nget predictions from it).\\n\\nWhen you create a custom model in MLflow, it has the `python_function` or pyfunc model flavor,\\nwhich is a kind of \u201cuniversal translator\u201d across formats in MLflow. When you save a model in MLflow\\nusing a built-in model flavor, e.g. with [mlflow.sklearn.log_model](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model),\\nthat model also has the pyfunc model flavor in addition to its framework-specific flavor.\\nHaving both framework-specific and pyfunc model flavors allows you to use the model via the\\nframework\u2019s native API (e.g., `scikit-learn`) or via the pyfunc flavor\u2019s framework-agnostic inference API.\\n\\nModels with the pyfunc flavor are loaded as instances of the [mlflow.pyfunc.PyfuncModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.PyFuncModel)\\nclass, which exposes a standardized predict method. This enables straightforward inference through a single\\nfunction call, regardless of the underlying model\'s implementation details.\\n\\n## Defining Custom MLflow Pyfunc Models\\n\\nSaving a model from any supported machine learning framework as an MLflow model results in the\\ncreation of a pyfunc model flavor that provides a framework-agnostic interface for managing and\\nusing the model. But what if you\u2019re using a framework without an MLflow integration, or you\u2019re\\ntrying to elicit some custom behavior from a model? [Custom pyfunc models](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models)\\nallow you to work with essentially any framework and to integrate custom logic.\\n\\nTo implement a custom pyfunc model, define a new Python class inheriting from the PythonModel class\\nand implement the necessary methods. Minimally, this will involve implementing a custom predict\\nmethod. Next, create an instance of your model and log or save the model. Once you\u2019ve loaded the\\nsaved or logged model, you can use it for predictions.\\n\\n![Creating a custom model](custom-model-creation.png)\\n\\nLet\u2019s work through a few examples, each adding a little more complexity and highlighting different\\naspects of defining a custom pyfunc model. We\u2019ll cover four main techniques for implementing custom\\nbehaviors in pyfunc models:\\n\\n1. Implementing a custom `predict` method\\n2. Implementing a custom `__init__` method\\n3. Implementing a custom `load_context` method\\n4. Implementing user-defined custom methods\\n\\n![Pyfunc model customization](custom-pyfunc-types.png)\\n\\n### Defining a custom `predict` method\\n\\nAt a minimum, a pyfunc model should specify a custom predict method that defines what happens when\\nwe call `model.predict`. Here\u2019s an example of a custom model that applies a simple linear\\ntransformation to the model inputs, multiplying each input by two and adding three:\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass SimpleLinearModel(PythonModel):\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(2 * model_input + 3)\\n\\n\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"linear_model\\",\\n        python_model=SimpleLinearModel(),\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nNote that you can (and should) also include a [signature](https://mlflow.org/docs/latest/models.html#model-signature)\\nand an [input example](https://mlflow.org/docs/latest/models.html#model-input-example) when saving/logging a\\nmodel. If you pass an input example, the signature will be inferred automatically. The model\\nsignature provides a way for MLflow to enforce correct usage of your model.\\n\\nOnce we\u2019ve defined the model path and saved an instance of the model, we can load the saved model\\nand use it to generate predictions:\\n\\n```python\\n# Now the model can be loaded and used for predictions\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\nmodel_input = pd.DataFrame([1, 2, 3])  # Example input data\\nprint(loaded_model.predict(model_input))  # Outputs transformed data\\n```\\n\\nWhich will return:\\n\\n```text\\n:    0\\n: 0  5\\n: 1  7\\n: 2  9\\n```\\n\\nNote that if a custom `predict` method is all you need\u2014that is, if your model does not have any\\nartifacts that require special handling\u2014you can save or log the `predict` method directly without\\nneeding to wrap it in a Python class:\\n\\n```python\\nimport mlflow\\nimport pandas as pd\\n\\n\\ndef predict(model_input):\\n    \\"\\"\\"\\n    Applies a simple linear transformation\\n    to the input data. For example, y = 2x + 3.\\n    \\"\\"\\"\\n    # Assuming model_input is a pandas DataFrame with one column\\n    return pd.DataFrame(2 * model_input + 3)\\n\\n\\n# Pass predict method as python_model argument to save/log model\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"simple_function\\",\\n        python_model=predict,\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nNote that with this approach, we **must include** an input example along with the custom predict\\nmethod. We also have to modify the predict method such that it takes only one input (i.e., no self or context).\\nRunning this example and then loading with the same code as the preceding code block will retain the same output as\\nthe example using a class definiton.\\n\\n### Parameterizing the custom model\\n\\nNow suppose we want to parameterize the custom linear function model so that it can be used with\\ndifferent slopes and intercepts. We can define the `__init__` method to set up custom parameters,\\nas in the following example. Note that the custom model class\u2019s `__init__` method should not be used\\nto load external resources like data files or pretrained models; these are handled in the\\n`load_context` method, which we\u2019ll discuss shortly.\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass ParameterizedLinearModel(PythonModel):\\n    def __init__(self, slope, intercept):\\n        \\"\\"\\"\\n        Initialize the parameters of the model. Note that we are not loading\\n        any external resources here, just setting up the parameters.\\n        \\"\\"\\"\\n        self.slope = slope\\n        self.intercept = intercept\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(self.slope * model_input + self.intercept)\\n\\n\\nlinear_model = ParameterizedLinearModel(10, 20)\\n\\n# Saving the model with mlflow\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"parameter_model\\",\\n        python_model=linear_model,\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nAgain, we can load this model and make some predictions:\\n\\n```python\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\nmodel_input = pd.DataFrame([1, 2, 3])  # Example input data\\nprint(loaded_model.predict(model_input))  # Outputs transformed data\\n```\\n\\n```text\\n:     0\\n: 0  30\\n: 1  40\\n: 2  50\\n```\\n\\nThere are many cases where we might want to parameterize a model in this manner. We can define\\nvariables in the `__init__` method to:\\n\\n- Set model hyperparameters.\\n- A/B test models with different parameter sets.\\n- Set user-specific customizations.\\n- Toggle features.\\n- Set, e.g., access credentials and endpoints for models that access external APIs.\\n\\nIn some cases, we may want to be able to pass parameters at inference time rather than when we\\ninitialize the model. This can be accomplished with\\n[model inference params](https://mlflow.org/docs/latest/models.html#model-inference-params). To use\\ninference params, we must pass a valid model signature including `params`. Here\u2019s how to adapt the\\npreceding example to use inference params:\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.models import infer_signature\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass LinearFunctionInferenceParams(PythonModel):\\n    def predict(self, context, model_input, params):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        slope = params[\\"slope\\"]\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(slope * model_input + params[\\"intercept\\"])\\n\\n\\n# Set default params\\nparams = {\\"slope\\": 2, \\"intercept\\": 3}\\n\\n# Define model signature\\nsignature = infer_signature(model_input=pd.DataFrame([10, 20, 30]), params=params)\\n\\n# Save the model with mlflow\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"model_with_params\\",\\n        python_model=LinearFunctionInferenceParams(),\\n        signature=signature,\\n    )\\n```\\n\\nAfter loading the model as before, you can now pass a `params` argument to the `predict` method,\\nenabling you to use the same loaded model for different combinations of parameters:\\n\\n```python\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\n\\nparameterized_predictions = loaded_model.predict(\\n    pd.DataFrame([10, 20, 30]), params={\\"slope\\": 2, \\"intercept\\": 10}\\n)\\nprint(parameterized_predictions)\\n```\\n\\n```text\\n:     0\\n: 0  30\\n: 1  50\\n: 2  70\\n```\\n\\n### Loading external resources with `load_context`\\n\\nCustom models often require external files such as model weights in order to perform inference.\\nThese files, or artifacts, must be handled carefully to avoid unnecessarily loading files into\\nmemory or errors during model serialization. When building custom pyfunc models in MLflow, you can\\nuse the `load_context` method to handle model artifacts correctly.\\n\\nThe `load_context` method receives a `context` object containing artifacts the model can use during\\ninference. You can specify these artifacts using the `artifacts` argument when saving or logging\\nmodels, making them accessible to the `load_context` method via the `context.artifacts` dictionary.\\n\\nIn practice, the `load_context` method often initializes the model called by the `predict` method by\\nhandling the loading of model artifacts.\\n\\nThis raises an important question: why do we load artifacts and define the model in the `load_context`\\nmethod and not in `__init__` or directly in `predict`? Correct usage of `load_context` is essential\\nfor the maintainability, efficiency, scalability, and portability of MLflow pyfunc models. This is because:\\n\\n- The `load_context` method is executed once when the model is loaded via `mlflow.pyfunc.load_model`.\\n  This setup ensures that resource-intensive processes defined within this method, such as loading\\n  large model files, are not repeated unnecessarily. If artifact loading is done in the predict\\n  method, it will occur every single time a prediction is made. This is highly inefficient for\\n  resource-intensive models.\\n- Saving or logging an MLflow `pyfunc` model involves serializing the Python model class (the subclass\\n  of [mlflow.pyfunc.PythonModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel)\\n  you created) and its attributes. Complex ML models are not always compatible with the methods used\\n  to serialize the Python object, which can lead to errors if they are created as attributes of the Python object.\\n\\nAs an example, suppose we want to load a large language model (LLM) in the `gguf` model format\\n(a file format designed for storing models for inference) and run it with the\\n[ctransformers library](https://pypi.org/project/ctransformers). At the time of writing, there is\\nno built-in model flavor that lets us use `gguf` models for inference, so we\u2019ll create a custom\\npyfunc model that loads the required libraries and model files in the `load_context` method.\\nSpecifically, we\u2019re going to load a quantized version of the [AWQ version of the Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ).\\n\\nFirst, we\u2019ll download the model snapshot using the huggingface hub cli:\\n\\n```bash\\nhuggingface-cli download TheBloke/Mistral-7B-v0.1-GGUF \\\\\\n                mistral-7b-v0.1.Q4_K_M.gguf \\\\\\n                --local-dir /path/to/mistralfiles/ \\\\\\n                --local-dir-use-symlinks False\\n```\\n\\nAnd then we\u2019ll define our custom `pyfunc` model. Note the addition of the `load_context` method:\\n\\n```python\\nimport ctransformers\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\nclass CTransformersModel(PythonModel):\\n    def __init__(self, gpu_layers):\\n        \\"\\"\\"\\n        Initialize with GPU layer configuration.\\n        \\"\\"\\"\\n        self.gpu_layers = gpu_layers\\n        self.model = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Load the model from the specified artifacts directory.\\n        \\"\\"\\"\\n        model_file_path = context.artifacts[\\"model_file\\"]\\n\\n        # Load the model\\n        self.model = ctransformers.AutoModelForCausalLM.from_pretrained(\\n            model_path_or_repo_id=model_file_path,\\n            gpu_layers=self.gpu_layers,\\n        )\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Perform prediction using the loaded model.\\n        \\"\\"\\"\\n        if self.model is None:\\n            raise ValueError(\\n                \\"The model has not been loaded. \\"\\n                \\"Ensure that \'load_context\' is properly executed.\\"\\n            )\\n        return self.model(model_input)\\n```\\n\\nThere\u2019s a lot going on here, so let\u2019s break it down. Here are the key points:\\n\\n- As before, we use the `__init__` method to parameterize the model (in this case, to set the\\n  `gpu_layers` argument for the model).\\n- The purpose of the `load_context` method is to load the artifacts required for use in the\\n  `predict` method. In this case, we need to load the model and its weights.\\n- You\u2019ll notice that we reference `context.artifacts[\\"model_file\\"]`. This comes from the artifacts\\n  argument to `mlflow.pyfunc.save_model` or `mlflow.pyfunc.load_model`, as shown in the following\\n  code snippet. This is an important part of working with `pyfunc` models. The `predict` and\\n  `load_context` methods can access the artifacts defined in the artifacts argument to the\\n  `save_model` or `log_model` method via the `context.artifacts` object. `load_context` is executed\\n  when the model is loaded via `load_model`; as described earlier, this provides a way to ensure that\\n  the potentially time-consuming initialization of a model does not occur each time the model is used\\n  for prediction.\\n\\nNow we can initialize and save an instance of the model. Note the artifacts argument to the\\n`save_model` function:\\n\\n```python\\n# Create an instance of the model\\nmistral_model = CTransformersModel(gpu_layers=50)\\n\\n# Log the model using mlflow with the model file as an artifact\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"mistral_model\\",\\n        python_model=mistral_model,\\n        artifacts={\\"model_file\\": model_file_path},\\n        pip_requirements=[\\n            \\"ctransformers==0.2.27\\",\\n        ],\\n    )\\n\\n# Load the saved model\\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\\n\\n# Make a prediction with the model\\nloaded_model.predict(\\"Question: What is the MLflow Pyfunc model flavor?\\")\\n```\\n\\nTo recap: correct use of the `load_context` method helps to ensure efficient handling of model\\nartifacts and prevents errors in serialization that could result from attempting to define artifacts\\nas model attributes.\\n\\n### Defining custom methods\\n\\nYou can define your own methods in the custom `pyfunc` model to handle tasks like preprocessing\\ninputs or post-processing outputs. These custom methods can then be called by the predict method.\\nKeep in mind that these custom methods, just like `__init__` and `predict`, should **not be used** for\\nloading artifacts. Loading artifacts is the exclusive role of the `load_context` method.\\n\\nFor example, we can modify the `CTransformersModel` to incorporate some prompt formatting as follows:\\n\\n```python\\nimport ctransformers\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\nclass CTransformersModel(PythonModel):\\n    def __init__(self, gpu_layers):\\n        \\"\\"\\"\\n        Initialize with GPU layer configuration.\\n        \\"\\"\\"\\n        self.gpu_layers = gpu_layers\\n        self.model = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Load the model from the specified artifacts directory.\\n        \\"\\"\\"\\n        model_file_path = context.artifacts[\\"model_file\\"]\\n\\n        # Load the model\\n        self.model = ctransformers.AutoModelForCausalLM.from_pretrained(\\n            model_path_or_repo_id=model_file_path,\\n            gpu_layers=self.gpu_layers,\\n        )\\n\\n    @staticmethod\\n    def _format_prompt(prompt):\\n        \\"\\"\\"\\n        Formats the user\'s prompt\\n        \\"\\"\\"\\n        formatted_prompt = (\\n            \\"Question: What is an MLflow Model?\\\\n\\\\n\\"\\n            \\"Answer: An MLflow Model is a directory that includes \\"\\n            \\"everything that is needed to reproduce a machine \\"\\n            \\"learning model across different environments. \\"\\n            \\"It is essentially a container holding the trained model \\"\\n            \\"files, dependencies, environment details, input examples, \\"\\n            \\"and additional metadata. The directory also includes an \\"\\n            \\"MLmodel YAML file, which describes the different \\"\\n            f\\"flavors of the model.\\\\n\\\\nQuestion: {prompt}\\\\nAnswer: \\"\\n        )\\n\\n        return formatted_prompt\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Perform prediction using the loaded model.\\n        \\"\\"\\"\\n        if self.model is None:\\n            raise ValueError(\\n                \\"Model was not loaded. Ensure that \'load_context\' \\"\\n                \\"is properly executed.\\"\\n            )\\n        return self.model(self._format_prompt(model_input))\\n```\\n\\nNow the `predict` method can access the private `_format_prompt` static method to apply custom formatting to the prompts.\\n\\n### Dependencies and Source Code\\n\\nThe custom `CTransformersModel` defined above uses the `ctransformers` library. There are a few\\ndifferent approaches for making sure this library (and any other source code, including from your\\nlocal device) is correctly loaded with your model. Correctly specifying dependencies is essential\\nfor ensuring that custom models work as expected across environments.\\n\\nThere are three main approaches to be aware of for specifying dependencies:\\n\\n- Define pip requirements explicitly with the `pip_requirements` argument to `save_model` or `log_model`.\\n- Add extra pip requirements to an automatically generated set of requirements with the\\n  `extra_pip_requirements` argument to `save_model` or `log_model`.\\n- Define a Conda environment with the `conda_env` argument to `save_model` or `log_model`.\\n\\nEarlier, we used the first approach to specify that the `ctransformers` library was needed:\\n\\n```python\\n# Log the model using mlflow with the model file as an artifact\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.save_model(\\n        artifact_path=\\"mistralmodel\\",\\n        python_model=mistral_model,\\n        artifacts={\\"model_file\\": \\"path/to/mistral/model/on/local/filesystem\\"},\\n        pip_requirements=[\\n            \\"ctransformers==0.2.27\\",\\n        ],\\n    )\\n```\\n\\nIf you do not specify dependencies explicitly, MLflow will attempt to infer the correct set of\\nrequirements and environment details. To enable greater accuracy, it is **strongly recommended** to\\ninclude an `input_example` when saving or logging your model due to the internal execution of a\\nsample inference step that will capture any loaded library references associated with the inference\\nexecution, enabling a higher probability that the correct dependencies will be recorded.\\n\\nYou can also work with custom code on your own filesystem with the `code_path` argument.\\n`code_path` takes a list of paths to Python file dependencies and prepends them to the system\\npath before the model is loaded, so the custom pyfunc model can import from these modules.\\n\\nSee the documentation for the [log_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.log_model) and\\n[save_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.save_model)\\nfunctions for more details on the accepted formats for `pip`, `Conda`, and local code requirements.\\n\\n### Summary: Custom Pyfunc Models in MLflow\\n\\nMLflow has built-in methods for working with models from many popular machine learning frameworks,\\nsuch as [scikit-learn](https://www.mlflow.org/docs/latest/models.html#scikit-learn-sklearn),\\n[PyTorch](https://www.mlflow.org/docs/latest/models.html#pytorch-pytorch), and\\n[Transformers](https://www.mlflow.org/docs/latest/llms/transformers/index.html). You can define your own custom\\n`mlflow.pyfunc` model when you want to work with models that do not yet have built-in model\\nflavors, or when you want to implement a custom predict method for models with built-in model flavors.\\n\\nThere are several ways to customize `pyfunc` models to get the desired behavior. Minimally, you can\\nimplement a custom `predict` method. If your model requires saving or loading artifacts, you should also\\nimplement a `load_context` method. For further customization, you can use the `__init__` method for\\nsetting custom attributes and define your own custom methods for pre- and post-processing.\\nCombining these approaches gives you the ability to flexibly define custom logic for your machine\\nlearning models.\\n\\n### Further Learning\\n\\nInterested in learning more about custom `pyfunc` implementations? You can visit:\\n\\n- [Custom Pyfuncs for Advanced LLMs with MLflow](https://www.mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html)\\n- [Build Custom Python Function Models for traditional ML](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n- [Custom PyFunc notebook examples](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/index.html)"},{"id":"mlflow-autolog","metadata":{"permalink":"/mlflow-website/blog/mlflow-autolog","source":"@site/blog/2023-11-30-mlflow-autolog/index.md","title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","description":"Looking to learn more about the autologging functionality included in MLflow? Look no further than this primer on the basics of using this powerful and time-saving feature!","date":"2023-11-30T00:00:00.000Z","formattedDate":"November 30, 2023","tags":[{"label":"autolog","permalink":"/mlflow-website/blog/tags/autolog"}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/img/authors/daniel_liden.png","key":"daniel-liden"}],"frontMatter":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","slug":"mlflow-autolog","tags":["autolog"],"authors":["daniel-liden"],"thumbnail":"/img/blog/mlflow-autolog.png"},"unlisted":false,"prevItem":{"title":"Custom MLflow Models with mlflow.pyfunc","permalink":"/mlflow-website/blog/custom-pyfunc"},"nextItem":{"title":"MLflow Docs Overhaul","permalink":"/mlflow-website/blog/mlflow-docs-overhaul"}},"content":"Looking to learn more about the autologging functionality included in MLflow? Look no further than this primer on the basics of using this powerful and time-saving feature!\\n\\n# Introduction to [mlflow.autolog](https://www.mlflow.org/docs/latest/tracking/autolog.html)\\n\\nRobust logging practices are central to the iterative development and improvement of machine learning models. Carefully tracking metrics, parameters, and artifacts can be challenging when working with complex machine learning libraries or when experimenting with multiple different frameworks with varying APIs and selections of different objects and values to track.\\n\\n\x3c!-- truncate --\x3e\\n\\nMLflow\u2019s **automatic logging functionality** offers a simple solution that is compatible with many widely-used machine learning libraries, such as [PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html), [Scikit-learn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog), and [XGBoost](https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog). Using `mlflow.autolog()` instructs MLflow to capture essential data without requiring the user to specify what to capture manually. It is an accessible and powerful entrypoint for MLflow\u2019s logging capabilities.\\n\\nTo enable automatic logging, simply add the following line to your machine learning scripts/notebooks, before initiating activities like model training or evaluation that may include information or artifacts you would like to log:\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.autolog()\\n```\\n\\n## Autolog features\\n\\nWhen a data science workflow includes `mlflow.autolog()`, MLflow will automatically log:\\n\\n- **Metrics**: standard training and evaluation measures such as accuracy and F1 score;\\n- **Parameters**: hyperparameters, such as learning rate and number of estimators; and\\n- **Artifacts**: important files, such as trained models.\\n\\nMLflow\u2019s automatic logging captures details tailored to the specific activities of the library being used: different libraries will result in different logged objects and data. In addition, MLflow logs key metadata such as software versions, a git commit hash, and the file name from which the run was initiated. By documenting the system\'s state during model training, MLflow aims to facilitate environment reproducibility and provide audit lineage, minimizing the possibility of inference issues that could arise from package regressions or deprecations in newer library versions.\\n\\nThe specifics of what is captured through automatic logging depend on the libraries used. Additionally, MLflow captures contextual metadata such as software versions, git commit hash, and the name of the file from which the run was launched. By logging as much detail as possible about the state of the system that trained the model, MLflow can offer environment reproducibility and audit lineage, minimizing the possibility of inference issues resulting from, for example, package regressions or deprecations.\\n\\n## Basic Usage of `mlflow.autolog`\\n\\nYou can access auto logging functionality in two different ways:\\n\\n1. Via the `mlflow.autolog()` function, which enables and configures automatic logging across all supported libraries. This provides a broad, one-size-fits-all approach when working with multiple libraries and is ideal for prototyping and exploratory analysis of a machine learning pipeline.\\n2. Via the library-specific autolog functions, such as `mlflow.sklearn.autolog()`, which enable finer-grained logging configuration for individual libraries. For example, `mlflow.pytorch.autolog()` includes the `log_every_n_epoch` and `log_every_n_step` arguments for specifying how often to log metrics.\\n\\nRegardless of which of these two approaches you use, you do not need to manually initialize an MLflow run with [start_run()](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run) in order to have a run created and for your model, parameters, and metrics to be captured in MLflow.\\n\\n### Example\\n\\n```python\\nimport mlflow\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Generate a 3-class classification problem\\nX, y = datasets.make_classification(\\n    n_samples=1000,\\n    class_sep=0.5,\\n    random_state=42,\\n    n_classes=3,\\n    n_informative=3,\\n)\\n\\n# Split the data into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Enable autolog\\nmlflow.autolog()  # or mlflow.sklearn.autolog()\\n\\n# Initialize the classifier with n_estimators=200 and max_depth=10\\nclf = RandomForestClassifier(n_estimators=200, max_depth=10)\\n\\n# Fit the classifier to the data.\\n# The `fit` method is patched to perform autologging. When engaged in training, a\\n# run is created and the parameters are logged.\\n# After the fit is complete, the model artifact is logged to the run.\\nclf.fit(X_train, y_train)\\n\\n# Score the model on the data\\n# The current active run is retrieved during calling `score` and the loss metrics are logged\\n# to the active MLflow run.\\nclf.score(X_val, y_val)\\n\\n# Visualize the automatically logged run results to validate what we recorded\\nmlflow.last_active_run()\\n```\\n\\nThe above logs model parameters, metrics, and the model to an MLflow run. The output result of the final statement ([mlflow.last_active_run()](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.last_active_run)) in the above example, which will return data from the run on model metrics, parameters, and logged artifacts (results truncated) is as shown below:\\n\\n```text\\n<Run: data=<RunData:\\nmetrics={\'RandomForestClassifier_score_X_val\': 0.72,\\n         \'training_accuracy_score\': 0.99625,\\n         \'training_f1_score\': 0.9962547564333545,\\n         \'training_log_loss\': 0.3354604497935824,\\n         \'training_precision_score\': 0.9962921348314606,\\n         \'training_recall_score\': 0.99625,\\n         \'training_roc_auc\': 0.9998943433719795,\\n         \'training_score\': 0.99625\\n         },\\n params={\'bootstrap\': \'True\',\\n         \'ccp_alpha\': \'0.0\',\\n         \'class_weight\': \'None\',\\n         \'criterion\': \'gini\',\\n         \'max_depth\': \'10\',\\n         \'max_features\': \'sqrt\',\\n         \'max_leaf_nodes\': \'None\',\\n         [...],\\n         },\\ntags={\'estimator_class\': \'sklearn.ensemble._forest.RandomForestClassifier\',\\n      \'estimator_name\': \'RandomForestClassifier\',\\n      \'mlflow.autologging\': \'sklearn\',\\n      [...]\\n}, [...]>>\\n```\\n\\nYou can also access these in the mlflow ui by executing [mlflow ui](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) on a command line terminal.\\n\\n![The MLflow Tracking UI](./autolog_in_ui.png)\\n\\nThe MLflow UI also allows you to graphically compare different metrics and parameters across multiple runs, including those generated via `mlflow.autolog`.\\n\\n![Run comparison of autologged runs in the MLflow UI](./autolog_compare_runs.png)\\n\\n## Configuration and Customization\\n\\nThe automatic logging functions take many arguments that give the user greater control over logging behavior. For example, `mlflow.autolog()` includes `log_models` and `log_datasets` arguments (both `True` by default) that specify whether models and dataset information should be saved to the MLflow run, enabling you to specify what is actually logged. To disable automatic logging of datasets while continuing to log all the usual elements, simply disable the autologging of datasets feature by setting `mlflow.autolog(log_datasets=False)` before fitting a model. You can also adjust the behavior of library-specific autolog functions: for example, the `mlflow.sklearn.autolog()` function includes a `max_tuning_runs` argument that specifies how many nested runs to capture when performing hyperparameter searches.\\n\\n`mlflow.autolog()` can be used in combination with the library-specific autolog functions to control the logging behavior for specific libraries. The library-specific autolog call will always supersede `mlflow.autolog()`, regardless of the order in which they are called. For example, combining `mlflow.autolog()` with `mlflow.sklearn.autolog(disable=True)` will result in automatic logging for all supported libraries except for `scikit-learn`.\\n\\nIt is important to consult the documentation for the specific framework(s) you are using in order to understand what is logged automatically and what configuration options are available. See the [further reading section below](#further-reading) for links.\\n\\n## Conclusion and Next Steps\\n\\nMLflow\'s autologging capabilities and library-specific automatic logging functions provide a straightforward starting point for MLflow tracking with little or no required configuration. They log key metrics, parameters, and artifacts from many popular machine learning libraries, allowing users to track their machine learning workflows without writing custom tracking code.\\n\\nThey are not, however, the right solution for all use cases. If you only need to track a handful of specific metrics, enabling automatic logging may be inefficient, resulting in much more generated data and stored artifacts than needed. Furthermore, automatic logging is not available for every possible framework and custom values one might want to track. In such cases, it might be necessary to [manually specify what to track](https://mlflow.org/docs/latest/tracking/tracking-api.html#logging-functions).\\n\\n## Further Reading\\n\\n- [MLflow Documentation on Automatic Logging](https://mlflow.org/docs/latest/tracking/autolog.html)\\n- [Python API reference for mlflow.autolog](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.autolog)\\n- Python API references for library-specific autolog functions\\n  - [PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\\n  - [Tensorflow](https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.autolog)\\n  - [Scikit-learn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog)\\n  - [XGBoost](https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog)\\n  - [PySpark](https://mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog)\\n  - [Statsmodels](https://mlflow.org/docs/latest/python_api/mlflow.statsmodels.html#mlflow.statsmodels.autolog)\\n  - [LightGBM](https://mlflow.org/docs/latest/python_api/mlflow.lightgbm.html#mlflow.lightgbm.autolog)\\n  - [Paddle](https://mlflow.org/docs/latest/python_api/mlflow.paddle.html#mlflow.paddle.autolog)\\n  - [Fastai](https://mlflow.org/docs/latest/python_api/mlflow.fastai.html#mlflow.fastai.autolog)"},{"id":"mlflow-docs-overhaul","metadata":{"permalink":"/mlflow-website/blog/mlflow-docs-overhaul","source":"@site/blog/2023-10-31-mlflow-docs-overhaul.md","title":"MLflow Docs Overhaul","description":"The MLflow Documentation is getting an upgrade.","date":"2023-10-31T00:00:00.000Z","formattedDate":"October 31, 2023","tags":[{"label":"docs","permalink":"/mlflow-website/blog/tags/docs"}],"readingTime":5.11,"hasTruncateMarker":true,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers"}],"frontMatter":{"title":"MLflow Docs Overhaul","tags":["docs"],"slug":"mlflow-docs-overhaul","authors":["mlflow-maintainers"],"thumbnail":"/img/blog/docs-overhaul.png"},"unlisted":false,"prevItem":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","permalink":"/mlflow-website/blog/mlflow-autolog"}},"content":"The MLflow Documentation is getting an upgrade.\\n\\n## Overhauling the MLflow Docs\\n\\nWe\'re thrilled to announce a comprehensive overhaul of the MLflow Docs. This initiative is not just about refreshing the look and feel but about reimagining how our users interact with our content. Our primary goal is to enhance clarity, improve navigation, and provide more in-depth resources for our community.\\n\\n## A Renewed Focus on User Experience\\n\\nThe MLflow documentation has always been an essential resource for our users. Over time, we\'ve received invaluable feedback, and we\'ve listened. The modernization effort is a direct response to the needs and preferences of our community.\\n\\n\x3c!-- truncate --\x3e\\n\\nAlong with working on covering new cutting-edge features as part of this documentation overhaul, we\'re working on addressing the complexity of getting started. As the first part of a series of tutorials and guides focusing on the initial learning phase, we\'ve created a new [getting started guide](https://www.mlflow.org/docs/latest/getting-started/logging-first-model/index.html), the first of many in a new series we\'re working on in an effort to teach the fundamentals of using MLflow. We feel that more in-depth instructional tutorials for learning the concepts and tools of MLflow will help to enhance the user experience for not only new users, but experienced users who need a refresher of how to do certain tasks.\\n\\nThere are more of these coming in the future!\\n\\n### **Easier Navigation**\\n\\nOur first order of business is to declutter and reorganize. This is going to be a process, though. With some of the monolithic pages ([Mlflow Models](https://www.mlflow.org/docs/2.7.1/models.html)), this will be more of a marathon than a sprint.\\n\\nWe\'ve introduced a [new main navigation page](https://www.mlflow.org/docs/latest/index.html) in an effort to help steer you to the content that you\'re looking for based on end-use domain, rather than component of MLflow. We\'re hoping that this helps to bring new feature content and useful examples to your awareness, limiting the amount of exploratory discovery needed to understand how to use these new features.\\n\\nAnother priority for us was to make major new features easier to discover. While the [release notes](https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md) are useful, particularly for Engineers who are maintaining integrations with, or are managing a deployment of, MLflow, they\'re not particularly user-friendly for an end-user of MLflow. We felt that a curated list of major new features would help to distill the information in our release notes, so we built the [new features](https://www.mlflow.org/docs/latest/new-features/index.html) page. We sincerely hope it helps to reduce the amount of effort needed to know what new major features have been released.\\n\\n### **Interactive Learning with Notebooks**\\n\\nIn today\'s fast-paced tech world, interactive learning is becoming the norm. Recognizing this trend, we\'re embedding viewable notebooks directly within the docs. But we\'re not stopping there. These notebooks are downloadable, allowing you to run, modify, and experiment with them locally. It\'s a hands-on approach to learning, bridging the gap between theory and practice.\\n\\n### **In-depth Tutorials and Guides**\\n\\nWhile our previous documentation provided a solid foundation, we felt there was room for more detailed explorations. We\'re introducing comprehensive [tutorials](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html) and [guides](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html) that delve deep into MLflow\'s features, showing how to solve actual problems. These first new tutorials and guides are just the start. We\'re going to be spending a lot of time and effort on making much more of MLflow documented in this way, helping to dramatically reduce the amount of time you have to spend figuring out how to leverage features in MLflow.\\n\\n## Diving Deeper: Expanding on Guides and Tutorials\\n\\nOur dedication to simplifying the usage of MLflow shines through in our revamped tutorials and guides. We\'re not just providing instructions; we\'re offering [deep dives](https://www.mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html), [best practices](https://www.mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/index.html), and real-world applications. What you see in the MLflow 2.8.0 release is just the beginning. We\'re going to be heavily focusing on creating more content, showing the best way to leverage the many features and services within MLflow, all the while endeavoring to make it easier than ever to manage any ML project you\'re working on.\\n\\n- **LLMs**: With all of the [new LLM-focused features](https://www.mlflow.org/docs/latest/llms/llm-evaluate/notebooks/rag-evaluation.html) we\'ve been releasing in the past year, we feel the need to create easier getting started guides,\\n  [in-depth tutorials](https://www.mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html), runnable examples, and more teaching-oriented step-by-step introductions to these features.\\n\\n- **Tracking and the MLflow UI**: Our expanded section on tracking will cover everything from setting up your first experiment to advanced tracking techniques. The MLflow UI, an integral part of the platform, will also get its spotlight, ensuring you can make the most of its features.\\n\\n- **Model Registry**: The model registry is where MLflow truly shines, and our new guides will ensure you can harness its full power. From organizing models to version control, we\'ll cover it all.\\n\\n- **Recipes and LLM-focused Features**: MLflow\'s versatility is one of its strengths. Our new content will explore the breadth of features available, from recipes to LLM-focused tools like the AI Gateway, LLM Evaluation, and the PromptLab UI.\\n\\n## The Transformative Power of Interactive Notebooks\\n\\nInteractive notebooks have revolutionized data science and machine learning. By integrating them into our documentation, we aim to provide a holistic learning experience. You can see code in action, understand its impact, and then experiment on their own. It\'s a dynamic way to grasp complex concepts, ensuring that you not only understand but can also apply your knowledge in your actual project code.\\n\\n## Join Us on This Journey\\n\\nThe overhaul of the MLflow documentation is a significant milestone, but it\'s just the beginning. We have a roadmap full of exciting updates, new content, and features. And for those in our community with a passion for sharing knowledge, we have a message: We\'d love to [collaborate](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md)! Whether it\'s writing tutorials, sharing use-cases, or providing feedback, every contribution enriches the MLflow community.\\n\\nIn conclusion, our commitment to providing top-notch documentation is a new primary focus of the maintainer group. We believe that well-documented features, combined with interactive learning tools, can significantly enhance the experience of using any tool. We want to put in the effort and time to make sure that your journey with using MLflow is as simple and powerful as it can be.\\n\\nStay tuned for more updates, and as always, happy coding!"}]}')}}]);