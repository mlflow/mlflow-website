"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6130],{14551:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/custom-llm-judges-make-judge","source":"@site/blog/2025-09-15-make-judge/index.mdx","title":"Beyond Manually Crafted LLM Judges: Automate Building Domain-Specific Evaluators with MLflow","description":"How to easily create custom evaluators that understand the semantics of your domain and automatically align with human experts","date":"2025-09-15T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"evaluation","permalink":"/mlflow-website/blog/tags/evaluation"},{"inline":true,"label":"judges","permalink":"/mlflow-website/blog/tags/judges"},{"inline":true,"label":"llm","permalink":"/mlflow-website/blog/tags/llm"}],"readingTime":9.14,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"Beyond Manually Crafted LLM Judges: Automate Building Domain-Specific Evaluators with MLflow","description":"How to easily create custom evaluators that understand the semantics of your domain and automatically align with human experts","slug":"custom-llm-judges-make-judge","authors":["mlflow-maintainers"],"tags":["genai","evaluation","judges","llm"],"thumbnail":"/img/blog/make-judge-thumbnail.png","image":"/img/blog/make-judge-thumbnail.png"},"unlisted":false,"prevItem":{"title":"Rapidly Prototype and Evaluate Agents with Claude Agent SDK and MLflow","permalink":"/mlflow-website/blog/mlflow-autolog-claude-agents-sdk"},"nextItem":{"title":"Building and Managing an LLM-based OCR System with MLflow","permalink":"/mlflow-website/blog/mlflow-prompt-evaluate"}}')},28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>i});var a=n(96540);const r={},o=a.createContext(r);function s(e){const t=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(o.Provider,{value:t},e.children)}},31904:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/make_judge_human_feedback-302a6bf18fb35cb8e03bb18aa260c57c.gif"},52545:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/make_judge_eval-a148d94d666bbc0d856b84fa28932be0.gif"},65537:(e,t,n)=>{n.d(t,{A:()=>j});var a=n(96540),r=n(18215),o=n(65627),s=n(56347),i=n(50372),l=n(30604),c=n(11861),u=n(78749);function d(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return d(e).map((({props:{value:e,label:t,attributes:n,default:a}})=>({value:e,label:t,attributes:n,default:a})))}(n);return function(e){const t=(0,c.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h({value:e,tabValues:t}){return t.some((t=>t.value===e))}function p({queryString:e=!1,groupId:t}){const n=(0,s.W6)(),r=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,l.aZ)(r),(0,a.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(n.location.search);t.set(r,e),n.replace({...n.location,search:t.toString()})}),[r,n])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:r}=e,o=m(e),[s,l]=(0,a.useState)((()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find((e=>e.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[c,d]=p({queryString:n,groupId:r}),[g,f]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,r]=(0,u.Dv)(t);return[n,(0,a.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:r}),w=(()=>{const e=c??g;return h({value:e,tabValues:o})?e:null})();(0,i.A)((()=>{w&&l(w)}),[w]);return{selectedValue:s,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),f(e)}),[d,f,o]),tabValues:o}}var f=n(9136);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=n(74848);function v({className:e,block:t,selectedValue:n,selectValue:a,tabValues:s}){const i=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const t=e.currentTarget,r=i.indexOf(t),o=s[r].value;o!==n&&(l(t),a(o))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=i.indexOf(e.currentTarget)+1;t=i[n]??i[0];break}case"ArrowLeft":{const n=i.indexOf(e.currentTarget)-1;t=i[n]??i[i.length-1];break}}t?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},e),children:s.map((({value:e,label:t,attributes:a})=>(0,b.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{i.push(e)},onKeyDown:u,onClick:c,...a,className:(0,r.A)("tabs__item",w.tabItem,a?.className,{"tabs__item--active":n===e}),children:t??e},e)))})}function x({lazy:e,children:t,selectedValue:n}){const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=o.find((e=>e.props.value===n));return e?(0,a.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:o.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==n})))})}function y(e){const t=g(e);return(0,b.jsxs)("div",{className:(0,r.A)("tabs-container",w.tabList),children:[(0,b.jsx)(v,{...t,...e}),(0,b.jsx)(x,{...t,...e})]})}function j(e){const t=(0,f.A)();return(0,b.jsx)(y,{...e,children:d(e.children)},String(t))}},79329:(e,t,n)=>{n.d(t,{A:()=>s});n(96540);var a=n(18215);const r={tabItem:"tabItem_Ymn6"};var o=n(74848);function s({children:e,hidden:t,className:n}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,n),hidden:t,children:e})}},92804:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var a=n(14551),r=n(74848),o=n(28453);n(65537),n(79329);const s={title:"Beyond Manually Crafted LLM Judges: Automate Building Domain-Specific Evaluators with MLflow",description:"How to easily create custom evaluators that understand the semantics of your domain and automatically align with human experts",slug:"custom-llm-judges-make-judge",authors:["mlflow-maintainers"],tags:["genai","evaluation","judges","llm"],thumbnail:"/img/blog/make-judge-thumbnail.png",image:"/img/blog/make-judge-thumbnail.png"},i=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Creating Your First Scorer: A Customer Support Evaluator",id:"creating-your-first-scorer-a-customer-support-evaluator",level:2},{value:"Scorers as Agents: Trace-Based Evaluation",id:"scorers-as-agents-trace-based-evaluation",level:2},{value:"Agent Scorers Evaluating Complex Agents",id:"agent-scorers-evaluating-complex-agents",level:3},{value:"Why We Call Them &quot;Agentic&quot;",id:"why-we-call-them-agentic",level:3},{value:"A Real-World Example: Customer Support Agent",id:"a-real-world-example-customer-support-agent",level:3},{value:"The Game Changer: Human Feedback Alignment",id:"the-game-changer-human-feedback-alignment",level:2},{value:"Step 1: Collect Human Feedback",id:"step-1-collect-human-feedback",level:3},{value:"Option A: Using the Trace UI / Evaluation UI",id:"option-a-using-the-trace-ui--evaluation-ui",level:4},{value:"Option B: Programmatic Feedback",id:"option-b-programmatic-feedback",level:4},{value:"Step 2: Align Your Scorer with Expert Feedback",id:"step-2-align-your-scorer-with-expert-feedback",level:3},{value:"Pluggable Optimization Framework",id:"pluggable-optimization-framework",level:4},{value:"Conclusion",id:"conclusion",level:2},{value:"Next Steps",id:"next-steps",level:3}];function u(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:"If you've tried evaluating GenAI applications, you know generic LLM judges are not enough. Your customer support bot needs to be evaluated on empathy and problem resolution. Your code generator needs to be checked for security vulnerabilities. Your medical advisor needs domain-specific accuracy checks. At the same time, translating these requirements into custom LLM judges is non-trivial. The judge has to focus on the right parts of the evaluated GenAI trace, and its prompt needs to be tuned to capture the nuances and preferences of human experts. This requires significant effort and often results in suboptimal judges, which eventually detracts from the main goal of building an effective GenAI application."}),"\n",(0,r.jsxs)(t.p,{children:["In the MLflow 3.4 release, we introduce the ",(0,r.jsx)(t.code,{children:"make_judge"})," method as a powerful new API to create ",(0,r.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/",children:"MLflow Scorers"}),", which are the framework's core abstraction for automated evaluation. The API enables you to create scorers in a declarative manner using simple instructions. With ",(0,r.jsx)(t.code,{children:"make_judge"}),", you can easily build judges that understand your domain-specific quality requirements and automatically align with feedback from human experts."]}),"\n",(0,r.jsx)(t.p,{children:"This post demonstrates the power of MLflow Scorers by showing how to:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Create custom scorers with ",(0,r.jsx)(t.code,{children:"make_judge"})," using simple declarative instructions"]}),"\n",(0,r.jsx)(t.li,{children:"Build scorers that act as agents with built-in tools for trace introspection, enabling complex evaluation tasks without complicated prompts or complex span parsing logic"}),"\n",(0,r.jsx)(t.li,{children:"Automatically align scorers with subject-matter expert preferences to improve scorer accuracy over time"}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"To illustrate the power of the new API, we will build a customer support quality scorer, show how it can evaluate complex agent behaviors, and demonstrate how human feedback makes it more effective. The built-in scorers MLflow provides are just pre-defined versions using these same capabilities. Once you understand this, you'll see how flexible and powerful MLflow's evaluation framework really is."}),"\n",(0,r.jsx)(t.h2,{id:"creating-your-first-scorer-a-customer-support-evaluator",children:"Creating Your First Scorer: A Customer Support Evaluator"}),"\n",(0,r.jsx)(t.p,{children:"Let's start with a practical example. Say you're building a customer support chatbot. You need a scorer that evaluates whether responses are actually helpful\u2014not just grammatically correct."}),"\n",(0,r.jsx)(t.p,{children:"First, install the latest version of MLflow:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-shell",children:"pip install -U mlflow\n"})}),"\n",(0,r.jsx)(t.p,{children:"Now let's create a scorer that understands what makes a good support response:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\n\n# Create a scorer for customer support quality\nsupport_scorer = make_judge(\n    name="support_quality",\n    instructions=(\n        "Evaluate if the response in {{ outputs }} shows appropriate empathy "\n        "for the customer issue in {{ inputs }}.\\n\\n"\n        "Check if the response acknowledges the customer\'s frustration and "\n        "responds with understanding and care.\\n"\n        "Rate as: \'empathetic\' or \'not empathetic\'"\n    ),\n    model="openai:/gpt-4o"\n)\n'})}),"\n",(0,r.jsxs)(t.p,{children:["The key here is that we're defining a scorer using simple declarative instructions that describe ",(0,r.jsx)(t.em,{children:"what"})," to evaluate, not ",(0,r.jsx)(t.em,{children:"how"})," to evaluate it. You don't need complex scoring functions or regex patterns. Just describe what you're looking for. This scorer is now a reusable evaluation component that works anywhere in MLflow's ecosystem."]}),"\n",(0,r.jsx)(t.p,{children:"Let's see how our scorer handles a terrible support response:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Test the scorer on a support interaction\nresult = support_scorer(\n    inputs={"issue": "Can\'t reset my password"},\n    outputs={"response": "Have you tried turning it off and on again?"}\n)\n\nprint(f"Rating: {result.value}")\nprint(f"Reasoning: {result.rationale}")\n'})}),"\n",(0,r.jsx)(t.p,{children:"Output:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Rating: not empathetic\nReasoning: The response completely ignores the customer's frustration with\nthe password reset issue and provides an irrelevant, dismissive suggestion\nthat shows no understanding of their problem...\n"})}),"\n",(0,r.jsxs)(t.p,{children:["The scorer we just created can be directly plugged into ",(0,r.jsx)(t.code,{children:"mlflow.genai.evaluate()"})," to scale evaluations to entire datasets or monitoring APIs to apply scorers on production traffic:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\n\n# Your support conversations dataset\ntest_data = pd.DataFrame([\n    {\n        "inputs": {"issue": "Billing error - charged twice"},\n        "outputs": {"response": "I\'ll immediately refund the duplicate charge."}\n    },\n    {\n        "inputs": {"issue": "Feature request for dark mode"},\n        "outputs": {"response": "Great suggestion! I\'ve forwarded this to our product team."}\n    }\n])\n\n# Run evaluation with your custom scorer\nresults = mlflow.genai.evaluate(\n    data=test_data,\n    scorers=[support_scorer]\n)\n\n# View results\nprint(results.tables["eval_results_table"])\n'})}),"\n",(0,r.jsx)(t.h2,{id:"scorers-as-agents-trace-based-evaluation",children:"Scorers as Agents: Trace-Based Evaluation"}),"\n",(0,r.jsx)(t.p,{children:"So far our scorer has evaluated simple question-answer pairs. But MLflow Scorers can act agentically to analyze complex AI agents that make multiple LLM calls, use tools, and follow reasoning chains."}),"\n",(0,r.jsx)(t.h3,{id:"agent-scorers-evaluating-complex-agents",children:"Agent Scorers Evaluating Complex Agents"}),"\n",(0,r.jsx)(t.p,{children:"One of the most powerful aspects of Agent-based Scorers is their ability to diagnose complex tool calling and logical chains in complex agent workflows. For example, the following clip shows an advanced agent that can answer questions about the logistics of fantasy currency systems. It has at its disposal dozens of tools that can perform physics calculations, logistical assessments, and standard reference information to determine just how ridiculous a fantasy character's treasure haul can be."}),"\n",(0,r.jsx)(t.p,{children:"The Agent-based judges used for evaluation are capable of reviewing the entire process flow of each span, permitting the scorer to not only be more accurate in its overall rating, but allows its rationale to be targeted and more evidence-based."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"MLflow Trace UI showing detailed execution flow",src:n(52545).A+"",width:"2048",height:"1237"})}),"\n",(0,r.jsx)(t.h3,{id:"why-we-call-them-agentic",children:'Why We Call Them "Agentic"'}),"\n",(0,r.jsxs)(t.p,{children:["Without ",(0,r.jsx)(t.code,{children:"make_judge"}),", implementing a comprehensive trace evaluator would require significant effort. You would need to:"]}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:"Write code to extract different parts of the trace (tool calls, LLM interactions, error handling)"}),"\n",(0,r.jsx)(t.li,{children:"Create separate prompts to evaluate each criterion (Are the tool calls reasonable? Any issues in the trajectory? Are errors handled well?)"}),"\n",(0,r.jsx)(t.li,{children:"Develop logic to combine these signals into a final assessment"}),"\n",(0,r.jsx)(t.li,{children:"Implement more complex reasoning strategies for nuanced evaluation"}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["Instead, ",(0,r.jsx)(t.code,{children:"make_judge"})," provides built-in tools for trace introspection and implements a reasoning loop to evaluate your declarative quality criteria. This lets you focus on ",(0,r.jsx)(t.em,{children:"what"})," to evaluate rather than ",(0,r.jsx)(t.em,{children:"how"})," to implement the evaluation."]}),"\n",(0,r.jsx)(t.h3,{id:"a-real-world-example-customer-support-agent",children:"A Real-World Example: Customer Support Agent"}),"\n",(0,r.jsx)(t.p,{children:"Let's evaluate a more complex customer support agent that uses multiple tools to resolve issues:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Example: A customer support agent handling a refund request\nimport mlflow\nfrom mlflow.genai.judges import make_judge\n\n# Create a comprehensive agent evaluator\nagent_scorer = make_judge(\n    name="support_agent_quality",\n    instructions=(\n        "Analyze the {{ trace }} for a customer support interaction.\\n\\n"\n        "Evaluate the following:\\n"\n        "1. Did the agent use the correct tools (search_orders, process_refund)?\\n"\n        "2. Were authentication steps properly followed?\\n"\n        "3. Was the customer kept informed throughout the process?\\n"\n        "4. Were any errors handled gracefully?\\n\\n"\n        "Rate as: \'excellent\', \'satisfactory\', or \'needs_improvement\'"\n    ),\n    model="openai:/gpt-4o"\n)\n\n# Simulate a complex agent execution with multiple tool calls\nwith mlflow.start_span("handle_refund_request") as main_span:\n    with mlflow.start_span("search_customer_orders"):\n        # Agent searches for customer orders\n        orders = search_orders(customer_id="12345")\n        mlflow.log_input({"customer_id": "12345"})\n        mlflow.log_output({"orders_found": 3})\n\n    with mlflow.start_span("verify_refund_eligibility"):\n        # Agent verifies refund eligibility\n        eligible = check_refund_policy(order_id="ORD-789")\n        mlflow.log_output({"eligible": True, "reason": "Within 30-day window"})\n\n    with mlflow.start_span("process_refund"):\n        # Agent processes the refund\n        refund_result = process_refund(order_id="ORD-789", amount=99.99)\n        mlflow.log_output({"status": "success", "refund_id": "REF-123"})\n\n    with mlflow.start_span("notify_customer"):\n        # Agent sends confirmation to customer\n        notification_sent = send_email(\n            to="customer@example.com",\n            subject="Refund Processed",\n            body="Your refund of $99.99 has been processed..."\n        )\n        mlflow.log_output({"notification_sent": True})\n\n    trace_id = main_span.trace_id\n\n# The scorer analyzes the entire trace\ntrace = mlflow.get_trace(trace_id)\nevaluation = agent_scorer(trace=trace)\n\nprint(f"Agent Performance: {evaluation.value}")\nprint(f"Analysis: {evaluation.rationale}")\n'})}),"\n",(0,r.jsx)(t.p,{children:"Output:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Agent Performance: excellent\nAnalysis: The agent followed all required procedures correctly:\n1. Properly searched for customer orders before processing\n2. Verified refund eligibility according to policy\n3. Successfully processed the refund with correct amount\n4. Notified the customer of the completion\nAll tool calls were necessary and executed in the optimal order.\n"})}),"\n",(0,r.jsx)(t.h2,{id:"the-game-changer-human-feedback-alignment",children:"The Game Changer: Human Feedback Alignment"}),"\n",(0,r.jsx)(t.p,{children:"Here's where things get interesting. What happens when your scorer disagrees with your human experts? Instead of rewriting prompts endlessly, MLflow Scorers can learn from human corrections."}),"\n",(0,r.jsx)(t.h3,{id:"step-1-collect-human-feedback",children:"Step 1: Collect Human Feedback"}),"\n",(0,r.jsx)(t.p,{children:"You can provide feedback either through the MLflow Trace UI or programmatically:"}),"\n",(0,r.jsx)(t.h4,{id:"option-a-using-the-trace-ui--evaluation-ui",children:"Option A: Using the Trace UI / Evaluation UI"}),"\n",(0,r.jsx)(t.p,{children:"Navigate to your MLflow tracking server and use the feedback interface directly on the evaluation traces. This is ideal for subject matter experts who prefer a visual interface or for comprehensive feedback to be added that directly corrects rationale that the agent provided."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"MLflow Trace UI with human feedback interface",src:n(31904).A+"",width:"2048",height:"1327"})}),"\n",(0,r.jsx)(t.p,{children:"The feedback you provide here will be logged as part of the trace and can be used for alignment."}),"\n",(0,r.jsx)(t.h4,{id:"option-b-programmatic-feedback",children:"Option B: Programmatic Feedback"}),"\n",(0,r.jsx)(t.p,{children:"The example below shows how to log feedback programmatically. This is useful for integrating expert reviews into your existing workflows. You can even embed feedback from users of your application directly within your application using the MLflow SDK APIs."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from mlflow.entities import AssessmentSource, AssessmentSourceType\n\n# Search for traces from production or development environments\nproduction_traces = mlflow.search_traces(\n    experiment_ids=[experiment_id],\n    filter_string="attributes.environment = \'production\'",\n    max_results=100\n)\n\n# Provide human feedback on specific traces\nfor trace in production_traces:\n    # Your expert reviews the trace and provides assessment\n    mlflow.log_feedback(\n        trace_id=trace.info.trace_id,\n        name="support_quality",  # Must match scorer name\n        value="empathetic",      # Expert\'s assessment\n        source=AssessmentSource(\n            source_type=AssessmentSourceType.HUMAN,\n            source_id="support_expert"\n        )\n    )\n'})}),"\n",(0,r.jsx)(t.h3,{id:"step-2-align-your-scorer-with-expert-feedback",children:"Step 2: Align Your Scorer with Expert Feedback"}),"\n",(0,r.jsx)(t.p,{children:"Once you have collected feedback (minimum 10 examples recommended), you can align your scorer:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Gather traces with human feedback\ntraces_with_feedback = mlflow.search_traces(\n    experiment_ids=[experiment_id],\n    return_type="list"\n)\n\n# Create an aligned scorer that learns from expert preferences\naligned_scorer = support_scorer.align(\n    traces=traces_with_feedback\n)\n\n# Use the aligned scorer in your evaluation pipeline\nresults = mlflow.genai.evaluate(\n    data=test_data,\n    scorers=[aligned_scorer]  # Now uses expert-aligned scoring\n)\n\nprint(f"Alignment improved accuracy by: {results.metrics[\'alignment_improvement\']}%")\n'})}),"\n",(0,r.jsx)(t.h4,{id:"pluggable-optimization-framework",children:"Pluggable Optimization Framework"}),"\n",(0,r.jsxs)(t.p,{children:["MLflow's alignment implementation is designed to be extensible. While the current release includes a built-in SIMBA optimizer from DSPy, the architecture supports custom alignment optimizers through the ",(0,r.jsx)(t.code,{children:"AlignmentOptimizer"})," abstract base class:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"# Use the default SIMBA optimizer\naligned_scorer = support_scorer.align(traces=traces_with_feedback)\n\n# Or provide your own custom optimizer\nfrom mlflow.genai.judges.base import AlignmentOptimizer\n\nclass CustomOptimizer(AlignmentOptimizer):\n    def align(self, judge, traces):\n        # Your optimization logic here\n        return optimized_judge\n\ncustom_optimizer = CustomOptimizer()\naligned_scorer = support_scorer.align(\n    traces=traces_with_feedback,\n    optimizer=custom_optimizer\n)\n"})}),"\n",(0,r.jsx)(t.p,{children:"This pluggable design means you can leverage external optimization frameworks or implement domain-specific alignment strategies while maintaining compatibility with MLflow's evaluation ecosystem."}),"\n",(0,r.jsx)(t.p,{children:"In practice, aligned scorers achieve 30-50% reduction in evaluation errors compared to generic prompts. The more feedback you provide, the better they get at matching your team's quality standards."}),"\n",(0,r.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsxs)(t.p,{children:["The new ",(0,r.jsx)(t.code,{children:"make_judge"})," API transforms how you build and maintain LLM judges for GenAI applications. We've shown how this declarative approach eliminates the manual effort traditionally required for creating domain-specific evaluators."]}),"\n",(0,r.jsx)(t.p,{children:"The key takeaways are:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Declarative over imperative"}),": Define ",(0,r.jsx)(t.em,{children:"what"})," to evaluate with simple instructions, not ",(0,r.jsx)(t.em,{children:"how"})," to implement the evaluation"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Agentic capabilities built-in"}),": Scorers come with tools for trace introspection, eliminating the need for complex extraction and evaluation logic"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Automatic alignment with experts"}),": Scorers learn from subject-matter expert feedback, achieving 30-50% reduction in evaluation errors"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Seamless integration"}),": Works directly with ",(0,r.jsx)(t.code,{children:"mlflow.genai.evaluate()"})," for dataset evaluation and production monitoring"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"The built-in scorers MLflow provides are simply pre-defined versions using these same capabilities. Once you understand this, creating domain-specific evaluators becomes straightforward. You can have a working custom scorer in minutes that automatically improves as it learns from your team's expertise."}),"\n",(0,r.jsx)(t.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Explore the ",(0,r.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/",children:"MLflow Evaluation and Monitoring framework"})," for the complete evaluation ecosystem"]}),"\n",(0,r.jsxs)(t.li,{children:["Read the ",(0,r.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge.html",children:"make_judge documentation"})," for advanced scorer features"]}),"\n",(0,r.jsxs)(t.li,{children:["Learn about ",(0,r.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/scorers/index.html",children:"built-in scorers"})," available out of the box"]}),"\n",(0,r.jsxs)(t.li,{children:["See how to create ",(0,r.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/datasets/index.html",children:"evaluation datasets"})," for systematic testing"]}),"\n"]})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}}}]);