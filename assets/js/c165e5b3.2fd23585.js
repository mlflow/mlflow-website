"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7623],{26483:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>d});var n=a(91450),i=a(74848),s=a(28453);const o={title:"Tracking Image Datasets with MLflow",tags:["mlflow","datasets","mlops","computer-vision"],slug:"tracking-image-datasets",authors:["thorsteen"],thumbnail:"/img/blog/coco-dataset.png",image:"/img/blog/coco-dataset.png"},r=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Why Track Image Datasets?",id:"why-track-image-datasets",level:2},{value:"Understanding Image Dataset Formats",id:"understanding-image-dataset-formats",level:2},{value:"COCO: Common Objects in Context",id:"coco-common-objects-in-context",level:3},{value:"Hugging Face Image Datasets",id:"hugging-face-image-datasets",level:3},{value:"Tracking Datasets with MLflow",id:"tracking-datasets-with-mlflow",level:2},{value:"Example Using Computer Vision Model and Image Datasets Tracking",id:"example-using-computer-vision-model-and-image-datasets-tracking",level:2},{value:"Why two approaches?",id:"why-two-approaches",level:3},{value:"Install MLflow and Other Dependencies",id:"install-mlflow-and-other-dependencies",level:3},{value:"Logging Dataset As An Artifact Together with Model",id:"logging-dataset-as-an-artifact-together-with-model",level:3},{value:"Logging of Dataset Together with Model",id:"logging-of-dataset-together-with-model",level:3},{value:"Limitations",id:"limitations",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:"Dataset tracking is fundamental to building robust and reproducible machine learning models. Among all data types, images present unique tracking challenges due to their high dimensionality, variability, and storage requirements. In this post, we'll demonstrate how to effectively track image datasets using MLflow's experiment tracking capabilities and UI, providing you with practical techniques to enhance your computer vision workflows' tracking of data and models."}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"NOTE"}),": This guide assumes familiarity with MLflow and its tracking capabilities as well as PyTorch. For beginners, refer to the ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/getting-started/",children:"MLflow starters tutorial"})," and this ",(0,i.jsx)(t.a,{href:"https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html",children:"PyTorch Vision tutorial"}),"."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"why-track-image-datasets",children:"Why Track Image Datasets?"}),"\n",(0,i.jsx)(t.p,{children:"Tracking image datasets is essential for structured machine learning projects. It ensures:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"High-quality training data"}),": Unlike structured datasets, image datasets are hard to understand and read, but their importance for model performance is the same, and they still require annotation, curation, and feature transformations."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Reproducibility"}),": Experiments can be replicated with the same dataset version and preprocessing steps."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Data and model lineage"}),": Tracking maintains a record of data usage, essential for adhering to data governance as well as tracking the lineage of the model and data used in training, validation, and testing."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Debugging"}),": Tracking helps identify issues related to data quality or preprocessing that may affect model performance."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"understanding-image-dataset-formats",children:"Understanding Image Dataset Formats"}),"\n",(0,i.jsx)(t.p,{children:"There exist many formats for datasets in the global machine-learning community.\nIn this blog post, we will use a widely used format for computer-vision models, COCO, in the traditional file format and a Hugging Face version of the same dataset. These two formats have pros and cons and different possibilities for MLflow tracking."}),"\n",(0,i.jsx)(t.p,{children:"Pros using the native file format:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["dataloaders ready to use in ",(0,i.jsx)(t.code,{children:"pycocotools"})," / ",(0,i.jsx)(t.code,{children:"torchvision"})]}),"\n",(0,i.jsx)(t.li,{children:"fast to load"}),"\n",(0,i.jsx)(t.li,{children:"smaller file size"}),"\n",(0,i.jsx)(t.li,{children:"simple directory structure"}),"\n",(0,i.jsx)(t.li,{children:"artifacts can be tracked and shown, e.g. as image"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Cons:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"not trackable"}),"\n",(0,i.jsx)(t.li,{children:"unstructured data so can be messy to search and explore"}),"\n",(0,i.jsx)(t.li,{children:"not queryable"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Pros using Hugging Face datasets / tabular datasets:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"very structured"}),"\n",(0,i.jsxs)(t.li,{children:["fully trackable as a training dataset using ",(0,i.jsx)(t.code,{children:"mlflow.data"})," (see more below)"]}),"\n",(0,i.jsx)(t.li,{children:"metadata can be added for data in MLflow"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Cons:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"huge file size due to binary written as text in table entries"}),"\n",(0,i.jsx)(t.li,{children:"requires a custom dataloader"}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"coco-common-objects-in-context",children:"COCO: Common Objects in Context"}),"\n",(0,i.jsx)(t.p,{children:"COCO is a widely used dataset format in computer vision, known for its rich annotations.\nIt supports:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Object Detection, Keypoint Detection, Stuff Segmentation, Image Captioning, and more."}),"\n",(0,i.jsx)(t.li,{children:"JSON-based annotations for storing metadata."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"We will be using this dataset throughout this blog post."}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsxs)(t.p,{children:["It is important to visualize annotations in image datasets to perform comprehensive quality checks. You can explore the dataset on the ",(0,i.jsx)(t.a,{href:"https://cocodataset.org/#explore",children:"COCO dataset official site"})," to understand more about the nature of the data included within it."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Image datasets have annotations which can be segments of an object in picture or an bounding box. This means that for every image, there will be a class and a set of coordinates for each identified object. See the following example from the COCO dataset:"}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"COCO dataset annotated image example",src:a(33865).A+"",width:"640",height:"427"})}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"hugging-face-image-datasets",children:"Hugging Face Image Datasets"}),"\n",(0,i.jsxs)(t.p,{children:["Hugging Face offers a simple ",(0,i.jsx)(t.code,{children:"Image Folder"})," type for creating datasets from local files. It supports:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Metadata integration for text captions and object detection."}),"\n",(0,i.jsx)(t.li,{children:"Quick dataset creation using directory file paths."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"This can be used with several formats such as COCO and is supported by MLflow."}),"\n",(0,i.jsx)(t.h2,{id:"tracking-datasets-with-mlflow",children:"Tracking Datasets with MLflow"}),"\n",(0,i.jsx)(t.p,{children:"Understanding the properties of your dataset is crucial for effective ML model training, testing, and evaluation. This includes analyzing class balance, annotation quality, and other key characteristics. By thoroughly reviewing these aspects of your source data, you can ensure that your dataset aligns with the requirements of your machine learning tasks and identify potential biases or gaps that may impact model performance. Therefore, datasets need to be tracked together along with your model during experimentation and refinement of your project."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"MLflow"})," provides robust tools to ensure reproducibility and model lineage:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Log dataset metadata, such as format (e.g., COCO, Hugging Face Image Folder)."}),"\n",(0,i.jsx)(t.li,{children:"Log parameters used in feature transformation/data augmentation steps."}),"\n",(0,i.jsx)(t.li,{children:"Track dataset versions for reproducibility."}),"\n",(0,i.jsx)(t.li,{children:"Store and retrieve dataset-related artifacts like dataset descriptions."}),"\n",(0,i.jsx)(t.li,{children:"Link datasets with specific model training runs."}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["One of the key APIs that enable us to track our source data is through the use of the ",(0,i.jsx)(t.code,{children:"mlflow.log_artifacts"})," ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html#mlflow.log_artifact",children:"method"}),", the ",(0,i.jsx)(t.code,{children:"mlflow.log_input"})," ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html#mlflow.log_input",children:"method"}),", and we will see how incorporating the use of the ",(0,i.jsx)(t.code,{children:"mlflow.data"})," ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.data.html#mlflow-data",children:"module"})," can add more structure to dataset tracking when used with Hugging Face.\nWe will use the ",(0,i.jsx)(t.code,{children:"mlflow.pytorch"})," ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pytorch.html#mlflow.pytorch.log_model",children:"module doc"})," to log a model along with our dataset tracking."]}),"\n",(0,i.jsx)(t.h2,{id:"example-using-computer-vision-model-and-image-datasets-tracking",children:"Example Using Computer Vision Model and Image Datasets Tracking"}),"\n",(0,i.jsx)(t.p,{children:"There are two approaches to log an image dataset:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["using ",(0,i.jsx)(t.code,{children:"mlflow.artifacts"})]}),"\n",(0,i.jsxs)(t.li,{children:["using ",(0,i.jsx)(t.code,{children:"mlflow.data"})," (the datasets API)."]}),"\n"]}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsxs)(t.p,{children:["You can also log an ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/ml/evaluation/dataset-eval",children:"evaluation dataset"}),", which I will not cover in this post."]}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"why-two-approaches",children:"Why two approaches?"}),"\n",(0,i.jsx)(t.p,{children:"Converting an image dataset from file-based formats like COCO to a tabular format is challenging, as most data loaders expect the file-based COCO format. Logging artifacts provides a quick and straightforward solution without the need to reformat files. However, this can also get a bit messy if you're not careful about organizing your files within a directory structure. Make sure to create meaningful paths for your artifacts."}),"\n",(0,i.jsxs)(t.p,{children:["The key artifact for COCO datasets is the ",(0,i.jsx)(t.code,{children:"instances.json"})," file, which describes your image dataset's metadata and annotations. This file can, for example, be used to check the class balance in your dataset by analyzing the ",(0,i.jsx)(t.code,{children:"category"})," field."]}),"\n",(0,i.jsxs)(t.p,{children:["If you are not too concerned about this, Hugging Face can help with logging datasets the MLflow way. Some Hugging Face datasets contain rich metadata that can be transferred to MLflow's tracking functionality. This is where ",(0,i.jsx)(t.code,{children:"mlflow.data"})," comes in. Compared to logging artifacts, this add much more rich metadata and structure to your dataset and makes it easier to manage and see within a given experiment run. If you can fit your dataset into a Hugging Face type dataset and make that work in your dataloader or training script, this is the recommended approach."]}),"\n",(0,i.jsx)(t.p,{children:"In this post, I will go through both approaches in code."}),"\n",(0,i.jsx)(t.h3,{id:"install-mlflow-and-other-dependencies",children:"Install MLflow and Other Dependencies"}),"\n",(0,i.jsxs)(t.p,{children:["Start by installing the dependencies for both code examples in your ",(0,i.jsx)(t.code,{children:"python >= 3.10"})," environment. ",(0,i.jsx)(t.code,{children:"opencv"})," can be omitted if only using the first example and ",(0,i.jsx)(t.code,{children:"pycocotools"})," can be omitted if only using the second example."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.code,{children:"pip install mlflow datasets torch torchvision pycocotools opencv-python-headless psutil"})}),"\n",(0,i.jsxs)(t.p,{children:["Also install ",(0,i.jsx)(t.code,{children:"pynvml"})," if you want to track GPU metrics."]}),"\n",(0,i.jsx)(t.p,{children:"For the Hugging Face dataset download, also make sure to log in."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.code,{children:"huggingface-cli login"})}),"\n",(0,i.jsxs)(t.p,{children:["One of the examples requires compute; therefore, be sure to turn on ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/system-metrics/",children:"MLflow system metrics"})," to track what happens on your compute during training."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.code,{children:"export MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true"})}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:'Note: The validation split is used to save space, but you can also use the "train" split\nif you want to train/finetune the model on the full dataset (requires +25 GB storage).\nNumber of epochs and a subset of the dataset are also used during training.'}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"logging-dataset-as-an-artifact-together-with-model",children:"Logging Dataset As An Artifact Together with Model"}),"\n",(0,i.jsx)(t.p,{children:"Since the COCO dataset is file-based, files need to be downloaded first. We use the smallest version of the newest version of the dataset from the official author's website."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bash",children:"# download the COCO val 2017 dataset\nwget -P datasets http://images.cocodataset.org/zips/val2017.zip\nunzip -q datasets/val2017.zip -d datasets\nwget -P datasets http://images.cocodataset.org/annotations/annotations_trainval2017.zip\nunzip -q datasets/annotations_trainval2017.zip -d datasets\nrm datasets/val2017.zip & rm datasets/annotations_trainval2017.zip\n"})}),"\n",(0,i.jsx)(t.p,{children:"We can now train a model and track the artifacts of the training dataset as well as inputs in the same run."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import json\n\nfrom torchvision.datasets import CocoDetection\nfrom torchvision import models\nimport mlflow\n\n# Load a COCO Dataset (val used to limit size)\nimg_folder = "datasets/val2017"\ncoco_annotation_file = "datasets/annotations/instances_val2017.json"\n\n# Download dataset\ndataset = CocoDetection(img_folder, coco_annotation_file)\n\n# Load a pre-trained model from COCO\nmodel = models.detection.fasterrcnn_resnet50_fpn(weights=\'COCO_V1\')\n\n# Set experiment name\nmlflow.set_experiment("coco_experiment")\n\n# Save dataset artifacts and model\nwith mlflow.start_run():\n\n  # log dataset\n  with open(coco_annotation_file, \'r\') as f:\n      dataset_metadata = json.load(f)\n  mlflow.log_dict(dataset_metadata, "coco_annotation_file")\n\n  # log images\n  mlflow.log_artifact(img_folder, artifact_path="images")\n\n  # log model\n  mlflow.pytorch.log_model(model, "model")\n\n  # register model with a meaningful name\n  mlflow.register_model(\n      "runs:/{}/model".format(mlflow.active_run().info.run_id),\n      "fasterrcnn_resnet50_fpn_coco_2017_model"\n  )\n'})}),"\n",(0,i.jsx)(t.p,{children:"We can go into our MLflow UI and see the dataset registered under the experiment run of our model."}),"\n",(0,i.jsx)(t.p,{children:"Image and text visualization of files is supported."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"COCO dataset annotated image example",src:a(85735).A+"",width:"1200",height:"773"})}),"\n",(0,i.jsx)(t.h3,{id:"logging-of-dataset-together-with-model",children:"Logging of Dataset Together with Model"}),"\n",(0,i.jsx)(t.p,{children:"We can do this in a more structured way using a Hugging Face dataset and leverage a convenient way to read in the data. In this way, we have our MLflow tracked dataset, training metrics and models all in the same experiment run!"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import numpy as np\nimport cv2\nimport io\nimport mlflow\nfrom torchvision import models\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\nimport os\n\nos.environ["MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING"] = "true"\n\n# Load the COCO dataset from Hugging Face\ndataset = load_dataset("detection-datasets/coco", split="val")\n\n# Transform to MLFlow Dataset\nmlflow_dataset = mlflow.data.huggingface_dataset.from_huggingface(dataset)\n\n# For this example we create a subset of the dataset with the first 100 rows\nsubset_dataset = dataset.select(range(100))\n\n# Load a pre-trained object detection / segmentation model\nmodel = models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n# Let\u2019s fine-tune it, log dataset, metrics, and model in an MLflow Experiment run\n\nmlflow.set_experiment("hg_image_experiment")\n\nwith mlflow.start_run():\n\n  # log training dataset in model training run\n  mlflow.log_input(mlflow_dataset, context="training")\n  optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n\n  for epoch in range(1): # We train for 1 epoch in this example\n\n    print(f"Training object detection model, epoch {epoch+1}...")\n\n    for row in subset_dataset: # We run a subset of the dataset to save time\n\n      # In this example we are not using a dataloader but just converting image bytes to ndarray\n      image_bytes = io.BytesIO()\n      row["image"].save(image_bytes, format="JPEG")\n      image_bytes = image_bytes.getvalue()\n      if isinstance(image_bytes, bytes):\n          image_array = np.frombuffer(image_bytes, np.uint8)\n          image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n      else:\n          raise TypeError("Expected bytes object for \'image\', got {}".format(type(image_bytes)))\n      image = np.array(image)\n\n      # Prepare annotations as target\n      annotations = row["objects"]\n      target = []\n      for i in range(len(annotations[\'category\'])):\n          d = {}\n          d[\'boxes\'] = torch.tensor(annotations[\'bbox\'][i], dtype=torch.float32).reshape(-1, 4)  # Ensure shape [N, 4]\n          d[\'labels\'] = torch.tensor([annotations[\'category\'][i]], dtype=torch.int64)  # Wrap in a list for correct shape\n          target.append(d)\n\n      # Convert the image to a PyTorch tensor and normalize it\n      image_tensor = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n\n      # Perform forward pass in batches of one\n      input_batch = [image_tensor]\n      output = model(input_batch, target)\n\n      # Compute loss\n      loss_dict = output[0] if isinstance(output, list) else output\n      loss = sum(loss for loss in loss_dict.values())\n\n      # Backpropagation and optimization step\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      # Pretty print the loss\n      print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")\n\n      mlflow.log_metrics({"epoch": epoch+1})\n      mlflow.log_metrics({"loss": loss.item()})\n\n    # finally log model\n    mlflow.pytorch.log_model(\n      model,\n      "model",\n      input_example=input_batch\n    )\n'})}),"\n",(0,i.jsx)(t.p,{children:"We have shown how to work with our images in a tabular format to simplify the use of the Hugging Face dataset in our training run."}),"\n",(0,i.jsx)(t.p,{children:"Under the second experiment, you will now have a dataset logged."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Dataset huggingface training example",src:a(36843).A+"",width:"1920",height:"968"})}),"\n",(0,i.jsx)(t.h2,{id:"limitations",children:"Limitations"}),"\n",(0,i.jsx)(t.p,{children:"While MLflow is powerful in itself, it needs support. Consider these limitations:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Storage Overhead"}),": Logging large datasets can require significant storage."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Annotation Complexity"}),": Managing complex annotations may need custom scripts like ",(0,i.jsx)(t.code,{children:"pycocotools"})," or open-source tools like ",(0,i.jsx)(t.a,{href:"https://docs.cvat.ai/docs/getting_started/overview/",children:"CVAT"})," which also provides an extensive UI feature for image dataset management."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Visualization"}),": MLflow's UI and Databricks are not optimized for visualizing image dataset annotations and require tools like ",(0,i.jsx)(t.code,{children:"CVAT"})," or custom scripts."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Central Dataset Management"}),": ",(0,i.jsx)(t.code,{children:"CVAT"})," can also help manage and version datasets to be used across MLflow experiment runs."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/dataset/",children:"MLflow tutorial on how to use the dataset module"}),"."]}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://cocodataset.org/#format-data",children:"COCO Dataset Format"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://huggingface.co/docs/datasets/en/image_dataset",children:"Hugging Face Image Datasets"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/model#evaluating-with-a-static-dataset",children:"MLflow tutorial on how to evaluate a model on a dataset"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html",children:"Documentation of the computer-vision model in this example"})}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"We hope this guide helps you streamline your image dataset tracking with MLflow and gives you some new ideas about image datasets. Happy ML model training!"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"And never let those GPU/CPUs cool down."})," Check your system metrics during model training in the MLflow UI."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"System metrics",src:a(42232).A+"",width:"1901",height:"935"})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var n=a(96540);const i={},s=n.createContext(i);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(s.Provider,{value:t},e.children)}},33865:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/id-337458-COCO-67392c6b44363ef91481b529bf3680d6.png"},36843:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/dataset-training-e77e2edfdf4057ede11814418c099c60.png"},42232:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/systemmetrics-63ddfa43b7d44e7e4c738ef7564dc73b.png"},85735:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/coco-dataset-32e6cae4b4598ff0f818f6021937de01.png"},91450:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/tracking-image-datasets","source":"@site/blog/2025-05-04-mlflow-image-datasets/index.md","title":"Tracking Image Datasets with MLflow","description":"Dataset tracking is fundamental to building robust and reproducible machine learning models. Among all data types, images present unique tracking challenges due to their high dimensionality, variability, and storage requirements. In this post, we\'ll demonstrate how to effectively track image datasets using MLflow\'s experiment tracking capabilities and UI, providing you with practical techniques to enhance your computer vision workflows\' tracking of data and models.","date":"2025-05-04T00:00:00.000Z","tags":[{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"inline":true,"label":"datasets","permalink":"/mlflow-website/blog/tags/datasets"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"},{"inline":true,"label":"computer-vision","permalink":"/mlflow-website/blog/tags/computer-vision"}],"readingTime":9.93,"hasTruncateMarker":false,"authors":[{"name":"Thor Steen Larsen","title":"Machine-learning engineer at Danske Statsbaner","url":"https://www.linkedin.com/in/thor-steen-larsen-00ab7448/","imageURL":"/mlflow-website/img/authors/ThorSteenLarsen_26-06-2019_ArthurCammelbeeck_003.png","key":"thorsteen","page":null}],"frontMatter":{"title":"Tracking Image Datasets with MLflow","tags":["mlflow","datasets","mlops","computer-vision"],"slug":"tracking-image-datasets","authors":["thorsteen"],"thumbnail":"/img/blog/coco-dataset.png","image":"/img/blog/coco-dataset.png"},"unlisted":false,"prevItem":{"title":"Announcing MLflow 3","permalink":"/mlflow-website/blog/mlflow-3-launch"},"nextItem":{"title":"MLflow Go","permalink":"/mlflow-website/blog/mlflow-go"}}')}}]);