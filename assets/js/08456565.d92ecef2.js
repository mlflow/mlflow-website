"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1154],{19841:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/2_eval_ui-fb093c18e47d491a8c4a5c23721fc58f.png"},21426:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/1_trace-ff3ddd2c139d1b9725b8e6746dc62ec5.png"},27933:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/tlm-tracing","source":"@site/blog/2025-04-01-tlm-tracing/index.mdx","title":"Automatically find the bad LLM responses in your LLM Evals with Cleanlab","description":"A practical guide to using Cleanlab\'s Trustworthy Language Models (TLM) to evaluate LLM responses captured in MLflow","date":"2025-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"observability","permalink":"/mlflow-website/blog/tags/observability"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":10.6,"hasTruncateMarker":false,"authors":[{"name":"Chris Mauck","title":"Data Scientist at Cleanlab","url":"https://www.linkedin.com/in/chris-mauck/","imageURL":"/mlflow-website/img/authors/chris_mauck.jpg","key":"chris-mauck","page":null}],"frontMatter":{"title":"Automatically find the bad LLM responses in your LLM Evals with Cleanlab","description":"A practical guide to using Cleanlab\'s Trustworthy Language Models (TLM) to evaluate LLM responses captured in MLflow","slug":"tlm-tracing","authors":["chris-mauck"],"tags":["genai","observability","tracing"],"thumbnail":"/img/blog/tlm-tracing-thumbnail.png"},"unlisted":false,"prevItem":{"title":"MLflow Go","permalink":"/mlflow-website/blog/mlflow-go"},"nextItem":{"title":"Practical AI Observability: Getting Started with MLflow Tracing","permalink":"/mlflow-website/blog/ai-observability-mlflow-tracing"}}')},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>i});var s=n(96540);const a={},r=s.createContext(a);function o(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:t},e.children)}},78136:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=n(27933),a=n(74848),r=n(28453);const o={title:"Automatically find the bad LLM responses in your LLM Evals with Cleanlab",description:"A practical guide to using Cleanlab's Trustworthy Language Models (TLM) to evaluate LLM responses captured in MLflow",slug:"tlm-tracing",authors:["chris-mauck"],tags:["genai","observability","tracing"],thumbnail:"/img/blog/tlm-tracing-thumbnail.png"},i="Automatically find the bad LLM responses in your LLM Evals with Cleanlab",l={authorsImageUrls:[void 0]},c=[{value:"Install dependencies &amp; Set environment variables",id:"install-dependencies--set-environment-variables",level:2},{value:"API Keys",id:"api-keys",level:3},{value:"Set Up MLflow Tracking Server and Logging",id:"set-up-mlflow-tracking-server-and-logging",level:3},{value:"Trace Some LLM Interactions with MLflow",id:"trace-some-llm-interactions-with-mlflow",level:2},{value:"Download Traces from the MLflow Tracking Server",id:"download-traces-from-the-mlflow-tracking-server",level:2},{value:"Evaluate Trustworthiness with TLM",id:"evaluate-trustworthiness-with-tlm",level:2},{value:"Tag Traces with TLM Evaluations",id:"tag-traces-with-tlm-evaluations",level:3},{value:"Using TLM with MLflow Evaluation",id:"using-tlm-with-mlflow-evaluation",level:2},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"This guide will walk you through the process of evaluating LLM responses captured in MLflow with Cleanlab's Trustworthy Language Models (TLM)."}),"\n",(0,a.jsxs)(t.p,{children:["TLM boosts the reliability of any LLM application by indicating when the model\u2019s response is untrustworthy. It works by analyzing the prompt and the generated response to calculate a ",(0,a.jsx)(t.code,{children:"trustworthiness_score"}),", helping to automatically identify potentially incorrect or hallucinated outputs without needing ground truth labels. TLM can also provide explanations for its assessment."]}),"\n",(0,a.jsx)(t.p,{children:"MLflow provides tracing and evaluation capabilities that can be used to monitor, review, and debug the performance of AI applications. This post will show how to apply Cleanlab's TLM to LLM responses recorded with MLflow tracing. Using Cleanlab's TLM with MLflow enables you to systematically log, track, and analyze the trustworthiness evaluations provided by TLM for your LLM interactions."}),"\n",(0,a.jsxs)(t.p,{children:["You can find a notebook version of this guide ",(0,a.jsx)(t.a,{href:"https://github.com/cleanlab/cleanlab-tools/blob/main/TLM-MLflow-Integration/evaluating_traces_TLM_mlflow_dl.ipynb",children:"here"}),"."]}),"\n",(0,a.jsx)(t.admonition,{type:"info",children:(0,a.jsxs)(t.p,{children:["This guide requires a Cleanlab TLM API key. If you don't have one, you can sign up for a free trial ",(0,a.jsx)(t.a,{href:"https://tlm.cleanlab.ai/",children:"here"}),"."]})}),"\n",(0,a.jsx)(t.h2,{id:"install-dependencies--set-environment-variables",children:"Install dependencies & Set environment variables"}),"\n",(0,a.jsx)(t.p,{children:"To work through this guide, you'll need to install the MLflow, OpenAI, and Cleanlab TLM Python packages:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"pip install -q mlflow openai cleanlab-tlm --upgrade\n"})}),"\n",(0,a.jsx)(t.p,{children:"Next, import the dependencies:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"import mlflow\nimport os\nimport json\nimport pandas as pd\n\nfrom rich import print\nfrom openai import OpenAI\nfrom getpass import getpass\n"})}),"\n",(0,a.jsx)(t.h3,{id:"api-keys",children:"API Keys"}),"\n",(0,a.jsx)(t.p,{children:"This guide requires two API keys:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://platform.openai.com/api-keys",children:"OpenAI API Key"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://tlm.cleanlab.ai/",children:"Cleanlab TLM API Key"})}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"If they are not already set as environment variables, you can set them manually as follows:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'if not (openai_api_key := os.getenv("OPENAI_API_KEY")):\n    openai_api_key = getpass("\ud83d\udd11 Enter your OpenAI API key: ")\nif not (cleanlab_tlm_api_key := os.getenv("CLEANLAB_TLM_API_KEY")):\n    cleanlab_tlm_api_key = getpass("\ud83d\udd11 Enter your Cleanlab TLM API key: ")\n\nos.environ["OPENAI_API_KEY"] = openai_api_key\nos.environ["CLEANLAB_TLM_API_KEY"] = cleanlab_tlm_api_key\n'})}),"\n",(0,a.jsx)(t.h3,{id:"set-up-mlflow-tracking-server-and-logging",children:"Set Up MLflow Tracking Server and Logging"}),"\n",(0,a.jsxs)(t.p,{children:["To manage our experiments, parameters, and results effectively, we'll ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server",children:"start a local MLflow Tracking Server"}),". This provides a dedicated UI for ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracking/",children:"monitoring and managing our experiments"})," and allows us to ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/getting-started/",children:"configure MLflow to connect to this server"}),". We'll then enable autologging for OpenAI to automatically capture relevant information from our API calls."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# This will start a server on port 8080, in the background\n# Navigate to http://localhost:8080 to see the MLflow UI\n%%bash --bg\nmlflow server --host 127.0.0.1 --port 8080\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Set up MLflow tracking server\nmlflow.set_tracking_uri("http://localhost:8080")\n\n# Enable logging for OpenAI SDK\nmlflow.openai.autolog()\n\n# Set experiment name\nmlflow.set_experiment("Eval OpenAI Traces with TLM")\n\n# Get experiment ID\nexperiment_id = mlflow.get_experiment_by_name("Eval OpenAI Traces with TLM").experiment_id\n'})}),"\n",(0,a.jsx)(t.h2,{id:"trace-some-llm-interactions-with-mlflow",children:"Trace Some LLM Interactions with MLflow"}),"\n",(0,a.jsx)(t.p,{children:'For the sake of demonstration purposes, we\'ll briefly generate some traces and track them in MLflow. Typically, you would have already captured traces in MLflow and would skip to "Download Traces from the MLflow Tracking Server."'}),"\n",(0,a.jsx)(t.p,{children:"In this example, we'll use some tricky trivia questions to generate some traces."}),"\n",(0,a.jsx)(t.admonition,{type:"info",children:(0,a.jsx)(t.p,{children:"TLM requires the entire input to the LLM to be provided. This includes any system prompts, context, or other information that was originally provided to the LLM to generate the response. Notice below that we include the system prompt in the trace metadata since by default the trace does not include the system prompt within the input."})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Let\'s use some tricky trivia questions to generate some traces\ntrivia_questions = [\n    "What is the 3rd month of the year in alphabetical order?",\n    "What is the capital of France?",\n    "How many seconds are in 100 years?",\n    "Alice, Bob, and Charlie went to a caf\xe9. Alice paid twice as much as Bob, and Bob paid three times as much as Charlie. If the total bill was $72, how much did each person pay?",\n    "When was the Declaration of Independence signed?"\n]\n\nclient = OpenAI()\n\ndef generate_answers(trivia_question, client=client):\n    system_prompt = "You are a trivia master."\n\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": trivia_question},\n        ],\n    )\n\n    answer = response.choices[0].message.content\n    return answer\n\n\n# Generate answers\nanswers = []\nfor i in range(len(trivia_questions)):\n    answer = generate_answers(trivia_questions[i])\n    answers.append(answer)\n    print(f"Question {i+1}: {trivia_questions[i]}")\n    print(f"Answer {i+1}:\\n{answer}\\n")\n\nprint(f"Generated {len(answers)} answers and tracked them in MLflow.")\n'})}),"\n",(0,a.jsxs)(t.p,{children:["We can see the resulting traces in the MLflow UI and, if you are running this code in a Jupyter notebook, you can see the traces ",(0,a.jsx)(t.a,{href:"https://mlflow.org/blog/mlflow-tracing-in-jupyter",children:"in the notebook cell output"}),"."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"MLflow Traces",src:n(21426).A+"",width:"2962",height:"2118"})}),"\n",(0,a.jsx)(t.p,{children:"Next, we'll download the generated traces from the MLflow tracking server so we can evaluate them with TLM. This illustrates how MLflow tracing can be used to generate datasets for downstream tasks like evaluation."}),"\n",(0,a.jsx)(t.h2,{id:"download-traces-from-the-mlflow-tracking-server",children:"Download Traces from the MLflow Tracking Server"}),"\n",(0,a.jsxs)(t.p,{children:["Fetching traces from MLflow is straightforward. Just set up the MLflow client and use the ",(0,a.jsx)(t.code,{children:"search_traces()"})," function to fetch the data. We'll fetch the traces and evaluate them. After that, we'll add our scores back into MLflow."]}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"search_traces()"})," function has arguments to filter the traces by tags, timestamps, and beyond. You can find more about other methods to ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.search_traces",children:"query traces"})," in the docs."]}),"\n",(0,a.jsx)(t.p,{children:"In this example, we'll fetch all traces from the experiment."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"client = mlflow.client.MlflowClient()\ntraces = client.search_traces(experiment_ids=[experiment_id])\n\n# Print the first trace\nprint(traces[0].data)\n"})}),"\n",(0,a.jsx)(t.h2,{id:"evaluate-trustworthiness-with-tlm",children:"Evaluate Trustworthiness with TLM"}),"\n",(0,a.jsx)(t.p,{children:"Now that we have our traces, we can use TLM to generate trustworthiness scores and explanations for each trace."}),"\n",(0,a.jsxs)(t.p,{children:["Instead of running TLM individually on each trace, we'll provide all of the ",(0,a.jsx)(t.code,{children:"(prompt, response)"})," pairs in a list to TLM in a single call. This is more efficient and allows us to get scores and explanations for all of the traces at once. Then, using the trace request IDs, we can attach the scores and explanations back to the correct trace in MLflow."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from cleanlab_tlm import TLM\n\ntlm = TLM(options={"log": ["explanation"]})\n'})}),"\n",(0,a.jsx)(t.p,{children:"We'll use the following helper function to extract the prompt and response from each trace and return three lists: request IDs, prompts, and responses. We can then construct a DataFrame with the evaluation results, sort and filter the results by trustworthiness score, and tag the traces in MLflow with the scores and explanations."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'def get_prompt_response_pairs(traces):\n    prompts = []\n    responses = []\n    for trace in traces:\n        # Parse request and response JSON\n        request_data = json.loads(trace.data.request)\n        response_data = json.loads(trace.data.response)\n\n        # Extract system prompt and user message from request\n        system_prompt = request_data["messages"][0]["content"]\n        user_message = request_data["messages"][1]["content"]\n\n        # Extract assistant\'s response from response\n        assistant_response = response_data["choices"][0]["message"]["content"]\n\n        prompts.append(system_prompt + "\\n" + user_message)\n        responses.append(assistant_response)\n    return prompts, responses\n\nrequest_ids = [trace.info.request_id for trace in traces]\nprompts, responses = get_prompt_response_pairs(traces)\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Now, let's use TLM to generate a ",(0,a.jsx)(t.code,{children:"trustworthiness score"})," and ",(0,a.jsx)(t.code,{children:"explanation"})," for each trace."]}),"\n",(0,a.jsx)(t.admonition,{type:"info",children:(0,a.jsxs)(t.p,{children:["It is essential to always include any system prompts, context, or other information that was originally provided to the LLM to generate the response. You should construct the prompt input to ",(0,a.jsx)(t.code,{children:"get_trustworthiness_score()"})," in a way that is as similar as possible to the original prompt. This is why we included the system prompt in the trace metadata."]})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Evaluate each of the prompt, response pairs using TLM\nevaluations = tlm.get_trustworthiness_score(prompts, responses)\n\n# Extract the trustworthiness scores and explanations from the evaluations\ntrust_scores = [entry[\"trustworthiness_score\"] for entry in evaluations]\nexplanations = [entry[\"log\"][\"explanation\"] for entry in evaluations]\n\n# Create a DataFrame with the evaluation results\ntrace_evaluations = pd.DataFrame({\n    'request_id': request_ids,\n    'prompt': prompts,\n    'response': responses,\n    'trust_score': trust_scores,\n    'explanation': explanations\n})\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Now we have a DataFrame mapping trace IDs to their scores and explanations. We've also included the prompt and response for each trace for demonstration purposes to find the ",(0,a.jsx)(t.strong,{children:"least trustworthy trace!"})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'sorted_df = trace_evaluations.sort_values(by="trust_score", ascending=True)\nsorted_df.head(3)\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Let\'s look at the least trustworthy trace.\nprint("Prompt: ", sorted_df.iloc[0]["prompt"], "\\n")\nprint("OpenAI Response: ", sorted_df.iloc[0]["response"], "\\n")\nprint("TLM Trust Score: ", sorted_df.iloc[0]["trust_score"], "\\n")\nprint("TLM Explanation: ", sorted_df.iloc[0]["explanation"])\n'})}),"\n",(0,a.jsx)(t.p,{children:"Which returns the following:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"Prompt:  You are a trivia master.\nWhat is the 3rd month of the year in alphabetical order?\n\nOpenAI Response:  The 3rd month of the year in alphabetical order is March. The months in alphabetical order are:\n\n1. April\n2. August\n3. December\n4. February\n5. January\n6. July\n7. June\n8. March\n9. May\n10. November\n11. October\n12. September\n\nSo, March is the 8th month, not the 3rd. The 3rd month in alphabetical order is February.\n\nTLM Trust Score:  0.04388514403195165\n\nTLM Explanation:  The proposed response incorrectly identifies the 3rd month of the year in alphabetical order. The\nmonths of the year, when arranged alphabetically, are as follows:\n\n1. April\n2. August\n3. December\n4. February\n5. January\n6. July\n7. June\n8. March\n9. May\n10. November\n11. October\n12. September\n\nIn this list, February is indeed the 4th month, not the 3rd. The 3rd month in alphabetical order is actually\nDecember. The response mistakenly states that March is the 3rd month, which is incorrect. Therefore, the answer to\nthe user's request is that the 3rd month in alphabetical order is December, not February or March.\nThis response is untrustworthy due to lack of consistency in possible responses from the model. Here's one\ninconsistent alternate response that the model considered (which may not be accurate either):\nDecember.\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Awesome! TLM was able to identify multiple traces that contained incorrect answers from OpenAI. In the example above, it correctly noted that the original response actually included ",(0,a.jsx)(t.em,{children:"two"})," incorrect answers, making it both wrong and inconsistent."]}),"\n",(0,a.jsxs)(t.admonition,{title:"Tracing TLM",type:"info",children:[(0,a.jsxs)(t.p,{children:["You could also trace the TLM API call itself. This will log the trustworthiness scores and explanations for each trace. Here's an example of how to do this by wrapping the TLM API call in a custom function and tracing it with the ",(0,a.jsx)(t.code,{children:"@mlflow.trace"})," decorator."]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Tracing TLM\n\n@mlflow.trace\ndef tlm_trustworthiness_wrapper(inputs, predictions):\n    tlm = TLM(options={"log": ["explanation"]})\n    evaluations = tlm.get_trustworthiness_score(inputs, predictions)\n    return evaluations\n\ntlm_trustworthiness_wrapper(prompts[0], answers[0])\n'})})]}),"\n",(0,a.jsxs)(t.p,{children:["Now, let's upload the ",(0,a.jsx)(t.code,{children:"trust_score"})," and ",(0,a.jsx)(t.code,{children:"explanation"})," columns to MLflow."]}),"\n",(0,a.jsx)(t.h3,{id:"tag-traces-with-tlm-evaluations",children:"Tag Traces with TLM Evaluations"}),"\n",(0,a.jsxs)(t.p,{children:["We'll use the ",(0,a.jsx)(t.code,{children:"set_trace_tag()"})," function to save the TLM scores and explanations as tags on the traces."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'for idx, row in trace_evaluations.iterrows():\n    request_id = row["request_id"]\n    trust_score = row["trust_score"]\n    explanation = row["explanation"]\n\n    # Add the trustworthiness score and explanation to the trace as a tag\n    client.set_trace_tag(request_id=request_id, key="trust_score", value=trust_score)\n    client.set_trace_tag(request_id=request_id, key="explanation", value=explanation)\n'})}),"\n",(0,a.jsx)(t.p,{children:"You should now see the TLM trustworthiness score and explanation in the MLflow UI! From here you can continue collecting and evaluating traces."}),"\n",(0,a.jsx)(t.h2,{id:"using-tlm-with-mlflow-evaluation",children:"Using TLM with MLflow Evaluation"}),"\n",(0,a.jsx)(t.p,{children:"MLflow Evaluation helps assess AI applications using built-in and custom metrics to score model outputs. Results, including scores and justifications, are logged and can be compared in the MLflow UI for systematic performance tracking. In this section, we will create a custom metric that uses TLM to evaluate the trustworthiness of LLM responses, providing a straightforward way to integrate TLM into your MLflow workflows."}),"\n",(0,a.jsx)(t.p,{children:"Using MLflow Evaluation with our custom TLM metric will log a table of trustworthiness scores and explanations and also provide an interface in the UI for comparing scores across runs. For example, you could use this to compare the trustworthiness scores of different models across the same set of prompts."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.metrics import MetricValue, make_metric\nfrom cleanlab_tlm import TLM\n\ndef _tlm_eval_fn(predictions, inputs, targets=None):\n    """\n    Evaluate trustworthiness using Cleanlab TLM.\n\n    Args:\n        predictions: The model outputs/answers\n        targets: Not used for this metric\n        **kwargs: Should contain \'inputs\' with the prompts\n    """\n    # Initialize TLM\n    tlm = TLM(options={"log": ["explanation"]})\n    inputs = inputs.to_list()\n    predictions = predictions.to_list()\n\n    # Get trustworthiness scores\n    evaluations = tlm.get_trustworthiness_score(inputs, predictions)\n\n    # Extract scores and explanations\n    scores = [float(eval_result["trustworthiness_score"]) for eval_result in evaluations]\n    justifications = [eval_result["log"]["explanation"] for eval_result in evaluations]\n\n    # Return metric value\n    return MetricValue(\n        scores=scores,\n        justifications=justifications,\n        aggregate_results={\n            "mean": sum(scores) / len(scores),\n            "min": min(scores),\n            "max": max(scores)\n        }\n    )\n\ndef tlm_trustworthiness():\n    """Creates a metric for evaluating trustworthiness using Cleanlab TLM"""\n    return make_metric(\n        eval_fn=_tlm_eval_fn,\n        greater_is_better=True,\n        name="tlm_trustworthiness"\n    )\n'})}),"\n",(0,a.jsx)(t.p,{children:"Now that we have defined the custom metric, let's use it to evaluate the trustworthiness of our LLM responses. We will use the same responses and prompts that we collected from our traces before."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'tlm_metric = tlm_trustworthiness()\n\neval_df = pd.DataFrame({\n    \'inputs\': prompts,\n    \'outputs\': responses\n})\n\n\nresults = mlflow.evaluate(\n    data=eval_df,\n    predictions="outputs",\n    model=None,\n    extra_metrics=[tlm_metric],\n    evaluator_config={\n        "col_mapping": {\n            "inputs": "inputs",\n            "predictions": "outputs"\n        }\n    }\n)\n'})}),"\n",(0,a.jsx)(t.p,{children:"Now we can see the results in the MLflow Evaluation UI."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"MLflow Evaluation",src:n(19841).A+"",width:"3760",height:"2304"})}),"\n",(0,a.jsx)(t.p,{children:"This approach is especially useful once you start comparing scores across different models, prompts, or other criteria."}),"\n",(0,a.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(t.p,{children:"In this post, we showed how to use Cleanlab's TLM to evaluate the trustworthiness of LLM responses captured in MLflow. We demonstrated how to use TLM to generate trustworthiness scores and explanations for each trace, tag the traces with the scores and explanations, and use MLflow Evaluation to log and compare the scores across runs."}),"\n",(0,a.jsx)(t.p,{children:"This approach provides a straightforward way to integrate TLM into your MLflow workflows for systematic performance tracking and debugging of AI applications. It also highlights a key use case for MLflow tracing: generating datasets for downstream tasks like evaluation."})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);