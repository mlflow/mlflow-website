"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6255],{54520:(e,n,t)=>{t.r(n),t.d(n,{default:()=>u});var a=t(57031),o=t(86387);const i=t.p+"assets/images/classical_tracking_hero-1a7b6069167f597a29b55d6f1774a082.png",s=t.p+"assets/images/classical_tracking_1-fbef55c4db4a133c2a27ef517e91cb5d.png",r=t.p+"assets/images/classical_tracking_2-eab3a9da0aeb4adbb72100aded77a0b4.png",l=t.p+"assets/images/classical_tracking_3-b9e91b8207e7f8d640467e0921925363.png",c=t.p+"assets/images/classical_tracking_4-659b275b0bb1fc60048089f0fb6489be.png";var d=t(74848);function u(){return(0,d.jsxs)(o.PE,{children:[(0,d.jsx)(o.Wh,{sectionLabel:"Experiment tracking",title:"Comprehensive experiment tracking",body:" Track, compare, and reproduce your machine learning experiments with MLflow's powerful tracking capabilities.",hasGetStartedButton:a.OU,children:(0,d.jsx)(o.Cv,{src:i,alt:""})}),(0,d.jsxs)(o.xA,{columns:2,children:[(0,d.jsx)(o.Ei,{width:"wide",children:(0,d.jsx)(o.Zp,{title:"Visual comparison",body:"Compare results across multiple experiments with MLflow's powerful visualization tools. Quickly identify best-performing models and retrieve their corresponding code and parameters based on different metrics of interest across various projects.",image:(0,d.jsx)("img",{src:s,alt:""})})}),(0,d.jsx)(o.Ei,{width:"wide",children:(0,d.jsx)(o.Zp,{title:"Native ML library integrations with mlflow.autolog",body:"mlflow.autolog() integrates with popular ML libraries to automatically log hyperparameters, metrics, and artifacts\u2014enabling efficient tracking and comparison of tuning experiments with no manual effort.",image:(0,d.jsx)("img",{src:s,alt:""})})}),(0,d.jsx)(o.Ei,{width:"wide",children:(0,d.jsx)(o.Zp,{title:"Reliable reproducibility",body:"Reliably logs parameters, weights, artifacts, code, metrics, and dependencies to ensure experiments can be restored accurately, enabling confident governance for enterprise deployments.",image:(0,d.jsx)("img",{src:r,alt:""})})}),(0,d.jsx)(o.Ei,{width:"wide",children:(0,d.jsx)(o.Zp,{title:"Track hyperparameter tuning runs",body:"Leverage the native integration between MLflow and Optuna to run distributed hyperparameter optimization at scale using Spark UDFs. The MLflow tracking server provides robust trial data storage that persists through node failures, ensuring your optimization jobs complete successfully even in complex scalable distributed environments.",image:(0,d.jsx)("img",{src:l,alt:""})})}),(0,d.jsx)(o.Ei,{width:"wide",children:(0,d.jsx)(o.Zp,{title:"Identify the best model for production",body:"By visualizing metrics across runs directly in the MLflow UI, users can quickly evaluate tradeoffs and identify the best model for production.",image:(0,d.jsx)("img",{src:l,alt:""})})}),(0,d.jsx)(o.Ei,{width:"wide",children:(0,d.jsx)(o.Zp,{title:"Complete experiment lifecycle",body:"MLflow Tracking automatically captures parameters, code versions, metrics, and model weights for each training iteration. Log trained models, visualizations, interface signatures, and data samples to ensure complete reproducibility across your entire ML workflow.",image:(0,d.jsx)("img",{src:c,alt:""})})})]}),(0,d.jsx)(o.gQ,{})]})}},57031:(e,n,t)=>{t.d(n,{OU:()=>a,Q5:()=>o});const a="https://mlflow.org/docs/latest/",o="https://signup.databricks.com/?destination_url=/ml/experiments-signup?source=MLFLOW_ORG&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW"},86387:(e,n,t)=>{t.d(n,{Wh:()=>ke,gQ:()=>xe,nB:()=>le,$n:()=>r,Zp:()=>Se,C1:()=>Ie,R1:()=>ze,aO:()=>be,GN:()=>Te,nN:()=>ve,AO:()=>Le,rX:()=>Me,xA:()=>_,Ei:()=>v,DZ:()=>se,Cv:()=>qe,r3:()=>me,PE:()=>R,Hl:()=>M,wn:()=>W,eN:()=>F,ej:()=>de,vU:()=>Ce});var a=t(96540),o=t(22732),i=t(74848);const s=(0,o.F)("rounded-xl inline-flex items-center justify-center whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0 hover:cursor-pointer",{variants:{variant:{primary:"bg-white text-black hover:bg-gray-900",outline:"box-border border border-white bg-transparent text-white hover:text-gray-900",secondary:"bg-black text-white hover:bg-black/80",dark:"bg-white/12 text-white hover:bg-gray-100 border border-gray-200",blue:"bg-[#0094E2] text-white hover:bg-[#0094E2]/80"},size:{small:"text-[15px] px-4 py-2",medium:"text-base px-4 py-4",large:"text-base px-5 py-4"},width:{default:"w-fit",full:"w-full"}},defaultVariants:{variant:"primary",size:"medium",width:"default"}}),r=a.forwardRef((({className:e,variant:n,size:t,width:a,asChild:o=!1,...r},l)=>(0,i.jsx)("button",{className:s({variant:n,size:t,width:a,className:e}),ref:l,...r})));r.displayName="Button";var l;function c(){return c=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},c.apply(null,arguments)}const d=({title:e,titleId:n,...t})=>a.createElement("svg",c({xmlns:"http://www.w3.org/2000/svg",width:109,height:40,fill:"none",viewBox:"0 0 109 40","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,l||(l=a.createElement("path",{fill:"#fff",d:"M0 31.032v-15.53h3.543v1.968c.893-1.594 2.84-2.425 4.593-2.425 2.042 0 3.828.926 4.658 2.745 1.216-2.043 3.034-2.745 5.043-2.745 2.81 0 5.49 1.787 5.49 5.904v10.083h-3.574v-9.478c0-1.818-.926-3.19-3-3.19-1.947 0-3.224 1.53-3.224 3.445v9.22H9.893v-9.475c0-1.786-.898-3.18-3.003-3.18-1.977 0-3.223 1.467-3.223 3.444v9.221ZM27.855 31.032V7.929h3.703v23.103ZM30.07 39.487c.833.232 1.582.383 3.17.383 2.953 0 6.436-1.665 7.353-6.339l3.786-18.739h5.629l.687-3.117h-5.686l.765-3.725c.586-2.892 2.186-4.358 4.754-4.358.668 0 .48.058 1.076.17L52.427.57c-.793-.237-1.503-.379-3.049-.379a7.32 7.32 0 0 0-4.528 1.484c-1.441 1.114-2.393 2.75-2.827 4.863l-1.066 5.137h-5.034l-.411 3.12h4.823L36.859 32.12c-.383 1.965-1.5 4.315-4.708 4.315-.727 0-.463-.055-1.121-.162ZM53.342 30.905H49.64l5.074-23.309h3.701ZM71.807 16.477A7.962 7.962 0 0 0 60.97 27.904l2.425-1.78a5.005 5.005 0 0 1 3.845-8.145v1.895ZM62.618 29.472a7.962 7.962 0 0 0 10.836-11.428l-2.425 1.78a5.005 5.005 0 0 1-3.845 8.145v-1.894ZM78.092 15.493h4.044l.823 10.612 5.759-10.612 3.839.055 1.508 10.557 5.074-10.612 3.701.055-7.678 15.493H91.46l-1.783-11.106-5.895 11.106h-3.84ZM105.072 15.768h-.766v-.266h1.845v.272h-.765v2.243h-.314ZM106.614 15.502h.383l.482 1.34q.091.259.178.524h.018c.059-.176.113-.352.172-.524l.478-1.34h.383v2.515h-.298V16.63c0-.22.024-.523.04-.747h-.016l-.191.574-.475 1.304h-.208l-.481-1.302-.191-.574h-.015c.017.224.042.527.042.747v1.387h-.291Z"})));var u=t(56289);const m=e=>(0,i.jsx)("div",{className:"min-w-[120px]",children:(0,i.jsx)(u.A,{...e})});var h=t(57031);const p=(0,o.F)("pb-30 flex flex-col pt-30 bg-bottom bg-no-repeat bg-cover text-center bg-size-[auto_360px] 2xl:bg-size-[100%_360px]",{variants:{variant:{blue:["bg-[linear-gradient(to_top,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_10%,rgba(14,20,20,100)_40%),url('/img/footer-blue-bg.png')]"],red:["bg-[linear-gradient(to_top,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_10%,rgba(14,20,20,100)_40%),url('/img/footer-red-bg.png')]"],colorful:["bg-[linear-gradient(to_top,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_10%,rgba(14,20,20,100)_40%),url('/img/footer-colorful-bg.png')]"]}}}),g=({variant:e})=>(0,i.jsx)("footer",{className:p({variant:e}),children:(0,i.jsxs)("div",{className:"flex flex-row justify-between items-start px-6 lg:px-20 gap-10 xs:gap-0 max-w-container",children:[(0,i.jsxs)("div",{className:"flex flex-col gap-8",children:[(0,i.jsx)(d,{className:"h-[36px] shrink-0"}),(0,i.jsx)("div",{className:"text-xs text-gray-800 text-left md:text-nowrap md:w-0",children:"\xa9 2025 MLflow Project, a Series of LF Projects, LLC."})]}),(0,i.jsxs)("div",{className:"flex flex-col md:flex-row gap-10",children:[(0,i.jsx)(m,{href:"/",children:"Components"}),(0,i.jsx)(m,{href:"/releases",children:"Releases"}),(0,i.jsx)(m,{href:"/blog",children:"Blog"}),(0,i.jsx)(m,{href:h.OU,children:"Docs"}),(0,i.jsx)(m,{href:"/ambassadors",children:"Ambassador Program"})]})]})});var f=t(50856);const w=(0,o.F)("w-full overflow-hidden"),y=(0,o.F)(["grid w-full bg-[rgba(255,255,255,0.08)] gap-[1px]","border-[rgba(255,255,255,0.08)] border-t border-b","-mx-10 w-[calc(100%+var(--spacing)*10*2)]"],{variants:{columns:{default:"md:auto-cols-fr md:grid-flow-col",2:"md:grid-cols-2",3:"md:grid-cols-3",4:"md:grid-cols-4"}},defaultVariants:{columns:"default"}}),_=({children:e,className:n,columns:t})=>{let o=0;a.Children.forEach(e,(e=>{a.isValidElement(e)&&("width"in e.props&&"wide"===e.props.width?o+=2:o+=1)}));let s=0;if("number"==typeof t){s=Math.ceil(o/t)*t-o}return(0,i.jsx)("div",{className:w(),children:(0,i.jsxs)("div",{className:(r={columns:t,className:n},(0,f.QP)(y(r))),children:[e,Array.from({length:s},((e,n)=>(0,i.jsx)(v,{},n)))]})});var r},b=(0,o.F)(["flex flex-col items-start p-10 gap-20 justify-between w-full bg-[#0E1416]"],{variants:{width:{narrow:"",wide:"md:col-span-2 md:flex-row *:flex-1 md:items-center"},direction:{reverse:"md:flex-col-reverse"}},compoundVariants:[{width:"wide",direction:"reverse",className:"md:flex-row-reverse"}],defaultVariants:{width:"narrow"}}),v=({children:e,width:n,direction:t})=>{return(0,i.jsx)("div",{className:(a={width:n,direction:t},(0,f.QP)(b(a))),children:e});var a};var k=t(29030);const x=["/img/companies/databricks.svg","/img/companies/microsoft.svg","/img/companies/meta.svg","/img/companies/mosaicml.svg","/img/companies/zillow.svg","/img/companies/toyota.svg","/img/companies/booking.svg","/img/companies/wix.svg","/img/companies/accenture.svg","/img/companies/asml.svg"],L=(0,o.cx)("shrink-0 animate-slide-left-infinite"),M=()=>{const e=x.map(((e,n)=>(0,i.jsx)("img",{className:"inline h-10 mx-10 opacity-20",src:(0,k.Ay)(e)},n)));return(0,i.jsxs)("div",{className:"flex overflow-x-hidden [mask-image:_linear-gradient(to_right,_transparent_0,_white_128px,white_calc(100%-128px),_transparent_100%)] w-full p-8",children:[(0,i.jsx)("div",{className:L,children:e}),(0,i.jsx)("div",{className:L,"aria-hidden":!0,children:e})]})};var T;function I(){return I=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},I.apply(null,arguments)}const A=({title:e,titleId:n,...t})=>a.createElement("svg",I({xmlns:"http://www.w3.org/2000/svg",width:16,height:17,fill:"none",viewBox:"0 0 16 17","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,T||(T=a.createElement("path",{fill:"#F7F8F8",fillRule:"evenodd",d:"m8 10.776 3.61-3.61-.943-.942L8 8.89 5.333 6.224l-.942.943z",clipRule:"evenodd"})));var E=t(34164);function S(...e){return(0,f.QP)((0,E.$)(e))}const q=({href:e,label:n,className:t,onMouseEnter:a,hasDropdown:o,...s})=>(0,i.jsxs)(u.A,{href:e,className:S("flex items-center gap-2 py-2 text-white text-lg w-full md:w-auto cursor-pointer",t),...s,children:[n,o&&(0,i.jsx)(A,{className:"w-6 h-6"})]}),C=()=>(0,i.jsxs)("div",{className:"flex flex-col md:flex-row md:max-w-4xl mx-auto gap-6 md:gap-8 lg:gap-10 px-4 lg:pl-0 products-submenu",children:[(0,i.jsxs)("div",{className:"flex flex-col gap-1 md:gap-4",children:[(0,i.jsx)("div",{className:"flex flex-col gap-1 md:gap-4 md:border-b border-[#F7F8F8]/8 pb-4",children:(0,i.jsxs)(u.A,{to:"/genai",children:[(0,i.jsx)("h3",{className:"text-white",children:"Gen AI"}),(0,i.jsx)("p",{className:"text-[#F7F8F8]/60 m-0",children:"Ship high-quality GenAI, fast"})]})}),(0,i.jsxs)("div",{className:"flex flex-col gap-3",children:[(0,i.jsx)("span",{className:"text-[#F7F8F8]/60 text-sm",children:"Features"}),(0,i.jsxs)("div",{className:"flex flex-row gap-4",children:[(0,i.jsxs)("div",{className:"min-w-50 flex flex-col gap-4 md:gap-1",children:[(0,i.jsx)(q,{href:"/genai/observability",label:"Observability"}),(0,i.jsx)(q,{href:"/genai/evaluations",label:"Evaluations"}),(0,i.jsx)(q,{href:"/genai/prompt-registry",label:"Prompt Registry"})]}),(0,i.jsxs)("div",{className:"min-w-40 flex flex-col gap-4 md:gap-1",children:[(0,i.jsx)(q,{href:"/genai/app-versioning",label:"App versioning"}),(0,i.jsx)(q,{href:"/genai/ai-gateway",label:"AI Gateway"})]})]})]})]}),(0,i.jsxs)("div",{className:"flex flex-col gap-4",children:[(0,i.jsx)("div",{className:"flex flex-col gap-1 md:gap-4 md:border-b border-[#F7F8F8]/8 pb-4",children:(0,i.jsxs)(u.A,{to:"/classical-ml",children:[(0,i.jsx)("h3",{className:"text-white",children:"Model training"}),(0,i.jsx)("p",{className:"text-[#F7F8F8]/60",children:"Mastering the ML lifecycle"})]})}),(0,i.jsxs)("div",{className:"flex flex-col gap-3",children:[(0,i.jsx)("span",{className:"text-[#F7F8F8]/60 text-sm",children:"Features"}),(0,i.jsxs)("div",{className:"flex flex-row gap-8",children:[(0,i.jsxs)("div",{className:"min-w-40 flex flex-col gap-4 md:gap-1",children:[(0,i.jsx)(q,{href:"/classical-ml/experiment-tracking",label:"Experiment tracking"}),(0,i.jsx)(q,{href:"/classical-ml/model-evaluation",label:"Model evaluation"})]}),(0,i.jsxs)("div",{className:"min-w-40 flex flex-col gap-4 md:gap-1",children:[(0,i.jsx)(q,{href:"/classical-ml/models",label:"MLflow models"}),(0,i.jsx)(q,{href:"/classical-ml/model-registry",label:"Model Registry & deployment"})]})]})]})]})]}),P=({layoutType:e})=>{const[n,t]=(0,a.useState)(!1),[o,s]=(0,a.useState)(!1),[l,c]=(0,a.useState)(!1);return(0,a.useLayoutEffect)((()=>{const e=()=>{window.innerWidth>=640&&t(!1)};return window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)}),[]),(0,i.jsxs)("nav",{className:"fixed w-full z-20 top-0 start-0 bg-[#F7F8F8]/1 border-b border-[#F7F8F8]/8 backdrop-blur-[20px] drop-shadow-[0px_1px_2px_rgba(0_0_0/75%),0px_1px_12px_rgba(0_0_0/75%)]",children:[(0,i.jsxs)("div",{className:"flex flex-wrap items-center mx-auto px-6 lg:px-20 py-2 max-w-container",children:[(0,i.jsx)(u.A,{href:"/",className:"flex items-center space-x-3 rtl:space-x-reverse grow basis-0",children:(0,i.jsx)(d,{className:"h-[36px]"})}),(0,i.jsxs)("div",{className:"flex flex-row items-center gap-6 md:order-2 space-x-3 md:space-x-0 rtl:space-x-reverse grow justify-end basis-0",children:[(0,i.jsx)(u.A,{href:h.OU,className:"hidden md:block",children:(0,i.jsx)(r,{variant:"primary",size:"small",children:"Get started"})}),(0,i.jsxs)("button",{"data-collapse-toggle":"navbar-sticky",type:"button",className:"inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-white md:hidden focus:outline-none cursor-pointer",onClick:()=>t(!n),children:[(0,i.jsx)("span",{className:"sr-only",children:"Open main menu"}),(0,i.jsx)("svg",{className:"w-5 h-5","aria-hidden":"true",xmlns:"http://www.w3.org/2000/svg",fill:"none",viewBox:"0 0 17 14",children:(0,i.jsx)("path",{stroke:"currentColor",strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:"2",d:"M1 1h15M1 7h15M1 13h15"})})]})]}),(0,i.jsx)("div",{className:S("items-center justify-between w-full md:flex md:w-auto md:order-1 mt-4 md:mt-0",n?"flex":"hidden md:flex"),children:(0,i.jsxs)("ul",{className:"flex flex-col font-medium md:flex-row gap-6 md:gap-10 w-full md:w-auto",children:[(0,i.jsxs)("li",{className:"w-full md:w-auto md:hidden",onClick:()=>{c(!l)},children:[(0,i.jsxs)("span",{className:"flex items-center gap-2 py-2 text-white text-[15px] w-full md:w-auto cursor-pointer",children:["Components",(0,i.jsx)(A,{className:"w-6 h-6"})]}),(0,i.jsx)("div",{className:S("transition-all duration-300 ease-in",l?"h-auto min-h-50":"h-0 overflow-hidden"),children:(0,i.jsx)(C,{})})]}),(0,i.jsx)("li",{className:"w-full md:w-auto hidden md:block",onMouseEnter:()=>{s(!0)},onClick:()=>{s(!o)},children:(0,i.jsx)(q,{label:"Components",hasDropdown:!0})}),(0,i.jsx)("li",{className:"w-full md:w-auto",children:(0,i.jsx)(q,{href:"/releases",label:"Releases"})}),(0,i.jsx)("li",{className:"w-full md:w-auto",children:(0,i.jsx)(q,{href:"/blog",label:"Blog"})}),(0,i.jsx)("li",{className:"w-full md:w-auto",children:(0,i.jsx)(q,{href:h.OU,label:"Docs"})}),(0,i.jsx)("li",{className:"w-full md:w-auto",children:(0,i.jsx)(q,{href:"/ambassadors",label:"Ambassador Program"})}),(0,i.jsx)("li",{className:"w-full md:w-auto md:hidden",children:(0,i.jsx)(u.A,{href:h.OU,children:(0,i.jsx)(r,{variant:"primary",size:"small",width:"full",children:"Get started"})})})]})})]}),(0,i.jsx)("div",{className:S("flex-col w-full py-6 backdrop-blur bg-black/10",o?"flex":"hidden"),onMouseLeave:()=>{s(!1)},children:(0,i.jsx)(C,{})})]})};var D=t(56347);const N=(0,o.F)("flex flex-col gap-20 bg-no-repeat w-full pt-42 pb-20 py-20",{variants:{variant:{red:"",blue:"",colorful:""},direction:{down:"bg-size-[auto_1000px] 2xl:bg-size-[100%_1000px] bg-[center_top]",up:"bg-size-[auto_820px] 2xl:bg-size-[100%_820px] bg-[center_top]"}},compoundVariants:[{variant:"red",direction:"down",className:["bg-[linear-gradient(to_bottom,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_50%,rgba(14,20,20,100)_75%),url('/img/background-image-2.png')]"]},{variant:"blue",direction:"down",className:["bg-[linear-gradient(to_bottom,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_50%,rgba(14,20,20,100)_75%),url('/img/background-image-3.png')]"]},{variant:"colorful",direction:"down",className:["bg-[linear-gradient(to_bottom,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_50%,rgba(14,20,20,100)_75%),url('/img/background-image-1.png')]"]},{variant:"red",direction:"up",className:["bg-[linear-gradient(to_top,rgba(12,20,20,0)_0%,rgba(12,20,20,0)_10%,rgba(14,20,20,100)_40%),url('/img/background-image-2-flipped.png')]"]},{variant:"blue",direction:"up",className:["bg-[linear-gradient(to_top,rgba(11,20,20,0)_0%,rgba(12,20,20,0)_10%,rgba(14,20,20,100)_40%),url('/img/background-image-3-flipped.png')]"]},{variant:"colorful",direction:"up",className:["bg-[linear-gradient(to_top,rgba(11,20,20,0)_0%,rgba(12,20,20,0)_10%,rgba(14,20,20,100)_40%),url('/img/background-image-1-flipped.png')]"]}],defaultVariants:{variant:"colorful",direction:"down"}}),O=(0,a.createContext)("colorful");function j(){const e=(0,a.useContext)(O);if(void 0===e)throw new Error("useLayoutVariant must be used within a Layout");return e}const R=({children:e})=>{const n=function(e){const n=(0,k.Ay)("/genai"),t=(0,k.Ay)("/classical-ml");return e.startsWith(n)?e===n||e===`${n}/`?"genai":"genai-subpage":e.startsWith(t)?e===t||e===`${t}/`?"classical-ml":"classical-ml-subpage":e===(0,k.Ay)("/")?"home":"default"}((0,D.zy)().pathname),t=n.startsWith("genai")?"red":n.startsWith("classical-ml")?"blue":"home"===n?"colorful":null,a=n.endsWith("subpage")?"up":"down";return(0,i.jsx)(O.Provider,{value:t,children:(0,i.jsxs)("div",{className:"flex flex-col min-h-screen w-full bg-[#0E1416]",children:[(0,i.jsx)(P,{layoutType:n}),(0,i.jsx)("main",{className:"flex flex-col",children:(0,i.jsx)("div",{className:N({variant:t,direction:a}),children:(0,i.jsx)("div",{className:"flex flex-col gap-24 w-full px-6 md:px-20 max-w-container",children:e})})}),(0,i.jsx)(g,{variant:null===t?"colorful":t})]})})},F=({label:e})=>{const n="red"===("blue"===j()?"green":"red")?"bg-brand-red":"bg-brand-teal";return(0,i.jsxs)("div",{className:"flex flex-row gap-3 justify-center items-center",children:[(0,i.jsx)("div",{className:`w-2 h-2 rotate-45 ${n}`}),(0,i.jsx)("div",{className:"text-sm font-medium uppercase text-white",children:e})]})};var G=t(25246);function W({id:e,label:n,title:t,body:a,children:o}){return(0,G.A)().collectAnchor(e),(0,i.jsxs)("div",{id:e,className:"flex flex-col w-full items-center justify-center gap-16",children:[(0,i.jsxs)("div",{className:"flex flex-col w-full max-w-5xl items-center justify-center gap-6",children:[n&&(0,i.jsx)(F,{label:n}),(0,i.jsx)(se,{level:1,"aria-level":2,children:t}),a&&(0,i.jsx)(le,{size:"l",children:a})]}),o]})}var z=t(82016);a.forwardRef((({className:e,...n},t)=>(0,i.jsx)(z.bL,{ref:t,orientation:"vertical",className:S("flex w-full",e),...n}))).displayName=z.bL.displayName;a.forwardRef((({className:e,...n},t)=>(0,i.jsx)(z.B8,{ref:t,className:S("flex flex-col w-[440px]",e),...n}))).displayName=z.B8.displayName;a.forwardRef((({className:e,label:n,description:t,...a},o)=>(0,i.jsx)(z.l9,{ref:o,className:S("vertical-tabs-trigger w-full p-5 inline-flex whitespace-nowrap rounded-lg data-[state=active]:bg-[#fff]/4 data-[state=active]:text-foreground data-[state=active]:border data-[state=active]:border-[#fff]/8 cursor-pointer text-white",e),...a,children:(0,i.jsxs)("div",{className:"flex flex-col gap-3 w-max",children:[(0,i.jsx)("span",{className:"text-lg font-medium text-white text-left text-wrap",children:n}),(0,i.jsx)("span",{className:"vertical-tabs-trigger-description text-md text-gray-500 text-left text-wrap h-0 overflow-hidden",children:t})]})}))).displayName=z.l9.displayName;var U;function B(){return B=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},B.apply(null,arguments)}a.forwardRef((({className:e,...n},t)=>(0,i.jsx)(z.UC,{ref:t,className:S("ring-offset-background ml-10 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 w-full h-full",e),...n}))).displayName=z.UC.displayName;const Q=({title:e,titleId:n,...t})=>a.createElement("svg",B({xmlns:"http://www.w3.org/2000/svg",width:48,height:48,fill:"none",viewBox:"0 0 48 48","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,U||(U=a.createElement("path",{fill:"#fff",d:"M24 3.901c11.05 0 20 8.95 20 20a20.03 20.03 0 0 1-13.625 18.975c-1 .2-1.375-.425-1.375-.95 0-.675.025-2.825.025-5.5 0-1.875-.625-3.075-1.35-3.7 4.45-.5 9.125-2.2 9.125-9.875 0-2.2-.775-3.975-2.05-5.375.2-.5.9-2.55-.2-5.3 0 0-1.675-.55-5.5 2.05-1.6-.45-3.3-.675-5-.675s-3.4.225-5 .675c-3.825-2.575-5.5-2.05-5.5-2.05-1.1 2.75-.4 4.8-.2 5.3-1.275 1.4-2.05 3.2-2.05 5.375 0 7.65 4.65 9.375 9.1 9.875-.575.5-1.1 1.375-1.275 2.675-1.15.525-4.025 1.375-5.825-1.65-.375-.6-1.5-2.075-3.075-2.05-1.675.025-.675.95.025 1.325.85.475 1.825 2.25 2.05 2.825.4 1.125 1.7 3.275 6.725 2.35 0 1.675.025 3.25.025 3.725 0 .525-.375 1.125-1.375.95A19.99 19.99 0 0 1 4 23.901c0-11.05 8.95-20 20-20"})));var H;function Y(){return Y=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},Y.apply(null,arguments)}const V=({title:e,titleId:n,...t})=>a.createElement("svg",Y({xmlns:"http://www.w3.org/2000/svg",width:49,height:48,fill:"none",viewBox:"0 0 49 48","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,H||(H=a.createElement("path",{fill:"#fff",fillRule:"evenodd",d:"M41.075 8.016c1.808.56 3.23 2.203 3.713 4.294.875 3.786.879 11.69.879 11.69s0 7.904-.879 11.69c-.484 2.092-1.905 3.735-3.713 4.294C37.801 41 24.667 41 24.667 41s-13.135 0-16.409-1.016c-1.808-.56-3.23-2.202-3.713-4.293C3.667 31.904 3.667 24 3.667 24s0-7.904.878-11.69c.484-2.091 1.905-3.735 3.713-4.294C11.532 7 24.667 7 24.667 7S37.8 7 41.075 8.016M31.693 24 20.263 30.6V17.4z",clipRule:"evenodd"})));var K;function X(){return X=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},X.apply(null,arguments)}const J=({title:e,titleId:n,...t})=>a.createElement("svg",X({xmlns:"http://www.w3.org/2000/svg",width:49,height:48,fill:"none",viewBox:"0 0 49 48","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,K||(K=a.createElement("path",{stroke:"#fff",strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:3,d:"M38.833 24v9.5h-24.5a4.5 4.5 0 0 0 0 9h6m-2.5-28.5h13m-13 8h7m-11-16.5h25v37h-25a4 4 0 0 1-4-4v-29a4 4 0 0 1 4-4"})));var Z;function $(){return $=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},$.apply(null,arguments)}const ee=({title:e,titleId:n,...t})=>a.createElement("svg",$({xmlns:"http://www.w3.org/2000/svg",width:48,height:48,fill:"none",viewBox:"0 0 48 48","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,Z||(Z=a.createElement("path",{fill:"#fff",d:"M39.3 6H8.7A2.7 2.7 0 0 0 6 8.7v30.6A2.7 2.7 0 0 0 8.7 42h30.6a2.7 2.7 0 0 0 2.7-2.7V8.7A2.7 2.7 0 0 0 39.3 6M16.8 36.6h-5.4V20.4h5.4zm-2.7-19.35a3.15 3.15 0 1 1 3.24-3.15 3.204 3.204 0 0 1-3.24 3.15M36.6 36.6h-5.4v-8.532c0-2.556-1.08-3.474-2.484-3.474a3.13 3.13 0 0 0-2.916 3.348 1.2 1.2 0 0 0 0 .252V36.6h-5.4V20.4h5.22v2.34a5.6 5.6 0 0 1 4.86-2.52c2.79 0 6.048 1.548 6.048 6.588z"})));var ne;function te(){return te=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},te.apply(null,arguments)}const ae=({title:e,titleId:n,...t})=>a.createElement("svg",te({xmlns:"http://www.w3.org/2000/svg",width:25,height:24,fill:"none",viewBox:"0 0 25 24","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,ne||(ne=a.createElement("path",{stroke:"#fff",strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"m9.667 4 8 8-8 8"}))),oe=({href:e,icon:n,label:t,description:a})=>(0,i.jsxs)(u.A,{href:e,target:"_blank",className:"flex sm:flex-row md:flex-col xl:flex-row group w-full sm:items-center md:items-start xl:items-center p-8 h-full cursor-pointer gap-6 hover:bg-white/4 bg-[#0E1416]",children:[n,(0,i.jsxs)("div",{className:"flex flex-row justify-between items-center w-full",children:[(0,i.jsxs)("div",{className:"flex flex-col",children:[(0,i.jsx)("span",{className:"text-white text-lg font-medium",children:t}),(0,i.jsx)("span",{className:"text-gray-600 text-base",children:a})]}),(0,i.jsx)("div",{className:"invisible group-hover:visible",children:(0,i.jsx)(ae,{})})]})]}),ie=(0,o.F)("",{variants:{level:{1:"text-balance font-light text-[64px] leading-[100%] tracking-[-3%] text-center",2:"text-wrap font-light text-[52px] leading-[120%] tracking-[-1%] text-center text-white",3:"text-wrap font-regular text-[40px] leading-[120%] tracking-[-1%]",4:"text-wrap font-regular text-[32px] leading-[120%] tracking-[0%]",5:"text-wrap font-medium text-[24px] leading-[120%] tracking-[0%]"}}}),se=({level:e,children:n,"aria-level":t=e})=>(0,i.jsx)("div",{role:"heading","aria-level":t,className:ie({level:e}),children:n}),re=(0,o.F)(["font-regular leading-[140%] tracking-[0%]"],{variants:{size:{l:"text-[18px]",m:"text-[16px]",s:"text-[14px]"},align:{center:"text-center"},margin:{tight:"",regular:"mb-4"},color:{default:"text-gray-600",white:"text-white"}}}),le=({size:e,align:n,margin:t="regular",color:a="default",children:o})=>(0,i.jsx)("div",{role:"paragraph",className:re({size:e,align:n,margin:t,color:a}),children:o}),ce=[{key:"docs",icon:(0,i.jsx)("div",{children:(0,i.jsx)(J,{})}),label:"Documentation",description:"Read documentation",href:h.OU},{key:"github",icon:(0,i.jsx)(Q,{}),label:"GitHub",description:"20k stars",href:"https://github.com/mlflow/mlflow"},{key:"linkedin",icon:(0,i.jsx)(ee,{}),label:"LinkedIn",description:"69k followers",href:"https://www.linkedin.com/company/mlflow-org"},{key:"youtube",icon:(0,i.jsx)(V,{}),label:"YouTube",description:"View tutorials",href:"https://www.youtube.com/@mlflowoss"}],de=()=>(0,i.jsxs)("div",{className:"flex flex-col w-full gap-16",children:[(0,i.jsxs)("div",{className:"flex flex-col w-full gap-6 items-center justify-center text-center",children:[(0,i.jsx)(F,{label:"GET INVOLVED"}),(0,i.jsx)(se,{level:2,children:"Connect with the open source community"}),(0,i.jsx)(le,{size:"l",children:"Join millions of MLflow users"})]}),(0,i.jsx)(_,{className:"px-10",children:ce.map((e=>(0,i.jsx)(oe,{href:e.href,icon:e.icon,label:e.label,description:e.description},e.key)))})]}),ue=JSON.parse('[{"id":"mlflow-go","metadata":{"permalink":"/mlflow-website/blog/mlflow-go","source":"@site/blog/2025-04-28-mlflow-go.md","title":"MLflow Go","description":"The MLflow project is a cornerstone of the machine learning community. It is highly flexible and makes machine learning workflows more reproducible, manageable and collaborative.","date":"2025-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"}],"readingTime":2.985,"hasTruncateMarker":false,"authors":[{"name":"Florian Verdonck","title":"Open Source Developer at G-Research","url":"https://www.linkedin.com/in/florian-verdonck/","imageURL":"/mlflow-website/img/authors/florian_verdonck.jpeg","key":"florian-verdonck","page":null}],"frontMatter":{"title":"MLflow Go","tags":["mlflow"],"slug":"mlflow-go","authors":["florian-verdonck"],"thumbnail":"/img/blog/mlflow-go.jpeg"},"unlisted":false,"nextItem":{"title":"Automatically find the bad LLM responses in your LLM Evals with Cleanlab","permalink":"/mlflow-website/blog/tlm-tracing"}},"content":"The [MLflow project](https://mlflow.org) is a cornerstone of the machine learning community. It is highly flexible and makes machine learning workflows more reproducible, manageable and collaborative.\\n\\nA key part of MLflow is experiment tracking, the system for logging and querying experiments. Developers can log parameters, metrics, inputs, and artifacts to keep track of different experiment runs, store them into a database, and then compare results.\\n\\nThis experiment tracking in MLflow is written in Python and has performance limitations when dealing with huge volumes of data. At [G-Research](https://www.gresearch.com/), 10TB amount of data in 2 months amount of time is not uncommon. In these cases, our researchers can become frustrated with just how long it takes the system to execute operations.\\n\\nTo address this situation, we took apart the component parts of MLflow, and soon saw where the system could be improved \u2013 its backend. Writing to the Python-scripted database just wasn\u2019t going to cut it for the sheer number of operations we had to perform. We started to imagine swapping out the experiment tracking with another backend. This train of thought evolved into the project called [FastTrackML](https://fasttrackml.io/).\\n\\n## Fasttrack ML\\n\\nFasttrack ML was a [huge success for G-Research](https://www.gresearch.com/news/fasttrackml-the-fastest-ml-experiment-tracker-yet/), as its implementation in Go was significantly faster and able to deal with the high volumes of data. Go was chosen for this project because its concurrency model is based on lightweight goroutines and channels, which make it extremely efficient at handling multiple simultaneous tasks (e.g., logging experiment data, handling user requests). Go is a compiled language, and produces native machine code, resulting in faster execution compared to Python, which is interpreted. Go is statically typed which leads to a more memory efficient usage compared to Python\'s dynamic typing.\\n\\nThe success of FastTrackML, to a certain degree, unlocked the usage of MLflow within G-Research. It allowed quant researchers to process their datasets and experiment tracking in reasonable timeframes.\\n\\n## Back to MLflow\\n\\nThen the [Open Source team at G-Research](https://www.gresearch.com/teams/open-source-software/) looked at incorporating the lessons learned from FastTrackML into MLflow itself - the mlflow-go project is a first step in that direction. It is a Python package that is meant to be a drop-in replacement for MLflow\'s experiment tracking server. Once the mlflow-go package is proven technology, we hope it could be absorbed by MLflow.\\n\\nGetting started with `mlflow-go` is as simple as installing MLflow, `pip install mlflow-go-backend` and then swapping `mlflow` for `mlflow-go` when running it from the command line. This will spin up a Go webserver with the exact API as the [mlflow REST API](https://mlflow.org/docs/latest/api_reference/rest-api.html).\\n\\nAt the time of this writing, users are required to pass a database connection string as \'--backend-store-uri\' argument, because the Go implementation is currently database-only. As the intention is to use mlflow-go for its performance, it makes sense to use an actual production worthy database and not the filesystem.\\n\\n## Closing thoughts\\n\\nAlthough the backend we\u2019ve built in MLflow-go is more performant for certain big data applications, it doesn\u2019t necessarily mean it will replace the existing Python implementation. As the MLflow maintainers within Databricks are not specifically Golang developers, MLflow-go has to develop its own user base and established community of active contributors before it can become widely adopted.\\n\\nWe found that G-Research really liked MLflow but needed it to be more performant, notably for large datasets. We anticipate that other organizations, especially those with the massive data-processing needs, have run into similar issues. We are keen to learn if our efforts have benefited others, so please let us know how you\u2019re using MLflow and if FastTrackML is faster for you on [GitHub](https://github.com/mlflow/mlflow-go-backend) or [Slack](https://mlflow.org/community/#slack)."},{"id":"tlm-tracing","metadata":{"permalink":"/mlflow-website/blog/tlm-tracing","source":"@site/blog/2025-04-01-tlm-tracing/index.mdx","title":"Automatically find the bad LLM responses in your LLM Evals with Cleanlab","description":"A practical guide to using Cleanlab\'s Trustworthy Language Models (TLM) to evaluate LLM responses captured in MLflow","date":"2025-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"observability","permalink":"/mlflow-website/blog/tags/observability"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":10.6,"hasTruncateMarker":false,"authors":[{"name":"Chris Mauck","title":"Data Scientist at Cleanlab","url":"https://www.linkedin.com/in/chris-mauck/","imageURL":"/mlflow-website/img/authors/chris_mauck.jpg","key":"chris-mauck","page":null}],"frontMatter":{"title":"Automatically find the bad LLM responses in your LLM Evals with Cleanlab","description":"A practical guide to using Cleanlab\'s Trustworthy Language Models (TLM) to evaluate LLM responses captured in MLflow","slug":"tlm-tracing","authors":["chris-mauck"],"tags":["genai","observability","tracing"],"thumbnail":"/img/blog/tlm-tracing-thumbnail.png"},"unlisted":false,"prevItem":{"title":"MLflow Go","permalink":"/mlflow-website/blog/mlflow-go"},"nextItem":{"title":"Practical AI Observability: Getting Started with MLflow Tracing","permalink":"/mlflow-website/blog/ai-observability-mlflow-tracing"}},"content":"This guide will walk you through the process of evaluating LLM responses captured in MLflow with Cleanlab\'s Trustworthy Language Models (TLM).\\n\\nTLM boosts the reliability of any LLM application by indicating when the model\u2019s response is untrustworthy. It works by analyzing the prompt and the generated response to calculate a `trustworthiness_score`, helping to automatically identify potentially incorrect or hallucinated outputs without needing ground truth labels. TLM can also provide explanations for its assessment.\\n\\nMLflow provides tracing and evaluation capabilities that can be used to monitor, review, and debug the performance of AI applications. This post will show how to apply Cleanlab\'s TLM to LLM responses recorded with MLflow tracing. Using Cleanlab\'s TLM with MLflow enables you to systematically log, track, and analyze the trustworthiness evaluations provided by TLM for your LLM interactions.\\n\\nYou can find a notebook version of this guide [here](https://github.com/cleanlab/cleanlab-tools/blob/main/TLM-MLflow-Integration/evaluating_traces_TLM_mlflow_dl.ipynb).\\n\\n:::info\\n\\nThis guide requires a Cleanlab TLM API key. If you don\'t have one, you can sign up for a free trial [here](https://tlm.cleanlab.ai/).\\n:::\\n\\n## Install dependencies & Set environment variables\\n\\nTo work through this guide, you\'ll need to install the MLflow, OpenAI, and Cleanlab TLM Python packages:\\n\\n```bash\\npip install -q mlflow openai cleanlab-tlm --upgrade\\n```\\n\\nNext, import the dependencies:\\n\\n```python\\nimport mlflow\\nimport os\\nimport json\\nimport pandas as pd\\n\\nfrom rich import print\\nfrom openai import OpenAI\\nfrom getpass import getpass\\n```\\n\\n### API Keys\\n\\nThis guide requires two API keys:\\n\\n- [OpenAI API Key](https://platform.openai.com/api-keys)\\n- [Cleanlab TLM API Key](https://tlm.cleanlab.ai/)\\n\\nIf they are not already set as environment variables, you can set them manually as follows:\\n\\n```python\\nif not (openai_api_key := os.getenv(\\"OPENAI_API_KEY\\")):\\n    openai_api_key = getpass(\\"\ud83d\udd11 Enter your OpenAI API key: \\")\\nif not (cleanlab_tlm_api_key := os.getenv(\\"CLEANLAB_TLM_API_KEY\\")):\\n    cleanlab_tlm_api_key = getpass(\\"\ud83d\udd11 Enter your Cleanlab TLM API key: \\")\\n\\nos.environ[\\"OPENAI_API_KEY\\"] = openai_api_key\\nos.environ[\\"CLEANLAB_TLM_API_KEY\\"] = cleanlab_tlm_api_key\\n```\\n\\n### Set Up MLflow Tracking Server and Logging\\n\\nTo manage our experiments, parameters, and results effectively, we\'ll [start a local MLflow Tracking Server](https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server). This provides a dedicated UI for [monitoring and managing our experiments](https://mlflow.org/docs/latest/tracking/) and allows us to [configure MLflow to connect to this server](https://mlflow.org/docs/latest/getting-started/). We\'ll then enable autologging for OpenAI to automatically capture relevant information from our API calls.\\n\\n```python\\n# This will start a server on port 8080, in the background\\n# Navigate to http://localhost:8080 to see the MLflow UI\\n%%bash --bg\\nmlflow server --host 127.0.0.1 --port 8080\\n```\\n\\n```python\\n# Set up MLflow tracking server\\nmlflow.set_tracking_uri(\\"http://localhost:8080\\")\\n\\n# Enable logging for OpenAI SDK\\nmlflow.openai.autolog()\\n\\n# Set experiment name\\nmlflow.set_experiment(\\"Eval OpenAI Traces with TLM\\")\\n\\n# Get experiment ID\\nexperiment_id = mlflow.get_experiment_by_name(\\"Eval OpenAI Traces with TLM\\").experiment_id\\n```\\n\\n## Trace Some LLM Interactions with MLflow\\n\\nFor the sake of demonstration purposes, we\'ll briefly generate some traces and track them in MLflow. Typically, you would have already captured traces in MLflow and would skip to \\"Download Traces from the MLflow Tracking Server.\\"\\n\\nIn this example, we\'ll use some tricky trivia questions to generate some traces.\\n\\n:::info\\n\\nTLM requires the entire input to the LLM to be provided. This includes any system prompts, context, or other information that was originally provided to the LLM to generate the response. Notice below that we include the system prompt in the trace metadata since by default the trace does not include the system prompt within the input.\\n\\n:::\\n\\n```python\\n# Let\'s use some tricky trivia questions to generate some traces\\ntrivia_questions = [\\n    \\"What is the 3rd month of the year in alphabetical order?\\",\\n    \\"What is the capital of France?\\",\\n    \\"How many seconds are in 100 years?\\",\\n    \\"Alice, Bob, and Charlie went to a caf\xe9. Alice paid twice as much as Bob, and Bob paid three times as much as Charlie. If the total bill was $72, how much did each person pay?\\",\\n    \\"When was the Declaration of Independence signed?\\"\\n]\\n\\nclient = OpenAI()\\n\\ndef generate_answers(trivia_question, client=client):\\n    system_prompt = \\"You are a trivia master.\\"\\n\\n    response = client.chat.completions.create(\\n        model=\\"gpt-4o-mini\\",\\n        messages=[\\n            {\\"role\\": \\"system\\", \\"content\\": system_prompt},\\n            {\\"role\\": \\"user\\", \\"content\\": trivia_question},\\n        ],\\n    )\\n\\n    answer = response.choices[0].message.content\\n    return answer\\n\\n\\n# Generate answers\\nanswers = []\\nfor i in range(len(trivia_questions)):\\n    answer = generate_answers(trivia_questions[i])\\n    answers.append(answer)\\n    print(f\\"Question {i+1}: {trivia_questions[i]}\\")\\n    print(f\\"Answer {i+1}:\\\\n{answer}\\\\n\\")\\n\\nprint(f\\"Generated {len(answers)} answers and tracked them in MLflow.\\")\\n```\\n\\nWe can see the resulting traces in the MLflow UI and, if you are running this code in a Jupyter notebook, you can see the traces [in the notebook cell output](https://mlflow.org/blog/mlflow-tracing-in-jupyter).\\n\\n![MLflow Traces](/img/blog/tlm-tracing/1_trace.png)\\n\\nNext, we\'ll download the generated traces from the MLflow tracking server so we can evaluate them with TLM. This illustrates how MLflow tracing can be used to generate datasets for downstream tasks like evaluation.\\n\\n## Download Traces from the MLflow Tracking Server\\n\\nFetching traces from MLflow is straightforward. Just set up the MLflow client and use the `search_traces()` function to fetch the data. We\'ll fetch the traces and evaluate them. After that, we\'ll add our scores back into MLflow.\\n\\nThe `search_traces()` function has arguments to filter the traces by tags, timestamps, and beyond. You can find more about other methods to [query traces](https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.search_traces) in the docs.\\n\\nIn this example, we\'ll fetch all traces from the experiment.\\n\\n```python\\nclient = mlflow.client.MlflowClient()\\ntraces = client.search_traces(experiment_ids=[experiment_id])\\n\\n# Print the first trace\\nprint(traces[0].data)\\n```\\n\\n## Evaluate Trustworthiness with TLM\\n\\nNow that we have our traces, we can use TLM to generate trustworthiness scores and explanations for each trace.\\n\\nInstead of running TLM individually on each trace, we\'ll provide all of the `(prompt, response)` pairs in a list to TLM in a single call. This is more efficient and allows us to get scores and explanations for all of the traces at once. Then, using the trace request IDs, we can attach the scores and explanations back to the correct trace in MLflow.\\n\\n```python\\nfrom cleanlab_tlm import TLM\\n\\ntlm = TLM(options={\\"log\\": [\\"explanation\\"]})\\n```\\n\\nWe\'ll use the following helper function to extract the prompt and response from each trace and return three lists: request IDs, prompts, and responses. We can then construct a DataFrame with the evaluation results, sort and filter the results by trustworthiness score, and tag the traces in MLflow with the scores and explanations.\\n\\n```python\\ndef get_prompt_response_pairs(traces):\\n    prompts = []\\n    responses = []\\n    for trace in traces:\\n        # Parse request and response JSON\\n        request_data = json.loads(trace.data.request)\\n        response_data = json.loads(trace.data.response)\\n\\n        # Extract system prompt and user message from request\\n        system_prompt = request_data[\\"messages\\"][0][\\"content\\"]\\n        user_message = request_data[\\"messages\\"][1][\\"content\\"]\\n\\n        # Extract assistant\'s response from response\\n        assistant_response = response_data[\\"choices\\"][0][\\"message\\"][\\"content\\"]\\n\\n        prompts.append(system_prompt + \\"\\\\n\\" + user_message)\\n        responses.append(assistant_response)\\n    return prompts, responses\\n\\nrequest_ids = [trace.info.request_id for trace in traces]\\nprompts, responses = get_prompt_response_pairs(traces)\\n```\\n\\nNow, let\'s use TLM to generate a `trustworthiness score` and `explanation` for each trace.\\n\\n:::info\\n\\nIt is essential to always include any system prompts, context, or other information that was originally provided to the LLM to generate the response. You should construct the prompt input to `get_trustworthiness_score()` in a way that is as similar as possible to the original prompt. This is why we included the system prompt in the trace metadata.\\n\\n:::\\n\\n```python\\n# Evaluate each of the prompt, response pairs using TLM\\nevaluations = tlm.get_trustworthiness_score(prompts, responses)\\n\\n# Extract the trustworthiness scores and explanations from the evaluations\\ntrust_scores = [entry[\\"trustworthiness_score\\"] for entry in evaluations]\\nexplanations = [entry[\\"log\\"][\\"explanation\\"] for entry in evaluations]\\n\\n# Create a DataFrame with the evaluation results\\ntrace_evaluations = pd.DataFrame({\\n    \'request_id\': request_ids,\\n    \'prompt\': prompts,\\n    \'response\': responses,\\n    \'trust_score\': trust_scores,\\n    \'explanation\': explanations\\n})\\n```\\n\\nNow we have a DataFrame mapping trace IDs to their scores and explanations. We\'ve also included the prompt and response for each trace for demonstration purposes to find the **least trustworthy trace!**\\n\\n```python\\nsorted_df = trace_evaluations.sort_values(by=\\"trust_score\\", ascending=True)\\nsorted_df.head(3)\\n```\\n\\n```python\\n# Let\'s look at the least trustworthy trace.\\nprint(\\"Prompt: \\", sorted_df.iloc[0][\\"prompt\\"], \\"\\\\n\\")\\nprint(\\"OpenAI Response: \\", sorted_df.iloc[0][\\"response\\"], \\"\\\\n\\")\\nprint(\\"TLM Trust Score: \\", sorted_df.iloc[0][\\"trust_score\\"], \\"\\\\n\\")\\nprint(\\"TLM Explanation: \\", sorted_df.iloc[0][\\"explanation\\"])\\n```\\n\\nWhich returns the following:\\n\\n```text\\nPrompt:  You are a trivia master.\\nWhat is the 3rd month of the year in alphabetical order?\\n\\nOpenAI Response:  The 3rd month of the year in alphabetical order is March. The months in alphabetical order are:\\n\\n1. April\\n2. August\\n3. December\\n4. February\\n5. January\\n6. July\\n7. June\\n8. March\\n9. May\\n10. November\\n11. October\\n12. September\\n\\nSo, March is the 8th month, not the 3rd. The 3rd month in alphabetical order is February.\\n\\nTLM Trust Score:  0.04388514403195165\\n\\nTLM Explanation:  The proposed response incorrectly identifies the 3rd month of the year in alphabetical order. The\\nmonths of the year, when arranged alphabetically, are as follows:\\n\\n1. April\\n2. August\\n3. December\\n4. February\\n5. January\\n6. July\\n7. June\\n8. March\\n9. May\\n10. November\\n11. October\\n12. September\\n\\nIn this list, February is indeed the 4th month, not the 3rd. The 3rd month in alphabetical order is actually\\nDecember. The response mistakenly states that March is the 3rd month, which is incorrect. Therefore, the answer to\\nthe user\'s request is that the 3rd month in alphabetical order is December, not February or March.\\nThis response is untrustworthy due to lack of consistency in possible responses from the model. Here\'s one\\ninconsistent alternate response that the model considered (which may not be accurate either):\\nDecember.\\n```\\n\\nAwesome! TLM was able to identify multiple traces that contained incorrect answers from OpenAI. In the example above, it correctly noted that the original response actually included _two_ incorrect answers, making it both wrong and inconsistent.\\n\\n:::info Tracing TLM\\n\\nYou could also trace the TLM API call itself. This will log the trustworthiness scores and explanations for each trace. Here\'s an example of how to do this by wrapping the TLM API call in a custom function and tracing it with the `@mlflow.trace` decorator.\\n\\n```python\\n# Tracing TLM\\n\\n@mlflow.trace\\ndef tlm_trustworthiness_wrapper(inputs, predictions):\\n    tlm = TLM(options={\\"log\\": [\\"explanation\\"]})\\n    evaluations = tlm.get_trustworthiness_score(inputs, predictions)\\n    return evaluations\\n\\ntlm_trustworthiness_wrapper(prompts[0], answers[0])\\n```\\n\\n:::\\n\\nNow, let\'s upload the `trust_score` and `explanation` columns to MLflow.\\n\\n### Tag Traces with TLM Evaluations\\n\\nWe\'ll use the `set_trace_tag()` function to save the TLM scores and explanations as tags on the traces.\\n\\n```python\\nfor idx, row in trace_evaluations.iterrows():\\n    request_id = row[\\"request_id\\"]\\n    trust_score = row[\\"trust_score\\"]\\n    explanation = row[\\"explanation\\"]\\n\\n    # Add the trustworthiness score and explanation to the trace as a tag\\n    client.set_trace_tag(request_id=request_id, key=\\"trust_score\\", value=trust_score)\\n    client.set_trace_tag(request_id=request_id, key=\\"explanation\\", value=explanation)\\n```\\n\\nYou should now see the TLM trustworthiness score and explanation in the MLflow UI! From here you can continue collecting and evaluating traces.\\n\\n## Using TLM with MLflow Evaluation\\n\\nMLflow Evaluation helps assess AI applications using built-in and custom metrics to score model outputs. Results, including scores and justifications, are logged and can be compared in the MLflow UI for systematic performance tracking. In this section, we will create a custom metric that uses TLM to evaluate the trustworthiness of LLM responses, providing a straightforward way to integrate TLM into your MLflow workflows.\\n\\nUsing MLflow Evaluation with our custom TLM metric will log a table of trustworthiness scores and explanations and also provide an interface in the UI for comparing scores across runs. For example, you could use this to compare the trustworthiness scores of different models across the same set of prompts.\\n\\n```python\\nimport mlflow\\nfrom mlflow.metrics import MetricValue, make_metric\\nfrom cleanlab_tlm import TLM\\n\\ndef _tlm_eval_fn(predictions, inputs, targets=None):\\n    \\"\\"\\"\\n    Evaluate trustworthiness using Cleanlab TLM.\\n\\n    Args:\\n        predictions: The model outputs/answers\\n        targets: Not used for this metric\\n        **kwargs: Should contain \'inputs\' with the prompts\\n    \\"\\"\\"\\n    # Initialize TLM\\n    tlm = TLM(options={\\"log\\": [\\"explanation\\"]})\\n    inputs = inputs.to_list()\\n    predictions = predictions.to_list()\\n\\n    # Get trustworthiness scores\\n    evaluations = tlm.get_trustworthiness_score(inputs, predictions)\\n\\n    # Extract scores and explanations\\n    scores = [float(eval_result[\\"trustworthiness_score\\"]) for eval_result in evaluations]\\n    justifications = [eval_result[\\"log\\"][\\"explanation\\"] for eval_result in evaluations]\\n\\n    # Return metric value\\n    return MetricValue(\\n        scores=scores,\\n        justifications=justifications,\\n        aggregate_results={\\n            \\"mean\\": sum(scores) / len(scores),\\n            \\"min\\": min(scores),\\n            \\"max\\": max(scores)\\n        }\\n    )\\n\\ndef tlm_trustworthiness():\\n    \\"\\"\\"Creates a metric for evaluating trustworthiness using Cleanlab TLM\\"\\"\\"\\n    return make_metric(\\n        eval_fn=_tlm_eval_fn,\\n        greater_is_better=True,\\n        name=\\"tlm_trustworthiness\\"\\n    )\\n```\\n\\nNow that we have defined the custom metric, let\'s use it to evaluate the trustworthiness of our LLM responses. We will use the same responses and prompts that we collected from our traces before.\\n\\n```python\\ntlm_metric = tlm_trustworthiness()\\n\\neval_df = pd.DataFrame({\\n    \'inputs\': prompts,\\n    \'outputs\': responses\\n})\\n\\n\\nresults = mlflow.evaluate(\\n    data=eval_df,\\n    predictions=\\"outputs\\",\\n    model=None,\\n    extra_metrics=[tlm_metric],\\n    evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"inputs\\",\\n            \\"predictions\\": \\"outputs\\"\\n        }\\n    }\\n)\\n```\\n\\nNow we can see the results in the MLflow Evaluation UI.\\n\\n![MLflow Evaluation](/img/blog/tlm-tracing/2_eval_ui.png)\\n\\nThis approach is especially useful once you start comparing scores across different models, prompts, or other criteria.\\n\\n## Conclusion\\n\\nIn this post, we showed how to use Cleanlab\'s TLM to evaluate the trustworthiness of LLM responses captured in MLflow. We demonstrated how to use TLM to generate trustworthiness scores and explanations for each trace, tag the traces with the scores and explanations, and use MLflow Evaluation to log and compare the scores across runs.\\n\\nThis approach provides a straightforward way to integrate TLM into your MLflow workflows for systematic performance tracking and debugging of AI applications. It also highlights a key use case for MLflow tracing: generating datasets for downstream tasks like evaluation."},{"id":"ai-observability-mlflow-tracing","metadata":{"permalink":"/mlflow-website/blog/ai-observability-mlflow-tracing","source":"@site/blog/2025-03-06-tracing-introduction/index.mdx","title":"Practical AI Observability: Getting Started with MLflow Tracing","description":"A practical guide to implementing AI tracing in your GenAI applications","date":"2025-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"observability","permalink":"/mlflow-website/blog/tags/observability"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":6.95,"hasTruncateMarker":false,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/mlflow-website/img/authors/daniel_liden.png","key":"daniel-liden","page":null}],"frontMatter":{"title":"Practical AI Observability: Getting Started with MLflow Tracing","description":"A practical guide to implementing AI tracing in your GenAI applications","slug":"ai-observability-mlflow-tracing","authors":["daniel-liden"],"tags":["genai","observability","tracing"],"thumbnail":"/img/blog/tracing-intro-thumbnail.png"},"unlisted":false,"prevItem":{"title":"Automatically find the bad LLM responses in your LLM Evals with Cleanlab","permalink":"/mlflow-website/blog/tlm-tracing"},"nextItem":{"title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","permalink":"/mlflow-website/blog/custom-tracing"}},"content":"import Tabs from \\"@theme/Tabs\\";\\nimport TabItem from \\"@theme/TabItem\\";\\n\\n## MLflow Tracing: Observability for GenAI\\n\\nGenAI providers and frameworks often respond with complicated and hard-to-read data structures or with simple responses that hide intermediate steps. Furthermore, it can be hard to keep track of and compare GenAI model/framework calls over time, especially if you are moving between frameworks and scripts.\\n\\n[MLflow\'s LLM tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) solves these issues by recording all of your GenAI calls, including both individual LLM calls and multi-step agentic workflows, and providing an easy-to-read interface for browsing and comparing them. You can enable this functionality for most GenAI providers with a single line of code: `mlflow.<provider>.autolog()`.\\n\\nThis blog will show how to get started with MLflow tracing\u2014in about five minutes. It assumes some familiarity with GenAI APIs (e.g. the OpenAI API), but does not assume any prior familiarity with MLflow.\\n\\n<figure>\\n  <img src=\\"/img/blog/tracing-intro/05_langchain.png\\" alt=\\"Alt text\\" />\\n  <figcaption>\\n    <i>Tracing a LangChain Application with MLflow Tracing</i>\\n  </figcaption>\\n</figure>\\n\\n## Quickstart\\n\\nWe\'ll start by showing how to use MLflow autologging to automatically trace calls to OpenAI models, though MLflow supports automatic tracing for an [ever-growing number of providers and frameworks](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) including Anthropic, Ollama, Langchain, LlamaIndex, and may others. To get started, install the MLflow and OpenAI Python packages with:\\n\\n```shell\\npip install mlflow openai\\n```\\n\\n### Collecting Traces with Autologging\\n\\nIn a Python script or notebook, import MLflow and the GenAI provider you\'re working with, and enable tracing with `mlflow.<provider>.autolog`. Here\'s how to set up automatic tracing for OpenAI:\\n\\n```python\\nimport mlflow\\nfrom openai import OpenAI\\n\\nmlflow.openai.autolog()\\n```\\n\\nMake sure to [create and set your OpenAI API key](https://platform.openai.com/docs/quickstart?language=python#create-and-export-an-api-key)! You can set it in your environment with\\n\\n```shell\\nexport OPENAI_API_KEY=\\"your_api_key_here\\"\\n```\\n\\nNow, when you use the OpenAI library, MLflow will capture _traces_ of your model calls. For example, MLflow will log a trace of the following OpenAI invocation because we have enabled autologging.\\n\\n```python\\nclient = OpenAI()\\n\\ncompletion = client.chat.completions.create(\\n    model=\\"gpt-4o-mini\\",\\n    messages=[\\n        {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n        {\\n            \\"role\\": \\"user\\",\\n            \\"content\\": \\"What is an MLflow tracking server?\\"\\n        }\\n    ]\\n)\\n```\\n\\n### Viewing your LLM Traces\\n\\nThe MLflow UI provides an AI observability dashboard for viewing your traces. Start the MLflow UI from your terminal with:\\n\\n```shell\\nmlflow ui\\n```\\n\\nNavigate to the UI. the output of the `mlflow ui` command will tell you where to go (`http://localhost:5000` by default). In the UI, navigate to the \\"Traces\\" tab. This will list all of the collected traces. Click on a trace\'s Trace ID to open up a new pane with more details.\\n\\n![Traces in the MLflow UI](/img/blog/tracing-intro/01_tracing_ui.gif)\\n\\n:::tip\\n\\nBy default, the MLflow server will listen on `http://localhost:5000`. You can choose a different port with `mlflow ui -p <port>`. For example, to listen on port 5001, use `mlflow ui -p 5001`.\\n\\n:::\\n\\nStarting the MLflow tracking server with `mlflow ui` also enables you to [view traces right in a Jupyter notebook](https://mlflow.org/blog/mlflow-tracing-in-jupyter)! You just have to set the tracking uri to the location specified above:\\n\\n```python\\nmlflow.set_tracking_uri(\\"http://localhost:5000\\")\\n```\\n\\nThen, when you invoke an AI model/framework with tracing enabled, the generated trace(s) will appear right in the notebook outputs.\\n\\n![Tracing in Jupyter Notebooks](/img/blog/tracing-intro/02_jupyter.gif)\\n\\nYou can disable this functionality with `mlflow.tracing.disable_notebook_display()`.\\n\\n## Organizing your Traces\\n\\nIf you use tracing across multiple different projects and tasks, you might want to organize the traces into separate groups.\\n\\nThe easiest way to organize your traces is to separate them into [_experiments_](https://mlflow.org/docs/latest/tracking.html#experiments). Each experiment has its own traces tab, which displays the traces for that experiment.\\n\\nYou can create an experiment in the UI (With the \\"+\\" button next to \\"Experiments\\"), with the MLflow CLI, or with Python. Let\'s create a new experiment called \\"Quickstart\\" and log a trace.\\n\\n```python\\nmlflow.set_experiment(\\"quickstart\\")\\n\\ncompletion = client.chat.completions.create(\\n    model=\\"gpt-4o-mini\\",\\n    messages=[\\n        {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n        {\\n            \\"role\\": \\"user\\",\\n            \\"content\\": \\"What is an MLflow tracking server?\\"\\n        }\\n    ]\\n)\\n```\\n\\nWe can now find this trace in the \\"Traces\\" tab in the \\"quickstart\\" experiment.\\n\\n![Trace in Experiment](/img/blog/tracing-intro/03_experiment.png)\\n\\nThe `set_experiment` function specifies which experiment traces should be logged to, creating it if it does not exist, so the code snippet above created a new \\"quickstart\\" experiment.\\n\\nYou can also organize your traces with [tags](https://mlflow.org/docs/latest/tracing#trace-tags) and [runs](https://mlflow.org/docs/latest/tracing#q-how-can-i-associate-a-trace-with-an-mlflow-run).\\n\\n## Tracing Other Providers\\n\\nOur quickstart example focused on OpenAI, but MLflow supports automatic tracing of [many different AI providers and frameworks](https://mlflow.org/docs/latest/tracing#automatic-tracing). The approach is the same: just add the line `mlflow.<provider>.autolog` to your notebook or script.\\n\\nHere are a few examples. See [here](https://mlflow.org/docs/latest/tracing#automatic-tracing) for the full list of supported providers.\\n\\n<Tabs>\\n  <TabItem value=\\"Anthropic\\" label=\\"Anthropic\\" default>\\n  \\n  Enable automatic tracing for Anthropic model calls with `mlflow.anthropic.autolog()`.\\n\\n{/* prettier-ignore-start */}\\n```python\\nimport anthropic\\nimport mlflow\\n\\nmlflow.anthropic.autolog()\\n\\nclient = anthropic.Anthropic()\\n\\nmessage = client.messages.create(\\n    model=\\"claude-3-7-sonnet-20250219\\",\\n    max_tokens=1000,\\n    temperature=1,\\n    messages=[\\n        {\\n            \\"role\\": \\"user\\",\\n            \\"content\\": \\"What is an MLflow tracking server?\\"\\n        }\\n    ]\\n)\\n\\n````\\n{/* prettier-ignore-end */}\\n\\nThis returns the following in the MLflow UI:\\n\\n![Anthropic tracing](/img/blog/tracing-intro/04_anthropic.png)\\n\\n  </TabItem>\\n  <TabItem value=\\"langchain\\" label=\\"LangChain\\">\\n\\nEnable automatic tracing for LangChain and LangGraph with `mlflow.langchain.autolog()`. MLflow automatic tracing captures all LangChain component executions, including chains, LLMs, agents, tools, prompts, and retrievers.\\n\\n```python\\nimport mlflow\\n\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_openai import ChatOpenAI\\n\\nmlflow.set_experiment(\\"quickstart\\")\\n\\nmlflow.langchain.autolog()\\n\\nllm = ChatOpenAI(model=\\"gpt-4o-mini\\", temperature=1, max_tokens=500)\\n\\nprompt_template = PromptTemplate.from_template(\\n    \\"Explain the following MLflow concept at the specified technical level. \\"\\n    \\"For \'beginner\', use simple analogies and avoid complex terms. \\"\\n    \\"For \'intermediate\', include more technical details and some code examples. \\"\\n    \\"For \'advanced\', go deep into implementation details and provide comprehensive explanations. \\"\\n    \\"Technical level: {level}. Question: {question}\\"\\n)\\n\\nchain = prompt_template | llm | StrOutputParser()\\n\\nchain.invoke(\\n    {\\n        \\"level\\": \\"beginner\\",\\n        \\"question\\": \\"How do MLflow tracking servers help with experiment management?\\",\\n    }\\n)\\n```\\n\\nThis example LangChain chain includes multiple components:\\n\\n- The `PromptTemplate`, which assembles the prompt based on the user input\\n- The `ChatOpenAI` model, which is used to call the OpenAI `gpt-4o-mini` model\\n- The `StrOutputParser`, which returns the final answer to the user\'s query as a string\\n\\nWe can see each of these components in the MLflow UI, nested under the parent `RunnableSequence` chain.\\n\\n![LangChain Tracing](/img/blog/tracing-intro/05_langchain.png)\\n\\n  </TabItem>\\n  <TabItem value=\\"ollama\\" label=\\"Ollama\\">\\n\\n[Ollama](https://ollama.com/) is a tool for running open source AI models locally. You can enable automatic tracing of Ollama models via Ollama\'s [OpenAI-compatible API](https://github.com/ollama/ollama/blob/main/docs/openai.md) and MLflow\'s OpenAI autologging. You just need to set the base URL to your Ollama REST endpoint.\\n\\nThis pattern should work with any provider that offers an OpenAI-compatible endpoint, even those that are not explicitly referenced in the docs.\\n\\nHere\'s how it works for Ollama:\\n\\n1. First, run the Ollama server with your desired model.\\n\\n```bash\\nollama run phi3:latest\\n```\\n\\n2. Configure the OpenAI client, setting the `base_url` to the Ollama OpenAI-compatible endpoint.\\n\\n```python\\nfrom openai import OpenAI\\n\\nclient = OpenAI(\\n    base_url=\\"http://localhost:11434/v1\\",  # The local Ollama REST endpoint\\n    api_key=\\"dummy\\",  # Required to instantiate OpenAI client, it can be a random string\\n)\\n```\\n\\n3. Enable MLflow OpenAI autologging and query the model\\n\\n```python\\nmlflow.openai.autolog()\\n\\ncompletion = client.chat.completions.create(\\n    model=\\"phi3:latest\\",\\n    messages=[\\n        {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"What is an MLflow tracking server?\\"}\\n    ]\\n)\\n```\\n\\nHere is the trace of the Ollama model call in the MLflow UI.\\n\\n![Ollama Tracing](/img/blog/tracing-intro/06_ollama.png)\\n\\n  </TabItem>\\n</Tabs>\\n\\n## Conclusion: Effective LLM tracing with one line of code\\n\\nIn this guide, you have learned how to use MLflow\'s autologging capabilities to get a complete AI observability solution with a single line of code. If you are using one of the [many GenAI frameworks/providers](https://mlflow.org/docs/latest/tracing#automatic-tracing) for which MLflow offers automatic tracing\u2014including any providers with OpenAI-compatible endpoints\u2014automatic logging is the easiest way to visualize and debug your AI application behavior. All you need is `mlflow.<provider>.autolog()`.\\n\\n### Next Steps\\n\\nAutologging is a great place to start with MLflow tracing, but you may need more flexibility in how you collect and use traces as you develop more complex GenAI applications. Furthermore, MLflow includes many tools for working with GenAI applications beyond tracing.\\n\\n- For a longer conceptual introduction to tracing, read [this guide](https://mlflow.org/docs/latest/tracing/) on tracing concepts.\\n- MLflow traces can provide an excellent source of data for evaluation, SME review, fine-tuning, and more. Learn about searching and retrieving trace data [here](https://mlflow.org/docs/latest/tracing/api/search).\\n- MLflow provides [LLM evaluation functionality](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) for running structured experiments with your AI models and applications.\\n- You can add tracing to your own AI applications with the tracing [fluent APIs](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation) and [client APIs](https://mlflow.org/docs/latest/tracing/api/client). You can also [add tracing to libraries and frameworks](https://mlflow.org/blog/custom-tracing) that do not (yet) have autologging support."},{"id":"custom-tracing","metadata":{"permalink":"/mlflow-website/blog/custom-tracing","source":"@site/blog/2025-01-30-custom-tracing-provider/index.md","title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","description":"In this post, we will show how to add MLflow Tracing to a new LLM provider by adding tracing support to the chat method of the Ollama Python SDK.","date":"2025-01-30T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"},{"inline":true,"label":"ollama","permalink":"/mlflow-website/blog/tags/ollama"}],"readingTime":16.82,"hasTruncateMarker":false,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/mlflow-website/img/authors/daniel_liden.png","key":"daniel-liden","page":null}],"frontMatter":{"title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","tags":["genai","tracing","ollama"],"slug":"custom-tracing","authors":["daniel-liden"],"thumbnail":"/img/blog/tracing-new-provider.png"},"unlisted":false,"prevItem":{"title":"Practical AI Observability: Getting Started with MLflow Tracing","permalink":"/mlflow-website/blog/ai-observability-mlflow-tracing"},"nextItem":{"title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","permalink":"/mlflow-website/blog/from-natural-language-to-sql"}},"content":"In this post, we will show how to add MLflow Tracing to a new LLM provider by adding tracing support to the `chat` method of the Ollama Python SDK.\\n\\n[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) is an observability tool in MLflow that captures detailed execution traces for GenAI applications and workflows. In addition to inputs, outputs, and metadata for individual calls, MLflow tracing can also capture intermediate steps such as tool calls, reasoning steps, retrieval steps, or other custom steps.\\n\\nMLflow provides [built-in Tracing support](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) for many popular LLM providers and orchestration frameworks. If you are using one of these providers, you can enable tracing with a single line of code: `mlflow.<provider>.autolog()`. While MLflow\'s autologging capabilities cover many of the most widely-used LLM providers and orchestration frameworks, there may be times when you need to add tracing to an unsupported provider or customize tracing beyond what autologging provides. This post demonstrates how flexible and extensible MLflow Tracing can be by:\\n\\n- Adding basic tracing support to an unsupported provider (the Ollama Python SDK)\\n- Showing how to capture both simple completions and more complex tool-calling workflows\\n- Illustrating how tracing can be added with minimal changes to existing code\\n\\nWe\'ll use [the Ollama Python SDK](https://github.com/ollama/ollama-python), an open-source Python SDK for the [Ollama](https://ollama.ai/) LLM platform, as our example. We\'ll work through the process step-by-step, showing how to capture the key information with MLflow tracing while maintaining a clean integration with the provider\'s SDK. Note that MLflow _does_ have autologging support for Ollama, but currently only for use via the OpenAI client, not directly with the Ollama Python SDK.\\n\\n## Adding MLflow Tracing to a New Provider: General Principles\\n\\nThe MLflow docs have an [excellent guide](https://mlflow.org/docs/latest/llms/tracing/contribute.html) to contributing to MLflow tracing. Though we will not be contributing to MLflow itself in this example, we will follow the same general principles.\\n\\nThis post assumes that you have a basic understanding of what MLflow Tracing is and how it works. If you are just learning, or you need a refresher, take a look at the [Tracing Concepts](https://mlflow.org/docs/latest/llms/tracing/overview.html) guide.\\n\\nAdding tracing to a new provider involves a few key considerations:\\n\\n1. **Understand the Provider\'s key functionality:** We first need to understand what API methods need to be traced in order to get the tracing information we want. For LLM inference providers, this typically involves operations such as chat completions, tool calls, or embedding generation. In orchestration frameworks, this may involve operations such as retrieval, reasoning, routing, or any of a wide range of custom steps. In our Ollama example, we will focus on the chat completions API. This step will vary significantly depending on the provider.\\n\\n2. **Map operations to spans:** MLflow tracing uses different _span types_ to represent different types of operations. You can find descriptions of built-in span types [here](https://mlflow.org/docs/latest/llms/tracing/index.html#span-type). Different span types are displayed differently in the MLflow UI and can enable specific functionality. Within spans, we also want to map the provider\'s inputs and outputs to the formats expected by MLflow. MLflow offers utilities for recording chat and tool inputs and outputs, which are then displayed as formatted messages in the MLflow UI.\\n\\n   ![Chat Messages](./1_chat_type.png)\\n\\n   When adding tracing to a new provider, our main task is to map the provider\'s API methods to MLflow Tracing spans with appropriate span types.\\n\\n3. **Structure and preserve key data:** For each operation we want to trace, we need to identify the key information we want to preserve and make sure it is captured and displayed in a useful way. For example, we may want to capture the input and configuration data that control the operation\'s behavior, the outputs and metadata that explain the results, errors that terminated the operation prematurely, etc. Looking at traces and tracing implementations for similar providers can provide a good starting point for how to structure and preserve these data.\\n\\n## Adding tracing to the Ollama Python SDK\\n\\nNow that we have a high-level understanding of the key step of adding tracing to a new provider, let\'s work through the process and add tracing to the Ollama Python SDK.\\n\\n### Step 1: Install and Test the Ollama Python SDK\\n\\nFirst, we need to install the Ollama Python SDK and figure out what methods we need to pay attention to when adding tracing support. You can install the Ollama Python SDK with `pip install ollama-python`.\\n\\nIf you have used the OpenAI Python SDK, the Ollama Python SDK will feel quite familiar. Here\'s how we use it to make a chat completion call:\\n\\n```python\\nfrom ollama import chat\\nfrom rich import print\\n\\nresponse = chat(model=\\"llama3.2\\",\\n     messages = [\\n         {\\"role\\": \\"user\\", \\"content\\": \\"Briefly describe the components of an MLflow model\\"}\\n     ]\\n)\\n\\nprint(response)\\n```\\n\\nWhich will return:\\n\\n```python\\nChatResponse(\\n    model=\'llama3.2\',\\n    created_at=\'2025-01-30T15:57:39.097119Z\',\\n    done=True,\\n    done_reason=\'stop\',\\n    total_duration=7687553708,\\n    load_duration=823704250,\\n    prompt_eval_count=35,\\n    prompt_eval_duration=3414000000,\\n    eval_count=215,\\n    eval_duration=3447000000,\\n    message=Message(\\n        role=\'assistant\',\\n        content=\\"In MLflow, a model consists of several key components:\\\\n\\\\n1. **Model Registry**: A centralized\\nstorage for models, containing metadata such as the model\'s name, version, and description.\\\\n2. **Model Version**:\\nA specific iteration of a model, represented by a unique version number. This can be thought of as a snapshot of\\nthe model at a particular point in time.\\\\n3. **Model Artifacts**: The actual model code, parameters, and data used\\nto train the model. These artifacts are stored in the Model Registry and can be easily deployed or reused.\\\\n4.\\n**Experiment**: A collection of runs that use the same hyperparameters and model version to train and evaluate a\\nmodel. Experiments help track progress, provide reproducibility, and facilitate collaboration.\\\\n5. **Run**: An\\nindividual instance of training or testing a model using a specific experiment. Runs capture the output of each\\nrun, including metrics such as accuracy, loss, and more.\\\\n\\\\nThese components work together to enable efficient\\nmodel management, version control, and reproducibility in machine learning workflows.\\",\\n        images=None,\\n        tool_calls=None\\n    )\\n)\\n```\\n\\nWe have verified that the Ollama Python SDK is set up and working. We also know what method we need to focus on when adding tracing support: `ollama.chat`.\\n\\n### Step 2: Write a Tracing Decorator\\n\\nThere are several ways we could add tracing to Ollama\'s SDK\u2014we could modify the SDK code directly, create a wrapper class, or use Python\'s method patching capabilities. For this example, we\'ll use a decorator to patch the SDK\'s `chat` method. This approach lets us add tracing without modifying the SDK code or creating additional wrapper classes, though it does require understanding both Python\'s decorator pattern and how MLflow tracing works.\\n\\n```python\\nimport mlflow\\nfrom mlflow.entities import SpanType\\nfrom mlflow.tracing.utils import set_span_chat_messages\\nfrom functools import wraps\\nfrom ollama import chat as ollama_chat\\n\\ndef _get_span_type(task_name: str) -> str:\\n    span_type_mapping = {\\n        \\"chat\\": SpanType.CHAT_MODEL,\\n    }\\n    return span_type_mapping.get(task_name, SpanType.UNKNOWN)\\n\\ndef trace_ollama_chat(func):\\n    @wraps(func)\\n    def wrapper(*args, **kwargs):\\n        with mlflow.start_span(\\n            name=\\"ollama.chat\\",\\n            span_type=_get_span_type(\\"chat\\"),\\n        ) as span:\\n            # Set model name as a span attribute\\n            model_name = kwargs.get(\\"model\\", \\"\\")\\n            span.set_attribute(\\"model_name\\", model_name)\\n\\n            # Log the inputs\\n            input_messages = kwargs.get(\\"messages\\", [])\\n            span.set_inputs({\\n                \\"messages\\": input_messages,\\n                \\"model\\": model_name,\\n            })\\n\\n            # Set input messages\\n            set_span_chat_messages(span, input_messages)\\n\\n            # Make the API call\\n            response = func(*args, **kwargs)\\n\\n            # Log the outputs\\n            if hasattr(response, \'to_dict\'):\\n                output = response.to_dict()\\n            else:\\n                output = response\\n            span.set_outputs(output)\\n\\n            output_message = response.message\\n\\n            # Append the output message\\n            set_span_chat_messages(span, [{\\"role\\": output_message.role, \\"content\\": output_message.content}], append=True)\\n\\n            return response\\n    return wrapper\\n```\\n\\nLet\'s break down the code and see how it works.\\n\\n1. We start by defining a helper function, `_get_span_type`, that maps Ollama methods to MLflow span types. This isn\'t strictly necessary as we are currently only tracing the `chat` function, but it shows a pattern that could be applied to other methods. This follows the reference implementation for the [Anthropic provider](https://github.com/mlflow/mlflow/blob/master/mlflow/anthropic/autolog.py), as recommended in the tracing contribution guide.\\n\\n2. We define a decorator, `trace_ollama_chat`, using [`functools.wraps`](https://docs.python.org/3/library/functools.html#functools.wraps), that patches the `chat` function. There are a few key steps here:\\n\\n   1. We start a new span with `mlflow.start_span`. The span name is set to \\"ollama.chat\\" and the span type is set to the value returned by `_get_span_type`.\\n   2. We set `model_name` as an attribute on the span with `span.set_attribute`. This isn\'t strictly necessary as model name will be captured in the inputs, but it illustrates how to set arbitrary attributes on a span.\\n   3. We log the messages as inputs to the span with `span.set_inputs`. We get these from the `messages` argument by accessing the `kwargs` dictionary. These messages will be logged to the \\"inputs\\" section of the span in the MLflow UI. We also log the model name as an input, again to illustrate how to record arbitrary inputs.\\n\\n      ![Inputs](./2_log_inputs.png)\\n\\n   4. We use MLflow\'s `set_span_chat_messages` utility function to format the input messages in a way that will be displayed nicely in the MLflow UI\'s Chat panel. This helper ensures that the messages are properly formatted and displayed with appropriate styling for each message role.\\n   5. We call the original function with `func(*args, **kwargs)`. This is the Ollama `chat` function.\\n   6. We log the outputs of the function as a span attribute with `span.set_outputs`. This takes the response from the Ollama API and sets it as an attribute on the span. These outputs will be logged to the \\"outputs\\" section of the span in the MLflow UI.\\n\\n      ![Outputs](./3_log_outputs.png)\\n\\n   7. We extract the output message from the response and use `set_span_chat_messages` again to append it to the chat history, ensuring it appears in the Chat panel of the MLflow UI.\\n\\n      ![Messages Panel](./4_chat_panel.png)\\n\\n   8. Finally, we return the response from the API call, without any changes. Now, when we patch the chat function with `trace_ollama_chat`, the function will be traced, but will otherwise behave as normal.\\n\\nA few points to note:\\n\\n- This implementation uses a simple decorator pattern that adds tracing without modifying the underlying Ollama SDK code. This makes it a lightweight and maintainable approach.\\n- The use of `set_span_chat_messages` ensures that both input and output messages are displayed in a user-friendly way in the MLflow UI\'s Chat panel, making it easy to follow the conversation flow.\\n- There are several other ways we could have implemented this tracing behavior. We could have written a wrapper class or used a simple wrapper function that decorates the `chat` function with `@mlflow.trace`. Some orchestration frameworks may require a more complex approach, such as callbacks or API hooks. See the [MLflow Tracing Contribution Guide](https://mlflow.org/docs/latest/llms/tracing/contribute.html) for more details.\\n\\n### Step 3: Patch the `chat` method and try it out\\n\\nNow that we have a tracing decorator, we can patch Ollama\'s `chat` method and try it out.\\n\\n```python\\noriginal_chat = ollama_chat\\nchat = trace_ollama_chat(ollama_chat)\\n```\\n\\nThis code effectively patches the `ollama.chat` function in the current scope. We first store the original function in `original_chat` for safekeeping, then reassign `chat` to the decorated version. This means that any subsequent calls to `chat()` in our code will use the traced version, while still preserving the original functionality.\\n\\nNow, when we call `chat()`, the method will be traced and the results will be logged to the MLflow UI:\\n\\n```python\\nmlflow.set_experiment(\\"ollama-tracing\\")\\n\\nresponse = chat(model=\\"llama3.2\\",\\n     messages = [\\n         {\\"role\\": \\"user\\", \\"content\\": \\"Briefly describe the components of an MLflow model\\"}\\n     ]\\n)\\n```\\n\\n![Tracing results](./5_trace_results.png)\\n\\n## Tracing Tools and Tool Calls\\n\\nThe Ollama Python SDK supports tool calls. We want to record two main things:\\n\\n1. The tools that are available to the LLM\\n2. The actual tool calls, including the specific tool and the arguments passed to it.\\n\\nNote that a \\"tool call\\" refers to the LLM\'s specification of which tool to use and what arguments to pass to it\u2014not the actual execution of that tool. When an LLM makes a tool call, it\'s essentially saying \\"this tool should be run with these parameters\\" rather than running the tool itself. The actual execution of the tool happens separately, typically in the application code.\\n\\nHere is an updated version of the tracing code, patching the Ollama chat method, that records the available tools and captures tool calls:\\n\\n```python\\nfrom mlflow.entities import SpanType\\nfrom mlflow.tracing.utils import set_span_chat_messages, set_span_chat_tools\\nfrom functools import wraps\\nfrom ollama import chat as ollama_chat\\nimport json\\nfrom uuid import uuid4\\n\\ndef _get_span_type(task_name: str) -> str:\\n    span_type_mapping = {\\n        \\"chat\\": SpanType.CHAT_MODEL,\\n    }\\n    return span_type_mapping.get(task_name, SpanType.UNKNOWN)\\n\\ndef trace_ollama_chat(func):\\n    @wraps(func)\\n    def wrapper(*args, **kwargs):\\n        with mlflow.start_span(\\n            name=\\"ollama.chat\\",\\n            span_type=_get_span_type(\\"chat\\"),\\n        ) as span:\\n            # Set model name as a span attribute\\n            model_name = kwargs.get(\\"model\\", \\"\\")\\n            span.set_attribute(\\"model_name\\", model_name)\\n\\n            # Log the inputs\\n            input_messages = kwargs.get(\\"messages\\", [])\\n            tools = kwargs.get(\\"tools\\", [])\\n            span.set_inputs({\\n                \\"messages\\": input_messages,\\n                \\"model\\": model_name,\\n                \\"tools\\": tools,\\n            })\\n\\n            # Set input messages and tools\\n            set_span_chat_messages(span, input_messages)\\n            if tools:\\n                set_span_chat_tools(span, tools)\\n\\n            # Make the API call\\n            response = func(*args, **kwargs)\\n\\n            # Log the outputs\\n            if hasattr(response, \\"to_dict\\"):\\n                output = response.to_dict()\\n            else:\\n                output = response\\n            span.set_outputs(output)\\n\\n            output_message = response.message\\n\\n            # Prepare the output message for span\\n            output_span_message = {\\n                \\"role\\": output_message.role,\\n                \\"content\\": output_message.content,\\n            }\\n\\n            # Handle tool calls if present\\n            if output_message.tool_calls:\\n                tool_calls = []\\n                for tool_call in output_message.tool_calls:\\n                    tool_calls.append({\\n                        \\"id\\": str(uuid4()),\\n                        \\"type\\": \\"function\\",\\n                        \\"function\\": {\\n                            \\"name\\": tool_call.function.name,\\n                            \\"arguments\\": json.dumps(tool_call.function.arguments),\\n                        }\\n                    })\\n                output_span_message[\\"tool_calls\\"] = tool_calls\\n\\n            # Append the output message\\n            set_span_chat_messages(span, [output_span_message], append=True)\\n\\n            return response\\n\\n    return wrapper\\n```\\n\\nThe key changes here are:\\n\\n- We extracted the list of available tools from the `tools` argument with `tools = kwargs.get(\\"tools\\", [])`, logged them as inputs, and use `set_span_chat_tools` to capture them for inclusion in the Chat panel.\\n- We added a specific handling for tool calls in the output message, making sure to format them according to the [ToolCall](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ToolCall) specification.\\n\\nNow let\'s test this with a simple tip calculation tool. Tools are defined according to the [OpenAI specification](https://platform.openai.com/docs/guides/function-calling#defining-functions) for tool calls.\\n\\n```python\\nchat = trace_ollama_chat(ollama_chat)\\n\\ntools = [\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"calculate_tip\\",\\n            \\"description\\": \\"Calculate the tip amount based on the bill amount and tip percentage\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"bill_amount\\": {\\n                        \\"type\\": \\"number\\",\\n                        \\"description\\": \\"The total bill amount\\"\\n                    },\\n                    \\"tip_percentage\\": {\\n                        \\"type\\": \\"number\\",\\n                        \\"description\\": \\"The percentage of the bill to be given as a tip, given as a whole number.\\"\\n                    }\\n                },\\n                \\"required\\": [\\"bill_amount\\", \\"tip_percentage\\"]\\n            }\\n        }\\n    }\\n]\\n\\nresponse = chat(\\n    model=\\"llama3.2\\",\\n    messages=[\\n        {\\"role\\": \\"user\\", \\"content\\": \\"What is the tip for a $187.32 bill with a 22% tip?\\"}\\n    ],\\n    tools=tools,\\n)\\n\\n```\\n\\nWe can inspect the trace in the MLflow UI, now with both the available tools and the tool call results displayed:\\n\\n![Tool Call Results](./6_tool_call.png)\\n\\n## Orchestration: Building a tool calling loop\\n\\nSo far, the Ollama example just generates a single span whenever a chat completion is made. But many GenAI applications include multiple LLM calls, retrieval steps, tool executions, and other custom steps. While we won\'t go into detail on adding tracing to orchestration frameworks here, we will illustrate some of the key concepts by defining a tool calling loop based on the tool we defined earlier.\\n\\nThe tool calling loop will follow this pattern:\\n\\n1. Take user prompt as input\\n2. Respond with a tool call or calls\\n3. For each tool call, execute the tool and store the results\\n4. Append the tool call results to the message history with the `tool` role\\n5. Call the LLM again with the tool call results, prompting it for a final answer to the user\'s prompt\\n\\nHere\'s an implementation with just one tool call.\\n\\n```python\\nclass ToolExecutor:\\n    def __init__(self):\\n        self.tools = [\\n            {\\n                \\"type\\": \\"function\\",\\n                \\"function\\": {\\n                    \\"name\\": \\"calculate_tip\\",\\n                    \\"description\\": \\"Calculate the tip amount based on the bill amount and tip percentage\\",\\n                    \\"parameters\\": {\\n                        \\"type\\": \\"object\\",\\n                        \\"properties\\": {\\n                            \\"bill_amount\\": {\\n                                \\"type\\": \\"number\\",\\n                                \\"description\\": \\"The total bill amount\\"\\n                            },\\n                            \\"tip_percentage\\": {\\n                                \\"type\\": \\"number\\",\\n                                \\"description\\": \\"The percentage of the bill to be given as a tip, represented as a whole number.\\"\\n                            }\\n                        },\\n                        \\"required\\": [\\"bill_amount\\", \\"tip_percentage\\"]\\n                    }\\n                }\\n            }\\n        ]\\n\\n        # Map tool names to their Python implementations\\n        self.tool_implementations = {\\n            \\"calculate_tip\\": self._calculate_tip\\n        }\\n\\n    def _calculate_tip(self, bill_amount: float, tip_percentage: float) -> float:\\n        \\"\\"\\"Calculate the tip amount based on the bill amount and tip percentage.\\"\\"\\"\\n        bill_amount = float(bill_amount)\\n        tip_percentage = float(tip_percentage)\\n        return round(bill_amount * (tip_percentage / 100), 2)\\n    def execute_tool_calling_loop(self, messages):\\n        \\"\\"\\"Execute a complete tool calling loop with tracing.\\"\\"\\"\\n        with mlflow.start_span(\\n            name=\\"ToolCallingLoop\\",\\n            span_type=\\"CHAIN\\",\\n        ) as parent_span:\\n            # Set initial inputs\\n            parent_span.set_inputs({\\n                \\"initial_messages\\": messages,\\n                \\"available_tools\\": self.tools\\n            })\\n\\n            # Set input messages\\n            set_span_chat_messages(parent_span, messages)\\n\\n            # First LLM call (already traced by our chat method patch)\\n            response = chat(\\n                messages=messages,\\n                model=\\"llama3.2\\",\\n                tools=self.tools,\\n            )\\n\\n            messages.append(response.message)\\n\\n            tool_calls = response.message.tool_calls\\n            tool_results = []\\n\\n            # Execute tool calls\\n            for tool_call in tool_calls:\\n                with mlflow.start_span(\\n                    name=f\\"ToolExecution_{tool_call.function.name}\\",\\n                    span_type=\\"TOOL\\",\\n                ) as tool_span:\\n                    # Parse tool inputs\\n                    tool_inputs = tool_call.function.arguments\\n                    tool_span.set_inputs(tool_inputs)\\n\\n                    # Execute tool\\n                    func = self.tool_implementations.get(tool_call.function.name)\\n                    if func is None:\\n                        raise ValueError(f\\"No implementation for tool: {tool_call.function.name}\\")\\n\\n                    result = func(**tool_inputs)\\n                    tool_span.set_outputs({\\"result\\": result})\\n\\n                    tool_results.append({\\n                        \\"tool_call_id\\": str(uuid4()),\\n                        \\"output\\": str(result)\\n                    })\\n\\n                    messages.append({\\n                        \\"role\\": \\"tool\\",\\n                        \\"tool_call_id\\": str(uuid4()),\\n                        \\"content\\": str(result)\\n                    })\\n\\n            # Prepare messages for final response\\n            messages.append({\\n                \\"role\\": \\"user\\",\\n                \\"content\\": \\"Answer the initial question based on the tool call results. Do not refer to the tool call results in your response. Just give a direct answer.\\"\\n            })\\n\\n            # Final LLM call (already traced by our chat method patch)\\n            final_response = chat(\\n                messages=messages,\\n                model=\\"llama3.2\\"\\n            )\\n\\n            # Set the final output for the parent span\\n            parent_span.set_outputs({\\n                \\"final_response\\": final_response.message.content,\\n                \\"tool_results\\": tool_results\\n            })\\n\\n            print(final_response)\\n\\n            # set output messages\\n            set_span_chat_messages(parent_span, [final_response.message.model_dump()], append=True)\\n\\n            return final_response\\n```\\n\\nHere\'s how we handled tracing in this tool calling loop:\\n\\n1. We first set up a parent span for the tool calling loop with `mlflow.start_span`. We set the span name to \\"ToolCallingLoop\\" and the span type to \\"CHAIN\\", representing a chain of operations.\\n2. We record the initial messages and available tools as inputs to the span. This could be helpful for future debugging by allowing us to verify that tools are made available and configured correctly.\\n3. We make the first LLM call with our patched `chat` function. This call is already traced by our decorator, so we don\'t need to do anything special to trace it.\\n4. We iterate over the tool calls, executing each tool and storing the results. Each tool execution is traced with a new span, named after the tool function name. The inputs and outputs are logged as attributes on the span.\\n5. We append the tool call results to the message history with the `tool` role. This allows the LLM to see the results of the tool calls in subsequent requests. It also allows us to see the tool call results in the MLflow UI.\\n6. We prepare messages for the final response, including a prompt to answer the initial question based on the tool call results.\\n7. We make the final LLM call with our patched `chat` function. Again, because we are using the patched function, this call is already traced.\\n8. We set the final output for the parent span, including both the final response from the LLM and the tool results.\\n9. Finally, we use `set_span_chat_messages` to append the final response to the chat history in the MLflow UI. Note that, to keep things clean and simple, we only record the user\'s initial query and the final response to the parent span with `set_span_chat_messages`. We can click into the nested spans to see the tool call results and other details.\\n\\nThis process creates a comprehensive trace of the entire tool calling loop, from the initial request through tool executions and the final response.\\n\\nWe can execute this as follows. However, note that you should _not_ run arbitrary code generated or invoked by LLMs without fully understanding what it will do on your system.\\n\\n```python\\nexecutor = ToolExecutor()\\nresponse = executor.execute_tool_calling_loop(\\n    messages=[\\n        {\\"role\\": \\"user\\", \\"content\\": \\"What is the tip for a $235.32 bill with a 22% tip?\\"}\\n    ]\\n)\\n```\\n\\nResulting in the following trace:\\n\\n![Tool Calling Loop](./7_tool_loop.png)\\n\\n## Conclusion\\n\\nThis post has shown how to extend MLflow Tracing beyond its built-in provider support. We started with a simple example\u2014adding tracing to the Ollama Python SDK\'s `chat` method\u2014and saw how, with a lightweight patch, we could capture detailed information about each chat completion. We then built on this foundation to trace a more complex tool execution loop.\\n\\nThe key takeaways are:\\n\\n- MLflow Tracing is highly customizable and can be adapted to providers for which autologging is not available\\n- Adding basic tracing support can often be done with minimal code changes. In this case, we patched the Ollama Python SDK\'s `chat` method and wrote a few lines of code to add tracing support.\\n- The same principles used for simple API calls can be extended to complex workflows with multiple steps. In this case, we traced a tool calling loop that included multiple steps and tool calls."},{"id":"from-natural-language-to-sql","metadata":{"permalink":"/mlflow-website/blog/from-natural-language-to-sql","source":"@site/blog/2025-01-23-from-natural-language-to-sql/index.md","title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","description":"MLflow Models from Code and MLflow Tracing applied to AI Workflows","date":"2025-01-23T00:00:00.000Z","tags":[{"inline":true,"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"inline":true,"label":"sql-generator","permalink":"/mlflow-website/blog/tags/sql-generator"},{"inline":true,"label":"models-from-code","permalink":"/mlflow-website/blog/tags/models-from-code"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":39.44,"hasTruncateMarker":true,"authors":[{"name":"Hugo Carvalho","title":"Machine Learning Analyst at adidas","url":"https://www.linkedin.com/in/hugodscarvalho/","imageURL":"/mlflow-website/img/authors/hugo_carvalho.png","key":"hugo-carvalho","page":null},{"name":"Joana Ferreira","title":"Machine Learning Engineer at adidas","url":"https://www.linkedin.com/in/joanaferreira96/","imageURL":"/mlflow-website/img/authors/joana_ferreira.png","key":"joana-ferreira","page":null},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/mlflow-website/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey","page":null}],"frontMatter":{"title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","description":"MLflow Models from Code and MLflow Tracing applied to AI Workflows","tags":["pyfunc","mlflow","sql-generator","models-from-code","tracing"],"slug":"from-natural-language-to-sql","authors":["hugo-carvalho","joana-ferreira","rahul-pandey"],"thumbnail":"/img/blog/from-natural-language-to-sql.png"},"unlisted":false,"prevItem":{"title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","permalink":"/mlflow-website/blog/custom-tracing"},"nextItem":{"title":"MLflow Tracing in Jupyter Notebooks","permalink":"/mlflow-website/blog/mlflow-tracing-in-jupyter"}},"content":"If you\'re looking to build a Multi-Lingual Query Engine that combines natural language to SQL generation with query execution while fully leveraging MLflow\u2019s features, this blog post is your guide. We\u2019ll explore how to leverage **MLflow Models from Code** to enable seamless tracking and versioning of AI Workflows. Additionally, we\u2019ll deep dive into **MLflow\u2019s Tracing** feature, which introduces observability into the many different components of an AI Workflow by tracking inputs, outputs, and metadata at every intermediate step.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nSQL is a fundamental skill for managing and accessing data within relational databases. However, constructing complex SQL queries to answer intricate data questions can be challenging and time-consuming. This complexity can make it difficult to fully leverage data effectively. Natural language to SQL (NL2SQL) systems help in solving this problem by providing a translation from natural language to SQL commands allowing non-technical people to interact with data: users can just ask questions in a natural language they are comfortable speaking and these systems will assist them in returning the appropriate information.\\n\\nHowever, there are also a number of problems that remain when creating a NL2SQL system such as semantic ambiguity, schema mapping or error handling and user feedback. Therefore, it is very important that while building such systems, we must put some guardrails instead of completely relying on LLM.\\n\\nIn this blog post, we\u2019ll walk you through the process of building a Multi-Lingual Query Engine. This engine supports natural language inputs in multiple languages, generates an SQL query based on the translated user input, and executes the query. Let\'s jump into an example: using a database containing information about a company\'s customers, products, and orders, a user might ask a question in any language, such as \\"Quantos clientes temos por pa\xeds?\\" (Portuguese for \\"How many customers do we have per country?\\"). The AI Workflow translates the input into English, outputting \\"How many customers do we have per country?\\". It then validates the input for safety, checks if the question can be answered using the database schema, generates the appropriate SQL query (e.g., `SELECT COUNT(CustomerID) AS NumberOfCustomers, Country FROM Customers GROUP BY Country;`), and validates the query to ensure no harmful commands (e.g., DROP) are present. Finally, it executes the query against the database to retrieve the results.\\n\\nWe\u2019ll start by demonstrating how to leverage [LangGraph\u2019s](https://www.langchain.com/langgraph) capabilities to build a dynamic AI workflow. This workflow integrates OpenAI and external data sources, such as a Vector Store and a SQLite database, to process user input, perform safety checks, query databases, and generate meaningful responses.\\n\\nThroughout this post, we\u2019ll leverage [MLflow\u2019s Models from Code](https://mlflow.org/docs/latest/model/models-from-code.html) feature to enable seamless tracking and versioning of AI Workflows. Additionally, we\u2019ll deep dive into [MLflow\u2019s Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) feature, designed to enhance the observability of the many different components of an AI workflow by tracking inputs, outputs, and metadata associated with each intermediate step. This enables easy identification of bugs and unexpected behaviors, providing greater transparency over the workflow.\\n\\n## Prerequisites\\n\\nTo set up and run this project, ensure the following **Python packages** are installed:\\n\\n- `faiss-cpu`\\n- `langchain`\\n- `langchain-core`\\n- `langchain-openai`\\n- `langgraph`\\n- `langchain-community`\\n- `pydantic >=2`\\n- `typing_extensions`\\n- `python-dotenv`\\n\\nAdditionally, an **MLflow Tracking Server** is required to log and manage experiments, models, and traces effectively. For local setup, refer to the official MLflow documentation for instructions on [configuring a simple MLflow Tracking Server](https://mlflow.org/docs/latest/tracking/server.html).\\n\\nFinally, ensure that your OpenAI API key is saved within a .env file in the project directory. This allows the application to securely access the OpenAI services required for building the AI workflow. The .env file should include a line like:\\n\\n```\\nOPENAI_API_KEY=your_openai_api_key\\n```\\n\\n## Multi-Lingual Query Engine using LangGraph\\n\\nThe Multi-Lingual Query Engine leverages the [LangGraph](https://langchain-ai.github.io/langgraph/) library, an AI orchestration tool designed to create stateful, multi-agent, and cyclical graph architectures for applications powered by LLMs.\\n\\nCompared to other AI orchestrators, LangGraph offers three core benefits: cycles, controllability, and persistence. It allows the definition of AI workflows with cycles, which are essential for implementing retry mechanisms like the SQL query generation retries in the Multi-Lingual Query Engine (where the query loops back for regeneration if validation fails). This makes LangGraph the ideal tool for building our Multi-Lingual Query Engine.\\n\\n### Key LangGraph features:\\n\\n1. **Stateful Architecture**: The engine maintains a dynamic snapshot of the graph\u2019s execution status. This snapshot acts as a shared resource across nodes, enabling efficient decision-making and real-time updates at each node execution.\\n\\n2. **Multi-Agent Design**: The AI Workflow includes multiple interactions with OpenAI and other external tools throughout the workflow.\\n\\n3. **Cyclical Graph Structure**: The graph\u2019s cyclical nature introduces a robust retry mechanism. This mechanism dynamically addresses failures by looping back to previous stages when needed, ensuring continuous graph execution. (Details of this mechanism will be discussed later.)\\n\\n## AI Workflow Overview\\n\\nThe Multi-Lingual Query Engine\u2019s advanced AI workflow is composed of interconnected nodes and edges, each representing a crucial stage:\\n\\n1. **Translation Node**: Converts the user\u2019s input into English.\\n\\n2. **Pre-safety Check**: Ensures user input is free from toxic or inappropriate content and does not contain harmful SQL commands (e.g., `DELETE`, `DROP`).\\n\\n3. **Database Schema Extraction**: Retrieves the schema of the target database to understand its structure and available data.\\n\\n4. **Relevancy Validation**: Validates the user\u2019s input against the database schema to ensure alignment with the database\u2019s context.\\n\\n5. **SQL Query Generation**: Generates an SQL query based on the user\u2019s input and the current database schema.\\n\\n6. **Post-safety Check**: Ensures the generated SQL Query does not contain harmful SQL commands (e.g., `DELETE`, `DROP`).\\n\\n7. **SQL Query Validation**: Executes the SQL query in a rollback-safe environment to ensure its validity before running it.\\n\\n8. **Dynamic State Evaluation**: Determines the next steps based on the current state. If the SQL query validation fails, it loops back to Stage 5 to regenerate the query.\\n\\n9. **Query Execution and Result Retrieval**: Executes the SQL query and returns the results if it\u2019s a `SELECT` statement.\\n\\nThe retry mechanism is introduced in Stage 8, where the system dynamically evaluates the current graph state. Specifically, when the SQL query validation node (Stage 7) detects an issue, the state triggers a loop back to the SQL Generation node (Stage 5) for a new SQL Generation attempt (with a maximum of 3 attempts).\\n\\n### Components\\n\\nThe Multi-Lingual Query Engine interacts with several external components to transform natural language user inputs into SQL queries and execute them in a safe and robust manner. In this section, we will take a detailed look at the key AI Workflow components: OpenAI, Vector Store, SQLite Database, and SQL Generation Chain.\\n\\n#### OpenAI\\n\\nOpenAI, more specifically the `gpt-4o-mini` language model, plays a crucial role in multiple stages of the workflow. It provides the intelligence required for:\\n\\n1. **Translation**: Translates user input into English. If the text is already in English, it simply repeats the input.\\n\\n2. **Safety Checks**: Analyzes user input to ensure that it does not contain toxic or inappropriate content.\\n\\n3. **Relevance Checks**: Evaluates whether the user\'s question is relevant given the database schema.\\n\\n4. **SQL Generation**: Generates valid and executable SQL queries based on user input, SQL generation documentation, and the database schema.\\n\\nDetails on OpenAI implementation will be provided later on in the [Node Descriptions](#node-descriptions) section.\\n\\n#### FAISS Vector Store\\n\\nTo build an effective natural language to SQL engine capable of generating accurate and executable SQL queries, we leverage Langchain\'s FAISS Vector Store feature. This setup allows the system to search and extract SQL query generation guidelines from [W3Schools SQL documents](https://www.w3schools.com/sql/) previously stored in the Vector Database, enhancing the success of SQL query generation.\\n\\nFor demo purposes, we are using FAISS, an in-memory vector store where vectors are stored directly in RAM. This provides fast access but means data is not persisted between runs. For a more scalable solution that enables embeddings to be stored and shared across multiple projects, we recommend alternatives like [AWS OpenSearch](https://aws.amazon.com/what-is/opensearch/), [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview), [Azure Vector Search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview), or [Mosaic AI Vector Search](https://docs.databricks.com/en/generative-ai/vector-search.html). These cloud-based solutions offer persistent storage, automatic scaling, and seamless integration with other cloud services, making them well-suited for large-scale applications.\\n\\n##### Step 1: Load SQL Documentation\\n\\nThe first step in creating a FAISS Vector Store with SQL query generation guidelines is to load SQL documentation from the [W3Schools SQL page](https://www.w3schools.com/sql/) using LangChain\'s `RecursiveUrlLoader`. This tool retrieves the documentation, allowing us to use it as a knowledge base for our engine.\\n\\n##### Step 2: Split the Text into Manageable Chunks\\n\\nThe loaded SQL documentation is a lengthy text, making it difficult to be effectively ingested by the LLM. To address this, the next step involves splitting the text into smaller, manageable chunks using Langchain\'s `RecursiveCharacterTextSplitter`. By splitting the text into chunks of 500 characters with a 50-character overlap, we ensure the language model has sufficient context while minimizing the risk of losing important information that spans across chunks. The `split_text` method applies this splitting process, storing the resulting pieces in a list called \'documents\'.\\n\\n##### Step 3: Generate Embedding Model\\n\\nThe third step is to create a model that converts these chunks into embeddings (vectorized numerical representations of each text chunk). Embeddings enable the system to compare the similarity between chunks and the user\'s input, facilitating the retrieval of the most relevant matches for SQL query generation.\\n\\n##### Step 4: Create and Store Embeddings in FAISS Vector Store\\n\\nFinally, we create and store the embeddings using FAISS. The `FAISS.from_texts` method takes all the chunks, computes their embeddings, and stores them in a high speed searchable vector database. This searchable database allows the engine to efficiently retrieve relevant SQL guidelines, significantly improving the success rate of executable SQL query generation.\\n\\n```python\\nimport logging\\nimport os\\n\\nfrom bs4 import BeautifulSoup as Soup\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n\\ndef setup_vector_store(logger: logging.Logger):\\n    \\"\\"\\"Setup or load the vector store.\\"\\"\\"\\n    if not os.path.exists(\\"data\\"):\\n        os.makedirs(\\"data\\")\\n\\n    vector_store_dir = \\"data/vector_store\\"\\n\\n    if os.path.exists(vector_store_dir):\\n        # Load the vector store from disk\\n        logger.info(\\"Loading vector store from disk...\\")\\n        vector_store = FAISS.load_local(\\n            vector_store_dir,\\n            OpenAIEmbeddings(),\\n            allow_dangerous_deserialization=True,\\n        )\\n    else:\\n        logger.info(\\"Creating new vector store...\\")\\n        # Load SQL documentation\\n        url = \\"https://www.w3schools.com/sql/\\"\\n        loader = RecursiveUrlLoader(\\n            url=url, max_depth=2, extractor=lambda x: Soup(x, \\"html.parser\\").text\\n        )\\n        docs = loader.load()\\n\\n        # Split documents into chunks\\n        text_splitter = RecursiveCharacterTextSplitter(\\n            chunk_size=500,\\n            chunk_overlap=50,\\n            separators=[\\"\\\\n\\\\n\\", \\"\\\\n\\", \\".\\", \\"!\\", \\"?\\", \\",\\", \\" \\", \\"\\"],\\n        )\\n\\n        documents = []\\n        for doc in docs:\\n            splits = text_splitter.split_text(doc.page_content)\\n            for i, split in enumerate(splits):\\n                documents.append(\\n                    {\\n                        \\"content\\": split,\\n                        \\"metadata\\": {\\"source\\": doc.metadata[\\"source\\"], \\"chunk\\": i},\\n                    }\\n                )\\n\\n        # Compute embeddings and create vector store\\n        embedding_model = OpenAIEmbeddings()\\n        vector_store = FAISS.from_texts(\\n            [doc[\\"content\\"] for doc in documents],\\n            embedding_model,\\n            metadatas=[doc[\\"metadata\\"] for doc in documents],\\n        )\\n\\n        # Save the vector store to disk\\n        vector_store.save_local(vector_store_dir)\\n        logger.info(\\"Vector store created and saved to disk.\\")\\n\\n    return vector_store\\n```\\n\\n#### SQLite Database\\n\\nThe SQLite database is a key component of the Multi-Lingual Query Engine serving as the structured data repository. SQLite offers a lightweight, fast, and self-contained relational database engine that requires no server setup or installation. Its compact size (under 500KB) and zero-configuration nature make it incredibly easy to use, while its platform-agnostic database format ensures seamless portability across different systems. As a local disk database, SQLite was the ideal choice for avoiding the complexity of setting up MySQL or PostgreSQL, while still providing a reliable, full-featured SQL engine with outstanding performance.\\n\\nThe SQLite database supports efficient SQL query generation, validation and execution by enabling:\\n\\n1. **Schema Extraction**: Suplying schema information for user\u2019s input context validation (Stage 4) and executable SQL Query Generation (Stage 5).\\n\\n2. **Query Execution**: Executing SQL queries in a rollback-safe environment in Validation Stage (Stage 7) and in Query Execution Stage (Stage 9) fetching results for `SELECT` statements and committing changes for other query types.\\n\\n#### SQLite Database Initialization\\n\\nThe database is initialized using the `setup_database` function when the AI Workflow is initialized. This process involves:\\n\\n1. **Setting the SQLite Database Connection**: Establishes a connection to the SQLite database, enabling data interaction.\\n\\n2. **Table Creation**: Defines and creates the necessary database tables for the AI Workflow.\\n\\n3. **Data Population**: Populates the tables with sample data to support query execution and validation stages.\\n\\n```python\\nimport logging\\nimport os\\n\\nimport sqlite3\\n\\n\\ndef create_connection(db_file=\\"data/database.db\\"):\\n    \\"\\"\\"Create a database connection to the SQLite database.\\"\\"\\"\\n    conn = sqlite3.connect(db_file)\\n    return conn\\n\\n\\ndef create_tables(conn):\\n    \\"\\"\\"Create tables in the database.\\"\\"\\"\\n    cursor = conn.cursor()\\n    # Create Customers table\\n    cursor.execute(\\n        \\"\\"\\"\\n    CREATE TABLE IF NOT EXISTS Customers (\\n        CustomerID INTEGER PRIMARY KEY,\\n        CustomerName TEXT,\\n        ContactName TEXT,\\n        Address TEXT,\\n        City TEXT,\\n        PostalCode TEXT,\\n        Country TEXT\\n    )\\n    \\"\\"\\"\\n    )\\n\\n    # Create Orders table\\n    cursor.execute(\\n        \\"\\"\\"\\n    CREATE TABLE IF NOT EXISTS Orders (\\n        OrderID INTEGER PRIMARY KEY,\\n        CustomerID INTEGER,\\n        OrderDate TEXT,\\n        FOREIGN KEY (CustomerID) REFERENCES Customers (CustomerID)\\n    )\\n    \\"\\"\\"\\n    )\\n\\n    # Create OrderDetails table\\n    cursor.execute(\\n        \\"\\"\\"\\n    CREATE TABLE IF NOT EXISTS OrderDetails (\\n        OrderDetailID INTEGER PRIMARY KEY,\\n        OrderID INTEGER,\\n        ProductID INTEGER,\\n        Quantity INTEGER,\\n        FOREIGN KEY (OrderID) REFERENCES Orders (OrderID),\\n        FOREIGN KEY (ProductID) REFERENCES Products (ProductID)\\n    )\\n    \\"\\"\\"\\n    )\\n\\n    # Create Products table\\n    cursor.execute(\\n        \\"\\"\\"\\n    CREATE TABLE IF NOT EXISTS Products (\\n        ProductID INTEGER PRIMARY KEY,\\n        ProductName TEXT,\\n        Price REAL\\n    )\\n    \\"\\"\\"\\n    )\\n\\n    conn.commit()\\n\\n\\ndef populate_tables(conn):\\n    \\"\\"\\"Populate tables with sample data if they are empty.\\"\\"\\"\\n    cursor = conn.cursor()\\n\\n    # Populate Customers table if empty\\n    cursor.execute(\\"SELECT COUNT(*) FROM Customers\\")\\n    if cursor.fetchone()[0] == 0:\\n        customers = []\\n        for i in range(1, 51):\\n            customers.append(\\n                (\\n                    i,\\n                    f\\"Customer {i}\\",\\n                    f\\"Contact {i}\\",\\n                    f\\"Address {i}\\",\\n                    f\\"City {i % 10}\\",\\n                    f\\"{10000 + i}\\",\\n                    f\\"Country {i % 5}\\",\\n                )\\n            )\\n        cursor.executemany(\\n            \\"\\"\\"\\n        INSERT INTO Customers (CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\\n        VALUES (?, ?, ?, ?, ?, ?, ?)\\n        \\"\\"\\",\\n            customers,\\n        )\\n\\n    # Populate Products table if empty\\n    cursor.execute(\\"SELECT COUNT(*) FROM Products\\")\\n    if cursor.fetchone()[0] == 0:\\n        products = []\\n        for i in range(1, 51):\\n            products.append((i, f\\"Product {i}\\", round(10 + i * 0.5, 2)))\\n        cursor.executemany(\\n            \\"\\"\\"\\n        INSERT INTO Products (ProductID, ProductName, Price)\\n        VALUES (?, ?, ?)\\n        \\"\\"\\",\\n            products,\\n        )\\n\\n    # Populate Orders table if empty\\n    cursor.execute(\\"SELECT COUNT(*) FROM Orders\\")\\n    if cursor.fetchone()[0] == 0:\\n        orders = []\\n        from datetime import datetime, timedelta\\n\\n        base_date = datetime(2023, 1, 1)\\n        for i in range(1, 51):\\n            order_date = base_date + timedelta(days=i)\\n            orders.append(\\n                (\\n                    i,\\n                    i % 50 + 1,  # CustomerID between 1 and 50\\n                    order_date.strftime(\\"%Y-%m-%d\\"),\\n                )\\n            )\\n        cursor.executemany(\\n            \\"\\"\\"\\n        INSERT INTO Orders (OrderID, CustomerID, OrderDate)\\n        VALUES (?, ?, ?)\\n        \\"\\"\\",\\n            orders,\\n        )\\n\\n    # Populate OrderDetails table if empty\\n    cursor.execute(\\"SELECT COUNT(*) FROM OrderDetails\\")\\n    if cursor.fetchone()[0] == 0:\\n        order_details = []\\n        for i in range(1, 51):\\n            order_details.append(\\n                (\\n                    i,\\n                    i % 50 + 1,  # OrderID between 1 and 50\\n                    i % 50 + 1,  # ProductID between 1 and 50\\n                    (i % 5 + 1) * 2,  # Quantity between 2 and 10\\n                )\\n            )\\n        cursor.executemany(\\n            \\"\\"\\"\\n        INSERT INTO OrderDetails (OrderDetailID, OrderID, ProductID, Quantity)\\n        VALUES (?, ?, ?, ?)\\n        \\"\\"\\",\\n            order_details,\\n        )\\n\\n    conn.commit()\\n\\n\\ndef setup_database(logger: logging.Logger):\\n    \\"\\"\\"Setup the database and return the connection.\\"\\"\\"\\n    db_file = \\"data/database.db\\"\\n    if not os.path.exists(\\"data\\"):\\n        os.makedirs(\\"data\\")\\n\\n    db_exists = os.path.exists(db_file)\\n\\n    conn = create_connection(db_file)\\n\\n    if not db_exists:\\n        logger.info(\\"Setting up the database...\\")\\n        create_tables(conn)\\n        populate_tables(conn)\\n    else:\\n        logger.info(\\"Database already exists. Skipping setup.\\")\\n\\n    return conn\\n```\\n\\n#### SQL Generation Chain\\n\\nThe **SQL Generation Chain** (`sql_gen_chain`) is the backbone of automated SQL query generation in our workflow. This chain leverages LangChain\'s modular capabilities and OpenAI\'s advanced natural language processing to transform user questions into precise and executable SQL queries.\\n\\n**Core Features**:\\n\\n- **Prompt-Driven Generation**: Begins with a thoughtfully designed prompt that integrates the database schema and documentation snippets, ensuring queries are contextually accurate.\\n\\n- **Structured Responses**: Delivers outputs in a predefined format, including:\\n\\n  - A **description** of the query\'s purpose.\\n\\n  - The corresponding **SQL code** ready for execution.\\n\\n- **Adaptable and Reliable**: Uses `gpt-4o-mini` for robust, consistent query generation, minimizing manual effort and errors.\\n\\nThis chain is a critical component in our workflow, enabling seamless integration of SQL query generation with downstream processes, ensuring accuracy, and significantly improving efficiency.\\n\\n```python\\nfrom pydantic import BaseModel, Field\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\n\\nclass SQLQuery(BaseModel):\\n    \\"\\"\\"Schema for SQL query solutions to questions.\\"\\"\\"\\n    description: str = Field(description=\\"Description of the SQL query\\")\\n    sql_code: str = Field(description=\\"The SQL code block\\")\\n\\ndef get_sql_gen_chain():\\n    \\"\\"\\"Set up the SQL generation chain.\\"\\"\\"\\n    sql_gen_prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \\"system\\",\\n                \\"\\"\\"You are a SQL assistant with expertise in SQL query generation. \\\\n\\nAnswer the user\'s question based on the provided documentation snippets and the database schema provided below. Ensure any SQL query you provide is valid and executable. \\\\n\\nStructure your answer with a description of the query, followed by the SQL code block. Here are the documentation snippets:\\\\n{retrieved_docs}\\\\n\\\\nDatabase Schema:\\\\n{database_schema}\\"\\"\\",\\n            ),\\n            (\\"placeholder\\", \\"{messages}\\"),\\n        ]\\n    )\\n\\n    # Initialize the OpenAI LLM\\n    llm = ChatOpenAI(temperature=0, model=\\"gpt-4o-mini\\")\\n\\n    # Create the code generation chain\\n    sql_gen_chain = sql_gen_prompt | llm.with_structured_output(SQLQuery)\\n\\n    return sql_gen_chain\\n```\\n\\n#### Workflow Setup and Initialization\\n\\nBefore delving into the workflow nodes, it\'s crucial to set up the necessary components and define the structure of the workflow. This section explains the initialization of essential libraries, logging, and the custom `GraphState` class, as well as the main workflow compilation function.\\n\\n##### Defining `GraphState`\\n\\nThe `GraphState` class is a custom `TypedDict` that maintains the state information as the workflow progresses. It acts as a shared data structure across the nodes, ensuring continuity and consistency. Key fields include:\\n\\n- **`error`**: Tracks whether an error has occurred.\\n- **`messages`**: Stores a list of user and system messages.\\n- **`generation`**: Holds the generated SQL query.\\n- **`iterations`**: Tracks the number of retry attempts in case of errors.\\n- **`results`**: Stores the SQL execution results, if any.\\n- **`no_records_found`**: Flags if no records are returned by the query.\\n- **`translated_input`**: Contains the user\'s translated input.\\n- **`database_schema`**: Maintains the database schema for context validation.\\n\\n```python\\nimport logging\\nimport re\\nfrom typing import List, Optional\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import END, START, StateGraph\\nfrom sql_generation import get_sql_gen_chain\\nfrom typing_extensions import TypedDict\\n\\n# Initialize the logger\\n_logger = logging.getLogger(__name__)\\n_logger.setLevel(logging.INFO)\\nhandler = logging.StreamHandler()\\nformatter = logging.Formatter(\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\")\\nhandler.setFormatter(formatter)\\n_logger.addHandler(handler)\\n\\nclass GraphState(TypedDict):\\n    error: str  # Tracks if an error has occurred\\n    messages: List  # List of messages (user input and assistant messages)\\n    generation: Optional[str]  # Holds the generated SQL query\\n    iterations: int  # Keeps track of how many times the workflow has retried\\n    results: Optional[List]  # Holds the results of SQL execution\\n    no_records_found: bool  # Flag for whether any records were found in the SQL result\\n    translated_input: str  # Holds the translated user input\\n    database_schema: str  # Holds the extracted database schema for context checking\\n```\\n\\n##### Workflow Compilation Function\\n\\nThe main function, `get_workflow`, is responsible for defining and compiling the workflow. Key components include:\\n\\n- **`conn` and `cursor`**: Used for database connectivity and query execution.\\n- **`vector_store`**: A vector database for contextual retrieval.\\n- **`max_iterations`**: Sets a limit on retry attempts to prevent infinite loops.\\n- **`sql_gen_chain`**: Retrieves the SQL generation chain from `sql_generation` for producing SQL queries based on contextual inputs.\\n- **`ChatOpenAI`**: Initializes the OpenAI `gpt-4o-mini` model for tasks like safety checks and query translation.\\n\\n```python\\ndef get_workflow(conn, cursor, vector_store):\\n    \\"\\"\\"Define and compile the LangGraph workflow.\\"\\"\\"\\n\\n    # Max iterations: defines how many times the workflow should retry in case of errors\\n    max_iterations = 3\\n\\n    # SQL generation chain: this is a chain that will generate SQL based on retrieved docs\\n    sql_gen_chain = get_sql_gen_chain()\\n\\n    # Initialize OpenAI LLM for translation and safety checks\\n    llm = ChatOpenAI(temperature=0, model=\\"gpt-4o-mini\\")\\n\\n    # Define the individual nodes of the workflow\\n```\\n\\nThis function acts as the entry point for creating a complete workflow using `StateGraph`. Individual nodes within the workflow will be defined and connected in subsequent sections.\\n\\n#### Node Descriptions\\n\\n##### 1. Translate Input\\n\\nThe `translate_input` node translates user queries into English to standardize processing and ensure compatibility with downstream nodes. Translating user input as the first step in the AI Workflow ensures task segregation and improves observability. Task segregation simplifies the workflow by isolating translation from the other dowstream tasks like user input safety validation and SQL generation. Improved observability provides clear traces in MLflow, making it easier to debug and monitor the process.\\n\\n- **Examples:**\\n  - Input: _\\"Quantos pedidos foram realizados em Novembro?\\"_\\n  - Translated: _\\"How many orders were made in November?\\"_\\n  - Input: _\\"Combien de ventes avons-nous enregistr\xe9es en France ?\\"_\\n  - Translated: _\\"How many sales did we record in France?\\"_\\n- **Code:**\\n\\n```python\\ndef translate_input(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Translates user input to English using an LLM. If the input is already in English,\\n    it is returned as is. This ensures consistent input for downstream processing.\\n\\n    Args:\\n        state (GraphState): The current graph state containing user messages.\\n\\n    Returns:\\n        GraphState: The updated state with the translated input.\\n    \\"\\"\\"\\n    _logger.info(\\"Starting translation of user input to English.\\")\\n    messages = state[\\"messages\\"]\\n    user_input = messages[-1][1]  # Get the latest user input\\n\\n    # Translation prompt for the model\\n    translation_prompt = f\\"\\"\\"\\n    Translate the following text to English. If the text is already in English, repeat it exactly without any additional explanation.\\n\\n    Text:\\n    {user_input}\\n    \\"\\"\\"\\n\\n    # Call the OpenAI LLM to translate the text\\n    translated_response = llm.invoke(translation_prompt)\\n    translated_text = translated_response.content.strip()  # Access the \'content\' attribute and strip any extra spaces\\n\\n    # Update state with the translated input\\n    state[\\"translated_input\\"] = translated_text\\n    _logger.info(\\"Translation completed successfully. Translated input: %s\\", translated_text)\\n\\n    return state\\n```\\n\\n##### 2. Pre-safety Check\\n\\nThe `pre_safety_check` node ensures early detection of disallowed SQL operations and inappropriate content in the user\'s input. While the check for harmful SQL commands (e.g., `CREATE`, `DELETE`, `DROP`, `INSERT`, `UPDATE`) will occur again later in the workflow, specifically after generating the SQL query, this pre-safety check is crucial for identifying potential issues at the input stage. By doing so, it prevents unnecessary computation and offers immediate feedback to the user.\\n\\nWhile the use of a disallow list for harmful SQL operations provides a quick way to safeguard against destructive queries, maintaining a comprehensive disallow list can become hard to manage when dealing with complex SQL backends like T-SQL. An alternative approach is adopting an allowlist, restricting queries to only safe operations (e.g., `SELECT`, `JOIN`). This approach ensures a more robust solution by narrowing down permissible actions rather than attempting to block every risky command.\\n\\nTo achieve an enterprise-grade solution, the project could leverage frameworks like [Unity Catalog](https://github.com/unitycatalog/unitycatalog/blob/main/ai/core/README.md), which provide a centralized and robust approach to managing security-related functions, such as the `pre_safety_check` for AI workflows. By registering and managing reusable functions within such a framework, you can enforce consistent and reliable behavior across all AI workflows, enhancing both security and scalability.\\n\\nAdditionally, the node leverages the LLM to analyze the input for offensive or inappropriate content. If unsafe queries or inappropriate content are detected, the state is updated with an error flag and transparent feedback is provided, safeguarding the workflow from malicious or destructive elements early on.\\n\\n- **Examples:**\\n\\n1. **Disallowed Operations:**\\n\\n   - **Input:** _\\"DROP TABLE customers;\\"_\\n   - **Response:** _\\"Your query contains disallowed SQL operations and cannot be processed.\\"_\\n\\n   - **Input:** _\\"SELECT _ FROM orders;\\"\\\\*\\n   - **Response:** _\\"Query allowed.\\"_\\n\\n2. **Inappropriate Content:**\\n\\n   - **Input:** _\\"Show me orders where customers have names like \'John the Idiot\';\\"_\\n   - **Response:** _\\"Your query contains inappropriate content and cannot be processed.\\"_\\n   - **Input:** _\\"Find total sales by region.\\"_\\n   - **Response:** _\\"Input is safe to process.\\"_\\n\\n- **Code:**\\n\\n```python\\ndef pre_safety_check(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Perform safety checks on the user input to ensure that no dangerous SQL operations\\n    or inappropriate content is present. The function checks for SQL operations like\\n    DELETE, DROP, and others, and also evaluates the input for toxic or unsafe content.\\n\\n    Args:\\n        state (GraphState): The current graph state containing the translated user input.\\n\\n    Returns:\\n        GraphState: The updated state with error status and messages if any issues are found.\\n    \\"\\"\\"\\n    _logger.info(\\"Performing safety check.\\")\\n    translated_input = state[\\"translated_input\\"]\\n    messages = state[\\"messages\\"]\\n    error = \\"no\\"\\n\\n    # List of disallowed SQL operations (e.g., DELETE, DROP)\\n    disallowed_operations = [\'CREATE\', \'DELETE\', \'DROP\', \'INSERT\', \'UPDATE\', \'ALTER\', \'TRUNCATE\', \'EXEC\', \'EXECUTE\']\\n    pattern = re.compile(r\'\\\\b(\' + \'|\'.join(disallowed_operations) + r\')\\\\b\', re.IGNORECASE)\\n\\n    # Check if the input contains disallowed SQL operations\\n    if pattern.search(translated_input):\\n        _logger.warning(\\"Input contains disallowed SQL operations. Halting the workflow.\\")\\n        error = \\"yes\\"\\n        messages += [(\\"assistant\\", \\"Your query contains disallowed SQL operations and cannot be processed.\\")]\\n    else:\\n        # Check if the input contains inappropriate content\\n        safety_prompt = f\\"\\"\\"\\n        Analyze the following input for any toxic or inappropriate content.\\n\\n        Respond with only \\"safe\\" or \\"unsafe\\", and nothing else.\\n\\n        Input:\\n        {translated_input}\\n        \\"\\"\\"\\n        safety_invoke = llm.invoke(safety_prompt)\\n        safety_response = safety_invoke.content.strip().lower()  # Access the \'content\' attribute and strip any extra spaces\\n\\n        if safety_response == \\"safe\\":\\n            _logger.info(\\"Input is safe to process.\\")\\n        else:\\n            _logger.warning(\\"Input contains inappropriate content. Halting the workflow.\\")\\n            error = \\"yes\\"\\n            messages += [(\\"assistant\\", \\"Your query contains inappropriate content and cannot be processed.\\")]\\n\\n    # Update state with error status and messages\\n    state[\\"error\\"] = error\\n    state[\\"messages\\"] = messages\\n\\n    return state\\n```\\n\\n##### 3. Schema Extract\\n\\nThe `schema_extract` node dynamically retrieves the database schema, including table names and column details, by querying metadata. The formatted schema is stored in the state, enabling validation of user queries while adapting to the current database structure.\\n\\n- **Examples:**\\n  - Input: Request for schema extraction.  \\n     Schema Output:\\n    - Customers(CustomerID (INTEGER), CustomerName (TEXT), ContactName (TEXT), Address (TEXT), City (TEXT), PostalCode (TEXT), Country (TEXT))\\n    - Orders(OrderID (INTEGER), CustomerID (INTEGER), OrderDate (TEXT))\\n    - OrderDetails(OrderDetailID (INTEGER), OrderID (INTEGER), ProductID (INTEGER), Quantity (INTEGER))\\n    - Products(ProductID (INTEGER), ProductName (TEXT), Price (REAL))\\n- **Code:**\\n\\n```python\\ndef schema_extract(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Extracts the database schema, including all tables and their respective columns,\\n    from the connected SQLite database. This function retrieves the list of tables and\\n    iterates through each table to gather column definitions (name and data type).\\n\\n    Args:\\n        state (GraphState): The current graph state, which will be updated with the database schema.\\n\\n    Returns:\\n        GraphState: The updated state with the extracted database schema.\\n    \\"\\"\\"\\n    _logger.info(\\"Extracting database schema.\\")\\n\\n    # Extract the schema from the database\\n    cursor.execute(\\"SELECT name FROM sqlite_master WHERE type=\'table\';\\")\\n    tables = cursor.fetchall()\\n    schema_details = []\\n\\n    # Loop through each table and retrieve column information\\n    for table_name_tuple in tables:\\n        table_name = table_name_tuple[0]\\n        cursor.execute(f\\"PRAGMA table_info({table_name});\\")\\n        columns = cursor.fetchall()\\n\\n        # Format column definitions\\n        column_defs = \', \'.join([f\\"{col[1]} ({col[2]})\\" for col in columns])\\n        schema_details.append(f\\"- {table_name}({column_defs})\\")\\n\\n    # Save the schema in the state\\n    database_schema = \'\\\\n\'.join(schema_details)\\n    state[\\"database_schema\\"] = database_schema\\n    _logger.info(f\\"Database schema extracted:\\\\n{database_schema}\\")\\n\\n    return state\\n```\\n\\n##### 4. Context Check\\n\\nThe `context_check` node validates user queries by comparing them against the extracted database schema to ensure alignment and relevance. Queries that do not correspond to the schema are flagged as irrelevant, preventing resource waste and enabling user feedback for query reformulation.\\n\\n- **Examples:**\\n  - Input: _\\"What is the average order value?\\"_  \\n     Schema Match: Input is relevant to the database schema.\\n  - Input: _\\"Show me data from the inventory table.\\"_  \\n     Response: _\\"Your question is not related to the database and cannot be processed.\\"_\\n- **Code:**\\n\\n```python\\ndef context_check(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Checks whether the user\'s input is relevant to the database schema by comparing\\n    the user\'s question with the database schema. Uses a language model to determine if\\n    the question can be answered using the provided schema.\\n\\n    Args:\\n        state (GraphState): The current graph state, which contains the translated input\\n                            and the database schema.\\n\\n    Returns:\\n        GraphState: The updated state with error status and messages if the input is irrelevant.\\n    \\"\\"\\"\\n    _logger.info(\\"Performing context check.\\")\\n\\n    # Extract relevant data from the state\\n    translated_input = state[\\"translated_input\\"]\\n    messages = state[\\"messages\\"]\\n    error = \\"no\\"\\n    database_schema = state[\\"database_schema\\"]  # Get the schema from the state\\n\\n    # Use the LLM to determine if the input is relevant to the database schema\\n    context_prompt = f\\"\\"\\"\\n    Determine whether the following user input is a question that can be answered using the database schema provided below.\\n\\n    Respond with only \\"relevant\\" if the input is relevant to the database schema, or \\"irrelevant\\" if it is not.\\n\\n    User Input:\\n    {translated_input}\\n\\n    Database Schema:\\n    {database_schema}\\n    \\"\\"\\"\\n\\n    # Call the LLM for context check\\n    llm_invoke = llm.invoke(context_prompt)\\n    llm_response = llm_invoke.content.strip().lower()  # Access the \'content\' attribute and strip any extra spaces and lower case\\n\\n    # Process the response from the LLM\\n    if llm_response == \\"relevant\\":\\n        _logger.info(\\"Input is relevant to the database schema.\\")\\n    else:\\n        _logger.info(\\"Input is not relevant. Halting the workflow.\\")\\n        error = \\"yes\\"\\n        messages += [(\\"assistant\\", \\"Your question is not related to the database and cannot be processed.\\")]\\n\\n    # Update the state with error and messages\\n    state[\\"error\\"] = error\\n    state[\\"messages\\"] = messages\\n\\n    return state\\n```\\n\\n##### 5. Generate\\n\\nThe `generate` node constructs SQL queries from natural language input by retrieving relevant documentation from the vector store and leveraging a pre-defined SQL generation chain. It aligns the query with the user\u2019s intent and schema context, updating the state with the generated SQL and its description.\\n\\n- **Examples:**\\n  - Input: _\\"Find total sales.\\"_  \\n     Generated SQL: _\\"SELECT SUM(Products.Price \\\\* OrderDetails.Quantity) AS TotalSales FROM OrderDetails LEFT JOIN Products ON OrderDetails.ProductID = Products.ProductID;\\"_\\n  - Input: _\\"List all customers in New York.\\"_  \\n     Generated SQL: _\\"SELECT name FROM customers WHERE location = \'New York\';\\"_\\n- **Code:**\\n\\n```python\\ndef generate(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Generates an SQL query based on the user\'s input. The node retrieves relevant documents from\\n    the vector store and uses a generation chain to produce an SQL query.\\n\\n    Args:\\n        state (GraphState): The current graph state, which contains the translated input and\\n                            other relevant data such as messages and iteration count.\\n\\n    Returns:\\n        GraphState: The updated state with the generated SQL query and related messages.\\n    \\"\\"\\"\\n    _logger.info(\\"Generating SQL query.\\")\\n\\n    # Extract relevant data from the state\\n    messages = state[\\"messages\\"]\\n    iterations = state[\\"iterations\\"]\\n    translated_input = state[\\"translated_input\\"]\\n    database_schema = state[\\"database_schema\\"]\\n\\n    # Retrieve relevant documents from the vector store based on the translated user input\\n    docs = vector_store.similarity_search(translated_input, k=4)\\n    retrieved_docs = \\"\\\\n\\\\n\\".join([doc.page_content for doc in docs])\\n\\n    # Generate the SQL query using the SQL generation chain\\n    sql_solution = sql_gen_chain.invoke(\\n        {\\n            \\"retrieved_docs\\": retrieved_docs,\\n            \\"database_schema\\": database_schema,\\n            \\"messages\\": [(\\"user\\", translated_input)],\\n        }\\n    )\\n\\n    # Save the generated SQL query in the state\\n    messages += [\\n        (\\n            \\"assistant\\",\\n            f\\"{sql_solution.description}\\\\nSQL Query:\\\\n{sql_solution.sql_code}\\",\\n        )\\n    ]\\n    iterations += 1\\n\\n    # Log the generated SQL query\\n    _logger.info(\\"Generated SQL query:\\\\n%s\\", sql_solution.sql_code)\\n\\n    # Update the state with the generated SQL query and updated message list\\n    state[\\"generation\\"] = sql_solution\\n    state[\\"messages\\"] = messages\\n    state[\\"iterations\\"] = iterations\\n\\n    return state\\n```\\n\\n##### 6. Post-safety Check\\n\\nThe `post_safety_check` node ensures the generated SQL query is safe by performing a final validation for harmful SQL commands. While the earlier pre-safety check identifies disallowed operations in user inputs, this post-safety check verifies that the SQL query produced after generation adheres to security guidelines. This two-step approach ensures that even if disallowed operations are inadvertently introduced during query generation, they can be caught and flagged. If unsafe queries are detected, the node halts the workflow, updates the state with an error flag, and provides feedback to the user.\\n\\n- **Examples:**\\n\\n1. **Disallowed Operations:**\\n   - **Generated Query:** _\\"DROP TABLE orders;\\"_\\n   - **Response:** _\\"The generated SQL query contains disallowed SQL operations: DROP and cannot be processed.\\"_\\n   - **Generated Query:** _\\"SELECT name FROM customers;\\"_\\n   - **Response:** _\\"Query is valid.\\"_\\n\\n- **Code:**\\n\\n```python\\ndef post_safety_check(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Perform safety checks on the generated SQL query to ensure that it doesn\'t contain disallowed operations\\n    such as CREATE, DELETE, DROP, etc. This node checks the SQL query generated earlier in the workflow.\\n\\n    Args:\\n        state (GraphState): The current graph state containing the generated SQL query.\\n\\n    Returns:\\n        GraphState: The updated state with error status and messages if any issues are found.\\n    \\"\\"\\"\\n    _logger.info(\\"Performing post-safety check on the generated SQL query.\\")\\n\\n    # Retrieve the generated SQL query from the state\\n    sql_solution = state.get(\\"generation\\", {})\\n    sql_query = sql_solution.get(\\"sql_code\\", \\"\\").strip()\\n    messages = state[\\"messages\\"]\\n    error = \\"no\\"\\n\\n    # List of disallowed SQL operations\\n    disallowed_operations = [\'CREATE\', \'DELETE\', \'DROP\', \'INSERT\', \'UPDATE\', \'ALTER\', \'TRUNCATE\', \'EXEC\', \'EXECUTE\']\\n    pattern = re.compile(r\'\\\\b(\' + \'|\'.join(disallowed_operations) + r\')\\\\b\', re.IGNORECASE)\\n\\n    # Check if the generated SQL query contains disallowed SQL operations\\n    found_operations = pattern.findall(sql_query)\\n    if found_operations:\\n        _logger.warning(\\n            \\"Generated SQL query contains disallowed SQL operations: %s. Halting the workflow.\\",\\n            \\", \\".join(set(found_operations))\\n        )\\n        error = \\"yes\\"\\n        messages += [(\\"assistant\\", f\\"The generated SQL query contains disallowed SQL operations: {\', \'.join(set(found_operations))} and cannot be processed.\\")]\\n    else:\\n        _logger.info(\\"Generated SQL query passed the safety check.\\")\\n\\n    # Update state with error status and messages\\n    state[\\"error\\"] = error\\n    state[\\"messages\\"] = messages\\n\\n    return state\\n```\\n\\n##### 7. SQL Check\\n\\nThe `sql_check` node ensures the generated SQL query is safe and syntactically valid by executing it within a transactional savepoint. Any changes are rolled back after validation, with errors flagged and detailed feedback provided to maintain query integrity.\\n\\n- **Examples:**\\n  - Input SQL: _\\"SELECT name FROM customers WHERE city = \'New York\';\\"_  \\n     Validation: Query is valid.\\n  - Input SQL: _\\"SELECT MONTH(date) AS month, SUM(total) AS total_sales FROM orders GROUP BY MONTH(date);\\"_  \\n     Response: _\\"Your SQL query failed to execute: no such function: MONTH.\\"_\\n- **Code:**\\n\\n```python\\ndef sql_check(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Validates the generated SQL query by attempting to execute it on the database.\\n    If the query is valid, the changes are rolled back to ensure no data is modified.\\n    If there is an error during execution, the error is logged and the state is updated accordingly.\\n\\n    Args:\\n        state (GraphState): The current graph state, which contains the generated SQL query\\n                             and the messages to communicate with the user.\\n\\n    Returns:\\n        GraphState: The updated state with error status and messages if the query is invalid.\\n    \\"\\"\\"\\n    _logger.info(\\"Validating SQL query.\\")\\n\\n    # Extract relevant data from the state\\n    messages = state[\\"messages\\"]\\n    sql_solution = state[\\"generation\\"]\\n    error = \\"no\\"\\n\\n    sql_code = sql_solution.sql_code.strip()\\n\\n    try:\\n        # Start a savepoint for the transaction to allow rollback\\n        conn.execute(\'SAVEPOINT sql_check;\')\\n        # Attempt to execute the SQL query\\n        cursor.execute(sql_code)\\n        # Roll back to the savepoint to undo any changes\\n        conn.execute(\'ROLLBACK TO sql_check;\')\\n        _logger.info(\\"SQL query validation: success.\\")\\n    except Exception as e:\\n        # Roll back in case of error\\n        conn.execute(\'ROLLBACK TO sql_check;\')\\n        _logger.error(\\"SQL query validation failed. Error: %s\\", e)\\n        messages += [(\\"user\\", f\\"Your SQL query failed to execute: {e}\\")]\\n        error = \\"yes\\"\\n\\n    # Update the state with the error status\\n    state[\\"error\\"] = error\\n    state[\\"messages\\"] = messages\\n\\n    return state\\n```\\n\\n##### 8. Run Query\\n\\nThe `run_query` node executes the validated SQL query, connecting to the database to retrieve results. It updates the state with the query output, ensuring the data is formatted for further analysis or reporting while implementing robust error handling.\\n\\n- **Examples:**\\n  - Input SQL: _\\"SELECT COUNT(\\\\*) FROM Customers WHERE City = \'New York\';\\"_  \\n     Query Result: _\\"(0,)\\"_\\n  - Input SQL: _\\"SELECT SUM(Products.Price * OrderDetails.Quantity) AS TotalSales FROM OrderDetails LEFT JOIN Products ON OrderDetails.ProductID = Products.ProductID;\\"*  \\n     Query Result: _\\"(6925.0,)\\"\\\\_\\n- **Code:**\\n\\n```python\\ndef run_query(state: GraphState) -> GraphState:\\n    \\"\\"\\"\\n    Executes the generated SQL query on the database and retrieves the results if it is a SELECT query.\\n    For non-SELECT queries, commits the changes to the database. If no records are found for a SELECT query,\\n    the `no_records_found` flag is set to True.\\n\\n    Args:\\n        state (GraphState): The current graph state, which contains the generated SQL query and other relevant data.\\n\\n    Returns:\\n        GraphState: The updated state with the query results, or a flag indicating if no records were found.\\n    \\"\\"\\"\\n    _logger.info(\\"Running SQL query.\\")\\n\\n    # Extract the SQL query from the state\\n    sql_solution = state[\\"generation\\"]\\n    sql_code = sql_solution.sql_code.strip()\\n    results = None\\n    no_records_found = False  # Flag to indicate no records found\\n\\n    try:\\n        # Execute the SQL query\\n        cursor.execute(sql_code)\\n\\n        # For SELECT queries, fetch and store results\\n        if sql_code.upper().startswith(\\"SELECT\\"):\\n            results = cursor.fetchall()\\n            if not results:\\n                no_records_found = True\\n                _logger.info(\\"SQL query execution: success. No records found.\\")\\n            else:\\n                _logger.info(\\"SQL query execution: success.\\")\\n        else:\\n            # For non-SELECT queries, commit the changes\\n            conn.commit()\\n            _logger.info(\\"SQL query execution: success. Changes committed.\\")\\n    except Exception as e:\\n        _logger.error(\\"SQL query execution failed. Error: %s\\", e)\\n\\n    # Update the state with results and flag for no records found\\n    state[\\"results\\"] = results\\n    state[\\"no_records_found\\"] = no_records_found\\n\\n    return state\\n```\\n\\n##### Decision Step: Determine Next Action\\n\\nThe `decide_next_step` function acts as a control point in the workflow, deciding what action should be taken next based on the current state. It evaluates the `error` status and the number of iterations performed so far to determine if the query should be run, the workflow should be finished, or if the system should retry generating the SQL query.\\n\\n- **Process:**\\n\\n  - If there is no error (`error == \\"no\\"`), the system proceeds with running the SQL query.\\n  - If the maximum number of iterations (`max_iterations`) has been reached, the workflow ends.\\n  - If an error occurred and the maximum iterations haven\'t been reached, the system will retry the query generation.\\n\\n- **Example Workflow Decisions:**\\n\\n  - **No Error, Proceed with Query**: If no errors are found in the previous steps, the workflow moves forward to run the query.\\n  - **Maximum Iterations Reached, End Workflow**: If the workflow has already attempted a set number of times (`max_iterations`), it terminates.\\n  - **Error Detected, Retry SQL Generation**: If an error occurs and the system has not yet reached the retry limit, it will attempt to regenerate the SQL query.\\n\\n- **Code:**\\n\\n```python\\ndef decide_next_step(state: GraphState) -> str:\\n    \\"\\"\\"\\n    Determines the next step in the workflow based on the current state, including whether the query\\n    should be run, the workflow should be finished, or if the query generation needs to be retried.\\n\\n    Args:\\n        state (GraphState): The current graph state, which contains error status and iteration count.\\n\\n    Returns:\\n        str: The next step in the workflow, which can be \\"run_query\\", \\"generate\\", or END.\\n    \\"\\"\\"\\n    _logger.info(\\"Deciding next step based on current state.\\")\\n\\n    error = state[\\"error\\"]\\n    iterations = state[\\"iterations\\"]\\n\\n    if error == \\"no\\":\\n        _logger.info(\\"Error status: no. Proceeding with running the query.\\")\\n        return \\"run_query\\"\\n    elif iterations >= max_iterations:\\n        _logger.info(\\"Maximum iterations reached. Ending the workflow.\\")\\n        return END\\n    else:\\n        _logger.info(\\"Error detected. Retrying SQL query generation.\\")\\n        return \\"generate\\"\\n```\\n\\n#### Workflow Orchestration and Conditional Logic\\n\\nTo define the orchestration of tasks in our system, we construct a workflow graph using the `StateGraph` class. Each task is represented as a node, and the transitions between tasks are defined as edges. Conditional edges are used to control the flow based on the state of the workflow.\\n\\n```python\\ndef get_workflow(conn, cursor, vector_store):\\n    \\"\\"\\"Define and compile the LangGraph workflow.\\"\\"\\"\\n\\n    # Max iterations: defines how many times the workflow should retry in case of errors\\n    max_iterations = 3\\n\\n    # SQL generation chain: this is a chain that will generate SQL based on retrieved docs\\n    sql_gen_chain = get_sql_gen_chain()\\n\\n    # Initialize OpenAI LLM for translation and safety checks\\n    llm = ChatOpenAI(temperature=0, model=\\"gpt-4o-mini\\")\\n\\n    # Define the individual nodes of the workflow\\n    ... # Insert nodes code defined above here\\n\\n    # Build the workflow graph\\n    workflow = StateGraph(GraphState)\\n\\n    # Define workflow nodes\\n    workflow.add_node(\\"translate_input\\", translate_input)  # Translate user input to structured format\\n    workflow.add_node(\\"pre_safety_check\\", pre_safety_check)  # Perform a pre-safety check on input\\n    workflow.add_node(\\"schema_extract\\", schema_extract)  # Extract the database schema\\n    workflow.add_node(\\"context_check\\", context_check)  # Validate input relevance to context\\n    workflow.add_node(\\"generate\\", generate)  # Generate SQL query\\n    workflow.add_node(\\"post_safety_check\\", post_safety_check)  # Perform a post-safety check on generated SQL query\\n    workflow.add_node(\\"sql_check\\", sql_check)  # Validate the generated SQL query\\n    workflow.add_node(\\"run_query\\", run_query)  # Execute the SQL query\\n\\n    # Define workflow edges\\n    workflow.add_edge(START, \\"translate_input\\")  # Start at the translation step\\n    workflow.add_edge(\\"translate_input\\", \\"pre_safety_check\\")  # Move to safety checks\\n\\n    # Conditional edge after safety check\\n    workflow.add_conditional_edges(\\n        \\"pre_safety_check\\",  # Start at the pre_safety_check node\\n        lambda state: \\"schema_extract\\" if state[\\"error\\"] == \\"no\\" else END,  # Decide next step\\n        {\\"schema_extract\\": \\"schema_extract\\", END: END}  # Map states to nodes\\n    )\\n\\n    workflow.add_edge(\\"schema_extract\\", \\"context_check\\")  # Proceed to context validation\\n\\n    # Conditional edge after context check\\n    workflow.add_conditional_edges(\\n        \\"context_check\\",  # Start at the context_check node\\n        lambda state: \\"generate\\" if state[\\"error\\"] == \\"no\\" else END,  # Decide next step\\n        {\\"generate\\": \\"generate\\", END: END}\\n    )\\n\\n    workflow.add_edge(\\"generate\\", \\"post_safety_check\\")  # Proceed to post-safety check\\n\\n    # Conditional edge after post-safety check\\n    workflow.add_conditional_edges(\\n            \\"post_safety_check\\",  # Start at the post_safety_check node\\n            lambda state: \\"sql_check\\" if state[\\"error\\"] == \\"no\\" else END,  # If no error, proceed to sql_check, else END\\n            {\\"sql_check\\": \\"sql_check\\", END: END},\\n        )\\n\\n    # Conditional edge after SQL validation\\n    workflow.add_conditional_edges(\\n        \\"sql_check\\",  # Start at the sql_check node\\n        decide_next_step,  # Function to determine the next step\\n        {\\n            \\"run_query\\": \\"run_query\\",  # If SQL is valid, execute the query\\n            \\"generate\\": \\"generate\\",  # If retry is needed, go back to generation\\n            END: END  # Otherwise, terminate the workflow\\n        }\\n    )\\n\\n    workflow.add_edge(\\"run_query\\", END)  # Final step is to end the workflow\\n\\n    # Compile and return the workflow application\\n    app = workflow.compile()\\n\\n    return app\\n```\\n\\n1. **Start to `translate_input`**:  \\n   The workflow begins by translating user input into a structured format.\\n2. **`translate_input` to `pre_safety_check`**:  \\n   After translation, the workflow proceeds to check the safety of the input.\\n3. **`pre_safety_check` Conditional Rule**:\\n   - If the input passes the pre-safety check (`state[\\"error\\"] == \\"no\\"`), the workflow moves to `schema_extract`.\\n   - If the input fails the pre-safety check, the workflow terminates (`END`).\\n4. **`schema_extract` to `context_check`**:  \\n   The schema is extracted, and then the workflow validates the input\u2019s relevance to the database context.\\n5. **`context_check` Conditional Rule**:\\n   - If the input is relevant (`state[\\"error\\"] == \\"no\\"`), the workflow moves to `generate`.\\n   - If not, the workflow terminates (`END`).\\n6. **`generate` to `post_safety_check`**:  \\n   The workflow generates an SQL query and sends it for validation.\\\\\\n7. **`post_safety_check` Conditional Rule**: - If the input passes the post-safety check (`state[\\"error\\"] == \\"no\\"`), the workflow moves to `sql_check`. - If the input fails the post-safety check, the workflow terminates (`END`).\\n8. **`sql_check` Conditional Rule**:\\n   - If the query is valid, the workflow proceeds to `run_query`.\\n   - If the query needs adjustments and the number of iteractions is lower than 3, the workflow loops back to `generate`.\\n   - If validation fails and the number of iteractions is higher than 3, the workflow terminates (`END`).\\n9. **`run_query` to `END`**:  \\n   Once the query is executed, the workflow ends.\\n\\nThe schema above provides a representation of the LangGraph Nodes and Edges:\\n\\n<img src={require(\\"./graph.png\\").default} alt=\\"Graph Representation of our Agent\\" width=\\"50%\\"/>\\n\\n## Logging the Model in MLflow\\n\\nNow that we have built a Multi-Lingual Query Engine using LangGraph, we are ready to log the model using MLflow\u2019s [Models from Code](https://mlflow.org/docs/latest/model/models-from-code.html). Logging the model into MLflow allows us to treat the Multi-Lingual Query Engine as a traditional ML model, enabling seamless tracking, versioning, and packaging for deployment across diverse serving infrastructures. MLflow\u2019s Models from Code strategy, where we log the code that represents the model, contrasts with MLflow object-based logging, where a model object is created, serialized, and logged as a pickle or JSON object.\\n\\n### Step 1: Create our Model from Code File\\n\\nSo far, we have defined the `get_workflow` function in a file named `workflow.py`. In this step, we will create a new file, `sql_model.py`, which introduces the `SQLGenerator` class. This script will:\\n\\n1. Import the `get_workflow` function from `workflow.py`.\\n2. Define the `SQLGenerator` class as a `PythonModel`, including a predict method that utilizes the `get_workflow` function to initialize the LangGraph workflow.\\n3. Use `mlflow.models.set_model` to designate the `SQLGenerator PythonModel` class as the model of interest for MLflow.\\n\\n```python\\nimport mlflow\\nfrom definitions import REMOTE_SERVER_URI\\nfrom workflow import get_workflow\\n\\nmlflow.set_tracking_uri(REMOTE_SERVER_URI)\\n\\n\\nclass SQLGenerator(mlflow.pyfunc.PythonModel):\\n    def predict(self, context, model_input):\\n        return get_workflow(\\n            model_input[\\"conn\\"], model_input[\\"cursor\\"], model_input[\\"vector_store\\"]\\n        )\\n\\n\\nmlflow.models.set_model(SQLGenerator())\\n```\\n\\n### Step 2: Log with MLflow Models from Code feature\\n\\nWith the SQLGenerator custom Python model defined, the next step is to log it using MLflow\'s Models from Code feature. This involves using the log model standard API specifying the path to the sql_model.py script and using the code_paths parameter to include workflow.py as a dependency. This approach ensures that all necessary code files are available when the model is loaded in a different environment or on another machine.\\n\\n```python\\nimport mlflow\\nfrom definitions import (\\n    EXPERIMENT_NAME,\\n    MODEL_ALIAS,\\n    REGISTERED_MODEL_NAME,\\n    REMOTE_SERVER_URI,\\n)\\nfrom mlflow import MlflowClient\\n\\nclient = MlflowClient(tracking_uri=REMOTE_SERVER_URI)\\n\\nmlflow.set_tracking_uri(REMOTE_SERVER_URI)\\nmlflow.set_experiment(EXPERIMENT_NAME)\\n\\nwith mlflow.start_run():\\n    logged_model_info = mlflow.pyfunc.log_model(\\n        python_model=\\"sql_model.py\\",\\n        artifact_path=\\"sql_generator\\",\\n        registered_model_name=REGISTERED_MODEL_NAME,\\n        code_paths=[\\"workflow.py\\"],\\n    )\\n\\nclient.set_registered_model_alias(\\n    REGISTERED_MODEL_NAME, MODEL_ALIAS, logged_model_info.registered_model_version\\n)\\n```\\n\\nIn the MLflow UI, the stored model includes both the `sql_model.py` and `workflow.py` scripts as artifacts within the run. This logging from code feature not only records the model\'s parameters and metrics but also captures the code defining its functionality. This ensures observability, seamless tracking, and straightforward debugging directly through the UI. However, it is crucial to ensure that sensitive elements, such as API keys or credentials, are never hardcoded into these scripts. Since the code is stored as-is, any sensitive information included may lead to token leakage and pose security risks. Instead, sensitive data should be securely managed using environment variables, secret management systems, or other secure methods.\\n\\n![model_as_code_artifact](model_as_code.png)\\n\\n## Use the Logged Multi-Lingual Query Engine in `main.py`\\n\\nAfter logging the model, it can be loaded back from the MLflow Tracking Server using the model URI and the standard `mlflow.pyfunc.load_model` API. When the model is loaded, the `workflow.py` script will be executed along with the `sql_model.py` script, ensuring that the `get_workflow` function is available when the `predict` method is called.\\n\\nExecuting the code below, we demonstrate that our Multi-Lingual Query Engine is able to perform natural language to SQL generation and query execution.\\n\\n```python\\nimport os\\nimport logging\\n\\nimport mlflow\\nfrom database import setup_database\\nfrom definitions import (\\n    EXPERIMENT_NAME,\\n    MODEL_ALIAS,\\n    REGISTERED_MODEL_NAME,\\n    REMOTE_SERVER_URI,\\n)\\nfrom dotenv import load_dotenv\\nfrom vector_store import setup_vector_store\\n\\nmlflow.set_tracking_uri(REMOTE_SERVER_URI)\\nmlflow.set_experiment(EXPERIMENT_NAME)\\nmlflow.langchain.autolog()\\n\\n# Initialize the logger\\n_logger = logging.getLogger(__name__)\\n_logger.setLevel(logging.INFO)\\nhandler = logging.StreamHandler()\\nformatter = logging.Formatter(\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\")\\nhandler.setFormatter(formatter)\\n_logger.addHandler(handler)\\n\\n\\ndef main():\\n    # Load environment variables from .env file\\n    load_dotenv()\\n\\n    # Access secrets using os.getenv\\n    os.environ[\\"OPENAI_API_KEY\\"] = os.getenv(\\"OPENAI_API_KEY\\")\\n\\n    # Setup database and vector store\\n    conn = setup_database()\\n    cursor = conn.cursor()\\n    vector_store = setup_vector_store()\\n\\n    # Load the model\\n    model_uri = f\\"models:/{REGISTERED_MODEL_NAME}@{MODEL_ALIAS}\\"\\n    model = mlflow.pyfunc.load_model(model_uri)\\n    model_input = {\\"conn\\": conn, \\"cursor\\": cursor, \\"vector_store\\": vector_store}\\n    app = model.predict(model_input)\\n\\n    # save image\\n    app.get_graph().draw_mermaid_png(\\n        output_file_path=\\"sql_agent_with_safety_checks.png\\"\\n    )\\n\\n    # Example user interaction\\n    _logger.info(\\"Welcome to the SQL Assistant!\\")\\n    while True:\\n        question = input(\\"\\\\nEnter your SQL question (or type \'exit\' to quit): \\")\\n        if question.lower() == \\"exit\\":\\n            break\\n\\n        # Initialize the state with all required keys\\n        initial_state = {\\n            \\"messages\\": [(\\"user\\", question)],\\n            \\"iterations\\": 0,\\n            \\"error\\": \\"\\",\\n            \\"results\\": None,\\n            \\"generation\\": None,\\n            \\"no_records_found\\": False,\\n            \\"translated_input\\": \\"\\",  # Initialize translated_input\\n        }\\n\\n        solution = app.invoke(initial_state)\\n\\n        # Check if an error was set during the safety check\\n        if solution[\\"error\\"] == \\"yes\\":\\n            _logger.info(\\"\\\\nAssistant Message:\\\\n\\")\\n            _logger.info(solution[\\"messages\\"][-1][1])  # Display the assistant\'s message\\n            continue  # Skip to the next iteration\\n\\n        # Extract the generated SQL query from solution[\\"generation\\"]\\n        sql_query = solution[\\"generation\\"].sql_code\\n        _logger.info(\\"\\\\nGenerated SQL Query:\\\\n\\")\\n        _logger.info(sql_query)\\n\\n        # Extract and display the query results\\n        if solution.get(\\"no_records_found\\"):\\n            _logger.info(\\"\\\\nNo records found matching your query.\\")\\n        elif \\"results\\" in solution and solution[\\"results\\"] is not None:\\n            _logger.info(\\"\\\\nQuery Results:\\\\n\\")\\n            for row in solution[\\"results\\"]:\\n                _logger.info(row)\\n        else:\\n            _logger.info(\\"\\\\nNo results returned or query did not execute successfully.\\")\\n\\n    _logger.info(\\"Goodbye!\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n\\n```\\n\\n## Project File Structure\\n\\nThe Project follows a simple file structure:\\n\\n<img src={require(\\"./file_structure.png\\").default} alt=\\"file structure\\" width=\\"60%\\"/>\\n\\n## MLflow Tracing\\n\\n[MLflow Automated Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) provides fully automated integrations with various GenAI libraries such as LangChain, OpenAI, LlamaIndex, DSPy, and AutoGen. Since our AI Workflow is built using LangGraph, we can activate automated LangChain tracing by enabling `mlflow.langchain.autolog()`.\\n\\nWith LangChain autologging, traces are automatically logged to the active MLflow experiment whenever invocation APIs are called on chains. This seamless integration ensures that every interaction is captured for monitoring and analysis.\\n\\n### Viewing Traces in MLflow\\n\\nTraces can be easily accessed by navigating to the MLflow experiment of interest and clicking on the \\"Tracing\\" tab. Once inside, selecting a specific trace provides detailed execution information.\\n\\nEach trace includes:\\n\\n1. **Execution Graphs**: Visualizations of the workflow steps.\\n2. **Inputs and Outputs**: Detailed logs of data processed at each step.\\n\\nBy leveraging MLflow Tracing, we gain granular visibility into the whole graph execution. AI workflows graphs can often feel like a black box, making it challenging to debug and understand what is happening at each step. However, with just a single line of code to enable tracing, MLflow provides clear and detailed insights into the workflow, allowing developers to effectively debug, monitor, and optimize every node of the graph ensuring that our Multi-Lingual Query Engine remains transparent, auditable, and scalable.\\n\\n![mlflow_tracing_gif](mlflow_trace.gif)\\n\\n## Conclusion\\n\\nThroughout this post, we explored the process of building and managing a Multi-Lingual Query Engine using LangGraph and MLflow. By integrating LangGraph\u2019s dynamic AI workflows with MLflow\u2019s robust lifecycle management and tracing features, we\u2019ve created a system that not only delivers accurate and efficient natural language to SQL generation and execution but is also transparent and scalable."},{"id":"mlflow-tracing-in-jupyter","metadata":{"permalink":"/mlflow-website/blog/mlflow-tracing-in-jupyter","source":"@site/blog/2024-12-20-mlflow-tracing-in-jupyter/index.md","title":"MLflow Tracing in Jupyter Notebooks","description":"Introducing MLflow Tracing\'s Jupyter integration","date":"2024-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":2.385,"hasTruncateMarker":false,"authors":[{"name":"Daniel Lok","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/daniel-yk-lok/","imageURL":"/mlflow-website/img/authors/daniel_lok.png","key":"daniel-lok","page":null}],"frontMatter":{"title":"MLflow Tracing in Jupyter Notebooks","description":"Introducing MLflow Tracing\'s Jupyter integration","slug":"mlflow-tracing-in-jupyter","authors":["daniel-lok"],"tags":["genai","mlops","tracing"],"thumbnail":"/img/blog/mlflow-tracing-in-jupyter.png"},"unlisted":false,"prevItem":{"title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","permalink":"/mlflow-website/blog/from-natural-language-to-sql"},"nextItem":{"title":"Using Bedrock Agent as an MLflow ChatModel with Tracing","permalink":"/mlflow-website/blog/bedrock-chat-model-part-1"}},"content":"![Thumbnail](mlflow-tracing-in-jupyter-title.png)\\n\\n\ud83d\ude80 We\'re excited to announce a major upgrade to the [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)\\nexperience!\\n\\nIf you\'re not familiar with MLflow Tracing, it\'s an observability tool that allows you record the inputs and\\noutputs of intermediate function executions. It\'s particularly useful in debugging GenAI applications, and MLflow has over\\na [dozen integrations with popular GenAI frameworks](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing)\\nto automatically generate traces without requiring you to change your existing code.\\n\\nAs of **MLflow 2.20**, you can now view the MLflow Trace UI directly within Jupyter notebooks, allowing\\nyou to debug your applications without having to tab out of your development environment. Context\\nswitching can often be disruptive to one\'s workflow, and this feature makes it easier to stay focused while\\nstill being able to visualize the trace data that you generate.\\n\\n<figure>\\n  ![MLflow Trace UI in Jupyter Notebook](jupyter-trace-ui.png)\\n  <figcaption style={{ textAlign: \\"center\\" }}>An example of the UI in JupyterLab</figcaption>\\n</figure>\\n\\n## Getting Started\\n\\nTo get started, you\'ll need to be using an [MLflow Tracking Server](https://mlflow.org/docs/latest/tracking/server.html).\\nUnder the hood, the MLflow client needs to make network requests in order to fetch the UI assets and trace data.\\n\\nIf you don\'t use a remote server, you can always start one locally by running the `mlflow server`\\n[CLI command](https://mlflow.org/docs/latest/tracking/server.html#start-the-tracking-server). By default,\\nthe server will start up at `http://localhost:5000`.\\n\\nIn your notebook, simply ensure that the MLflow Tracking URI is set to your tracking server, and you\'re good to go!\\n\\n```python\\nimport mlflow\\n\\n# replace this with your own URI, if it differs\\ntracking_uri = \\"http://localhost:5000\\"\\nmlflow.set_tracking_uri(tracking_uri)\\n\\n# create a new experiment to avoid cluttering the default experiment\\nexperiment = mlflow.set_experiment(\\"mlflow-trace-ui-demo\\")\\n\\n# the trace UI should now show up whenever traces are generated,\\n# for example:\\n@mlflow.trace\\ndef add(a, b):\\n  return a + b\\n\\n# running the traced function triggers the UI display\\nadd(1, 2)\\n```\\n\\nThe trace UI will show up whenever any of the following events happen:\\n\\n1. A trace is generated in the cell (via automatic tracing, or when running manually traced functions)\\n2. When a trace object is explicitly displayed (e.g. via IPython\'s `display()` function)\\n3. When the `mlflow.search_traces()` API is called\\n\\nFor a hands-on experience with this feature, please try running our\\n[**demo notebook**](https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb)!\\nThe notebook contains detailed examples of all three scenarios above, as well as a short LangChain RAG demo to\\nget a more realistic impression of how this feature will feel during your development loop.\\n\\n## Disabling and Re-enabling the Display\\n\\nThis feature is enabled by default, but it can be turned off any time by calling `mlflow.tracing.disable_notebook_display()`.\\nTo remove the displays that have already rendered, you\'ll need to re-run the cells (or simply clear the cell output).\\n\\nIf you want to re-enable the display, you can call `mlflow.tracing.enable_notebook_display()`.\\n\\n## Bug Reports and Feedback\\n\\nTo report bugs or provide feedback, please file an issue in the\\n[MLflow GitHub repo](https://github.com/mlflow/mlflow/issues). We\'re looking forward to hearing from you!"},{"id":"bedrock-chat-model-part-1","metadata":{"permalink":"/mlflow-website/blog/bedrock-chat-model-part-1","source":"@site/blog/2024-11-07-bedrock-chat-model-part-1/index.md","title":"Using Bedrock Agent as an MLflow ChatModel with Tracing","description":"A guide for using BedRock Runtime Agent with ChatModel and custom trace handling.","date":"2024-11-07T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"inline":true,"label":"bedrock","permalink":"/mlflow-website/blog/tags/bedrock"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"}],"readingTime":61.59,"hasTruncateMarker":false,"authors":[{"name":"Jas Bali","title":"Lead Specialist Solutions Architect at Databricks","url":"https://www.linkedin.com/in/jas-bali-195ba410a/","imageURL":"/mlflow-website/img/authors/jas_bali.png","key":"jas-bali","page":null}],"frontMatter":{"title":"Using Bedrock Agent as an MLflow ChatModel with Tracing","description":"A guide for using BedRock Runtime Agent with ChatModel and custom trace handling.","slug":"bedrock-chat-model-part-1","authors":["jas-bali"],"tags":["genai","pyfunc","bedrock","tracing"],"thumbnail":"/img/blog/bedrock-chatmodel.png"},"unlisted":false,"prevItem":{"title":"MLflow Tracing in Jupyter Notebooks","permalink":"/mlflow-website/blog/mlflow-tracing-in-jupyter"},"nextItem":{"title":"Building Advanced RAG with MLflow and LlamaIndex Workflow","permalink":"/mlflow-website/blog/mlflow-llama-index-workflow"}},"content":"![Thumbnail](bedrock_chatmodel.png)\\n\\n**In this blog post, we delve into the integration of AWS Bedrock Agent as a ChatModel within MLflow, focusing on\\nhow to leverage Bedrock\'s [Action Groups](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-action-create.html)\\nand [Knowledge Bases](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-kb-add.html) to build a\\nconversational AI application. The blog will guide you through setting up the Bedrock Agent, configuring\\nAction Groups to enable custom actions with Lambda, and utilizing knowledge bases for context-aware interactions.\\nA special emphasis is placed on implementing tracing within MLflow.By the end of this article, you\'ll have a good\\nunderstanding of how to combine AWS Bedrock\'s advanced features with MLflow\'s capabilities such as agent request\\ntracing, model tracking and consistent signatures for input examples.**\\n\\n## What is AWS Bedrock?\\n\\nAmazon Bedrock is a managed service by AWS that simplifies the development of generative AI applications. It provides access to a variety of foundation models (FMs) from leading AI providers through a single API, enabling developers to build and scale AI solutions securely and efficiently.\\n\\nKey Components Relevant to This Integration:\\n\\n**Bedrock Agent**: At a high level, a bedrock agent is an abstraction within bedrock that consists of a foundation model,\\naction groups and knowledge bases.\\n\\n**Action Groups**: These are customizable sets of actions that define what tasks the Bedrock Agent can perform.\\nAction Groups consist of an OpenAPI Schema and the corresponding Lambda functions that will be used to execute tool calls.\\nThe OpenAPI Schema is used to define APIs available for the agent to invoke and complete tasks.\\n\\n**Knowledge Bases**: Amazon Bedrock supports the creation of Knowledge Bases to implement\\nRetrieval Augmented Generation workflows. It consists of data sources (on S3 or webpages)\\nand a vector store that contains the embedded references to this data.\\n\\nBedrock\'s Agent execution process and the corresponding tracing for Agent instrumentation is grouped as follows:\\n\\n**Pre-processing**\\nThis step validates, contextualizes and categorizes user input.\\n\\n**Orchestration**\\nThis step handles the interpretation of user inputs, deciding when to and which tasks to perform,\\nand iteratively refines responses\\n\\n**Post-processing (Optional)**\\nThis step formats the final response before returning to the user.\\n\\n**Traces**\\nEach step above has an execution trace, which consists of rationale, actions, queries and observations at each step\\nof the agent\'s response. This includes both the inputs and outputs of action groups and knowledge base queries.\\n\\nWe will look at these traces in detail below.\\n\\n## What is a ChatModel in MLflow?\\n\\nThe [ChatModel class](https://mlflow.org/docs/latest/llms/chat-model-guide/index.html) is specifically\\ndesigned to make it easier to implement models that are compatible with\\npopular large language model (LLM) chat APIs. It enables you to seamlessly bring in your own models or agents and\\nleverage MLflow\'s functionality, even if those models aren\'t natively supported as a flavor in MLflow. Additionally,\\nIt provides default signatures, which are static for ChatModel, unlike PythonModel.\\n\\nIn the following sections, we will use ChatModel to wrap the Bedrock Agent.\\n\\nFor more detailed information about ChatModel, you can read the MLflow documentation\\n[here](https://mlflow.org/docs/latest/llms/chat-model-guide/index.html) and\\n[here](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel)\\n\\n## Setting up AWS Bedrock Agent with an Action group\\n\\nIn this section, we will deploy all components of a bedrock agent so that we can invoke it as a `ChatModel` in MLflow.\\n\\n### Prerequisites\\n\\nYou will need to setup following items (either via the AWS console or SDKs):\\n\\n- Setting up role for the agent and Lambda function. [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L148)\\n- Create/deploy the agent. [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L191)\\n  - **Important**: Save the agent ID here as we will need this below.\\n- Creating a Lambda function. [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L218)\\n- Configuring IAM permissions for agent-Lambda interaction. [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L283) and [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L297)\\n- Creating an action group to link the agent and Lambda. [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L312)\\n  - **Important**:Save the agent alias ID here as we will need this below.\\n- Deploy Bedrock agent with an alias. [Example](https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py#L342)\\n\\n<b>In our case, we are going to deploy the following example action group, which calculates the next optimal departure\\ndate for a Hohmann transfer from Earth to Mars, based on the spacecraft\'s mass and specific impulse.</b>\\n\\n### OpenAPI schema for Action Groups\\n\\nAs described above, here is the OpenAPI Schema for our example action group:\\n\\n```yaml\\nopenapi: 3.0.0\\ninfo:\\n  title: Time API\\n  version: 1.0.0\\n  description: API to get the next optimal departure date for a Hohmann transfer from Earth to Mars.\\npaths:\\n  /get-next-mars-launch-window:\\n    get:\\n      summary: Gets the next optimal launch window to Mars.\\n      description: Gets the next optimal launch window to Mars.\\n      operationId: getNextMarsLaunchWindow\\n      parameters:\\n        - name: total_mass\\n          in: query\\n          description: Total mass of the spacecraft including fuel (kg)\\n          required: true\\n          schema:\\n            type: string\\n        - name: dry_mass\\n          in: query\\n          description: Mass of the spacecraft without fuel (kg).\\n          required: true\\n          schema:\\n            type: string\\n        - name: specific_impulse\\n          in: query\\n          description: Specific impulse of the propulsion system (s).\\n          required: true\\n          schema:\\n            type: string\\n      responses:\\n        \\"200\\":\\n          description: The next optimal departure date for a Hohmann transfer from Earth to Mars, based on the spacecraft\'s mass and specific impulse.\\n          content:\\n            \\"application/json\\":\\n              schema:\\n                type: object\\n                properties:\\n                  next_launch_window:\\n                    type: string\\n                    description: Next Mars Launch Window\\n```\\n\\n### Action groups - Lamda function\\n\\nHere is the code deployment for action group\'s example Lambda:\\n\\n```python\\nimport json\\nimport math\\nfrom datetime import datetime, timedelta\\n\\n\\ndef lambda_handler(event, context):\\n    def _calculate_optimal_departure_window(\\n        total_mass, dry_mass, specific_impulse\\n    ):\\n        \\"\\"\\"\\n        Calculate the next optimal departure date for a Hohmann transfer from Earth to Mars,\\n        based on the spacecraft\'s mass and specific impulse.\\n\\n        Parameters:\\n        - total_mass (float): Total mass of the spacecraft including fuel (kg).\\n        - dry_mass (float): Mass of the spacecraft without fuel (kg).\\n        - specific_impulse (float): Specific impulse of the propulsion system (s).\\n\\n        Returns:\\n        - dict: {\\n            \'next_launch_date\': datetime,\\n            \'synodic_period_days\': float,\\n            \'transfer_time_days\': float,\\n            \'delta_v_available_m_s\': float,\\n            \'delta_v_required_m_s\': float,\\n            \'is_feasible\': bool\\n          }\\n        \\"\\"\\"\\n        current_date = None\\n        # Constants\\n        G0 = 9.80665  # m/s^2, standard gravity\\n        MU_SUN = (\\n            1.32712440018e20  # m^3/s^2, standard gravitational parameter for the Sun\\n        )\\n        AU = 1.496e11  # meters, astronomical unit\\n        EARTH_ORBITAL_PERIOD = 365.25  # days\\n        MARS_ORBITAL_PERIOD = 686.98  # days\\n        SYNODIC_PERIOD = 1 / abs((1 / EARTH_ORBITAL_PERIOD) - (1 / MARS_ORBITAL_PERIOD))\\n        TRANSFER_TIME = 259  # days, approximate duration of Hohmann transfer\\n        BASE_LAUNCH_DATE = datetime(2020, 7, 1)  # A reference past launch window date\\n\\n        # Orbital Radii (assuming circular orbits for simplicity)\\n        r1 = AU  # Earth\'s orbital radius in meters\\n        r2 = 1.524 * AU  # Mars\' orbital radius in meters\\n\\n        # Calculate Required Delta-V for Hohmann Transfer\\n        # Using vis-viva equation for Hohmann transfer\\n        def calculate_hohmann_delta_v(mu, r_start, r_end):\\n            # Velocity of departure orbit (Earth)\\n            v_start = math.sqrt(mu / r_start)\\n            # Velocity of transfer orbit at departure\\n            a_transfer = (r_start + r_end) / 2\\n            v_transfer_start = math.sqrt(mu * (2 / r_start - 1 / a_transfer))\\n            delta_v1 = v_transfer_start - v_start\\n\\n            # Velocity of arrival orbit (Mars)\\n            v_end = math.sqrt(mu / r_end)\\n            # Velocity of transfer orbit at arrival\\n            v_transfer_end = math.sqrt(mu * (2 / r_end - 1 / a_transfer))\\n            delta_v2 = v_end - v_transfer_end\\n\\n            return delta_v1, delta_v2\\n\\n        delta_v1, delta_v2 = calculate_hohmann_delta_v(MU_SUN, r1, r2)\\n        delta_v_required = abs(delta_v1) + abs(delta_v2)  # Total delta-v in m/s\\n\\n        # Delta-V using Tsiolkovsky Rocket Equation\\n        if dry_mass <= 0 or total_mass <= dry_mass:\\n            raise ValueError(\\"Total mass must be greater than dry mass.\\")\\n\\n        delta_v_available = (\\n            specific_impulse * G0 * math.log(total_mass / dry_mass)\\n        )  # m/s\\n\\n        is_feasible = delta_v_available >= delta_v_required\\n\\n        if current_date is None:\\n            current_date = datetime.now()\\n\\n        days_since_base = (current_date - BASE_LAUNCH_DATE).days\\n        if days_since_base < 0:\\n            # Current date is before the base launch date\\n            next_launch_date = BASE_LAUNCH_DATE\\n        else:\\n            synodic_periods_passed = days_since_base / SYNODIC_PERIOD\\n            synodic_periods_passed_int = math.floor(synodic_periods_passed)\\n            next_launch_date = BASE_LAUNCH_DATE + timedelta(\\n                days=(synodic_periods_passed_int + 1) * SYNODIC_PERIOD\\n            )\\n\\n        next_launch_date = next_launch_date.replace(\\n            hour=0, minute=0, second=0, microsecond=0\\n        )\\n\\n        return {\\n            \\"next_launch_date\\": next_launch_date,\\n            \\"synodic_period_days\\": SYNODIC_PERIOD,\\n            \\"transfer_time_days\\": TRANSFER_TIME,\\n            \\"delta_v_available_m_s\\": delta_v_available,\\n            \\"delta_v_required_m_s\\": delta_v_required,\\n            \\"is_feasible\\": is_feasible,\\n        }\\n\\n    query_params = {\\n        event[\\"name\\"]: event[\\"value\\"] for event in event.get(\\"parameters\\", [])\\n    }\\n\\n    total_mass = float(query_params.get(\\"total_mass\\"))\\n    dry_mass = float(query_params.get(\\"dry_mass\\"))\\n    specific_impulse = float(query_params.get(\\"specific_impulse\\"))\\n\\n    response = {\\n        \\"next_launch_window\\": _calculate_optimal_departure_window(\\n            total_mass, dry_mass, specific_impulse\\n        )\\n    }\\n\\n    response_body = {\\"application/json\\": {\\"body\\": json.dumps(response)}}\\n\\n    action_response = {\\n        \\"actionGroup\\": event[\\"actionGroup\\"],\\n        \\"apiPath\\": event[\\"apiPath\\"],\\n        \\"httpMethod\\": event[\\"httpMethod\\"],\\n        \\"httpStatusCode\\": 200,\\n        \\"responseBody\\": response_body,\\n    }\\n\\n    session_attributes = event[\\"sessionAttributes\\"]\\n    prompt_session_attributes = event[\\"promptSessionAttributes\\"]\\n\\n    return {\\n        \\"messageVersion\\": \\"1.0\\",\\n        \\"response\\": action_response,\\n        \\"sessionAttributes\\": session_attributes,\\n        \\"promptSessionAttributes\\": prompt_session_attributes,\\n    }\\n```\\n\\nNext, we are going to wrap Bedrock agent as a ChatModel so that we can register and load it for inference.\\n\\n## Writing ChatModel for Bedrock agent\\n\\nHere are the top-level packages used for running the following example locally in **Python 3.12.7**:\\n\\n```text\\nboto3==1.35.31\\nmlflow==2.16.2\\n```\\n\\n### Implementing Bedrock Agent as an MLflow ChatModel with Tracing\\n\\n```python\\nimport copy\\nimport os\\nimport uuid\\nfrom typing import List, Optional\\n\\nimport boto3\\nimport mlflow\\nfrom botocore.config import Config\\nfrom mlflow.entities import SpanType\\nfrom mlflow.pyfunc import ChatModel\\nfrom mlflow.types.llm import ChatResponse, ChatMessage, ChatParams, ChatChoice\\n\\n\\nclass BedrockModel(ChatModel):\\n    def __init__(self):\\n        \\"\\"\\"\\n        Initializes the BedrockModel instance with placeholder values.\\n\\n        Note:\\n            The `load_context` method cannot create new instance variables; it can only modify existing ones.\\n            Therefore, all instance variables should be defined in the `__init__` method with placeholder values.\\n        \\"\\"\\"\\n        self.brt = None\\n        self._main_bedrock_agent = None\\n        self._bedrock_agent_id = None\\n        self._bedrock_agent_alias_id = None\\n        self._inference_configuration = None\\n        self._agent_instruction = None\\n        self._model = None\\n        self._aws_region = None\\n\\n    def __getstate__(self):\\n        \\"\\"\\"\\n        Prepares the instance state for pickling.\\n\\n        This method is needed because the `boto3` client (`self.brt`) cannot be pickled.\\n        By excluding `self.brt` from the state, we ensure that the model can be serialized and deserialized properly.\\n        \\"\\"\\"\\n        # Create a dictionary of the instance\'s state, excluding the boto3 client\\n        state = self.__dict__.copy()\\n        del state[\\"brt\\"]\\n        return state\\n\\n    def __setstate__(self, state):\\n        \\"\\"\\"\\n        Restores the instance state during unpickling.\\n\\n        This method is needed to reinitialize the `boto3` client (`self.brt`) after the instance is unpickled,\\n        because the client was excluded during pickling.\\n        \\"\\"\\"\\n        self.__dict__.update(state)\\n        self.brt = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Initializes the Bedrock client with AWS credentials.\\n\\n        Args:\\n            context: The MLflow context containing model configuration.\\n\\n        Note:\\n            Dependent secret variables must be in the execution environment prior to loading the model;\\n            else they will not be available during model initialization.\\n        \\"\\"\\"\\n        self._main_bedrock_agent = context.model_config.get(\\"agents\\", {}).get(\\n            \\"main\\", {}\\n        )\\n        self._bedrock_agent_id = self._main_bedrock_agent.get(\\"bedrock_agent_id\\")\\n        self._bedrock_agent_alias_id = self._main_bedrock_agent.get(\\n            \\"bedrock_agent_alias_id\\"\\n        )\\n        self._inference_configuration = self._main_bedrock_agent.get(\\n            \\"inference_configuration\\"\\n        )\\n        self._agent_instruction = self._main_bedrock_agent.get(\\"instruction\\")\\n        self._model = self._main_bedrock_agent.get(\\"model\\")\\n        self._aws_region = self._main_bedrock_agent.get(\\"aws_region\\")\\n\\n        # Initialize the Bedrock client\\n        self.brt = boto3.client(\\n            service_name=\\"bedrock-agent-runtime\\",\\n            config=Config(region_name=self._aws_region),\\n            aws_access_key_id=os.environ[\\"AWS_ACCESS_KEY\\"],\\n            aws_secret_access_key=os.environ[\\"AWS_SECRET_ACCESS_KEY\\"],\\n            aws_session_token=os.environ[\\"AWS_SESSION_TOKEN\\"],\\n            region_name=self._aws_region,\\n        )\\n\\n    @staticmethod\\n    def _extract_trace_groups(events):\\n        \\"\\"\\"\\n        Extracts trace groups from a list of events based on their trace IDs.\\n\\n        Args:\\n            events (list): A list of event dictionaries.\\n\\n        Returns:\\n            dict: A dictionary where keys are trace IDs and values are lists of trace items.\\n        \\"\\"\\"\\n        from collections import defaultdict\\n\\n        trace_groups = defaultdict(list)\\n\\n        def find_trace_ids(obj, original_trace, depth=0, parent_key=None):\\n            if depth > 5:\\n                return  # Stop recursion after 5 levels if no traceId has been found\\n            if isinstance(obj, dict):\\n                trace_id = obj.get(\\"traceId\\")\\n                if trace_id:\\n                    # Include the parent key as the \'type\'\\n                    item = {\\n                        \\"type\\": parent_key,\\n                        \\"data\\": obj,\\n                        \\"event_order\\": original_trace.get(\\"trace\\", {}).get(\\n                            \\"event_order\\"\\n                        ),\\n                    }\\n                    trace_groups[trace_id].append(item)\\n                else:\\n                    for key, value in obj.items():\\n                        find_trace_ids(\\n                            value, original_trace, depth=depth + 1, parent_key=key\\n                        )\\n            elif isinstance(obj, list):\\n                for item in obj:\\n                    find_trace_ids(item, item, depth=depth + 1, parent_key=parent_key)\\n\\n        find_trace_ids(events, {})\\n        return dict(trace_groups)\\n\\n    @staticmethod\\n    def _get_final_response_with_trace(trace_id_groups: dict[str, list[dict]]):\\n        \\"\\"\\"\\n        Processes trace groups to extract the final response and create relevant MLflow spans.\\n\\n        Args:\\n            trace_id_groups (dict): A dictionary of trace groups keyed by trace IDs.\\n\\n        Returns:\\n            str: The final response text extracted from the trace groups.\\n        \\"\\"\\"\\n        trace_id_groups_copy = copy.deepcopy(trace_id_groups)\\n        model_invocation_input_key = \\"modelInvocationInput\\"\\n\\n        def _create_trace_by_type(\\n            trace_name, _trace_id, context_input, optional_rationale_subtrace\\n        ):\\n            @mlflow.trace(\\n                name=trace_name,\\n                attributes={\\"trace_attributes\\": trace_id_groups[_trace_id]},\\n            )\\n            def _trace_agent_pre_context(inner_input_trace):\\n                return optional_rationale_subtrace.get(\\"data\\", {}).get(\\"text\\")\\n\\n            trace_id_groups_copy[_trace_id].remove(context_input)\\n            _trace_agent_pre_context(context_input.get(\\"data\\", {}).get(\\"text\\"))\\n\\n        def _extract_action_group_trace(\\n            _trace_id, trace_group, action_group_invocation_input: dict\\n        ):\\n            @mlflow.trace(\\n                name=\\"action-group-invocation\\",\\n                attributes={\\"trace_attributes\\": trace_id_groups[_trace_id]},\\n            )\\n            def _action_group_trace(inner_trace_group):\\n                for _trace in trace_group:\\n                    action_group_invocation_output = _trace.get(\\"data\\", {}).get(\\n                        \\"actionGroupInvocationOutput\\"\\n                    )\\n                    if action_group_invocation_output is not None:\\n                        action_group_response = str(\\n                            {\\n                                \\"action_group_name\\": action_group_invocation_input.get(\\n                                    \\"actionGroupName\\"\\n                                ),\\n                                \\"api_path\\": action_group_invocation_input.get(\\n                                    \\"apiPath\\"\\n                                ),\\n                                \\"execution_type\\": action_group_invocation_input.get(\\n                                    \\"executionType\\"\\n                                ),\\n                                \\"execution_output\\": action_group_invocation_output.get(\\n                                    \\"text\\"\\n                                ),\\n                            }\\n                        )\\n                        trace_group.remove(_trace)\\n                        return action_group_response\\n\\n            _action_group_trace(str(action_group_invocation_input))\\n\\n        def _extract_knowledge_base_trace(\\n            _trace_id, trace_group, knowledge_base_lookup_input\\n        ):\\n            @mlflow.trace(\\n                name=\\"knowledge-base-lookup\\",\\n                attributes={\\"trace_attributes\\": trace_id_groups[_trace_id]},\\n            )\\n            def _knowledge_base_trace(inner_trace_group):\\n                for _trace in trace_group:\\n                    knowledge_base_lookup_output = _trace.get(\\"data\\", {}).get(\\n                        \\"knowledgeBaseLookupOutput\\"\\n                    )\\n                    if knowledge_base_lookup_output is not None:\\n                        knowledge_base_response = str(\\n                            {\\n                                \\"knowledge_base_id\\": knowledge_base_lookup_input.get(\\n                                    \\"knowledgeBaseId\\"\\n                                ),\\n                                \\"text\\": knowledge_base_lookup_input.get(\\"text\\"),\\n                                \\"retrieved_references\\": knowledge_base_lookup_output.get(\\n                                    \\"retrievedReferences\\"\\n                                ),\\n                            }\\n                        )\\n                        trace_group.remove(_trace)\\n                        return knowledge_base_response\\n\\n            _knowledge_base_trace(str(trace_group))\\n\\n        def _trace_group_type(\\n            _trace_id, trace_group, _trace, optional_rationale_subtrace\\n        ):\\n            trace_name = \\"observation\\"\\n            pre_processing_trace_id_suffix = \\"-pre\\"\\n            if pre_processing_trace_id_suffix in _trace_id:\\n                trace_name = \\"agent-initial-context\\"\\n            else:\\n                for _inner_trace in trace_group:\\n                    action_group_invocation_input = _inner_trace.get(\\"data\\", {}).get(\\n                        \\"actionGroupInvocationInput\\"\\n                    )\\n                    if action_group_invocation_input is not None:\\n                        action_group_name = action_group_invocation_input.get(\\n                            \\"actionGroupName\\"\\n                        )\\n                        trace_name = f\\"ACTION-GROUP-{action_group_name}\\"\\n                        _create_trace_by_type(\\n                            trace_name, _trace_id, _trace, optional_rationale_subtrace\\n                        )\\n                        _extract_action_group_trace(\\n                            _trace_id, trace_group, action_group_invocation_input\\n                        )\\n                        trace_group.remove(_trace)\\n                    knowledge_base_lookup_input = _inner_trace.get(\\"data\\", {}).get(\\n                        \\"knowledgeBaseLookupInput\\"\\n                    )\\n                    if knowledge_base_lookup_input is not None:\\n                        knowledge_base_id = knowledge_base_lookup_input.get(\\n                            \\"knowledgeBaseId\\"\\n                        )\\n                        trace_name = f\\"KNOWLEDGE_BASE_{knowledge_base_id}\\"\\n                        _create_trace_by_type(\\n                            trace_name, _trace_id, _trace, optional_rationale_subtrace\\n                        )\\n                        _extract_knowledge_base_trace(\\n                            _trace_id, trace_group, knowledge_base_lookup_input\\n                        )\\n                        trace_group.remove(_trace)\\n            return trace_name\\n\\n        for _trace_id, _trace_group in trace_id_groups_copy.items():\\n            trace_group = sorted(_trace_group, key=lambda tg: tg[\\"event_order\\"])\\n            model_invocation_input_subtrace = None\\n            optional_rationale_subtrace = None\\n            for _trace in _trace_group:\\n                if model_invocation_input_key == _trace.get(\\"type\\", \\"\\"):\\n                    model_invocation_input_subtrace = _trace\\n                elif \\"rationale\\" == _trace.get(\\"type\\", \\"\\"):\\n                    optional_rationale_subtrace = _trace\\n            _trace_group_type(\\n                _trace_id,\\n                trace_group,\\n                model_invocation_input_subtrace,\\n                optional_rationale_subtrace,\\n            )\\n\\n        final_response = (\\n            list(trace_id_groups_copy.values())[-1][-1]\\n            .get(\\"data\\", {})\\n            .get(\\"finalResponse\\", {})\\n            .get(\\"text\\")\\n        )\\n        return final_response\\n\\n    @mlflow.trace(name=\\"Bedrock Input Prompt\\")\\n    def _get_agent_prompt(self, raw_input_question):\\n        \\"\\"\\"\\n        Constructs the agent prompt by combining the input question and the agent instruction.\\n\\n        Args:\\n            raw_input_question (str): The user\'s input question.\\n\\n        Returns:\\n            str: The formatted agent prompt.\\n        \\"\\"\\"\\n        return f\\"\\"\\"\\n        Answer the following question and pay strong attention to the prompt:\\n        <question>\\n        {raw_input_question}\\n        </question>\\n        <instruction>\\n        {self._agent_instruction}\\n        </instruction>\\n        \\"\\"\\"\\n\\n    @mlflow.trace(name=\\"bedrock-agent\\", span_type=SpanType.CHAT_MODEL)\\n    def predict(\\n        self, context, messages: List[ChatMessage], params: Optional[ChatParams]\\n    ) -> ChatResponse:\\n        \\"\\"\\"\\n        Makes a prediction using the Bedrock agent and processes the response.\\n\\n        Args:\\n            context: The MLflow context.\\n            messages (List[ChatMessage]): A list of chat messages.\\n            params (Optional[ChatParams]): Optional parameters for the chat.\\n\\n        Returns:\\n            ChatResponse: The response from the Bedrock agent.\\n        \\"\\"\\"\\n        formatted_input = messages[-1].content\\n        session_id = uuid.uuid4().hex\\n\\n        response = self.brt.invoke_agent(\\n            agentId=self._bedrock_agent_id,\\n            agentAliasId=self._bedrock_agent_alias_id,\\n            inputText=self._get_agent_prompt(formatted_input),\\n            enableTrace=True,\\n            sessionId=session_id,\\n            endSession=False,\\n        )\\n\\n        # Since this provider\'s output doesn\'t match the OpenAI specification,\\n        # we need to go through the returned trace data and map it appropriately\\n        # to create the MLflow span object.\\n        events = []\\n        for index, event in enumerate(response.get(\\"completion\\", [])):\\n            if \\"trace\\" in event:\\n                event[\\"trace\\"][\\"event_order\\"] = index\\n            events.append(event)\\n        trace_id_groups = self._extract_trace_groups(events)\\n        final_response = self._get_final_response_with_trace(trace_id_groups)\\n        with mlflow.start_span(\\n            name=\\"retrieved-response\\", span_type=SpanType.AGENT\\n        ) as span:\\n            span.set_inputs(messages)\\n            span.set_attributes({})\\n\\n            output = ChatResponse(\\n                choices=[\\n                    ChatChoice(\\n                        index=0,\\n                        message=ChatMessage(role=\\"user\\", content=final_response),\\n                    )\\n                ],\\n                usage={},\\n                model=self._model,\\n            )\\n\\n            span.set_outputs(output)\\n\\n        return output\\n```\\n\\nHere are some important remarks about this `BedrockModel` implementation:\\n\\n- AWS access key ID, secret key and the session token are externalized here. These need to be present in the environment before we can run inference.\\n  You will need to generate it for your IAM user and set them as environment variables.\\n\\n```bash\\naws sts get-session-token --duration-seconds 3600\\n```\\n\\nAnd then set the following:\\n\\n```python\\nimport os\\n\\nos.environ[\'AWS_ACCESS_KEY\'] = \\"<AccessKeyId>\\"\\nos.environ[\'AWS_SECRET_ACCESS_KEY\'] = \\"<SecretAccessKey>\\"\\nos.environ[\'AWS_SESSION_TOKEN\'] = \\"<SessionToken>\\"\\n\\n```\\n\\nAs noticed in the code above, these do not get logged with the model and are only set inside `load_context`.\\nThis method is called when ChatModel is constructed. Further details are [here](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel.load_context)\\n\\n- Bedrock agent ID and agent alias ID are passed via `model_config` that we will use below.\\n\\n- boto3 module has been excluded from getting pickled. This is done via `__getstate__` and `__setstate__` where we exclude it and reset it respectively\\n\\n### Log and load the BedrockModel\\n\\n```python\\nimport mlflow\\nfrom mlflow.models import infer_signature\\n\\ninput_example = [\\n    {\\n        \\"messages\\": [\\n            {\\n                \\"role\\": \\"user\\",\\n                \\"content\\": \\"When is the next launch window for Mars?\\",\\n            }\\n        ]\\n    }\\n]\\n\\noutput_example = {\\n    \\"choices\\": [\\n        {\\n            \\"index\\": 0,\\n            \\"finish_reason\\": \\"stop\\",\\n            \\"message\\": {\\"role\\": \\"assistant\\", \\"content\\": \\"test content\\"},\\n        }\\n    ]\\n}\\nsignature = infer_signature(input_example, output_example)\\n\\nwith mlflow.start_run():\\n\\n    model_config = {\\n        \\"agents\\": {\\n            \\"main\\": {\\n                \\"model\\": \\"anthropic.claude-v2\\",\\n                \\"aws_region\\": \\"us-east-1\\",\\n                \\"bedrock_agent_id\\": \\"O9KQSEVEFF\\",\\n                \\"bedrock_agent_alias_id\\": \\"3WHEEJKNUT\\",\\n                \\"instruction\\": (\\n                    \\"You have functions available at your disposal to use when anwering any questions about orbital mechanics.\\"\\n                    \\"if you can\'t find a function to answer a question about orbital mechanics, simply reply \\"\\n                    \\"\'I do not know\'\\"\\n                ),\\n                \\"inference_configuration\\": {\\n                    \\"temperature\\": 0.5,\\n                    \\"maximumLength\\": 2000,\\n                },\\n            },\\n        },\\n    }\\n\\n    # Input example for the model\\n    input_example = {\\n        \\"messages\\": [\\n            {\\n                \\"role\\": \\"user\\",\\n                \\"content\\": \\"When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\",\\n            }\\n        ]\\n    }\\n\\n    # Log and load the model using MLflow\\n    logged_chain_info = mlflow.pyfunc.log_model(\\n        python_model=BedrockModel(),\\n        model_config=model_config,\\n        artifact_path=\\"chain\\",  # This string is used as the path inside the MLflow model where artifacts are stored\\n        input_example=input_example,  # Must be a valid input to your chain\\n    )\\n\\nloaded = mlflow.pyfunc.load_model(logged_chain_info.model_uri)\\n\\n# Predict using the loaded model\\nresponse = loaded.predict(\\n    {\\n        \\"messages\\": [\\n            {\\n                \\"role\\": \\"user\\",\\n                \\"content\\": \\"When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\",\\n            }\\n        ]\\n    }\\n)\\nprint(response)\\n```\\n\\n```text\\n\\n```\\n\\n### Mapping Bedrock Agent Trace Data to MLflow Span Objects\\n\\nIn this step, we need to iterate over the data that is returned within the bedrock agent\'s response trace\\nto provide relevant mappings to create the MLflow span object.\\nAWS Bedrock agent\'s response is a flat list with trace events connected by `traceId`.\\nHere is the raw trace sent in the bedrock agent\'s response:\\n\\n<details>\\n<summary>Expand to see AWS Bedrock agent\'s raw trace</summary>\\n```text\\n[\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 0,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'preProcessingTrace\': {\\n          \'modelInvocationInput\': {\\n            \'inferenceConfiguration\': {\\n              ...\\n            },\\n            \'text\': \'\\\\n\\\\nHuman: You are a classifying agent that filters user inputs into categories. Your job is to sort these inputs before they...<thinking> XML tags before providing only the category letter to sort the input into within <category> XML tags.\\\\n\\\\nAssistant:\',\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-pre-0\',\\n            \'type\': \'PRE_PROCESSING\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 1,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'preProcessingTrace\': {\\n          \'modelInvocationOutput\': {\\n            \'parsedResponse\': {\\n              ...\\n            },\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-pre-0\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 2,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'modelInvocationInput\': {\\n            \'inferenceConfiguration\': {\\n              ...\\n            },\\n            \'text\': \'\\\\n\\\\nHuman:\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>...\\\\n\\\\nAssistant: <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\n\\\\n\',\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\',\\n            \'type\': \'ORCHESTRATION\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 3,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'modelInvocationOutput\': {\\n            \'metadata\': {\\n              ...\\n            },\\n            \'rawResponse\': {\\n              ...\\n            },\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 4,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'rationale\': {\\n            \'text\': \'To answer this question about the next Mars launch window, I will:\\\\n\\\\n1. Call the GET::optimal_departure_window_mars::getNext...lse values.\\\\n\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\',\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 5,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'invocationInput\': {\\n            \'actionGroupInvocationInput\': {\\n              ...\\n            },\\n            \'invocationType\': \'ACTION_GROUP\',\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 6,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'observation\': {\\n            \'actionGroupInvocationOutput\': {\\n              ...\\n            },\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\',\\n            \'type\': \'ACTION_GROUP\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 7,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'modelInvocationInput\': {\\n            \'inferenceConfiguration\': {\\n              ...\\n            },\\n            \'text\': \'\\\\n\\\\nHuman:\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>...lta_v_available_m_s\\": 39457.985759929674, \\"delta_v_required_m_s\\": 5595.997417810693, \\"is_feasible\\": true}}</function_result>\\\\n\',\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\',\\n            \'type\': \'ORCHESTRATION\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 8,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'modelInvocationOutput\': {\\n            \'metadata\': {\\n              ...\\n            },\\n            \'rawResponse\': {\\n              ...\\n            },\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'trace\': {\\n      \'agentAliasId\': \'3WHEEJKNUT\',\\n      \'agentId\': \'O9KQSEVEFF\',\\n      \'agentVersion\': \'1\',\\n      \'event_order\': 9,\\n      \'sessionId\': \'9566a6d78551434fb0409578ffed63c1\',\\n      \'trace\': {\\n        \'orchestrationTrace\': {\\n          \'observation\': {\\n            \'finalResponse\': {\\n              ...\\n            },\\n            \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\',\\n            \'type\': \'FINISH\'\\n          }\\n        }\\n      }\\n    }\\n  },\\n  {\\n    \'chunk\': {\\n      \'bytes\': b\\n      \'Based on the provided spacecraft dry mass of 10000 kg, total mass of 50000 kg, and specific impulse of 2500 s, the next optimal launch window for a Hohmann transfer from Earth to Mars is on November 26, 2026 UTC. The transfer will take 259 days.\'\\n    }\\n  }\\n]\\n```\\n</details>\\n\\nTo fit this structure into MLflow\'s span, we first need to go through the raw response trace and group events by their `traceId`.\\nAfter grouping the trace events by _`traceId`_, the structure looks like this:\\n\\n<details>\\n<summary>Expand to see trace grouped by _`traceId`_</summary>\\n```text\\n{\\n  \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\': [\\n    {\\n      \'data\': {\\n        \'inferenceConfiguration\': {\\n          \'maximumLength\': 2048,\\n          \'stopSequences\': [\\n            \'</function_call>\',\\n            \'</answer>\',\\n            \'</error>\'\\n          ],\\n          \'temperature\': 0.0,\\n          \'topK\': 250,\\n          \'topP\': 1.0\\n        },\\n        \'text\': \'\\\\n\\\\nHuman:\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>...\\\\n\\\\nAssistant: <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\n\\\\n\',\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\',\\n        \'type\': \'ORCHESTRATION\'\\n      },\\n      \'event_order\': 2,\\n      \'type\': \'modelInvocationInput\'\\n    },\\n    {\\n      \'data\': {\\n        \'metadata\': {\\n          \'usage\': {\\n            \'inputTokens\': 5160,\\n            \'outputTokens\': 135\\n          }\\n        },\\n        \'rawResponse\': {\\n          \'content\': \'To answer this question about the next Mars launch window, I will:\\\\n\\\\n1. Call the GET::optimal_departure_window_mars::getNext...l>\\\\nGET::optimal_departure_window_mars::getNextMarsLaunchWindow(specific_impulse=\\"2500\\", dry_mass=\\"10000\\", total_mass=\\"50000\\")\'\\n        },\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\'\\n      },\\n      \'event_order\': 3,\\n      \'type\': \'modelInvocationOutput\'\\n    },\\n    {\\n      \'data\': {\\n        \'text\': \'To answer this question about the next Mars launch window, I will:\\\\n\\\\n1. Call the GET::optimal_departure_window_mars::getNext...lse values.\\\\n\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\',\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\'\\n      },\\n      \'event_order\': 4,\\n      \'type\': \'rationale\'\\n    },\\n    {\\n      \'data\': {\\n        \'actionGroupInvocationInput\': {\\n          \'actionGroupName\': \'optimal_departure_window_mars\',\\n          \'apiPath\': \'/get-next-mars-launch-window\',\\n          \'executionType\': \'LAMBDA\',\\n          \'parameters\': [\\n            {\\n              ...\\n            },\\n            {\\n              ...\\n            },\\n            {\\n              ...\\n            }\\n          ],\\n          \'verb\': \'get\'\\n        },\\n        \'invocationType\': \'ACTION_GROUP\',\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\'\\n      },\\n      \'event_order\': 5,\\n      \'type\': \'invocationInput\'\\n    },\\n    {\\n      \'data\': {\\n        \'actionGroupInvocationOutput\': {\\n          \'text\': \'{\\"next_launch_window\\": {\\"next_launch_date\\": \\"2026-11-26 00:00:00\\", \\"synodic_period_days\\": 779.9068939794238, \\"transfer_time_days\\": 259, \\"delta_v_available_m_s\\": 39457.985759929674, \\"delta_v_required_m_s\\": 5595.997417810693, \\"is_feasible\\": true}}\'\\n        },\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-0\',\\n        \'type\': \'ACTION_GROUP\'\\n      },\\n      \'event_order\': 6,\\n      \'type\': \'observation\'\\n    }\\n  ],\\n  \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\': [\\n    {\\n      \'data\': {\\n        \'inferenceConfiguration\': {\\n          \'maximumLength\': 2048,\\n          \'stopSequences\': [\\n            \'</function_call>\',\\n            \'</answer>\',\\n            \'</error>\'\\n          ],\\n          \'temperature\': 0.0,\\n          \'topK\': 250,\\n          \'topP\': 1.0\\n        },\\n        \'text\': \'\\\\n\\\\nHuman:\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>...lta_v_available_m_s\\": 39457.985759929674, \\"delta_v_required_m_s\\": 5595.997417810693, \\"is_feasible\\": true}}</function_result>\\\\n\',\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\',\\n        \'type\': \'ORCHESTRATION\'\\n      },\\n      \'event_order\': 7,\\n      \'type\': \'modelInvocationInput\'\\n    },\\n    {\\n      \'data\': {\\n        \'metadata\': {\\n          \'usage\': {\\n            \'inputTokens\': 5405,\\n            \'outputTokens\': 64\\n          }\\n        },\\n        \'rawResponse\': {\\n          \'content\': \'<answer>\\\\nBased on the provided spacecraft dry mass of 10000 kg, total mass of 50000 kg, and specific impulse of 2500 s, the ... optimal launch window for a Hohmann transfer from Earth to Mars is on November 26, 2026 UTC. The transfer will take 259 days.\'\\n        },\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\'\\n      },\\n      \'event_order\': 8,\\n      \'type\': \'modelInvocationOutput\'\\n    },\\n    {\\n      \'data\': {\\n        \'finalResponse\': {\\n          \'text\': \'Based on the provided spacecraft dry mass of 10000 kg, total mass of 50000 kg, and specific impulse of 2500 s, the next optimal launch window for a Hohmann transfer from Earth to Mars is on November 26, 2026 UTC. The transfer will take 259 days.\'\\n        },\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-1\',\\n        \'type\': \'FINISH\'\\n      },\\n      \'event_order\': 9,\\n      \'type\': \'observation\'\\n    }\\n  ],\\n  \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-pre-0\': [\\n    {\\n      \'data\': {\\n        \'inferenceConfiguration\': {\\n          \'maximumLength\': 2048,\\n          \'stopSequences\': [\\n            \'\\\\n\\\\nHuman:\'\\n          ],\\n          \'temperature\': 0.0,\\n          \'topK\': 250,\\n          \'topP\': 1.0\\n        },\\n        \'text\': \'\\\\n\\\\nHuman: You are a classifying agent that filters user inputs into categories. Your job is to sort these inputs before they...<thinking> XML tags before providing only the category letter to sort the input into within <category> XML tags.\\\\n\\\\nAssistant:\',\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-pre-0\',\\n        \'type\': \'PRE_PROCESSING\'\\n      },\\n      \'event_order\': 0,\\n      \'type\': \'modelInvocationInput\'\\n    },\\n    {\\n      \'data\': {\\n        \'parsedResponse\': {\\n          \'isValid\': True,\\n          \'rationale\': \'Based on the provided instructions, this input appears to be a question about orbital mechanics that can be answered using th...equired arguments for that function - specific impulse, dry mass, and total mass. Therefore, this input should be sorted into:\'\\n        },\\n        \'traceId\': \'ca9880a2-dae7-46ac-a480-f38ca7e2d99f-pre-0\'\\n      },\\n      \'event_order\': 1,\\n      \'type\': \'modelInvocationOutput\'\\n    }\\n  ]\\n}\\n```\\n</details>\\n\\nEach group of events with the same _`traceId`_ will contain at least two events: one of type _`modelInvocationInput`_ and\\none of type _`modelInvocationOutput`_. Groups that involve action group traces will also include events of type\\n_`actionGroupInvocationInput`_ and _`actionGroupInvocationOutput`_. Similarly, groups that use knowledge bases will have\\nadditional events of type _`knowledgeBaseLookupInput`_ and _`knowledgeBaseLookupOutput`_.\\nIn the _`BedrockModel`_ mentioned above, it implements an approach to parse these event groups into trace nodes.\\nThis method allows the trace to display the reasoning behind selecting action groups/knowledge bases to answer queries and invoking\\nthe corresponding Lambda function calls, as defined in our example OpenAPI spec above.\\nThis structure helps to clearly show the flow of information and decision-making process that bedrock agent follows.\\n\\n<details>\\n<summary>Here is the final mlflow trace</summary>\\n```text\\n{\\n  \\"spans\\": [\\n    {\\n      \\"name\\": \\"Bedrock Agent Runtime\\",\\n      \\"context\\": {\\n        \\"span_id\\": \\"0xb802165d133a33aa\\",\\n        \\"trace_id\\": \\"0x9b8bd0b2e018d77f936e48a09e54fd44\\"\\n      },\\n      \\"parent_id\\": null,\\n      \\"start_time\\": 1731388531754725000,\\n      \\"end_time\\": 1731388550226771000,\\n      \\"status_code\\": \\"OK\\",\\n      \\"status_message\\": \\"\\",\\n      \\"attributes\\": {\\n        \\"mlflow.traceRequestId\\": \\"\\\\\\"1e036cc3a7f946ec995f7763b8dde51c\\\\\\"\\",\\n        \\"mlflow.spanType\\": \\"\\\\\\"CHAT_MODEL\\\\\\"\\",\\n        \\"mlflow.spanFunctionName\\": \\"\\\\\\"predict\\\\\\"\\",\\n        \\"mlflow.spanInputs\\": \\"{\\\\\\"context\\\\\\": \\\\\\"<mlflow.pyfunc.model.PythonModelContext object at 0x13397c530>\\\\\\", \\\\\\"messages\\\\\\": [{\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\", \\\\\\"name\\\\\\": null}], \\\\\\"params\\\\\\": {\\\\\\"temperature\\\\\\": 1.0, \\\\\\"max_tokens\\\\\\": null, \\\\\\"stop\\\\\\": null, \\\\\\"n\\\\\\": 1, \\\\\\"stream\\\\\\": false, \\\\\\"top_p\\\\\\": null, \\\\\\"top_k\\\\\\": null, \\\\\\"frequency_penalty\\\\\\": null, \\\\\\"presence_penalty\\\\\\": null}}\\",\\n        \\"mlflow.spanOutputs\\": \\"{\\\\\\"choices\\\\\\": [{\\\\\\"index\\\\\\": 0, \\\\\\"message\\\\\\": {\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"Based on the provided spacecraft dry mass of 10000 kg, total mass of 50000 kg, and specific impulse of 2500 s, the next optimal launch window for a Hohmann transfer from Earth to Mars is on November 26, 2026 UTC. The transfer will take 259 days.\\\\\\", \\\\\\"name\\\\\\": null}, \\\\\\"finish_reason\\\\\\": \\\\\\"stop\\\\\\", \\\\\\"logprobs\\\\\\": null}], \\\\\\"usage\\\\\\": {\\\\\\"prompt_tokens\\\\\\": null, \\\\\\"completion_tokens\\\\\\": null, \\\\\\"total_tokens\\\\\\": null}, \\\\\\"id\\\\\\": null, \\\\\\"model\\\\\\": \\\\\\"anthropic.claude-v2\\\\\\", \\\\\\"object\\\\\\": \\\\\\"chat.completion\\\\\\", \\\\\\"created\\\\\\": 1731388550}\\"\\n      },\\n      \\"events\\": []\\n    },\\n    {\\n      \\"name\\": \\"Bedrock Input Prompt\\",\\n      \\"context\\": {\\n        \\"span_id\\": \\"0x2e7cd730be70865b\\",\\n        \\"trace_id\\": \\"0x9b8bd0b2e018d77f936e48a09e54fd44\\"\\n      },\\n      \\"parent_id\\": \\"0xb802165d133a33aa\\",\\n      \\"start_time\\": 1731388531755172000,\\n      \\"end_time\\": 1731388531755252000,\\n      \\"status_code\\": \\"OK\\",\\n      \\"status_message\\": \\"\\",\\n      \\"attributes\\": {\\n        \\"mlflow.traceRequestId\\": \\"\\\\\\"1e036cc3a7f946ec995f7763b8dde51c\\\\\\"\\",\\n        \\"mlflow.spanType\\": \\"\\\\\\"UNKNOWN\\\\\\"\\",\\n        \\"mlflow.spanFunctionName\\": \\"\\\\\\"_get_agent_prompt\\\\\\"\\",\\n        \\"mlflow.spanInputs\\": \\"{\\\\\\"raw_input_question\\\\\\": \\\\\\"When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\"}\\",\\n        \\"mlflow.spanOutputs\\": \\"\\\\\\"\\\\\\\\n        Answer the following question and pay strong attention to the prompt:\\\\\\\\n        <question>\\\\\\\\n        When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\\\n        </question>\\\\\\\\n        <instruction>\\\\\\\\n        You have functions available at your disposal to use when anwering any questions about orbital mechanics.if you can\'t find a function to answer a question about orbital mechanics, simply reply \'I do not know\'\\\\\\\\n        </instruction>\\\\\\\\n        \\\\\\"\\"\\n      },\\n      \\"events\\": []\\n    },\\n    {\\n      \\"name\\": \\"ACTION GROUP DECISION -optimal_departure_window_mars\\",\\n      \\"context\\": {\\n        \\"span_id\\": \\"0x131e4e08cd5e95d9\\",\\n        \\"trace_id\\": \\"0x9b8bd0b2e018d77f936e48a09e54fd44\\"\\n      },\\n      \\"parent_id\\": \\"0xb802165d133a33aa\\",\\n      \\"start_time\\": 1731388550223219000,\\n      \\"end_time\\": 1731388550224592000,\\n      \\"status_code\\": \\"OK\\",\\n      \\"status_message\\": \\"\\",\\n      \\"attributes\\": {\\n        \\"mlflow.traceRequestId\\": \\"\\\\\\"1e036cc3a7f946ec995f7763b8dde51c\\\\\\"\\",\\n        \\"mlflow.spanType\\": \\"\\\\\\"UNKNOWN\\\\\\"\\",\\n        \\"trace_attributes\\": \\"[{\\\\\\"type\\\\\\": \\\\\\"modelInvocationInput\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"inferenceConfiguration\\\\\\": {\\\\\\"maximumLength\\\\\\": 2048, \\\\\\"stopSequences\\\\\\": [\\\\\\"</function_call>\\\\\\", \\\\\\"</answer>\\\\\\", \\\\\\"</error>\\\\\\"], \\\\\\"temperature\\\\\\": 0.0, \\\\\\"topK\\\\\\": 250, \\\\\\"topP\\\\\\": 1.0}, \\\\\\"text\\\\\\": \\\\\\"\\\\\\\\n\\\\\\\\nHuman:\\\\\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>. Your goal is to answer the user\'s question to the best of your ability, using the function(s) to gather more information if necessary to better answer the question. If you choose to call a function, the result of the function call will be added to the conversation history in <function_results> tags (if the call succeeded) or <error> tags (if the function failed). \\\\\\\\nYou were created with these instructions to consider as well:\\\\\\\\n<auxiliary_instructions>\\\\\\\\n            You are a friendly chat bot. You have access to a function called that returns\\\\\\\\n            information about the Mars launch window. When responding with Mars launch window,\\\\\\\\n            please make sure to add the timezone UTC.\\\\\\\\n            </auxiliary_instructions>\\\\\\\\n\\\\\\\\nHere are some examples of correct action by other, different agents with access to functions that may or may not be similar to ones you are provided.\\\\\\\\n\\\\\\\\n<examples>\\\\\\\\n    <example_docstring> Here is an example of how you would correctly answer a question using a <function_call> and the corresponding <function_result>. Notice that you are free to think before deciding to make a <function_call> in the <scratchpad>.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n\\\\\\\\n        <question>Can you show me my policy engine violation from 1st january 2023 to 1st february 2023? My alias is jsmith.</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            To answer this question, I will need to:\\\\\\\\n            1. I do not have knowledge to policy engine violations, so I should see if I can use any of the available functions to help. I have been equipped with get::policyengineactions::getpolicyviolations that gets the policy engine violations for a given alias, start date and end date. I will use this function to gather more information.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::policyengineactions::getpolicyviolations(alias=\\\\\\\\\\\\\\"jsmith\\\\\\\\\\\\\\", startDate=\\\\\\\\\\\\\\"1st January 2023\\\\\\\\\\\\\\", endDate=\\\\\\\\\\\\\\"1st February 2023\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: [{creationDate: \\\\\\\\\\\\\\"2023-06-01T09:30:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"High\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-001\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-001\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-001\\\\\\\\\\\\\\"}, {creationDate: \\\\\\\\\\\\\\"2023-06-02T14:45:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"Medium\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-002\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-002\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-002\\\\\\\\\\\\\\"}]}</function_result>\\\\\\\\n        <answer>The policy engine violations between 1st january 2023 to 1st february 2023 for alias jsmith are - Policy ID: POL-001, Policy ID: POL-002</answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    <example_docstring>Here is another example that utilizes multiple function calls.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n        <question>Can you check the policy engine violations under my manager between 2nd May to 5th May? My alias is john.</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            To answer this question, I will need to:\\\\\\\\n            1. Get the manager alias of the user using get::activedirectoryactions::getmanager function.\\\\\\\\n            2. Use the returned manager alias to get the policy engine violations using the get::policyengineactions::getpolicyviolations function.\\\\\\\\n\\\\\\\\n            I have double checked and made sure that I have been provided the get::activedirectoryactions::getmanager and the get::policyengineactions::getpolicyviolations functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::activedirectoryactions::getmanager(alias=\\\\\\\\\\\\\\"john\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: {managerAlias: \\\\\\\\\\\\\\"mark\\\\\\\\\\\\\\", managerLevel: \\\\\\\\\\\\\\"6\\\\\\\\\\\\\\", teamName: \\\\\\\\\\\\\\"Builder\\\\\\\\\\\\\\", managerName: \\\\\\\\\\\\\\"Mark Hunter\\\\\\\\\\\\\\"}}}}</function_result>\\\\\\\\n        <scratchpad>\\\\\\\\n            1. I have the managerAlias from the function results as mark and I have the start and end date from the user input. I can use the function result to call get::policyengineactions::getpolicyviolations function.\\\\\\\\n            2. I will then return the get::policyengineactions::getpolicyviolations function result to the user.\\\\\\\\n\\\\\\\\n            I have double checked and made sure that I have been provided the get::policyengineactions::getpolicyviolations functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::policyengineactions::getpolicyviolations(alias=\\\\\\\\\\\\\\"mark\\\\\\\\\\\\\\", startDate=\\\\\\\\\\\\\\"2nd May 2023\\\\\\\\\\\\\\", endDate=\\\\\\\\\\\\\\"5th May 2023\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: [{creationDate: \\\\\\\\\\\\\\"2023-05-02T09:30:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"High\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-001\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-001\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-001\\\\\\\\\\\\\\"}, {creationDate: \\\\\\\\\\\\\\"2023-05-04T14:45:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"Low\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-002\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-002\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-002\\\\\\\\\\\\\\"}]}</function_result>\\\\\\\\n        <answer>\\\\\\\\n            The policy engine violations between 2nd May 2023 to 5th May 2023 for your manager\'s alias mark are - Policy ID: POL-001, Policy ID: POL-002\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    <example_docstring>Functions can also be search engine API\'s that issue a query to a knowledge base. Here is an example that utilizes regular function calls in combination with function calls to a search engine API. Please make sure to extract the source for the information within the final answer when using information returned from the search engine.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::benefitsaction::getbenefitplanname</function_name>\\\\\\\\n                <function_description>Get\'s the benefit plan name for a user. The API takes in a userName and a benefit type and returns the benefit name to the user (i.e. Aetna, Premera, Fidelity, etc.).</function_description>\\\\\\\\n                <optional_argument>userName (string): None</optional_argument>\\\\\\\\n                <optional_argument>benefitType (string): None</optional_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::benefitsaction::increase401klimit</function_name>\\\\\\\\n                <function_description>Increases the 401k limit for a generic user. The API takes in only the current 401k limit and returns the new limit.</function_description>\\\\\\\\n                <optional_argument>currentLimit (string): None</optional_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_dentalinsurance::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Delta Dental benefits. It has information about covered dental benefits and other relevant information</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_401kplan::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Amazon 401k plan benefits. It can determine what a person\'s yearly 401k contribution limit is, based on their age.</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_healthinsurance::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Aetna and Premera health benefits. It has information about the savings plan and shared deductible plan, as well as others.</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n\\\\\\\\n        <question>What is my deductible? My username is Bob and my benefitType is Dental. Also, what is the 401k yearly contribution limit?</question>\\\\\\\\n        <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\\\\\n            To answer this question, I will:\\\\\\\\n            1. Call the get::benefitsaction::getbenefitplanname function to get the benefit plan name for the user Bob with benefit type Dental.\\\\\\\\n            2. Call the get::x_amz_knowledgebase_dentalinsurance::search function to search for information about deductibles for the plan name returned from step 1.\\\\\\\\n            3. Call the get::x_amz_knowledgebase_401k::search function to search for information about 401k yearly contribution limits.\\\\\\\\n            4. Return the deductible information from the search results to the user.\\\\\\\\n            I have checked that I have access to the get::benefitsaction::getbenefitplanname, x_amz_knowledgebase_dentalinsurance::search, and x_amz_knowledgebase_401k::search functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::benefitsaction::getbenefitplanname(userName=\\\\\\\\\\\\\\"Bob\\\\\\\\\\\\\\", benefitType=\\\\\\\\\\\\\\"Dental\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'planName\': \'Delta Dental\'}}}}</function_result>\\\\\\\\n        <scratchpad>\\\\\\\\n            I have received the plan name Delta Dental for the user Bob with Dental benefits. I will now call the x_amz_knowledgebase_dentalinsurance::search function to find deductible information for Delta Dental.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::x_amz_knowledgebase_dentalinsurance::search(searchQuery=\\\\\\\\\\\\\\"What is the deductible for Delta Dental?\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'responseCode\': \'200\', \'responseBody\': \\\\\\\\\\\\\\"\\\\\\\\\\\\\\"<answer>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>The user\'s individual deductible is $50 per benefit period</text>\\\\\\\\\\\\\\\\n<source>dfe040f8-46ed-4a65-b3ea-529fa55f6b9e</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>If they are enrolled with dependents, the maximum family deductible is $150 per benefit period.</text>\\\\\\\\\\\\\\\\n<source>0e666064-31d8-4223-b7ba-8eecf40b7b47</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n</answer>\\\\\\\\\\\\\\"}}}}</function_result> <scratchpad>\\\\\\\\n            I have found the deductible information for Dental benefits. I will now call the x_amz_knowledgebase_401k::search function to find yearly 401k contribution limits.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::x_amz_knowledgebase_401k::search(searchQuery=\\\\\\\\\\\\\\"What is the yearly 401k contribution limit?\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'responseCode\': \'200\', \'responseBody\': \\\\\\\\\\\\\\"<answer>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>The yearly 401k contribution limit is $20,500.</text>\\\\\\\\\\\\\\\\n<source>c546cbe8-07f6-45d1-90ca-74d87ab2885a</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n</answer>\\\\\\\\\\\\\\"}}}}</function_result>\\\\\\\\n        <answer>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>The deductible for your Delta Dental plan is $50 per benefit period.</text>\\\\\\\\n                <source>dfe040f8-46ed-4a65-b3ea-529fa55f6b9e</source>\\\\\\\\n            </answer_part>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>If you have dependents enrolled, the maximum family deductible is $150 per benefit period.</text>\\\\\\\\n                <source>0e666064-31d8-4223-b7ba-8eecf40b7b47</source>\\\\\\\\n            </answer_part>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>The yearly 401k contribution limit is $20,500.</text>\\\\\\\\n                <source>c546cbe8-07f6-45d1-90ca-74d87ab2885a</source>\\\\\\\\n            </answer_part>\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    \\\\\\\\n\\\\\\\\n    <example_docstring>Here\'s a final example where the question asked could not be answered with information gathered from calling the provided functions. In this example, notice how you respond by telling the user you cannot answer, without using a function that was not provided to you.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n        <question>Who are the reportees of David?</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            After reviewing the functions I was equipped with, I realize I am not able to accurately answer this question since I can\'t access reportees of David. Therefore, I should explain to the user I cannot answer this question.\\\\\\\\n        </scratchpad>\\\\\\\\n        <answer>\\\\\\\\n            Sorry, I am unable to assist you with this request.\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n</examples>\\\\\\\\n\\\\\\\\nThe above examples have been provided to you to illustrate general guidelines and format for use of function calling for information retrieval, and how to use your scratchpad to plan your approach. IMPORTANT: the functions provided within the examples should not be assumed to have been provided to you to use UNLESS they are also explicitly given to you within <functions></functions> tags below. All of the values and information within the examples (the questions, function results, and answers) are strictly part of the examples and have not been provided to you.\\\\\\\\n\\\\\\\\nNow that you have read and understood the examples, I will define the functions that you have available to you to use. Here is a comprehensive list.\\\\\\\\n\\\\\\\\n<functions>\\\\\\\\n<function>\\\\\\\\n<function_name>GET::optimal_departure_window_mars::getNextMarsLaunchWindow</function_name>\\\\\\\\n<function_description>Gets the next optimal launch window to Mars.</function_description>\\\\\\\\n<required_argument>specific_impulse (string): Specific impulse of the propulsion system (s).</required_argument>\\\\\\\\n<required_argument>dry_mass (string): Mass of the spacecraft without fuel (kg).</required_argument>\\\\\\\\n<required_argument>total_mass (string): Total mass of the spacecraft including fuel (kg)</required_argument>\\\\\\\\n<returns>object: The next optimal departure date for a Hohmann transfer from Earth to Mars, based on the spacecraft\'s mass and specific impulse.</returns>\\\\\\\\n</function>\\\\\\\\n\\\\\\\\n\\\\\\\\n</functions>\\\\\\\\n\\\\\\\\nNote that the function arguments have been listed in the order that they should be passed into the function.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nDo not modify or extend the provided functions under any circumstances. For example, GET::optimal_departure_window_mars::getNextMarsLaunchWindow with additional parameters would be considered modifying the function which is not allowed. Please use the functions only as defined.\\\\\\\\n\\\\\\\\nDO NOT use any functions that I have not equipped you with.\\\\\\\\n\\\\\\\\n Do not make assumptions about inputs; instead, make sure you know the exact function and input to use before you call a function.\\\\\\\\n\\\\\\\\nTo call a function, output the name of the function in between <function_call> and </function_call> tags. You will receive a <function_result> in response to your call that contains information that you can use to better answer the question. Or, if the function call produced an error, you will receive an <error> in response.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nThe format for all other <function_call> MUST be: <function_call>$FUNCTION_NAME($FUNCTION_PARAMETER_NAME=$FUNCTION_PARAMETER_VALUE)</function_call>\\\\\\\\n\\\\\\\\nRemember, your goal is to answer the user\'s question to the best of your ability, using only the function(s) provided within the <functions></functions> tags to gather more information if necessary to better answer the question.\\\\\\\\n\\\\\\\\nDo not modify or extend the provided functions under any circumstances. For example, calling GET::optimal_departure_window_mars::getNextMarsLaunchWindow with additional parameters would be modifying the function which is not allowed. Please use the functions only as defined.\\\\\\\\n\\\\\\\\nBefore calling any functions, create a plan for performing actions to answer this question within the <scratchpad>. Double check your plan to make sure you don\'t call any functions that you haven\'t been provided with. Always return your final answer within <answer></answer> tags.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nThe user input is <question>Answer the following question and pay strong attention to the prompt:\\\\\\\\n        <question>\\\\\\\\n        When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\\\n        </question>\\\\\\\\n        <instruction>\\\\\\\\n        You have functions available at your disposal to use when anwering any questions about orbital mechanics.if you can\'t find a function to answer a question about orbital mechanics, simply reply \'I do not know\'\\\\\\\\n        </instruction></question>\\\\\\\\n\\\\\\\\n\\\\\\\\nAssistant: <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\\\\\n\\\\\\\\n\\\\\\", \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\", \\\\\\"type\\\\\\": \\\\\\"ORCHESTRATION\\\\\\"}, \\\\\\"event_order\\\\\\": 2}, {\\\\\\"type\\\\\\": \\\\\\"modelInvocationOutput\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"metadata\\\\\\": {\\\\\\"usage\\\\\\": {\\\\\\"inputTokens\\\\\\": 5160, \\\\\\"outputTokens\\\\\\": 135}}, \\\\\\"rawResponse\\\\\\": {\\\\\\"content\\\\\\": \\\\\\"To answer this question about the next Mars launch window, I will:\\\\\\\\n\\\\\\\\n1. Call the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function to get the next optimal launch window, passing in the provided spacecraft mass and specific impulse values.\\\\\\\\n\\\\\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\\\\\\\\n\\\\\\\\n</scratchpad>\\\\\\\\n\\\\\\\\n<function_call>\\\\\\\\nGET::optimal_departure_window_mars::getNextMarsLaunchWindow(specific_impulse=\\\\\\\\\\\\\\"2500\\\\\\\\\\\\\\", dry_mass=\\\\\\\\\\\\\\"10000\\\\\\\\\\\\\\", total_mass=\\\\\\\\\\\\\\"50000\\\\\\\\\\\\\\")\\\\\\"}, \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\"}, \\\\\\"event_order\\\\\\": 3}, {\\\\\\"type\\\\\\": \\\\\\"rationale\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"text\\\\\\": \\\\\\"To answer this question about the next Mars launch window, I will:\\\\\\\\n\\\\\\\\n1. Call the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function to get the next optimal launch window, passing in the provided spacecraft mass and specific impulse values.\\\\\\\\n\\\\\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\\\\\\", \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\"}, \\\\\\"event_order\\\\\\": 4}, {\\\\\\"type\\\\\\": \\\\\\"invocationInput\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"actionGroupInvocationInput\\\\\\": {\\\\\\"actionGroupName\\\\\\": \\\\\\"optimal_departure_window_mars\\\\\\", \\\\\\"apiPath\\\\\\": \\\\\\"/get-next-mars-launch-window\\\\\\", \\\\\\"executionType\\\\\\": \\\\\\"LAMBDA\\\\\\", \\\\\\"parameters\\\\\\": [{\\\\\\"name\\\\\\": \\\\\\"total_mass\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"value\\\\\\": \\\\\\"50000\\\\\\"}, {\\\\\\"name\\\\\\": \\\\\\"dry_mass\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"value\\\\\\": \\\\\\"10000\\\\\\"}, {\\\\\\"name\\\\\\": \\\\\\"specific_impulse\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"value\\\\\\": \\\\\\"2500\\\\\\"}], \\\\\\"verb\\\\\\": \\\\\\"get\\\\\\"}, \\\\\\"invocationType\\\\\\": \\\\\\"ACTION_GROUP\\\\\\", \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\"}, \\\\\\"event_order\\\\\\": 5}, {\\\\\\"type\\\\\\": \\\\\\"observation\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"actionGroupInvocationOutput\\\\\\": {\\\\\\"text\\\\\\": \\\\\\"{\\\\\\\\\\\\\\"next_launch_window\\\\\\\\\\\\\\": {\\\\\\\\\\\\\\"next_launch_date\\\\\\\\\\\\\\": \\\\\\\\\\\\\\"2026-11-26 00:00:00\\\\\\\\\\\\\\", \\\\\\\\\\\\\\"synodic_period_days\\\\\\\\\\\\\\": 779.9068939794238, \\\\\\\\\\\\\\"transfer_time_days\\\\\\\\\\\\\\": 259, \\\\\\\\\\\\\\"delta_v_available_m_s\\\\\\\\\\\\\\": 39457.985759929674, \\\\\\\\\\\\\\"delta_v_required_m_s\\\\\\\\\\\\\\": 5595.997417810693, \\\\\\\\\\\\\\"is_feasible\\\\\\\\\\\\\\": true}}\\\\\\"}, \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\", \\\\\\"type\\\\\\": \\\\\\"ACTION_GROUP\\\\\\"}, \\\\\\"event_order\\\\\\": 6}]\\",\\n        \\"mlflow.spanFunctionName\\": \\"\\\\\\"_trace_agent_pre_context\\\\\\"\\",\\n        \\"mlflow.spanInputs\\": \\"{\\\\\\"inner_input_trace\\\\\\": \\\\\\"\\\\\\\\n\\\\\\\\nHuman:\\\\\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>. Your goal is to answer the user\'s question to the best of your ability, using the function(s) to gather more information if necessary to better answer the question. If you choose to call a function, the result of the function call will be added to the conversation history in <function_results> tags (if the call succeeded) or <error> tags (if the function failed). \\\\\\\\nYou were created with these instructions to consider as well:\\\\\\\\n<auxiliary_instructions>\\\\\\\\n            You are a friendly chat bot. You have access to a function called that returns\\\\\\\\n            information about the Mars launch window. When responding with Mars launch window,\\\\\\\\n            please make sure to add the timezone UTC.\\\\\\\\n            </auxiliary_instructions>\\\\\\\\n\\\\\\\\nHere are some examples of correct action by other, different agents with access to functions that may or may not be similar to ones you are provided.\\\\\\\\n\\\\\\\\n<examples>\\\\\\\\n    <example_docstring> Here is an example of how you would correctly answer a question using a <function_call> and the corresponding <function_result>. Notice that you are free to think before deciding to make a <function_call> in the <scratchpad>.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n\\\\\\\\n        <question>Can you show me my policy engine violation from 1st january 2023 to 1st february 2023? My alias is jsmith.</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            To answer this question, I will need to:\\\\\\\\n            1. I do not have knowledge to policy engine violations, so I should see if I can use any of the available functions to help. I have been equipped with get::policyengineactions::getpolicyviolations that gets the policy engine violations for a given alias, start date and end date. I will use this function to gather more information.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::policyengineactions::getpolicyviolations(alias=\\\\\\\\\\\\\\"jsmith\\\\\\\\\\\\\\", startDate=\\\\\\\\\\\\\\"1st January 2023\\\\\\\\\\\\\\", endDate=\\\\\\\\\\\\\\"1st February 2023\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: [{creationDate: \\\\\\\\\\\\\\"2023-06-01T09:30:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"High\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-001\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-001\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-001\\\\\\\\\\\\\\"}, {creationDate: \\\\\\\\\\\\\\"2023-06-02T14:45:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"Medium\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-002\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-002\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-002\\\\\\\\\\\\\\"}]}</function_result>\\\\\\\\n        <answer>The policy engine violations between 1st january 2023 to 1st february 2023 for alias jsmith are - Policy ID: POL-001, Policy ID: POL-002</answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    <example_docstring>Here is another example that utilizes multiple function calls.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n        <question>Can you check the policy engine violations under my manager between 2nd May to 5th May? My alias is john.</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            To answer this question, I will need to:\\\\\\\\n            1. Get the manager alias of the user using get::activedirectoryactions::getmanager function.\\\\\\\\n            2. Use the returned manager alias to get the policy engine violations using the get::policyengineactions::getpolicyviolations function.\\\\\\\\n\\\\\\\\n            I have double checked and made sure that I have been provided the get::activedirectoryactions::getmanager and the get::policyengineactions::getpolicyviolations functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::activedirectoryactions::getmanager(alias=\\\\\\\\\\\\\\"john\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: {managerAlias: \\\\\\\\\\\\\\"mark\\\\\\\\\\\\\\", managerLevel: \\\\\\\\\\\\\\"6\\\\\\\\\\\\\\", teamName: \\\\\\\\\\\\\\"Builder\\\\\\\\\\\\\\", managerName: \\\\\\\\\\\\\\"Mark Hunter\\\\\\\\\\\\\\"}}}}</function_result>\\\\\\\\n        <scratchpad>\\\\\\\\n            1. I have the managerAlias from the function results as mark and I have the start and end date from the user input. I can use the function result to call get::policyengineactions::getpolicyviolations function.\\\\\\\\n            2. I will then return the get::policyengineactions::getpolicyviolations function result to the user.\\\\\\\\n\\\\\\\\n            I have double checked and made sure that I have been provided the get::policyengineactions::getpolicyviolations functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::policyengineactions::getpolicyviolations(alias=\\\\\\\\\\\\\\"mark\\\\\\\\\\\\\\", startDate=\\\\\\\\\\\\\\"2nd May 2023\\\\\\\\\\\\\\", endDate=\\\\\\\\\\\\\\"5th May 2023\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: [{creationDate: \\\\\\\\\\\\\\"2023-05-02T09:30:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"High\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-001\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-001\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-001\\\\\\\\\\\\\\"}, {creationDate: \\\\\\\\\\\\\\"2023-05-04T14:45:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"Low\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-002\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-002\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-002\\\\\\\\\\\\\\"}]}</function_result>\\\\\\\\n        <answer>\\\\\\\\n            The policy engine violations between 2nd May 2023 to 5th May 2023 for your manager\'s alias mark are - Policy ID: POL-001, Policy ID: POL-002\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    <example_docstring>Functions can also be search engine API\'s that issue a query to a knowledge base. Here is an example that utilizes regular function calls in combination with function calls to a search engine API. Please make sure to extract the source for the information within the final answer when using information returned from the search engine.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::benefitsaction::getbenefitplanname</function_name>\\\\\\\\n                <function_description>Get\'s the benefit plan name for a user. The API takes in a userName and a benefit type and returns the benefit name to the user (i.e. Aetna, Premera, Fidelity, etc.).</function_description>\\\\\\\\n                <optional_argument>userName (string): None</optional_argument>\\\\\\\\n                <optional_argument>benefitType (string): None</optional_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::benefitsaction::increase401klimit</function_name>\\\\\\\\n                <function_description>Increases the 401k limit for a generic user. The API takes in only the current 401k limit and returns the new limit.</function_description>\\\\\\\\n                <optional_argument>currentLimit (string): None</optional_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_dentalinsurance::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Delta Dental benefits. It has information about covered dental benefits and other relevant information</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_401kplan::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Amazon 401k plan benefits. It can determine what a person\'s yearly 401k contribution limit is, based on their age.</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_healthinsurance::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Aetna and Premera health benefits. It has information about the savings plan and shared deductible plan, as well as others.</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n\\\\\\\\n        <question>What is my deductible? My username is Bob and my benefitType is Dental. Also, what is the 401k yearly contribution limit?</question>\\\\\\\\n        <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\\\\\n            To answer this question, I will:\\\\\\\\n            1. Call the get::benefitsaction::getbenefitplanname function to get the benefit plan name for the user Bob with benefit type Dental.\\\\\\\\n            2. Call the get::x_amz_knowledgebase_dentalinsurance::search function to search for information about deductibles for the plan name returned from step 1.\\\\\\\\n            3. Call the get::x_amz_knowledgebase_401k::search function to search for information about 401k yearly contribution limits.\\\\\\\\n            4. Return the deductible information from the search results to the user.\\\\\\\\n            I have checked that I have access to the get::benefitsaction::getbenefitplanname, x_amz_knowledgebase_dentalinsurance::search, and x_amz_knowledgebase_401k::search functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::benefitsaction::getbenefitplanname(userName=\\\\\\\\\\\\\\"Bob\\\\\\\\\\\\\\", benefitType=\\\\\\\\\\\\\\"Dental\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'planName\': \'Delta Dental\'}}}}</function_result>\\\\\\\\n        <scratchpad>\\\\\\\\n            I have received the plan name Delta Dental for the user Bob with Dental benefits. I will now call the x_amz_knowledgebase_dentalinsurance::search function to find deductible information for Delta Dental.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::x_amz_knowledgebase_dentalinsurance::search(searchQuery=\\\\\\\\\\\\\\"What is the deductible for Delta Dental?\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'responseCode\': \'200\', \'responseBody\': \\\\\\\\\\\\\\"\\\\\\\\\\\\\\"<answer>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>The user\'s individual deductible is $50 per benefit period</text>\\\\\\\\\\\\\\\\n<source>dfe040f8-46ed-4a65-b3ea-529fa55f6b9e</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>If they are enrolled with dependents, the maximum family deductible is $150 per benefit period.</text>\\\\\\\\\\\\\\\\n<source>0e666064-31d8-4223-b7ba-8eecf40b7b47</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n</answer>\\\\\\\\\\\\\\"}}}}</function_result> <scratchpad>\\\\\\\\n            I have found the deductible information for Dental benefits. I will now call the x_amz_knowledgebase_401k::search function to find yearly 401k contribution limits.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::x_amz_knowledgebase_401k::search(searchQuery=\\\\\\\\\\\\\\"What is the yearly 401k contribution limit?\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'responseCode\': \'200\', \'responseBody\': \\\\\\\\\\\\\\"<answer>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>The yearly 401k contribution limit is $20,500.</text>\\\\\\\\\\\\\\\\n<source>c546cbe8-07f6-45d1-90ca-74d87ab2885a</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n</answer>\\\\\\\\\\\\\\"}}}}</function_result>\\\\\\\\n        <answer>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>The deductible for your Delta Dental plan is $50 per benefit period.</text>\\\\\\\\n                <source>dfe040f8-46ed-4a65-b3ea-529fa55f6b9e</source>\\\\\\\\n            </answer_part>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>If you have dependents enrolled, the maximum family deductible is $150 per benefit period.</text>\\\\\\\\n                <source>0e666064-31d8-4223-b7ba-8eecf40b7b47</source>\\\\\\\\n            </answer_part>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>The yearly 401k contribution limit is $20,500.</text>\\\\\\\\n                <source>c546cbe8-07f6-45d1-90ca-74d87ab2885a</source>\\\\\\\\n            </answer_part>\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    \\\\\\\\n\\\\\\\\n    <example_docstring>Here\'s a final example where the question asked could not be answered with information gathered from calling the provided functions. In this example, notice how you respond by telling the user you cannot answer, without using a function that was not provided to you.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n        <question>Who are the reportees of David?</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            After reviewing the functions I was equipped with, I realize I am not able to accurately answer this question since I can\'t access reportees of David. Therefore, I should explain to the user I cannot answer this question.\\\\\\\\n        </scratchpad>\\\\\\\\n        <answer>\\\\\\\\n            Sorry, I am unable to assist you with this request.\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n</examples>\\\\\\\\n\\\\\\\\nThe above examples have been provided to you to illustrate general guidelines and format for use of function calling for information retrieval, and how to use your scratchpad to plan your approach. IMPORTANT: the functions provided within the examples should not be assumed to have been provided to you to use UNLESS they are also explicitly given to you within <functions></functions> tags below. All of the values and information within the examples (the questions, function results, and answers) are strictly part of the examples and have not been provided to you.\\\\\\\\n\\\\\\\\nNow that you have read and understood the examples, I will define the functions that you have available to you to use. Here is a comprehensive list.\\\\\\\\n\\\\\\\\n<functions>\\\\\\\\n<function>\\\\\\\\n<function_name>GET::optimal_departure_window_mars::getNextMarsLaunchWindow</function_name>\\\\\\\\n<function_description>Gets the next optimal launch window to Mars.</function_description>\\\\\\\\n<required_argument>specific_impulse (string): Specific impulse of the propulsion system (s).</required_argument>\\\\\\\\n<required_argument>dry_mass (string): Mass of the spacecraft without fuel (kg).</required_argument>\\\\\\\\n<required_argument>total_mass (string): Total mass of the spacecraft including fuel (kg)</required_argument>\\\\\\\\n<returns>object: The next optimal departure date for a Hohmann transfer from Earth to Mars, based on the spacecraft\'s mass and specific impulse.</returns>\\\\\\\\n</function>\\\\\\\\n\\\\\\\\n\\\\\\\\n</functions>\\\\\\\\n\\\\\\\\nNote that the function arguments have been listed in the order that they should be passed into the function.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nDo not modify or extend the provided functions under any circumstances. For example, GET::optimal_departure_window_mars::getNextMarsLaunchWindow with additional parameters would be considered modifying the function which is not allowed. Please use the functions only as defined.\\\\\\\\n\\\\\\\\nDO NOT use any functions that I have not equipped you with.\\\\\\\\n\\\\\\\\n Do not make assumptions about inputs; instead, make sure you know the exact function and input to use before you call a function.\\\\\\\\n\\\\\\\\nTo call a function, output the name of the function in between <function_call> and </function_call> tags. You will receive a <function_result> in response to your call that contains information that you can use to better answer the question. Or, if the function call produced an error, you will receive an <error> in response.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nThe format for all other <function_call> MUST be: <function_call>$FUNCTION_NAME($FUNCTION_PARAMETER_NAME=$FUNCTION_PARAMETER_VALUE)</function_call>\\\\\\\\n\\\\\\\\nRemember, your goal is to answer the user\'s question to the best of your ability, using only the function(s) provided within the <functions></functions> tags to gather more information if necessary to better answer the question.\\\\\\\\n\\\\\\\\nDo not modify or extend the provided functions under any circumstances. For example, calling GET::optimal_departure_window_mars::getNextMarsLaunchWindow with additional parameters would be modifying the function which is not allowed. Please use the functions only as defined.\\\\\\\\n\\\\\\\\nBefore calling any functions, create a plan for performing actions to answer this question within the <scratchpad>. Double check your plan to make sure you don\'t call any functions that you haven\'t been provided with. Always return your final answer within <answer></answer> tags.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nThe user input is <question>Answer the following question and pay strong attention to the prompt:\\\\\\\\n        <question>\\\\\\\\n        When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\\\n        </question>\\\\\\\\n        <instruction>\\\\\\\\n        You have functions available at your disposal to use when anwering any questions about orbital mechanics.if you can\'t find a function to answer a question about orbital mechanics, simply reply \'I do not know\'\\\\\\\\n        </instruction></question>\\\\\\\\n\\\\\\\\n\\\\\\\\nAssistant: <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\\\\\n\\\\\\\\n\\\\\\"}\\",\\n        \\"mlflow.spanOutputs\\": \\"\\\\\\"To answer this question about the next Mars launch window, I will:\\\\\\\\n\\\\\\\\n1. Call the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function to get the next optimal launch window, passing in the provided spacecraft mass and specific impulse values.\\\\\\\\n\\\\\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\\\\\\"\\"\\n      },\\n      \\"events\\": []\\n    },\\n    {\\n      \\"name\\": \\"Invoking Action Group\\",\\n      \\"context\\": {\\n        \\"span_id\\": \\"0x692bd6457647dc76\\",\\n        \\"trace_id\\": \\"0x9b8bd0b2e018d77f936e48a09e54fd44\\"\\n      },\\n      \\"parent_id\\": \\"0xb802165d133a33aa\\",\\n      \\"start_time\\": 1731388550224851000,\\n      \\"end_time\\": 1731388550225218000,\\n      \\"status_code\\": \\"OK\\",\\n      \\"status_message\\": \\"\\",\\n      \\"attributes\\": {\\n        \\"mlflow.traceRequestId\\": \\"\\\\\\"1e036cc3a7f946ec995f7763b8dde51c\\\\\\"\\",\\n        \\"mlflow.spanType\\": \\"\\\\\\"UNKNOWN\\\\\\"\\",\\n        \\"trace_attributes\\": \\"[{\\\\\\"type\\\\\\": \\\\\\"modelInvocationInput\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"inferenceConfiguration\\\\\\": {\\\\\\"maximumLength\\\\\\": 2048, \\\\\\"stopSequences\\\\\\": [\\\\\\"</function_call>\\\\\\", \\\\\\"</answer>\\\\\\", \\\\\\"</error>\\\\\\"], \\\\\\"temperature\\\\\\": 0.0, \\\\\\"topK\\\\\\": 250, \\\\\\"topP\\\\\\": 1.0}, \\\\\\"text\\\\\\": \\\\\\"\\\\\\\\n\\\\\\\\nHuman:\\\\\\\\nYou are a research assistant AI that has been equipped with one or more functions to help you answer a <question>. Your goal is to answer the user\'s question to the best of your ability, using the function(s) to gather more information if necessary to better answer the question. If you choose to call a function, the result of the function call will be added to the conversation history in <function_results> tags (if the call succeeded) or <error> tags (if the function failed). \\\\\\\\nYou were created with these instructions to consider as well:\\\\\\\\n<auxiliary_instructions>\\\\\\\\n            You are a friendly chat bot. You have access to a function called that returns\\\\\\\\n            information about the Mars launch window. When responding with Mars launch window,\\\\\\\\n            please make sure to add the timezone UTC.\\\\\\\\n            </auxiliary_instructions>\\\\\\\\n\\\\\\\\nHere are some examples of correct action by other, different agents with access to functions that may or may not be similar to ones you are provided.\\\\\\\\n\\\\\\\\n<examples>\\\\\\\\n    <example_docstring> Here is an example of how you would correctly answer a question using a <function_call> and the corresponding <function_result>. Notice that you are free to think before deciding to make a <function_call> in the <scratchpad>.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n\\\\\\\\n        <question>Can you show me my policy engine violation from 1st january 2023 to 1st february 2023? My alias is jsmith.</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            To answer this question, I will need to:\\\\\\\\n            1. I do not have knowledge to policy engine violations, so I should see if I can use any of the available functions to help. I have been equipped with get::policyengineactions::getpolicyviolations that gets the policy engine violations for a given alias, start date and end date. I will use this function to gather more information.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::policyengineactions::getpolicyviolations(alias=\\\\\\\\\\\\\\"jsmith\\\\\\\\\\\\\\", startDate=\\\\\\\\\\\\\\"1st January 2023\\\\\\\\\\\\\\", endDate=\\\\\\\\\\\\\\"1st February 2023\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: [{creationDate: \\\\\\\\\\\\\\"2023-06-01T09:30:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"High\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-001\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-001\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-001\\\\\\\\\\\\\\"}, {creationDate: \\\\\\\\\\\\\\"2023-06-02T14:45:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"Medium\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-002\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-002\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-002\\\\\\\\\\\\\\"}]}</function_result>\\\\\\\\n        <answer>The policy engine violations between 1st january 2023 to 1st february 2023 for alias jsmith are - Policy ID: POL-001, Policy ID: POL-002</answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    <example_docstring>Here is another example that utilizes multiple function calls.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n        <question>Can you check the policy engine violations under my manager between 2nd May to 5th May? My alias is john.</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            To answer this question, I will need to:\\\\\\\\n            1. Get the manager alias of the user using get::activedirectoryactions::getmanager function.\\\\\\\\n            2. Use the returned manager alias to get the policy engine violations using the get::policyengineactions::getpolicyviolations function.\\\\\\\\n\\\\\\\\n            I have double checked and made sure that I have been provided the get::activedirectoryactions::getmanager and the get::policyengineactions::getpolicyviolations functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::activedirectoryactions::getmanager(alias=\\\\\\\\\\\\\\"john\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: {managerAlias: \\\\\\\\\\\\\\"mark\\\\\\\\\\\\\\", managerLevel: \\\\\\\\\\\\\\"6\\\\\\\\\\\\\\", teamName: \\\\\\\\\\\\\\"Builder\\\\\\\\\\\\\\", managerName: \\\\\\\\\\\\\\"Mark Hunter\\\\\\\\\\\\\\"}}}}</function_result>\\\\\\\\n        <scratchpad>\\\\\\\\n            1. I have the managerAlias from the function results as mark and I have the start and end date from the user input. I can use the function result to call get::policyengineactions::getpolicyviolations function.\\\\\\\\n            2. I will then return the get::policyengineactions::getpolicyviolations function result to the user.\\\\\\\\n\\\\\\\\n            I have double checked and made sure that I have been provided the get::policyengineactions::getpolicyviolations functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::policyengineactions::getpolicyviolations(alias=\\\\\\\\\\\\\\"mark\\\\\\\\\\\\\\", startDate=\\\\\\\\\\\\\\"2nd May 2023\\\\\\\\\\\\\\", endDate=\\\\\\\\\\\\\\"5th May 2023\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{response: [{creationDate: \\\\\\\\\\\\\\"2023-05-02T09:30:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"High\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-001\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-001\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-001\\\\\\\\\\\\\\"}, {creationDate: \\\\\\\\\\\\\\"2023-05-04T14:45:00Z\\\\\\\\\\\\\\", riskLevel: \\\\\\\\\\\\\\"Low\\\\\\\\\\\\\\", policyId: \\\\\\\\\\\\\\"POL-002\\\\\\\\\\\\\\", policyUrl: \\\\\\\\\\\\\\"https://example.com/policies/POL-002\\\\\\\\\\\\\\", referenceUrl: \\\\\\\\\\\\\\"https://example.com/violations/POL-002\\\\\\\\\\\\\\"}]}</function_result>\\\\\\\\n        <answer>\\\\\\\\n            The policy engine violations between 2nd May 2023 to 5th May 2023 for your manager\'s alias mark are - Policy ID: POL-001, Policy ID: POL-002\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    <example_docstring>Functions can also be search engine API\'s that issue a query to a knowledge base. Here is an example that utilizes regular function calls in combination with function calls to a search engine API. Please make sure to extract the source for the information within the final answer when using information returned from the search engine.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::benefitsaction::getbenefitplanname</function_name>\\\\\\\\n                <function_description>Get\'s the benefit plan name for a user. The API takes in a userName and a benefit type and returns the benefit name to the user (i.e. Aetna, Premera, Fidelity, etc.).</function_description>\\\\\\\\n                <optional_argument>userName (string): None</optional_argument>\\\\\\\\n                <optional_argument>benefitType (string): None</optional_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::benefitsaction::increase401klimit</function_name>\\\\\\\\n                <function_description>Increases the 401k limit for a generic user. The API takes in only the current 401k limit and returns the new limit.</function_description>\\\\\\\\n                <optional_argument>currentLimit (string): None</optional_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_dentalinsurance::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Delta Dental benefits. It has information about covered dental benefits and other relevant information</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_401kplan::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Amazon 401k plan benefits. It can determine what a person\'s yearly 401k contribution limit is, based on their age.</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::x_amz_knowledgebase_healthinsurance::search</function_name>\\\\\\\\n                <function_description>This is a search tool that provides information about Aetna and Premera health benefits. It has information about the savings plan and shared deductible plan, as well as others.</function_description>\\\\\\\\n                <required_argument>query(string): A full sentence query that is fed to the search tool</required_argument>\\\\\\\\n                <returns>Returns string  related to the user query asked.</returns>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n\\\\\\\\n        <question>What is my deductible? My username is Bob and my benefitType is Dental. Also, what is the 401k yearly contribution limit?</question>\\\\\\\\n        <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\\\\\n            To answer this question, I will:\\\\\\\\n            1. Call the get::benefitsaction::getbenefitplanname function to get the benefit plan name for the user Bob with benefit type Dental.\\\\\\\\n            2. Call the get::x_amz_knowledgebase_dentalinsurance::search function to search for information about deductibles for the plan name returned from step 1.\\\\\\\\n            3. Call the get::x_amz_knowledgebase_401k::search function to search for information about 401k yearly contribution limits.\\\\\\\\n            4. Return the deductible information from the search results to the user.\\\\\\\\n            I have checked that I have access to the get::benefitsaction::getbenefitplanname, x_amz_knowledgebase_dentalinsurance::search, and x_amz_knowledgebase_401k::search functions.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::benefitsaction::getbenefitplanname(userName=\\\\\\\\\\\\\\"Bob\\\\\\\\\\\\\\", benefitType=\\\\\\\\\\\\\\"Dental\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'planName\': \'Delta Dental\'}}}}</function_result>\\\\\\\\n        <scratchpad>\\\\\\\\n            I have received the plan name Delta Dental for the user Bob with Dental benefits. I will now call the x_amz_knowledgebase_dentalinsurance::search function to find deductible information for Delta Dental.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::x_amz_knowledgebase_dentalinsurance::search(searchQuery=\\\\\\\\\\\\\\"What is the deductible for Delta Dental?\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'responseCode\': \'200\', \'responseBody\': \\\\\\\\\\\\\\"\\\\\\\\\\\\\\"<answer>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>The user\'s individual deductible is $50 per benefit period</text>\\\\\\\\\\\\\\\\n<source>dfe040f8-46ed-4a65-b3ea-529fa55f6b9e</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>If they are enrolled with dependents, the maximum family deductible is $150 per benefit period.</text>\\\\\\\\\\\\\\\\n<source>0e666064-31d8-4223-b7ba-8eecf40b7b47</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n</answer>\\\\\\\\\\\\\\"}}}}</function_result> <scratchpad>\\\\\\\\n            I have found the deductible information for Dental benefits. I will now call the x_amz_knowledgebase_401k::search function to find yearly 401k contribution limits.\\\\\\\\n        </scratchpad>\\\\\\\\n        <function_call>get::x_amz_knowledgebase_401k::search(searchQuery=\\\\\\\\\\\\\\"What is the yearly 401k contribution limit?\\\\\\\\\\\\\\")</function_call>\\\\\\\\n        <function_result>{{\'response\': {{\'responseCode\': \'200\', \'responseBody\': \\\\\\\\\\\\\\"<answer>\\\\\\\\\\\\\\\\n<answer_part>\\\\\\\\\\\\\\\\n<text>The yearly 401k contribution limit is $20,500.</text>\\\\\\\\\\\\\\\\n<source>c546cbe8-07f6-45d1-90ca-74d87ab2885a</source>\\\\\\\\\\\\\\\\n</answer_part>\\\\\\\\\\\\\\\\n</answer>\\\\\\\\\\\\\\"}}}}</function_result>\\\\\\\\n        <answer>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>The deductible for your Delta Dental plan is $50 per benefit period.</text>\\\\\\\\n                <source>dfe040f8-46ed-4a65-b3ea-529fa55f6b9e</source>\\\\\\\\n            </answer_part>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>If you have dependents enrolled, the maximum family deductible is $150 per benefit period.</text>\\\\\\\\n                <source>0e666064-31d8-4223-b7ba-8eecf40b7b47</source>\\\\\\\\n            </answer_part>\\\\\\\\n            <answer_part>\\\\\\\\n                <text>The yearly 401k contribution limit is $20,500.</text>\\\\\\\\n                <source>c546cbe8-07f6-45d1-90ca-74d87ab2885a</source>\\\\\\\\n            </answer_part>\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n\\\\\\\\n    \\\\\\\\n\\\\\\\\n    <example_docstring>Here\'s a final example where the question asked could not be answered with information gathered from calling the provided functions. In this example, notice how you respond by telling the user you cannot answer, without using a function that was not provided to you.</example_docstring>\\\\\\\\n    <example>\\\\\\\\n        <functions>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::policyengineactions::getpolicyviolations</function_name>\\\\\\\\n                <function_description>Returns a list of policy engine violations for the specified alias within the specified date range.</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <required_argument>startDate (string): The start date of the range to filter violations. The format for startDate is MM/DD/YYYY.</required_argument>\\\\\\\\n                <required_argument>endDate (string): The end date of the range to filter violations</required_argument>\\\\\\\\n                <returns>array: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>post::policyengineactions::acknowledgeviolations</function_name>\\\\\\\\n                <function_description>Acknowledge policy engine violation. Generally used to acknowledge violation, once user notices a violation under their alias or their managers alias.</function_description>\\\\\\\\n                <required_argument>policyId (string): The ID of the policy violation</required_argument>\\\\\\\\n                <required_argument>expectedDateOfResolution (string): The date by when the violation will be addressed/resolved</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            <function>\\\\\\\\n                <function_name>get::activedirectoryactions::getmanager</function_name>\\\\\\\\n                <function_description>This API is used to identify the manager hierarchy above a given person. Every person could have a manager and the manager could have another manager to which they report to</function_description>\\\\\\\\n                <required_argument>alias (string): The alias of the employee under whose name current violations needs to be listed</required_argument>\\\\\\\\n                <returns>object: Successful response</returns>\\\\\\\\n                <raises>object: Invalid request</raises>\\\\\\\\n            </function>\\\\\\\\n            \\\\\\\\n        </functions>\\\\\\\\n        <question>Who are the reportees of David?</question>\\\\\\\\n        <scratchpad>\\\\\\\\n            After reviewing the functions I was equipped with, I realize I am not able to accurately answer this question since I can\'t access reportees of David. Therefore, I should explain to the user I cannot answer this question.\\\\\\\\n        </scratchpad>\\\\\\\\n        <answer>\\\\\\\\n            Sorry, I am unable to assist you with this request.\\\\\\\\n        </answer>\\\\\\\\n    </example>\\\\\\\\n</examples>\\\\\\\\n\\\\\\\\nThe above examples have been provided to you to illustrate general guidelines and format for use of function calling for information retrieval, and how to use your scratchpad to plan your approach. IMPORTANT: the functions provided within the examples should not be assumed to have been provided to you to use UNLESS they are also explicitly given to you within <functions></functions> tags below. All of the values and information within the examples (the questions, function results, and answers) are strictly part of the examples and have not been provided to you.\\\\\\\\n\\\\\\\\nNow that you have read and understood the examples, I will define the functions that you have available to you to use. Here is a comprehensive list.\\\\\\\\n\\\\\\\\n<functions>\\\\\\\\n<function>\\\\\\\\n<function_name>GET::optimal_departure_window_mars::getNextMarsLaunchWindow</function_name>\\\\\\\\n<function_description>Gets the next optimal launch window to Mars.</function_description>\\\\\\\\n<required_argument>specific_impulse (string): Specific impulse of the propulsion system (s).</required_argument>\\\\\\\\n<required_argument>dry_mass (string): Mass of the spacecraft without fuel (kg).</required_argument>\\\\\\\\n<required_argument>total_mass (string): Total mass of the spacecraft including fuel (kg)</required_argument>\\\\\\\\n<returns>object: The next optimal departure date for a Hohmann transfer from Earth to Mars, based on the spacecraft\'s mass and specific impulse.</returns>\\\\\\\\n</function>\\\\\\\\n\\\\\\\\n\\\\\\\\n</functions>\\\\\\\\n\\\\\\\\nNote that the function arguments have been listed in the order that they should be passed into the function.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nDo not modify or extend the provided functions under any circumstances. For example, GET::optimal_departure_window_mars::getNextMarsLaunchWindow with additional parameters would be considered modifying the function which is not allowed. Please use the functions only as defined.\\\\\\\\n\\\\\\\\nDO NOT use any functions that I have not equipped you with.\\\\\\\\n\\\\\\\\n Do not make assumptions about inputs; instead, make sure you know the exact function and input to use before you call a function.\\\\\\\\n\\\\\\\\nTo call a function, output the name of the function in between <function_call> and </function_call> tags. You will receive a <function_result> in response to your call that contains information that you can use to better answer the question. Or, if the function call produced an error, you will receive an <error> in response.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nThe format for all other <function_call> MUST be: <function_call>$FUNCTION_NAME($FUNCTION_PARAMETER_NAME=$FUNCTION_PARAMETER_VALUE)</function_call>\\\\\\\\n\\\\\\\\nRemember, your goal is to answer the user\'s question to the best of your ability, using only the function(s) provided within the <functions></functions> tags to gather more information if necessary to better answer the question.\\\\\\\\n\\\\\\\\nDo not modify or extend the provided functions under any circumstances. For example, calling GET::optimal_departure_window_mars::getNextMarsLaunchWindow with additional parameters would be modifying the function which is not allowed. Please use the functions only as defined.\\\\\\\\n\\\\\\\\nBefore calling any functions, create a plan for performing actions to answer this question within the <scratchpad>. Double check your plan to make sure you don\'t call any functions that you haven\'t been provided with. Always return your final answer within <answer></answer> tags.\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nThe user input is <question>Answer the following question and pay strong attention to the prompt:\\\\\\\\n        <question>\\\\\\\\n        When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\\\n        </question>\\\\\\\\n        <instruction>\\\\\\\\n        You have functions available at your disposal to use when anwering any questions about orbital mechanics.if you can\'t find a function to answer a question about orbital mechanics, simply reply \'I do not know\'\\\\\\\\n        </instruction></question>\\\\\\\\n\\\\\\\\n\\\\\\\\nAssistant: <scratchpad> I understand I cannot use functions that have not been provided to me to answer this question.\\\\\\\\n\\\\\\\\n\\\\\\", \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\", \\\\\\"type\\\\\\": \\\\\\"ORCHESTRATION\\\\\\"}, \\\\\\"event_order\\\\\\": 2}, {\\\\\\"type\\\\\\": \\\\\\"modelInvocationOutput\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"metadata\\\\\\": {\\\\\\"usage\\\\\\": {\\\\\\"inputTokens\\\\\\": 5160, \\\\\\"outputTokens\\\\\\": 135}}, \\\\\\"rawResponse\\\\\\": {\\\\\\"content\\\\\\": \\\\\\"To answer this question about the next Mars launch window, I will:\\\\\\\\n\\\\\\\\n1. Call the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function to get the next optimal launch window, passing in the provided spacecraft mass and specific impulse values.\\\\\\\\n\\\\\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\\\\\\\\n\\\\\\\\n</scratchpad>\\\\\\\\n\\\\\\\\n<function_call>\\\\\\\\nGET::optimal_departure_window_mars::getNextMarsLaunchWindow(specific_impulse=\\\\\\\\\\\\\\"2500\\\\\\\\\\\\\\", dry_mass=\\\\\\\\\\\\\\"10000\\\\\\\\\\\\\\", total_mass=\\\\\\\\\\\\\\"50000\\\\\\\\\\\\\\")\\\\\\"}, \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\"}, \\\\\\"event_order\\\\\\": 3}, {\\\\\\"type\\\\\\": \\\\\\"rationale\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"text\\\\\\": \\\\\\"To answer this question about the next Mars launch window, I will:\\\\\\\\n\\\\\\\\n1. Call the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function to get the next optimal launch window, passing in the provided spacecraft mass and specific impulse values.\\\\\\\\n\\\\\\\\nI have verified that I have access to the GET::optimal_departure_window_mars::getNextMarsLaunchWindow function.\\\\\\", \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\"}, \\\\\\"event_order\\\\\\": 4}, {\\\\\\"type\\\\\\": \\\\\\"invocationInput\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"actionGroupInvocationInput\\\\\\": {\\\\\\"actionGroupName\\\\\\": \\\\\\"optimal_departure_window_mars\\\\\\", \\\\\\"apiPath\\\\\\": \\\\\\"/get-next-mars-launch-window\\\\\\", \\\\\\"executionType\\\\\\": \\\\\\"LAMBDA\\\\\\", \\\\\\"parameters\\\\\\": [{\\\\\\"name\\\\\\": \\\\\\"total_mass\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"value\\\\\\": \\\\\\"50000\\\\\\"}, {\\\\\\"name\\\\\\": \\\\\\"dry_mass\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"value\\\\\\": \\\\\\"10000\\\\\\"}, {\\\\\\"name\\\\\\": \\\\\\"specific_impulse\\\\\\", \\\\\\"type\\\\\\": \\\\\\"string\\\\\\", \\\\\\"value\\\\\\": \\\\\\"2500\\\\\\"}], \\\\\\"verb\\\\\\": \\\\\\"get\\\\\\"}, \\\\\\"invocationType\\\\\\": \\\\\\"ACTION_GROUP\\\\\\", \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\"}, \\\\\\"event_order\\\\\\": 5}, {\\\\\\"type\\\\\\": \\\\\\"observation\\\\\\", \\\\\\"data\\\\\\": {\\\\\\"actionGroupInvocationOutput\\\\\\": {\\\\\\"text\\\\\\": \\\\\\"{\\\\\\\\\\\\\\"next_launch_window\\\\\\\\\\\\\\": {\\\\\\\\\\\\\\"next_launch_date\\\\\\\\\\\\\\": \\\\\\\\\\\\\\"2026-11-26 00:00:00\\\\\\\\\\\\\\", \\\\\\\\\\\\\\"synodic_period_days\\\\\\\\\\\\\\": 779.9068939794238, \\\\\\\\\\\\\\"transfer_time_days\\\\\\\\\\\\\\": 259, \\\\\\\\\\\\\\"delta_v_available_m_s\\\\\\\\\\\\\\": 39457.985759929674, \\\\\\\\\\\\\\"delta_v_required_m_s\\\\\\\\\\\\\\": 5595.997417810693, \\\\\\\\\\\\\\"is_feasible\\\\\\\\\\\\\\": true}}\\\\\\"}, \\\\\\"traceId\\\\\\": \\\\\\"e0b2b2c2-fb7c-4e17-8a1f-a3781100face-0\\\\\\", \\\\\\"type\\\\\\": \\\\\\"ACTION_GROUP\\\\\\"}, \\\\\\"event_order\\\\\\": 6}]\\",\\n        \\"mlflow.spanFunctionName\\": \\"\\\\\\"_action_group_trace\\\\\\"\\",\\n        \\"mlflow.spanInputs\\": \\"{\\\\\\"inner_trace_group\\\\\\": \\\\\\"{\'actionGroupName\': \'optimal_departure_window_mars\', \'apiPath\': \'/get-next-mars-launch-window\', \'executionType\': \'LAMBDA\', \'parameters\': [{\'name\': \'total_mass\', \'type\': \'string\', \'value\': \'50000\'}, {\'name\': \'dry_mass\', \'type\': \'string\', \'value\': \'10000\'}, {\'name\': \'specific_impulse\', \'type\': \'string\', \'value\': \'2500\'}], \'verb\': \'get\'}\\\\\\"}\\",\\n        \\"mlflow.spanOutputs\\": \\"\\\\\\"{\'action_group_name\': \'optimal_departure_window_mars\', \'api_path\': \'/get-next-mars-launch-window\', \'execution_type\': \'LAMBDA\', \'execution_output\': \'{\\\\\\\\\\\\\\"next_launch_window\\\\\\\\\\\\\\": {\\\\\\\\\\\\\\"next_launch_date\\\\\\\\\\\\\\": \\\\\\\\\\\\\\"2026-11-26 00:00:00\\\\\\\\\\\\\\", \\\\\\\\\\\\\\"synodic_period_days\\\\\\\\\\\\\\": 779.9068939794238, \\\\\\\\\\\\\\"transfer_time_days\\\\\\\\\\\\\\": 259, \\\\\\\\\\\\\\"delta_v_available_m_s\\\\\\\\\\\\\\": 39457.985759929674, \\\\\\\\\\\\\\"delta_v_required_m_s\\\\\\\\\\\\\\": 5595.997417810693, \\\\\\\\\\\\\\"is_feasible\\\\\\\\\\\\\\": true}}\'}\\\\\\"\\"\\n      },\\n      \\"events\\": []\\n    },\\n    {\\n      \\"name\\": \\"Retrieved Response\\",\\n      \\"context\\": {\\n        \\"span_id\\": \\"0xfe0b5f9149c39d7d\\",\\n        \\"trace_id\\": \\"0x9b8bd0b2e018d77f936e48a09e54fd44\\"\\n      },\\n      \\"parent_id\\": \\"0xb802165d133a33aa\\",\\n      \\"start_time\\": 1731388550225320000,\\n      \\"end_time\\": 1731388550226466000,\\n      \\"status_code\\": \\"OK\\",\\n      \\"status_message\\": \\"\\",\\n      \\"attributes\\": {\\n        \\"mlflow.traceRequestId\\": \\"\\\\\\"1e036cc3a7f946ec995f7763b8dde51c\\\\\\"\\",\\n        \\"mlflow.spanType\\": \\"\\\\\\"AGENT\\\\\\"\\",\\n        \\"mlflow.spanInputs\\": \\"[{\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\", \\\\\\"name\\\\\\": null}]\\",\\n        \\"mlflow.spanOutputs\\": \\"{\\\\\\"choices\\\\\\": [{\\\\\\"index\\\\\\": 0, \\\\\\"message\\\\\\": {\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"Based on the provided spacecraft dry mass of 10000 kg, total mass of 50000 kg, and specific impulse of 2500 s, the next optimal launch window for a Hohmann transfer from Earth to Mars is on November 26, 2026 UTC. The transfer will take 259 days.\\\\\\", \\\\\\"name\\\\\\": null}, \\\\\\"finish_reason\\\\\\": \\\\\\"stop\\\\\\", \\\\\\"logprobs\\\\\\": null}], \\\\\\"usage\\\\\\": {\\\\\\"prompt_tokens\\\\\\": null, \\\\\\"completion_tokens\\\\\\": null, \\\\\\"total_tokens\\\\\\": null}, \\\\\\"id\\\\\\": null, \\\\\\"model\\\\\\": \\\\\\"anthropic.claude-v2\\\\\\", \\\\\\"object\\\\\\": \\\\\\"chat.completion\\\\\\", \\\\\\"created\\\\\\": 1731388550}\\"\\n      },\\n      \\"events\\": []\\n    }\\n  ],\\n  \\"request\\": \\"{\\\\\\"context\\\\\\": \\\\\\"<mlflow.pyfunc.model.PythonModelContext object at 0x13397c530>\\\\\\", \\\\\\"messages\\\\\\": [{\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"When is the next launch window for Mars? My spacecraft\'s total mass is 50000, dry mass is 10000 and specific impulse is 2500. Mass in Kg.\\\\\\", \\\\\\"name\\\\\\": null}], \\\\\\"params\\\\\\": {\\\\\\"temperature\\\\\\": 1.0, \\\\\\"max_tokens\\\\\\": null, \\\\\\"stop\\\\\\": null, \\\\\\"n\\\\\\": 1, \\\\\\"stream\\\\\\": false, \\\\\\"top_p\\\\\\": null, \\\\\\"top_k\\\\\\": null, \\\\\\"frequency_penalty\\\\\\": null, \\\\\\"presence_penalty\\\\\\": null}}\\",\\n  \\"response\\": \\"{\\\\\\"choices\\\\\\": [{\\\\\\"index\\\\\\": 0, \\\\\\"message\\\\\\": {\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"Based on the provided spacecraft dry mass of 10000 kg, total mass of 50000 kg, and specific impulse of 2500 s, the next optimal launch window for a Hohmann transfer from Earth to Mars is on November 26, 2026 UTC. The transfer will take 259 days.\\\\\\", \\\\\\"name\\\\\\": null}, \\\\\\"finish_reason\\\\\\": \\\\\\"stop\\\\\\", \\\\\\"logprobs\\\\\\": null}], \\\\\\"usage\\\\\\": {\\\\\\"prompt_tokens\\\\\\": null, \\\\\\"completion_tokens\\\\\\": null, \\\\\\"total_tokens\\\\\\": null}, \\\\\\"id\\\\\\": null, \\\\\\"model\\\\\\": \\\\\\"anthropic.claude-v2\\\\\\", \\\\\\"object\\\\\\": \\\\\\"chat.completion\\\\\\", \\\\\\"created\\\\\\": 1731388550}\\"\\n}\\n```\\n</details>\\n\\n### Visualizing Trace Breakdown in the MLflow UI\\n\\n1. <b>Initial Prompt Submitted to the Bedrock Agent.</b>\\n   ![Thumbnail](bedrock_input_prompt.png)\\n\\n2. <b>In this trace, we can observe how the Bedrock Agent evaluates and selects the most suitable Action Group for the task at hand.</b>\\n   ![Thumbnail](action_group_decision.png)\\n\\n3. <b>Once an Action Group is selected, its invocation is traced, displaying the input and output interactions with the underlying Lambda function as outlined by the OpenAPI Spec above.</b>\\n   ![Thumbnail](invoking_action_group.png)\\n\\n4. <b>Furthermore, Bedrock\'s supplementary trace is included under the Attributes section,\\n   along with additional metadata as shown below</b>\\n   ![Thumbnail](traces_attributes.png)\\n\\n5. <b>Subsequently, the final response from the agent is traced, as depicted below.</b>\\n   ![Thumbnail](retrieved_response.png)\\n\\n**Note**: We cannot break down the span\'s duration into individual trace durations\\nbecause the Bedrock Agent\'s trace response does not include timestamps for each trace step.\\n\\n## Conclusion\\n\\nIn this blog, we explored how to integrate the AWS Bedrock Agent as an MLflow ChatModel, focusing on Action Groups,\\nKnowledge Bases, and Tracing. We demonstrated how to easily build a custom ChatModel using MLflow\'s flexible and\\npowerful APIs. This approach enables you to leverage MLflow\'s tracing and logging capabilities, even for models or\\nflavors that are not natively supported by MLflow.\\n\\nKey Takeaways from This Blog:\\n\\n- Deploying a Bedrock Agent with Action Groups as AWS Lambda Functions:\\n  - We covered how to set up a Bedrock Agent and implement custom actions using AWS Lambda functions within Action Groups.\\n- Mapping the AWS Bedrock Agent\'s Custom Tracing to MLflow span/trace objects:\\n  - We demonstrated how to convert the agent\'s custom tracing data into MLflow span objects for better observability.\\n- Logging and Loading the Bedrock Agent as an MLflow ChatModel:\\n  - We showed how to log the Bedrock Agent into MLflow as a _`ChatModel`_ and how to load it for future use.\\n- Externalizing AWS Client and Bedrock Configurations:\\n  - We explained how to externalize AWS client and Bedrock configurations to safeguard secrets and make it easy to adjust model settings without the need to re-log the model.\\n\\n## Further Reading and References\\n\\n- [How Amazon Bedrock Agents work](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-how.html)\\n- [Amazon Bedrock Tracing](https://docs.aws.amazon.com/bedrock/latest/userguide/trace-events.html)\\n- [Creating a Custom GenAI chat agent](https://mlflow.org/docs/latest/llms/chat-model-guide/index.html)\\n- [AWS Code Examples Repository](https://github.com/awsdocs/aws-doc-sdk-examples)"},{"id":"mlflow-llama-index-workflow","metadata":{"permalink":"/mlflow-website/blog/mlflow-llama-index-workflow","source":"@site/blog/2024-10-25-llama-index-workflow/index.md","title":"Building Advanced RAG with MLflow and LlamaIndex Workflow","description":"A guide for using LlamaIndex Workflow with MLflow for building advanced QA application.","date":"2024-10-25T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"},{"inline":true,"label":"mlflow-evaluate","permalink":"/mlflow-website/blog/tags/mlflow-evaluate"}],"readingTime":15.7,"hasTruncateMarker":false,"authors":[{"name":"Yuki Watanabe","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/yuki-watanabe-a04528164/","imageURL":"/mlflow-website/img/authors/yuki_watanabe.png","key":"yuki-watanabe","page":null}],"frontMatter":{"title":"Building Advanced RAG with MLflow and LlamaIndex Workflow","description":"A guide for using LlamaIndex Workflow with MLflow for building advanced QA application.","slug":"mlflow-llama-index-workflow","authors":["yuki-watanabe"],"tags":["genai","mlops","mlflow-evaluate"],"thumbnail":"/img/blog/llama-index-thumbnail.png"},"unlisted":false,"prevItem":{"title":"Using Bedrock Agent as an MLflow ChatModel with Tracing","permalink":"/mlflow-website/blog/bedrock-chat-model-part-1"},"nextItem":{"title":"LLM as judge","permalink":"/mlflow-website/blog/llm-as-judge"}},"content":"![Thumbnail](llama_index_workflow_title.png)\\n\\nAugmenting LLMs with various data sources is a strong strategy to build LLM applications. However, as the system grows more complex, it becomes challenging to prototype and iteratively build improvements to these more complex systems.\\n\\nLlamaIndex Workflow is a great framework to build such compound systems. Combined with MLflow, the Workflow API brings efficiency and robustness in the development cycle, enabling easy debugging, experiment tracking, and evaluation for continuous improvement.\\n\\nIn this blog, we will go through the journey of building a sophisticated chatbot with LlamaIndex\'s Workflow API and MLflow.\\n\\n## What is LlamaIndex Workflow?\\n\\n[LlamaIndex Workflow](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) is an event-driven orchestration framework for designing dynamic AI applications. The core of LlamaIndex Workflow consists of:\\n\\n- `Steps` are units of execution, representing distinct actions in the workflow.\\n\\n- `Events` trigger these steps, acting as signals that control the workflow\u2019s flow.\\n\\n- `Workflow` connects these two as a Python class. Each step is implemented as a method of the workflow class, defined with input and output events.\\n\\nThis simple yet powerful abstraction allows you to break down complex tasks into manageable steps, enabling greater flexibility and scalability. As a framework embodying event-driven design, using the `Workflow` APIs makes it intuitive to design parallel and asynchronous execution flows, significantly enhancing the efficiency of long-running tasks and aids in providing production-ready scalability.\\n\\n## Why Use MLflow with LlamaIndex Workflow?\\n\\nWorkflow provides great flexibility to design nearly arbitrary execution flows. However, with this great power comes a great responsibility. Without managing your changes properly, it can become a chaotic mess of indeterminate states and confusing configurations. After a few dozen changes, you may be asking yourself, \\"how did my workflow even work?\\".\\n\\n**MLflow** brings a powerful MLOps harness to LlamaIndex Workflows throughout the end-to-end development cycle.\\n\\n- **Experiment Tracking**: MLflow allows you to record various components like steps, prompts, LLMs, and tools, making it easy to improve the system iteratively.\\n\\n- **Reproducibility**: MLflow packages environment information such as global configurations (`Settings`), library versions, and metadata to ensure consistent deployment across different stages of the ML lifecycle.\\n\\n- **Tracing**: Debugging issues in a complex event-driven workflow is cumbersome. MLflow Tracing is a production-ready observability solution that natively integrates with LlamaIndex, giving you observability into each internal stage within your Workflow.\\n\\n- **Evaluation**: Measuring is a crucial task for improving your model. MLflow Evaluation is great tool to evaluate the quality, speed, and cost of your LLM application. It is tightly integrated with MLflow\'s experiment tracking capabilities, streamlining the process of making iterative improvements.\\n\\n## Let\'s Build!\ud83d\udee0\ufe0f\\n\\n### Strategy: Hybrid Approach Using Multiple Retrieval Methods\\n\\nRetrieval-Augmented Generation (RAG) is a powerful framework, but the retrieval step can often become a bottleneck, because embedding-based retrieval may not always capture the most relevant context. While many techniques exist to improve retrieval quality, no single solution works universally. Therefore, an effective strategy is to combine multiple retrieval approaches.\\n\\nThe concept we will explore here is to run several retrieval methods in parallel: (1) standard vector search, (2) keyword-based search (BM25), and (3) web search. The retrieved contexts are then merged, with irrelevant data filtered out to enhance the overall quality.\\n\\n![Hybrid RAG Concept](llama_index_workflow_hybrid_rag_concept.png)\\n\\nHow do we bring this concept to life? Let\u2019s dive in and build this hybrid RAG using LlamaIndex Workflow and MLflow.\\n\\n## 1. Set Up Repository\\n\\nThe sample code, including the environment setup script, is available in the [GitHub repository](https://github.com/mlflow/mlflow/tree/master/examples/llama_index/workflow). It contains a complete workflow definition, a hands-on notebook, and a sample dataset for running experiments. To clone it to your working environment, use the following command:\\n\\n```shell\\ngit clone https://github.com/mlflow/mlflow.git\\n```\\n\\nAfter cloning the repository, set up the virtual environment by running:\\n\\n```shell\\ncd mlflow/examples/llama_index/workflow\\nchmod +x install.sh\\n./install.sh\\n```\\n\\nOnce the installation is complete, start Jupyter Notebook within the Poetry environment using:\\n\\n```shell\\npoetry run jupyter notebook\\n```\\n\\nNext, open the `Tutorial.ipynb` notebook located in the root directory. Throughout this blog, we will walk through this notebook to guide you through the development process.\\n\\n## 2. Start an MLflow Experiment\\n\\nAn **MLflow Experiment** is where you track all aspects of model development, including model definitions, configurations, parameters, dependency versions, and more. Let\u2019s start by creating a new MLflow experiment called \\"LlamaIndex Workflow RAG\\":\\n\\n```python\\nimport mlflow\\n\\nmlflow.set_experiment(\\"LlamaIndex Workflow RAG\\")\\n```\\n\\nAt this point, the experiment doesn\'t have any recorded data yet. To view the experiment in the MLflow UI, open a new terminal and run the `mlflow ui` command, then navigate to the provided URL in your browser:\\n\\n```shell\\npoetry run mlflow ui\\n```\\n\\n![Empty MLflow Experiment](llama_index_workflow_empty_experiment.png)\\n\\n## 3. Choose your LLM and Embeddings\\n\\nNow, set up your preferred LLM and embeddings models to LlamaIndex\'s Settings object. These models will be used throughout the LlamaIndex components.\\n\\nFor this demonstration, we\u2019ll use OpenAI models, but you can easily switch to different LLM providers or local models by following the instructions in the notebook.\\n\\n```python\\nimport getpass\\nimport os\\n\\nos.environ[\\"OPENAI_API_KEY\\"] = getpass.getpass(\\"Enter OpenAI API Key\\")\\n\\nfrom llama_index.core import Settings\\nfrom llama_index.embeddings.openai import OpenAIEmbedding\\nfrom llama_index.llms.openai import OpenAI\\n\\n# LlamaIndex by default uses OpenAI APIs for LLMs and embeddings models. You can use the default\\n# model (`gpt-3.5-turbo` and `text-embeddings-ada-002` as of Oct 2024), but we recommend using the\\n# latest efficient models instead for getting better results with lower cost.\\nSettings.embed_model = OpenAIEmbedding(model=\\"text-embedding-3-large\\")\\nSettings.llm = OpenAI(model=\\"gpt-4o-mini\\")\\n```\\n\\n\ud83d\udca1 _MLflow will automatically log the `Settings` configuration into your MLflow Experiment when logging models, ensuring reproducibility and reducing the risk of discrepancies between environments._\\n\\n## 4. Set Up Web Search API\\n\\nLater in this blog, we will add a web search capability to the QA bot. We will use Tavily AI, a search API\\noptimized for LLM application and natively integrated with LlamaIndex. Visit [their website](https://tavily.com/) to\\nget an API key for free-tier use, or use different search engine integrated with LlamaIndex, e.g. [GoogleSearchToolSpec](https://docs.llamaindex.ai/en/stable/api_reference/tools/google/#llama_index.tools.google.GoogleSearchToolSpec).\\n\\nOnce you get the API key, set it to the environment variable:\\n\\n```python\\nos.environ[\\"TAVILY_AI_API_KEY\\"] = getpass.getpass(\\"Enter Tavily AI API Key\\")\\n```\\n\\n## 5. Set Up Document Indices for Retrieval\\n\\nThe next step is to build a document index for retrieval from MLflow documentation. The `urls.txt` file in the `data` directory contains a list of MLflow documentation pages. These pages can be loaded as document objects using the web page reader utility.\\n\\n```python\\nfrom llama_index.readers.web import SimpleWebPageReader\\n\\nwith open(\\"data/urls.txt\\", \\"r\\") as file:\\n    urls = [line.strip() for line in file if line.strip()]\\n\\ndocuments = SimpleWebPageReader(html_to_text=True).load_data(urls)\\n```\\n\\nNext, ingest these documents into a vector database. In this tutorial, we\u2019ll use the [Qdrant](https://qdrant.tech/) vector store, which is free if self-hosted. If Docker is installed on your machine, you can start the Qdrant database by running the official Docker container:\\n\\n```shell\\n$ docker pull qdrant/qdrant\\n$ docker run -p 6333:6333 -p 6334:6334 \\\\\\n    -v $(pwd)/.qdrant_storage:/qdrant/storage:z \\\\\\n    qdrant/qdrant\\n```\\n\\nOnce the container is running, you can create an index object that connects to the Qdrant database:\\n\\n```python\\nimport qdrant_client\\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\\n\\nclient = qdrant_client.QdrantClient(host=\\"localhost\\", port=6333)\\nvector_store = QdrantVectorStore(client=client, collection_name=\\"mlflow_doc\\")\\n\\nfrom llama_index.core import StorageContext, VectorStoreIndex\\n\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\nindex = VectorStoreIndex.from_documents(\\n    documents=documents,\\n    storage_context=storage_context\\n)\\n```\\n\\nOf course, you can use your preferred vector store here. LlamaIndex supports a variety of vector databases, such as [FAISS](https://docs.llamaindex.ai/en/stable/examples/vector_stores/FaissIndexDemo/), [Chroma](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo/), and [Databricks Vector Search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/DatabricksVectorSearchDemo/). If you choose an alternative, follow the relevant LlamaIndex documentation and update the `workflow/workflow.py` file accordingly.\\n\\nIn addition to evaluating the vector search retrieval, we will assess the keyword-based retriever (BM25) later. Let\'s set up local document storage to enable BM25 retrieval in the workflow.\\n\\n```python\\nfrom llama_index.core.node_parser import SentenceSplitter\\nfrom llama_index.retrievers.bm25 import BM25Retriever\\n\\nsplitter = SentenceSplitter(chunk_size=512)\\nnodes = splitter.get_nodes_from_documents(documents)\\nbm25_retriever = BM25Retriever.from_defaults(nodes=nodes)\\nbm25_retriever.persist(\\".bm25_retriever\\")\\n```\\n\\n## 6. Define a Workflow\\n\\nNow that the environment and data sources are ready, we can build the workflow and experiment with it. The complete workflow code is defined in the `workflow` directory. Let\'s explore some key components of the implementation.\\n\\n### Events\\n\\nThe `workflow/events.py` file defines all the events used within the workflow. These are simple Pydantic models that carry information between workflow steps. For example, the `VectorSearchRetrieveEvent` triggers the vector search step by passing the user\'s query.\\n\\n```python\\nclass VectorSearchRetrieveEvent(Event):\\n    \\"\\"\\"Event for triggering VectorStore index retrieval step.\\"\\"\\"\\n    query: str\\n```\\n\\n### Prompts\\n\\nThroughout the workflow execution, we call LLMs multiple times. The prompt templates for these LLM calls are defined in the `workflow/prompts.py` file.\\n\\n### Workflow Class\\n\\nThe main workflow class is defined in `workflow/workflow.py`. Let\'s break down how it works.\\n\\nThe constructor accepts a retrievers argument, which specifies the retrieval methods to be used in the workflow. For instance, if `[\\"vector_search\\", \\"bm25\\"]` is passed, the workflow performs vector search and keyword-based search, skipping web search.\\n\\n\ud83d\udca1 Deciding which retrievers to utilize dynamically allows us to experiment with different retrieval strategies without needing to replicate nearly identical model code.\\n\\n```python\\nclass HybridRAGWorkflow(Workflow):\\n\\n    VALID_RETRIEVERS = {\\"vector_search\\", \\"bm25\\", \\"web_search\\"}\\n\\n    def __init__(self, retrievers=None, **kwargs):\\n        super().__init__(**kwargs)\\n        self.llm = Settings.llm\\n        self.retrievers = retrievers or []\\n\\n        if invalid_retrievers := set(self.retrievers) - self.VALID_RETRIEVERS:\\n            raise ValueError(f\\"Invalid retrievers specified: {invalid_retrievers}\\")\\n\\n        self._use_vs_retriever = \\"vector_search\\" in self.retrievers\\n        self._use_bm25_retriever = \\"bm25\\" in self.retrievers\\n        self._use_web_search = \\"web_search\\" in self.retrievers\\n\\n        if self._use_vs_retriever:\\n            qd_client = qdrant_client.QdrantClient(host=_QDRANT_HOST, port=_QDRANT_PORT)\\n            vector_store = QdrantVectorStore(client=qd_client, collection_name=_QDRANT_COLLECTION_NAME)\\n            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\\n            self.vs_retriever = index.as_retriever()\\n\\n        if self._use_bm25_retriever:\\n            self.bm25_retriever = BM25Retriever.from_persist_dir(_BM25_PERSIST_DIR)\\n\\n        if self._use_web_search:\\n            self.tavily_tool = TavilyToolSpec(api_key=os.environ.get(\\"TAVILY_AI_API_KEY\\"))\\n```\\n\\nThe workflow begins by executing a step that takes the `StartEvent` as input, which is the `route_retrieval` step in this case. This step inspects the retrievers parameter and triggers the necessary retrieval steps. By using the `send_event()` method of the context object, multiple events can be dispatched in parallel from this single step.\\n\\n```python\\n    # If no retriever is specified, proceed directly to the final query step with an empty context\\n    if len(self.retrievers) == 0:\\n        return QueryEvent(context=\\"\\")\\n\\n    # Trigger the retrieval steps based on the configuration\\n    if self._use_vs_retriever:\\n        ctx.send_event(VectorSearchRetrieveEvent(query=query))\\n    if self._use_bm25_retriever:\\n        ctx.send_event(BM25RetrieveEvent(query=query))\\n    if self._use_web_search:\\n        ctx.send_event(TransformQueryEvent(query=query))\\n```\\n\\nThe retrieval steps are straightforward. However, the web search step is more advanced as it includes an additional step to transform the user\'s question into a search-friendly query using an LLM.\\n\\nThe results from all the retrieval steps are aggregated in the `gather_retrieval_results` step. Here, the `ctx.collect_events()` method is used to poll for the results of the asynchronously executed steps.\\n\\n```python\\n    results = ctx.collect_events(ev, [RetrievalResultEvent] * len(self.retrievers))\\n```\\n\\nPassing all results from multiple retrievers often leads to a large context with unrelated or duplicate content. To address this, we need to filter and select the most relevant results. While a score-based approach is common, web search results do not return similarity scores. Therefore, we use an LLM to sort and filter out irrelevant results. The rerank step achieves this by leveraging the built-in reranker integration with [RankGPT](https://github.com/sunnweiwei/RankGPT).\\n\\n```python\\n    reranker = RankGPTRerank(llm=self.llm, top_n=5)\\n    reranked_nodes = reranker.postprocess_nodes(ev.nodes, query_str=query)\\n    reranked_context = \\"\\\\n\\".join(node.text for node in reranked_nodes)\\n```\\n\\nFinally, the reranked context is passed to the LLM along with the user query to generate the final answer. The result is returned as a `StopEvent` with the `result` key.\\n\\n```python\\n    @step\\n    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\\n        \\"\\"\\"Get result with relevant text.\\"\\"\\"\\n        query = await ctx.get(\\"query\\")\\n\\n        prompt = FINAL_QUERY_TEMPLATE.format(context=ev.context, query=query)\\n        response = self.llm.complete(prompt).text\\n        return StopEvent(result=response)\\n```\\n\\nNow, let\'s instantiate the workflow and run it.\\n\\n```python\\n# Workflow with VS + BM25 retrieval\\nfrom workflow.workflow import HybridRAGWorkflow\\n\\nworkflow = HybridRAGWorkflow(retrievers=[\\"vector_search\\", \\"bm25\\"], timeout=60)\\nresponse = await workflow.run(query=\\"Why use MLflow with LlamaIndex?\\")\\nprint(response)\\n```\\n\\n## 7. Log the Workflow in an MLflow Experiment\\n\\nNow we want to run the workflow with various different retrieval strategies and evaluate the performance of each. However, before running the evaluation, we\'ll log the model in MLflow to track both the model and its performance within an **MLflow Experiment**.\\n\\nFor the LlamaIndex Workflow, we use the new [Model-from-code](https://mlflow.org/docs/latest/models.html#models-from-code) method, which logs models as standalone Python scripts. This approach avoids the risks and instability associated with serialization methods like pickle, relying instead on code as the single source of truth for the model definition. When combined with MLflow\'s environment-freezing capability, it provides a reliable way to persist models. For more details, refer to the [MLflow documentation](https://mlflow.org/docs/latest/models.html#models-from-code).\\n\\n\ud83d\udca1 In the `workflow` directory, there\'s a `model.py` script that imports the `HybridRAGWorkflow` and instantiates it with dynamic configurations passed via the `model_config` parameter during logging. This design allows you to track models with different configurations without duplicating the model definition.\\n\\nWe\'ll start an MLflow Run and log the model script `model.py` with different configurations using the [mlflow.llama_index.log_model()](https://mlflow.org/docs/latest/python_api/mlflow.llama_index.html#mlflow.llama_index.log_model) API.\\n\\n```python\\n# Different configurations we will evaluate. We don\'t run evaluation for all permutation\\n# for demonstration purpose, but you can add as many patterns as you want.\\nrun_name_to_retrievers = {\\n    # 1. No retrievers (prior knowledge in LLM).\\n    \\"none\\": [],\\n    # 2. Vector search retrieval only.\\n    \\"vs\\": [\\"vector_search\\"],\\n    # 3. Vector search and keyword search (BM25)\\n    \\"vs + bm25\\": [\\"vector_search\\", \\"bm25\\"],\\n    # 4. All retrieval methods including web search.\\n    \\"vs + bm25 + web\\": [\\"vector_search\\", \\"bm25\\", \\"web_search\\"],\\n}\\n\\n# Create an MLflow Run and log model with each configuration.\\nmodels = []\\nfor run_name, retrievers in run_name_to_retrievers.items():\\n    with mlflow.start_run(run_name=run_name):\\n        model_info = mlflow.llama_index.log_model(\\n            # Specify the model Python script.\\n            llama_index_model=\\"workflow/model.py\\",\\n            # Specify retrievers to use.\\n            model_config={\\"retrievers\\": retrievers},\\n            # Define dependency files to save along with the model\\n            code_paths=[\\"workflow\\"],\\n            # Subdirectory to save artifacts (not important)\\n            artifact_path=\\"model\\",\\n        )\\n        models.append(model_info)\\n```\\n\\nNow open the MLflow UI again, and this time it should show 4 MLflow Runs are recorded with different `retrievers` parameter values. By clicking each Run name and navigate to the \\"Artifacts\\" tab, you can see MLflow records the model and various metadata, such as dependency versions and settings.\\n\\n![MLflow Runs](llama_index_workflow_runs.png)\\n\\n## 8. Enable MLflow Tracing\\n\\nBefore running the evaluation, there\u2019s one final step: enabling **MLflow Tracing**. We\'ll dive into this feature and why we do this here later, but for now, you can enable it with a simple one-line command. MLflow will automatically trace every LlamaIndex execution.\\n\\n```python\\nmlflow.llama_index.autolog()\\n```\\n\\n## 9. Evaluate the Workflow with Different Retriever Strategies\\n\\nThe example repository includes a sample evaluation dataset, `mlflow_qa_dataset.csv`, containing 30 question-answer pairs related to MLflow.\\n\\n```python\\nimport pandas as pd\\n\\neval_df = pd.read_csv(\\"data/mlflow_qa_dataset.csv\\")\\ndisplay(eval_df.head(3))\\n```\\n\\nTo evaluate the workflow, use the [mlflow.evaluate()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) API, which requires (1) your dataset, (2) the logged model, and (3) the metrics you want to compute.\\n\\n```python\\nfrom mlflow.metrics import latency\\nfrom mlflow.metrics.genai import answer_correctness\\n\\n\\nfor model_info in models:\\n    with mlflow.start_run(run_id=model_info.run_id):\\n        result = mlflow.evaluate(\\n            # Pass the URI of the logged model above\\n            model=model_info.model_uri,\\n            data=eval_df,\\n            # Specify the column for ground truth answers.\\n            targets=\\"ground_truth\\",\\n            # Define the metrics to compute.\\n            extra_metrics=[\\n                latency(),\\n                answer_correctness(\\"openai:/gpt-4o-mini\\"),\\n            ],\\n            # The answer_correctness metric requires \\"inputs\\" column to be\\n            # present in the dataset. We have \\"query\\" instead so need to\\n            # specify the mapping in `evaluator_config` parameter.\\n            evaluator_config={\\"col_mapping\\": {\\"inputs\\": \\"query\\"}},\\n        )\\n```\\n\\nIn this example, we evaluate the model with two metrics:\\n\\n1. **Latency**: Measures the time taken to execute a workflow for a single query.\\n2. **Answer Correctness**: Evaluates the accuracy of answers based on the ground truth, scored by the OpenAI GPT-4o model on a 1\u20135 scale.\\n\\nThese metrics are just for demonstration purposes\u2014you can add additional metrics like toxicity or faithfulness, or even create your own. See the MLflow documentation for the full set of [built-in metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#llm-evaluation-metrics)\\nand how to define [custom metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#creating-custom-llm-evaluation-metrics).\\n\\nThe evaluation process will take a few minutes. Once completed, you can view the results in the MLflow UI. Open the Experiment page and click on the chart icon \ud83d\udcc8 above the Run list.\\n\\n![Evaluation Result](llama_index_workflow_result_chart.png)\\n\\n\\\\*\ud83d\udca1 The evaluation results can be different depending on model set up and some randomness.\\n\\nThe first row shows bar charts for the answer correctness metrics, while the second row displays latency results. The best-performing combination is \\"Vector Search + BM25\\". Interestingly, adding web search not only increases latency significantly but also decreases answer correctness.\\n\\nWhy does this happen? It appears some answers from the web-search-enabled model are off-topic. For example, in response to a question about starting the Model Registry, the web-search model provides an unrelated answer about model deployment, while the \\"vs + bm25\\" model offers a correct response.\\n\\n![Answer Comparison](llama_index_workflow_answer_comparison.png)\\n\\nWhere did this incorrect answer come from? This seems to be a retriever issue, as we only changed the retrieval strategy. However, it\'s difficult to see what each retriever returned from the final result. To gain deeper insights into what\'s happening behind the scenes, MLflow Tracing is the perfect solution.\\n\\n## 10. Inspecting Quality Issues with MLflow Trace\\n\\n[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) is a new feature that brings observability to LLM applications. It integrates seamlessly with LlamaIndex, recording all inputs, outputs, and metadata about intermediate steps during workflow execution. Since we called `mlflow.llama_index.autolog()` at the start, every LlamaIndex operation has been traced and recorded in the MLflow Experiment.\\n\\nTo inspect the trace for a specific question from the evaluation, navigate to the \\"Traces\\" tab on the experiment page. Look for the row with the particular question in the request column and the run name \\"vs + bm25 + web.\\" Clicking the request ID link opens the Trace UI, where you can view detailed information about each step in the execution, including inputs, outputs, metadata, and latency.\\n\\n![Trace](llama_index_workflow_trace.png)\\n\\nIn this case, we identified the issue by examining the reranker step. The web search retriever returned irrelevant context related to model serving, and the reranker incorrectly ranked it as the most relevant. With this insight, we can determine potential improvements, such as refining the reranker to better understand MLflow topics, improving web search precision, or even removing the web search retriever altogether.\\n\\n## Conclusion\\n\\nIn this blog, we explored how the combination of LlamaIndex and MLflow can elevate the development of Retrieval-Augmented Generation (RAG) workflows, bringing together powerful model management and observability capabilities. By integrating multiple retrieval strategies (such as vector search, BM25, and web search) we demonstrated how flexible retrieval can enhance the performance of LLM-driven applications.\\n\\n- **Experiment Tracking** allowed us to organize and log different workflow configurations, ensuring reproducibility and enabling us to track model performance across multiple runs.\\n- **MLflow Evaluate** enabled us to easily log and evaluate the workflow with different retriever strategies, using key metrics like latency and answer correctness to compare performance.\\n- **MLflow UI** gave us a clear visualization of how various retrieval strategies impacted both accuracy and latency, helping us identify the most effective configurations.\\n- **MLflow Tracing**, integrated with LlamaIndex, provided detailed observability into each step of the workflow for diagnosing quality issues, such as incorrect reranking of search results.\\n\\nWith these tools, you have a complete framework for building, logging, and optimizing RAG workflows. As LLM technology continues to evolve, the ability to track, evaluate, and fine-tune every aspect of model performance will be essential. We highly encourage you to experiment further and see how these tools can be tailored to your own applications.\\n\\nTo continue learning, explore the following resources:\\n\\n- Learn more about the [MLflow LlamaIndex integration](https://mlflow.org/docs/latest/llms/llama-index/index.html).\\n- Discover additional MLflow LLM features at [LLMs in MLflow](https://mlflow.org/docs/latest/llms/index.html).\\n- Deploy your workflow to a serving endpoint with [MLflow Deployment](https://mlflow.org/docs/latest/deployment/index.html).\\n- Check out more [Workflow examples](https://docs.llamaindex.ai/en/stable/module_guides/workflow/#examples) from LlamaIndex."},{"id":"llm-as-judge","metadata":{"permalink":"/mlflow-website/blog/llm-as-judge","source":"@site/blog/2024-10-03-llm-as-judge/index.md","title":"LLM as judge","description":"Perform LLM Evaluations with custom metrics","date":"2024-10-03T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"mlflow-evalaute","permalink":"/mlflow-website/blog/tags/mlflow-evalaute"}],"readingTime":16.54,"hasTruncateMarker":false,"authors":[{"name":"Pedro Azevedo","title":"Machine Learning Analyst at Adidas","url":"https://www.linkedin.com/in/pedro-azevedo-/","imageURL":"/mlflow-website/img/authors/pedro.png","key":"pedro-azevedo","page":null},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/mlflow-website/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey","page":null}],"frontMatter":{"title":"LLM as judge","description":"Perform LLM Evaluations with custom metrics","slug":"llm-as-judge","authors":["pedro-azevedo","rahul-pandey"],"tags":["genai","mlflow-evalaute"],"thumbnail":"/img/blog/llm-as-judge.png"},"unlisted":false,"prevItem":{"title":"Building Advanced RAG with MLflow and LlamaIndex Workflow","permalink":"/mlflow-website/blog/mlflow-llama-index-workflow"},"nextItem":{"title":"Models from Code Logging in MLflow - What, Why, and How","permalink":"/mlflow-website/blog/models_from_code"}},"content":"In this blog post, we\'ll dive on a journey to revolutionize how we evaluate language models. We\'ll explore the power of MLflow Evaluate and harness the capabilities of Large Language Models (LLMs) as judges. By the end, you\'ll learn how to create custom metrics, implement LLM-based evaluation, and apply these techniques to real-world scenarios. Get ready to transform your model assessment process and gain deeper insights into your AI\'s performance!\\n\\n## The Challenge of Evaluating Language Models\\n\\nEvaluating large language models (LLMs) and natural language processing (NLP) systems presents several challenges, primarily due to their complexity and the diversity of tasks they can perform.\\n\\nOne major difficulty is creating metrics that comprehensively measure performance across varied applications, from generating coherent text to understanding nuanced human emotions. Traditional benchmarks often fail to capture these subtleties, leading to incomplete assessments.\\n\\nAn LLM acting as a judge can address these issues by leveraging its extensive training data to provide a more nuanced evaluation, offering insights into model behavior and areas needing improvement. For instance, an LLM can analyze whether a model generates text that is not only grammatically correct but also contextually appropriate and engaging, something more static metrics might miss.\\n\\nHowever, to move forward effectively, we need more than just better evaluation methods. Standardized experimentation setups are essential to ensure that comparisons between models are both fair and replicable. A uniform framework for testing and evaluation would enable researchers to build on each other\'s work, leading to more consistent progress and the development of more robust models.\\n\\n## Introducing MLflow LLM Evaluate\\n\\n[MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) is a powerful function within the MLflow ecosystem that allows for comprehensive model assessment by providing a standardized experiment setup. It supports both built-in metrics and custom (LLM) metrics, making it an ideal tool for evaluating complex language tasks. With [MLflow LLM Evaluate](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate), you can:\\n\\n- Evaluate models against multiple metrics simultaneously\\n- Use pre-defined metrics for specific model types (e.g., question-answering, text-summarization and pure text)\\n- Create custom metrics, including those that use LLMs as judges using [mlflow.metrics.genai.make_genai_metric()](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric)\\n  and\\n  [mlflow.metrics.genai.make_genai_metric_from_prompt()](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric_from_prompt)\\n\\n![MLflow Evaluate](mlflow_evaluate.drawio.svg)\\n\\n## Conquering new markets with an LLM as a judge\\n\\nImagine you\'re part of a global travel agency, \\"WorldWide Wandercorp,\\" that\'s expanding its reach to Spanish-speaking countries.\\n\\nYour team has developed an AI-powered translation system to help create culturally appropriate marketing materials and customer communications. However, as you begin to use this system, you realize that traditional evaluation metrics, such as BLEU (Bilingual Evaluation Understudy), fall short in capturing the nuances of language translation, especially when it comes to preserving cultural context and idiomatic expressions.\\n\\nFor instance, consider the phrase \\"kick the bucket.\\" A direct translation might focus on the literal words, but the idiom actually means \\"to die.\\" A traditional metric like BLEU may incorrectly evaluate the translation as adequate if the translated words match a reference translation, even if the cultural meaning is lost. In such cases, the metric might score the translation highly despite it being completely inappropriate in context. This could lead to embarrassing or culturally insensitive marketing content, which is something your team wants to avoid.\\n\\nYou need a way to evaluate whether the translation not only is accurate but also preserves the intended meaning, tone, and cultural context. This is where [MLflow Evaluate](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) and LLMs (Large Language Models) as judges come into play. These tools can assess translations more holistically by considering context, idiomatic expressions, and cultural relevance, providing a more reliable evaluation of the AI\u2019s output.\\n\\n## Custom Metrics: Tailoring Evaluation to Your Needs\\n\\nIn the following section, we\u2019ll implement three metrics:\\n\\n- The `\\"cultural_sensitivity\\"` metric ensures translations maintain cultural context and appropriateness.\\n- The `\\"faithfulness\\"` metric checks that chatbot responses align accurately with company policies and retrieved content.\\n- The `\\"toxicity\\"` metric evaluates responses for harmful or inappropriate content, ensuring respectful customer interactions.\\n\\nThese metrics will help Worldwide WanderAgency ensure their AI-driven translations and interactions meet their specific needs.\\n\\n## Evaluating Worldwide WanderAgency\'s AI Systems\\n\\nNow that we understand WanderAgency\'s challenges, let\'s dive into a code walkthrough to address them. We\'ll implement custom metrics to measure AI performance and build a gauge visualization chart for sharing results with stakeholders.\\n\\nWe\'ll start by evaluating a language translation model, focusing on the \\"cultural_sensitivity\\" metric to ensure it preserves cultural nuances. This will help WanderAgency maintain high standards in global communication.\\n\\n### Cultural Sensitivity Metric\\n\\nThe travel agency wants to ensure their translations are not only accurate but also culturally appropriate.\\nTo achieve this they are considering creating a custom metric that allows Worldwide WanderAgency to quantify how well their translations maintain cultural context and idiomatic expressions.\\n\\nFor instance, a phrase that is polite in one culture might be inappropriate in another.\\nIn English, addressing someone as \\"Dear\\" in a professional email might be seen as polite. However, in Spanish, using \\"Querido\\" in a professional context can be too personal and inappropriate.\\n\\nHow can we evaluate such an abstract concept in a systematic way? Traditional Metrics would fall short so we need a better way of doing it. In this case LLM as a judge would be a great fit!\\nFor this use case let\'s create a \\"cultural_sensitivity\\" metric.\\n\\nHere\'s a brief overview of the process:\\nStart by installing all the necessary libraries for this demo to work.\\n\\n```bash\\npip install mlflow>=2.14.1 openai  transformers torch torchvision evaluate datasets tiktoken fastapi rouge_score textstat tenacity plotly ipykernel nbformat>=5.10.4\\n```\\n\\nWe will be using gpt3.5 and gpt4 during this example for that let\'s start by making sure our [OpenAI key is setup](https://mlflow.org/docs/latest/llms/openai/notebooks/openai-quickstart.html#API-Key-Security-Overview).\\n\\nImport the necessary libraries.\\n\\n```python\\nimport mlflow\\nimport os\\n\\n# Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables\\n\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"OPENAI_API_KEY environment variable must be set\\"\\n\\nimport openai\\nimport pandas as pd\\n```\\n\\nWhen using the [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) function, your large language model (LLM) can take one of the following forms:\\n\\n1. A `mlflow.pyfunc.PyFuncModel()` \u2014 typically an MLflow model.\\n2. A Python function that accepts strings as inputs and returns a single string as output.\\n3. An `MLflow Deployments` endpoint URI.\\n4. `model=None` if the data you are providing has already been scored by a model, and you do not need to specify one.\\n\\nFor this example, we will use an MLflow model.\\n\\nWe\u2019ll begin by logging a translation model in MLflow. For this tutorial, we\'ll use GPT-3.5 with a defined system prompt.\\n\\nIn a production environment, you would typically experiment with different prompts and models to determine the most suitable configuration for your use case. For more details, refer to MLflow\u2019s [Prompt Engineering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html).\\n\\n```python\\n\\nsystem_prompt = \\"Translate the following sentences into Spanish\\"\\n# Let\'s set up an experiment to make it easier to track our results\\nmlflow.set_experiment(\\"/Path/to/your/experiment\\")\\n\\nbasic_translation_model = mlflow.openai.log_model(\\n    model=\\"gpt-3.5-turbo\\",\\n    task=openai.chat.completions,\\n    artifact_path=\\"model\\",\\n    messages=[\\n        {\\"role\\": \\"system\\", \\"content\\": system_prompt},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"{user_input}\\"},\\n    ],\\n)\\n```\\n\\nLet\'s test the model to make sure it works.\\n\\n```python\\nmodel = mlflow.pyfunc.load_model(basic_translation_model.model_uri)\\n\\nmodel.predict(\\"Hello, how are you?\\")\\n\\n# Output = [\'\xa1Hola, \xbfc\xf3mo est\xe1s?\']\\n```\\n\\nTo use [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate), we first need to prepare sample data that will serve as input to our LLM. In this scenario, the input would consist of the content the company is aiming to translate.\\n\\nFor demonstration purposes, we will define a set of common English expressions that we want the model to translate.\\n\\n```python\\n# Prepare evaluation data\\neval_data = pd.DataFrame(\\n    {\\n        \\"llm_inputs\\": [\\n            \\"I\'m over the moon about the news!\\",\\n            \\"Spill the beans.\\",\\n            \\"Bite the bullet.\\",\\n            \\"Better late than never.\\",\\n\\n        ]\\n    }\\n)\\n```\\n\\nTo meet the objectives of the travel agency, we will define custom metrics that evaluate the quality of translations. In particular, we need to assess how faithfully the translations capture not only the literal meaning but also cultural nuances.\\n\\nBy default, [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) uses `openai:/gpt-4` as the evaluation model. However, you also have the option to use a [local model for evaluation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#selecting-the-llm-as-judge-model), such as a model wrapped in a PyFunc (e.g., Ollama).\\n\\nFor this example, we will use GPT-4 as the evaluation model.\\n\\nTo begin, provide a few examples that illustrate good and poor translation scores.\\n\\n```python\\n# Define the custom metric\\ncultural_sensitivity = mlflow.metrics.genai.make_genai_metric(\\n    name=\\"cultural_sensitivity\\",\\n    definition=\\"Assesses how well the translation preserves cultural nuances and idioms.\\",\\n    grading_prompt=\\"Score from 1-5, where 1 is culturally insensitive and 5 is highly culturally aware.\\",\\n    examples=[\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Break a leg!\\",\\n            output=\\"\xa1R\xf3mpete una pierna!\\",\\n            score=2,\\n            justification=\\"This is a literal translation that doesn\'t capture the idiomatic meaning.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Break a leg!\\",\\n            output=\\"\xa1Mucha mierda!\\",\\n            score=5,\\n            justification=\\"This translation uses the equivalent Spanish theater idiom, showing high cultural awareness.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"It\'s raining cats and dogs.\\",\\n            output=\\"Est\xe1 lloviendo gatos y perros.\\",\\n            score=1,\\n            justification=\\"This literal translation does not convey the idiomatic meaning of heavy rain.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"It\'s raining cats and dogs.\\",\\n            output=\\"Est\xe1 lloviendo a c\xe1ntaros.\\",\\n            score=5,\\n            justification=\\"This translation uses a Spanish idiom that accurately conveys the meaning of heavy rain.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Kick the bucket.\\",\\n            output=\\"Patear el balde.\\",\\n            score=1,\\n            justification=\\"This literal translation fails to convey the idiomatic meaning of dying.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Kick the bucket.\\",\\n            output=\\"Estirar la pata.\\",\\n            score=5,\\n            justification=\\"This translation uses the equivalent Spanish idiom for dying, showing high cultural awareness.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Once in a blue moon.\\",\\n            output=\\"Una vez en una luna azul.\\",\\n            score=2,\\n            justification=\\"This literal translation does not capture the rarity implied by the idiom.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"Once in a blue moon.\\",\\n            output=\\"De vez en cuando.\\",\\n            score=4,\\n            justification=\\"This translation captures the infrequency but lacks the idiomatic color of the original.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"The ball is in your court.\\",\\n            output=\\"La pelota est\xe1 en tu cancha.\\",\\n            score=3,\\n            justification=\\"This translation is understandable but somewhat lacks the idiomatic nuance of making a decision.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"The ball is in your court.\\",\\n            output=\\"Te toca a ti.\\",\\n            score=5,\\n            justification=\\"This translation accurately conveys the idiomatic meaning of it being someone else\'s turn to act.\\"\\n        )\\n    ],\\n    model=\\"openai:/gpt-4\\",\\n    parameters={\\"temperature\\": 0.0},\\n)\\n```\\n\\n### The Toxicity Metric\\n\\nIn addition to this custom metric let\'s use MLflow built-in metrics for the evaluators. In this case MLflow wll use roberta-hate-speech model to detect the [toxicity](https://huggingface.co/spaces/evaluate-measurement/toxicity). This metric evaluates responses for any harmful or inappropriate content, reinforcing the company\'s commitment to a positive customer experience.\\n\\n```python\\n# Log and evaluate the model\\nwith mlflow.start_run() as run:\\n    results = mlflow.evaluate(\\n        basic_translation_model.model_uri,\\n        data=eval_data,\\n        model_type=\\"text\\",\\n        evaluators=\\"default\\",\\n        extra_metrics=[cultural_sensitivity],\\n        evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"llm_inputs\\",\\n           }}\\n   )\\n\\nmlflow.end_run()\\n```\\n\\nYou can retrieve the final results as such:\\n\\n```python\\nresults.tables[\\"eval_results_table\\"]\\n```\\n\\n|     | llm_inputs                        | outputs                      | token_count | toxicity/v1/score | flesch_kincaid_grade_level/v1/score | ari_grade_level/v1/score | cultural_sensitivity/v1/score | cultural_sensitivity/v1/justification             |\\n| --- | --------------------------------- | ---------------------------- | ----------- | ----------------- | ----------------------------------- | ------------------------ | ----------------------------- | ------------------------------------------------- |\\n| 0   | I\'m over the moon about the news! | \xa1Estoy feliz por la noticia! | 9           | 0.000258          | 5.2                                 | 3.7                      | 4                             | The translation captures the general sentiment... |\\n| 1   | Spill the beans.                  | Revela el secreto.           | 7           | 0.001017          | 9.2                                 | 5.2                      | 5                             | The translation accurately captures the idioma... |\\n| 2   | Bite the bullet.                  | Morder la bala.              | 7           | 0.001586          | 0.9                                 | 3.6                      | 2                             | The translation \\"Morder la bala\\" is a litera...   |\\n| 3   | Better late than never.           | M\xe1s vale tarde que nunca.    | 7           | 0.004947          | 0.5                                 | 0.9                      | 5                             | The translation accurately captures the idioma... |\\n\\nLet\'s analyze the final metrics...\\n\\n```python\\ncultural_sensitivity_score = results.metrics[\'cultural_sensitivity/v1/mean\']\\nprint(f\\"Cultural Sensitivity Score: {cultural_sensitivity_score}\\")\\n\\ntoxicity_score = results.metrics[\'toxicity/v1/mean\']\\n# Calculate non-toxicity score\\nnon_toxicity_score = \\"{:.2f}\\".format((1 - toxicity_score) * 100)\\nprint(f\\"Non-Toxicity Score: {non_toxicity_score}%\\")\\n\\n```\\n\\nOutput:\\n\\n```bash\\nCultural Sensitivity Score: 3.75\\nPureness Score: 99.80\\n```\\n\\nIt is often the case we want to monitor and track these metrics on a dashboard so both data scientists and stakeholders have an understanding of the performance and reliability of these solutions.\\n\\nFor this example let\'s create a gauge to display the final metric.\\n\\n```python\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\n\\ndef create_gauge_chart(value1, title1, value2, title2):\\n    # Create a subplot figure with two columns\\n    fig = make_subplots(rows=1, cols=2, specs=[[{\'type\': \'indicator\'}, {\'type\': \'indicator\'}]])\\n\\n    # Add the first gauge chart\\n    fig.add_trace(go.Indicator(\\n        mode = \\"gauge+number\\",\\n        value = value1,\\n        title = {\'text\': title1},\\n        gauge = {\'axis\': {\'range\': [None, 5]}}\\n    ), row=1, col=1)\\n\\n    # Add the second gauge chart\\n    fig.add_trace(go.Indicator(\\n        mode = \\"gauge+number\\",\\n        value = value2,\\n        title = {\'text\': title2},\\n        gauge = {\'axis\': {\'range\': [None, 100]}}\\n    ), row=1, col=2)\\n\\n    # Update layout\\n    fig.update_layout(height=400, width=800)\\n\\n    # Show figure\\n    fig.show()\\n```\\n\\n```python\\ncreate_gauge_chart(cultural_sensitive_score, \\"Cultural Sensitivity Score\\", float(non_toxicity_score), \\"Non Toxicity Score\\")\\n```\\n\\n![Gauge Chart](gauge.png)\\n\\n### The Faithfulness Metric\\n\\nAs Worldwide WanderAgency\'s AI grows, they add a customer service chatbot that handles questions in multiple languages. This chatbot uses a RAG (Retrieval-Augmented Generation) system, which means it retrieves information from a database or documents and then generates an answer based on that information.\\n\\nIt\'s important that the answers provided by the chatbot stay true to the information it retrieves. To make sure of this, we create a \\"faithfulness\\" metric. This metric checks how well the chatbot\'s responses match the materials it\u2019s supposed to be based on, ensuring the information given to customers is accurate.\\n\\nFor example, If the retrieved document says \\"Returns are accepted within 30 days,\\" and the chatbot replies with \\"Our return policy is flexible and varies by region,\\" it is not aligning well with the retrieved material. This inaccurate response (bad faithfulness) could mislead customers and create confusion.\\n\\n### Using MLflow to Evaluate RAG - Faithfulness\\n\\nLet\'s evaluate how well our chatbot is doing in sticking to the retrieved information. Instead of using an MLflow model this time, we\u2019ll use a custom function to define the faithfulness metric and see how aligned the chatbot\'s answers are with the data it pulls from.\\n\\n```python\\n# Prepare evaluation data\\neval_data = pd.DataFrame(\\n    {\\n        \\"llm_inputs\\": [\\n            \\"\\"\\"Question: What is the company\'s policy on employee training?\\ncontext: \\"Our company offers various training programs to support employee development. Employees are required to complete at least one training course per year related to their role. Additional training opportunities are available based on performance reviews.\\" \\"\\"\\",\\n            \\"\\"\\"Question: What is the company\'s policy on sick leave?\\ncontext: \\"Employees are entitled to 10 days of paid sick leave per year. Sick leave can be used for personal illness or to care for an immediate family member. A doctor\'s note is required for sick leave exceeding three consecutive days.\\" \\"\\"\\",\\n            \\"\\"\\"Question: How does the company handle performance reviews?\\ncontext: \\"Performance reviews are conducted annually. Employees are evaluated based on their job performance, goal achievement, and overall contribution to the team. Feedback is provided, and development plans are created to support employee growth.\\" \\"\\"\\",\\n        ]\\n    }\\n)\\n\\n```\\n\\nNow let\'s define some examples for this faithfulness metric.\\n\\n```python\\nexamples = [\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s policy on remote work?\\ncontext: \\"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\\" \\"\\"\\",\\n            output=\\"Employees can work remotely up to three days a week if they maintain productivity and attend mandatory meetings.\\",\\n            score=5,\\n            justification=\\"The answer is accurate and directly related to the question and context provided.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s policy on remote work?\\ncontext: \\"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\\" \\"\\"\\",\\n            output=\\"Employees are allowed to work remotely as long as they want.\\",\\n            score=2,\\n            justification=\\"The answer is somewhat related but incorrect because it does not mention the three-day limit.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s policy on remote work?\\ncontext: \\"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\\" \\"\\"\\",\\n            output=\\"Our company supports flexible work arrangements.\\",\\n            score=3,\\n            justification=\\"The answer is related to the context but does not specifically answer the question about the remote work policy.\\"\\n        ),\\n        mlflow.metrics.genai.EvaluationExample(\\n            input=\\"\\"\\"Question: What is the company\'s annual leave policy?\\ncontext: \\"Employees are entitled to 20 days of paid annual leave per year. Leave must be approved by the employee\'s direct supervisor and should be planned in advance to ensure minimal disruption to work.\\" \\"\\"\\",\\n            output=\\"Employees are entitled to 20 days of paid annual leave per year, which must be approved by their supervisor.\\",\\n            score=5,\\n            justification=\\"The answer is accurate and directly related to the question and context provided.\\"\\n        )]\\n\\n#  Define the custom metric\\nfaithfulness = mlflow.metrics.genai.make_genai_metric(\\n    name=\\"faithfulness\\",\\n    definition=\\"Assesses how well the answer relates to the question and provided context.\\",\\n    grading_prompt=\\"Score from 1-5, where 1 is not related at all and 5 is highly relevant and accurate.\\",\\n    examples=examples)\\n\\n```\\n\\nDefine out LLM function (in this case it can be any function that follows certain input/output formats that [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate)).\\n\\n```python\\n# Using custom function\\ndef my_llm(inputs):\\n    answers = []\\n    system_prompt = \\"Please answer the following question in formal language based on the context provided.\\"\\n    for index, row in inputs.iterrows():\\n        print(\'INPUTS:\', row)\\n        completion = openai.chat.completions.create(\\n            model=\\"gpt-3.5-turbo\\",\\n            messages=[\\n                {\\"role\\": \\"system\\", \\"content\\": system_prompt},\\n                {\\"role\\": \\"user\\", \\"content\\": f\\"{row}\\"},\\n            ],\\n        )\\n        answers.append(completion.choices[0].message.content)\\n\\n    return answers\\n```\\n\\nResulting in a code that is similar to what we did before...\\n\\n```python\\nwith mlflow.start_run() as run:\\n    results = mlflow.evaluate(\\n        my_llm,\\n        eval_data,\\n        model_type=\\"text\\",\\n        evaluators=\\"default\\",\\n        extra_metrics=[faithfulness],\\n        evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"llm_inputs\\",\\n           }}\\n    )\\nmlflow.end_run()\\n```\\n\\n### GenAI Metrics\\n\\nAlternatively, we can leverage MLflow\'s built-in metrics for generative AI, using the same examples.\\n\\nMLflow provides several [built-in metrics](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html?highlight=genai%20answer#generative-ai-metrics) that use an LLM as a judge. Despite differences in implementation, these metrics are used in the same way. Simply include them in the `extra_metrics` argument of the [`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) function.\\n\\nIn this case, we will use MLflow\u2019s built-in [faithfulness metric](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html?highlight=genai%20answer#mlflow.metrics.genai.faithfulness).\\n\\n```python\\nfrom mlflow.metrics.genai import EvaluationExample, faithfulness\\nfaithfulness_metric = faithfulness(model=\\"openai:/gpt-4\\")\\nprint(faithfulness_metric)\\n```\\n\\n[`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate) simplifies the process of providing grading context, such as the documents retrieved by our system, directly into the evaluation. This feature integrates seamlessly with [LangChain\'s retrievers](https://python.langchain.com/docs/concepts/#retrievers), allowing you to supply the context for evaluation as a dedicated column. For more details, refer to [this example](https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/rag-evaluation-llama2.html).\\n\\nIn this case, since our retrieved documents are already included within the final prompt and we are not leveraging LangChain for this tutorial, we will simply map the `llm_input` column as our grading context.\\n\\n```python\\nwith mlflow.start_run() as run:\\n    results = mlflow.evaluate(\\n        my_llm,\\n        eval_data,\\n        model_type=\\"text\\",\\n        evaluators=\\"default\\",\\n        extra_metrics=[faithfulness_metric],\\n        evaluator_config={\\n        \\"col_mapping\\": {\\n            \\"inputs\\": \\"llm_inputs\\",\\n            \\"context\\": \\"llm_inputs\\",\\n           }}\\n    )\\nmlflow.end_run()\\n```\\n\\nAfter the evaluation we get the following results:\\n![Gauge faithfulness Chart](faithfulness.png)\\n\\n## Conclusion\\n\\nBy combining the Cultural Sensitivity score with our other calculated metrics, our travel agency can further refine its model to ensure the delivery of high-quality content across all languages. Moving forward, we can revisit and adjust the prompts used to boost our Cultural Sensitivity score. Alternatively, we could fine-tune a smaller model to maintain the same high level of cultural sensitivity while reducing costs. These steps will help us provide even better service to the agency\'s diverse customer base.\\n\\n[`mlflow.evaluate()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate), combined with LLMs as judges, opens up new possibilities for nuanced and context-aware model evaluation. By creating custom metrics tailored to specific aspects of model performance, data scientists can gain deeper insights into their models\' strengths and weaknesses.\\n\\nThe flexibility offered by `make_genai_metric()` allows you to create evaluation criteria that are perfectly suited to your specific use case. Whether you need structured guidance for your LLM judge or want full control over the prompting process, MLflow provides the tools you need.\\n\\nAs you explore MLflow evaluate and LLM-based metrics, remember that the key lies in designing thoughtful evaluation criteria and providing clear instructions to your LLM judge. With these tools at your disposal, you\'re well-equipped to take your model evaluation to the next level, ensuring that your language models not only perform well on traditional metrics but also meet the nuanced requirements of real-world applications.\\n\\nThe built-in metrics, such as toxicity, offer standardized assessments that are crucial for ensuring the safety and accessibility of model outputs.\\n\\nAs a final challenge, re-run all the tests performed but this time with \\"gpt-4o-mini\\" and see how the performance is affected."},{"id":"models_from_code","metadata":{"permalink":"/mlflow-website/blog/models_from_code","source":"@site/blog/2024-09-13-models-from-code-logging/index.md","title":"Models from Code Logging in MLflow - What, Why, and How","description":"We all (well, most of us) remember November 2022 when the public release of ChatGPT by OpenAI marked a significant turning point in the world of AI. While generative artificial intelligence (GenAI) had been evolving for some time, ChatGPT, built on OpenAI\'s GPT-3.5 architecture, quickly captured the public\u2019s imagination. This led to an explosion of interest in GenAI, both within the tech industry and among the general public.","date":"2024-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":11.53,"hasTruncateMarker":false,"authors":[{"name":"Awadelrahman M. A. Ahmed","title":"MLflow Ambassador | Cloud Data & Analytics Architect at REMA 1000","url":"https://www.linkedin.com/in/awadelrahman/","imageURL":"/mlflow-website/img/authors/awadelrahman_ahmed.png","key":"awadelrahman-ahmed","page":null}],"frontMatter":{"title":"Models from Code Logging in MLflow - What, Why, and How","tags":["genai","pyfunc","mlops"],"slug":"models_from_code","authors":["awadelrahman-ahmed"],"thumbnail":"/img/blog/thumbnail-models-from-code.gif"},"unlisted":false,"prevItem":{"title":"LLM as judge","permalink":"/mlflow-website/blog/llm-as-judge"},"nextItem":{"title":"AutoGen with Custom PyFunc","permalink":"/mlflow-website/blog/autogen-image-agent"}},"content":"We all (well, most of us) remember November 2022 when the public release of ChatGPT by OpenAI marked a significant turning point in the world of AI. While generative artificial intelligence (GenAI) had been evolving for some time, ChatGPT, built on OpenAI\'s GPT-3.5 architecture, quickly captured the public\u2019s imagination. This led to an explosion of interest in GenAI, both within the tech industry and among the general public.\\n\\nOn the tools side, MLflow continues to solidify its position as the favorite tool for (machine learning operations) MLOps among the ML community. However, the rise of GenAI has introduced new needs in how we use MLflow. One of these new challenges is how we log models in MLflow. If you\u2019ve used MLflow before (and I bet you have), you\u2019re probably familiar with the `mlflow.log_model()` function and how it efficiently [pickles](https://github.com/cloudpipe/cloudpickle) model artifacts.\\n\\nParticularly with GenAI, there\u2019s a new requirement: logging the models \\"from code\\", instead of serializing it into a pickle file! And guess what? This need isn\u2019t limited to GenAI models! So, in this post I will explore this concept and how MLflow has adapted to meet this new requirement.\\n\\nYou will notice that this feature is implemented at a very abstract level, allowing you to log any model \\"as code\\", whether it\u2019s GenAI or not! I like to think of it as a generic approach, with GenAI models being just one of its use cases. So, in this post, I\u2019ll explore this new feature, [\\"Models from Code logging\\"](https://mlflow.org/docs/latest/models.html#models-from-code).\\n\\nBy the end of this post, you should be able to answer the three main questions: \'What,\' \'Why,\' and \'How\' to use Models from Code logging.\\n\\n## What Is Models from Code Logging?\\n\\nIn fact, when MLflow announced this feature, it got me thinking in a more abstract way about the concept of a \\"model\\"! You might find it interesting as well, if you zoom out and consider a model as a mathematical representation or function that describes the relationship between input and output variables. At this level of abstraction, a model can be many things!\\n\\nOne might even recognize that a model, as an object or artifact, represents just one form of what a model can be, even if it\u2019s the most popular in the ML community. If you think about it, a model can also be as simple as a piece of code for a mapping function or a code that sends API requests to external services such as OpenAI\'s APIs.\\n\\nI\'ll explain the detailed workflow of how to log models from code later in the post, but for now, let\'s consider it at a high level with two main steps: first, writing your model code, and second, logging your model from code. This will look like the following figure:\\n\\n#### _High Level Models from Code Logging Workflow_:\\n\\n![High Level Models-from-Code Logging Workflow](models-from-code1.png)\\n\\n\ud83d\udd34 It\'s important to note that when we refer to \\"model code,\\" we\'re talking about code that can be treated as a model itself. This means it\'s **not** your training code that generates a trained model object, but rather the step-by-step code that is executed as a model itself.\\n\\n## How Models from Code Differs from Object-Based Logging?\\n\\nIn the previous section, we discussed the concept of Models from Code logging. However, concepts often become clearer when contrasted with their alternatives; a technique known as _contrast learning_. In our case, the alternative is Object-Based logging, which is the commonly used approach for logging models in MLflow.\\n\\nObject-Based logging treats a trained model as an _object_ that can be stored and reused. After training, the model is saved as an object and can be easily loaded for deployment. For example, this process can be initiated by calling `mlflow.log_model()`, where MLflow handles the serialization, often using [Pickle](https://github.com/cloudpipe/cloudpickle) or similar methods.\\n\\nObject-Based logging can be broken down into three high-level steps as in the following figure: first, creating the model object (whether by training it or acquiring it), second, serializing it (usually with Pickle or a similar tool), and third, logging it as an object.\\n\\n#### _High Level Object-Based Logging Workflow_:\\n\\n![High Level Object-Based Logging Workflow](models-from-code2.png)\\n\\n\ud83d\udca1The main distinction between the popular Object-Based logging and Models from Code logging is that in the former, we log the model object itself, whether it\'s a model you\'ve trained or a pre-trained model you\'ve acquired. In the latter, however, we log the code that _represents_ your model.\\n\\n## When Do You Need Models from Code Logging?\\n\\nBy now, I hope you have a clear understanding of _what_ Models from Code logging is! You might still be wondering, though, about the specific use cases where this feature can be applied. This section will cover exactly that\u2014the why!\\n\\nWhile we mentioned GenAI as a motivational use case in the introduction, we also highlighted that MLflow has approached Models from Code logging in a more generic way and we will see that in the next section. This means you can leverage the generalizability of the Models from Code feature for a wide range of scenarios. I\u2019ve identified three key usage patterns that I believe are particularly relevant:\\n\\n### 1\ufe0f\u20e3 When Your Model Relies on External Services:\\n\\nThis is one of the obvious and common use cases, especially with the rise of modern AI applications. It\u2019s becoming increasingly clear that we are shifting from building AI at the \\"model\\" granularity to the \\"system\\" granularity.\\n\\nIn other words, AI is no longer just about individual models; it\u2019s about how those models interact within a broader ecosystem. As we become more dependent on external AI services and APIs, the need for Models from Code logging becomes more pronounced.\\n\\nFor instance, frameworks like [LangChain](https://github.com/langchain-ai/langchain/) allow developers to build applications that chain together various AI models and services to perform complex tasks, such as language understanding and information retrieval. In such scenarios, the \\"model\\" is not just a set of trained parameters that can be _pickled_ but a \\"system\\" of interconnected services, often orchestrated by code that makes API calls to external platforms.\\n\\nModels from Code logging in these situations ensures that the entire workflow, including the logic and dependencies, is preserved. It offers is the ability to maintain the same model-like experience by capturing the code making it possible to faithfully recreate the model\u2019s behavior, even when the actual computational work is performed outside your domain.\\n\\n### 2\ufe0f\u20e3 When You\u2019re Combining Multiple Models to Calculate a Complex Metric:\\n\\nApart from GenAI, you can still benefit from the Models from Code feature in various other domains. There are many situations where multiple specialized models are combined to produce a comprehensive output. Note that we are not just referring to traditional ensemble modeling (predicting the same variable); often, you need to combine multiple models to predict different components of a complex inferential task.\\n\\nOne concrete example could be [Customer Lifetime Value (CLV)](https://en.wikipedia.org/wiki/Customer_lifetime_value) in customer analytics. In the context of CLV, you might have separate models for:\\n\\n- Customer Retention: Forecasting how long a customer will continue to engage with the business.\\n- Purchase Frequency: Predicting how often a customer will make a purchase.\\n- Average Order Value: Estimating the typical value of each transaction.\\n\\nEach of these models might already be logged and tracked properly using MLflow. Now, you need to \\"combine\\" these models into a single \\"system\\" that calculates CLV. We refer to it as a \\"system\\" because it contains multiple components.\\n\\nThe beauty of MLflow\'s Models from Code logging is that it allows you to treat this \\"CLV system\\" as a \\"CLV model\\". It enables you to leverage MLflow\'s capabilities, maintaining the MLflow-like model structure with all the advantages of tracking, versioning, and deploying your CLV model as a cohesive unit, even though it\'s built on top of other models. While such a complex model system is able to be built using a custom MLflow PythonModel, utilizing the Models from Code feature dramatically simplifies the serialization process, reducing the friction to building your solution.\\n\\n### 3\ufe0f\u20e3 When You Don\u2019t Have Serialization at All:\\n\\nDespite the rise of deep learning, industries still rely on rule-based algorithms that don\u2019t produce serialized models. In these cases, Models from Code logging can be beneficial for integrating these processes into the MLflow ecosystem.\\n\\nOne example is in industrial quality control, where the [Canny edge detection algorithm](https://en.wikipedia.org/wiki/Canny_edge_detector) is often used to identify defects. This rule-based algorithm doesn\u2019t involve serialization but is defined by specific steps.\\n\\nAnother example, which is gaining attention nowadays, is [Causal AI](https://en.wikipedia.org/wiki/Causal_AI). Constraint-based causal discovery algorithms like the [PC (Peter-Clark)](https://causal-learn.readthedocs.io/en/latest/search_methods_index/Constraint-based%20causal%20discovery%20methods/PC.html) algorithm that discover causal relationships in data but are implemented as code rather than as model objects.\\n\\nIn either case, with the Models from Code feature, you can log the entire process as a \\"model\\" in MLflow, preserving the logic and parameters while benefiting from MLflow\u2019s tracking and versioning features.\\n\\n## How To Implement Models from Code Logging?\\n\\nI hope that by this point, you have a clear understanding of the \\"What\\" and \\"Why\\" of Models from Code, and now you might be eager to get hands-on and focus on the _How_!\\n\\nIn this section, I\'ll provide a generic workflow for implementing MLflow\'s Models from Code logging, followed by a basic yet broadly applicable example. I hope the workflow provides a broad understanding that allows you to address a wide range of scenarios. I will also include links at the end to resources that cover more specific use cases (e.g., AI models).\\n\\n### Models from Code Workflow:\\n\\nA key \\"ingredient\\" of the implementation is MLflow\'s component [`pyfunc`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html). If you\'re not familiar with it, think of `pyfunc` as a universal interface in MLflow that lets you turn any model, from any framework, into an MLflow model by defining a _custom_ Python function. You can also refer to [this earlier post](https://mlflow.org/blog/custom-pyfunc) if you wish to gain a deeper understanding.\\n\\nFor our Models from Code logging, we\u2019ll particularly use the [`PythonModel`](https://mlflow.org/docs/latest/_modules/mlflow/pyfunc/model.html#PythonModel) class within `pyfunc`. This class in the MLflow Python client library allows us to create and manage Python functions as MLflow models. It enables us to define a custom function that processes input data and returns predictions or results. This model can then be deployed, tracked, and shared using MLflow\'s features.\\n\\nIt seems to be exactly what we\'re looking for\u2014we have some code that serves as our model, and we want to log it! That\'s why you\'ll soon see `mlflow.pyfunc.PythonModel` in our code example!\\n\\nNow, each time we need to implement Models from Code, we create _two_ separate Python files:\\n\\n1.  The first contains our model code (let\'s call it `model_code.py`). This file contains a class that inherits from the `mlflow.pyfunc.PythonModel` class.\\n    The class we\'re defining contains our model logic. It could be our calls to OpenAI APIs, CLV (Customer Lifetime Value) model, or our causal discovery code. We\'ll see a very simple 101 example soon.\\n\\n    \ud83d\udccc But wait! IMPORTANT:\\n\\n        - Our `model_code.py` script needs to call (i,e; include) [`mlflow.models.set_model()`](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.set_model) to set the model, which is crucial for loading the model back using `load_model()` for inference. You will notice this in the example.\\n\\n2.  The second file logs our class (that we defined in `model_code.py`). Think of it as the driver code; it can be either a notebook or a Python script (let\'s call it `driver.py`).\\n    In this file, we\'ll include the code that is responsible for logging our model code (essentially, providing the path to `model_code.py`) .\\n\\nThen we can deploy our model. Later, when the serving environment is loaded, `model_code.py` is executed, and when a serving request comes in, `PyFuncClass.predict()` is called.\\n\\nThis figure gives a generic template of these two files.\\n\\n![Models from Code files](models-from-code3.png)\\n\\n### A 101 Example of Model from Code Logging :\\n\\nLet\u2019s consider a straightforward example: a simple function to calculate the area of a circle based on its diameter. With Models from Code, we can log this calculation as a model! I like to think of it as framing the calculation as a prediction problem, allowing us to write our model code with a `predict` method.\\n\\n#### 1. Our `model_code.py` file :\\n\\n```python\\nimport mlflow\\nimport math\\n\\nclass CircleAreaModel(mlflow.pyfunc.PythonModel):\\n    def predict(self, context, model_input, params=None):\\n        return [math.pi * (r ** 2) for r in model_input]\\n\\n# It\'s important to call set_model() so it can be loaded for inference\\n# Also, note that it is set to an instance of the class, not the class itself.\\nmlflow.models.set_model(model=CircleAreaModel())\\n```\\n\\n#### 2. Our `driver.py` file :\\n\\nThis can be defined within a notebook as well. Here are its essential contents:\\n\\n```python\\nimport mlflow\\n\\ncode_path = \\"model_code.py\\" # make sure that you put the correct path\\n\\nwith mlflow.start_run():\\n  logged_model_info = mlflow.pyfunc.log_model(\\n                                            python_model=code_path,\\n                                            artifact_path=\\"test_code_logging\\"\\n                                            )\\n\\n#We can proint some info about the logged model\\nprint(f\\"MLflow Run: {logged_model_info.run_id}\\")\\nprint(f\\"Model URI: {logged_model_info.model_uri}\\")\\n```\\n\\n#### How that looks like on MLflow:\\n\\nExecuting the `driver.py` will start an MLflow run and log our model as code. The files can been as demonstrated below:\\n\\n![Models from Code files](models-from-code4.png)\\n\\n## Conclusion and Further Learning\\n\\nI hope that by this point, I have fulfilled the promises I made earlier! You should now have a clearer understanding of _What_ Models from Code is and how it differs from the popular Object-Based approach which logs models as serialized objects. You should also have a solid foundation of _Why_ and when to use it, as well as an understanding of _How_ to implement it through our general example.\\n\\nAs we mentioned in the introduction and throughout the post, there are various use cases where Models from Code can be beneficial. Our 101 example is just the beginning\u2014there is much more to explore. Below is a list of code examples that you may find helpful:\\n\\n1. Logging models from code using **Pyfunc** log model API ( [model code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/pyfunc/model_as_code.py) | [driver code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/pyfunc/model_as_code_driver.py) )\\n2. Logging model from code using **Langchain** log model API ( [model code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/langchain/chain_as_code.py) | [driver code](https://github.com/mlflow/mlflow/blob/a3454610285e3729266e5e94041d06bd2bc55ff6/examples/langchain/chain_as_code_driver.py) )"},{"id":"autogen-image-agent","metadata":{"permalink":"/mlflow-website/blog/autogen-image-agent","source":"@site/blog/2024-08-29-autogen-pyfunc/index.md","title":"AutoGen with Custom PyFunc","description":"A guide for building an autonomous image generation agent","date":"2024-08-29T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":21.295,"hasTruncateMarker":true,"authors":[{"name":"Michael Berk","title":"Sr. Resident Solutions Architect at Databricks","url":"https://www.linkedin.com/in/-michael-berk/","imageURL":"/mlflow-website/img/authors/michael_berk.png","key":"michael-berk","page":null},{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"AutoGen with Custom PyFunc","description":"A guide for building an autonomous image generation agent","tags":["genai","mlops"],"slug":"autogen-image-agent","authors":["michael-berk","mlflow-maintainers"],"thumbnail":"/img/blog/autogen-blog.png"},"unlisted":false,"prevItem":{"title":"Models from Code Logging in MLflow - What, Why, and How","permalink":"/mlflow-website/blog/models_from_code"},"nextItem":{"title":"LangGraph with Model From Code","permalink":"/mlflow-website/blog/langgraph-model-from-code"}},"content":"In this blog, we\'ll guide you through creating an [AutoGen](https://microsoft.github.io/autogen/) agent framework within an MLflow custom PyFunc. By combining MLflow with AutoGen\'s ability to create multi-agent frameworks, we are able to create scalable and stable GenAI applications.\\n\\n## Agent Frameworks\\n\\nAgent frameworks enable autonomous agents to handle complex, multi-turn tasks by integrating discrete logic at each step. These frameworks are crucial for LLM-driven workflows, where agents manage dynamic interactions across multiple stages. Each agent operates based on specific logic, enabling precise task automation, decision-making, and coordination. This is ideal for applications like workflow orchestration, customer support, and multi-agent systems, where LLMs must interpret evolving context and respond accordingly.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Agent Frameworks with AutoGen\\n\\nAutoGen is an open-source programming framework designed for building agent-based AI systems. It offers a multi-agent conversation framework, allowing users to build [complex LLM workflows](https://microsoft.github.io/autogen/docs/Examples/) using high-level abstractions. AutoGen simplifies the creation of diverse applications across various domains by providing pre-built systems. Additionally, it enhances LLM inference and optimization through specialized APIs, improving performance and reducing operational costs. The framework is tailored to streamline the development and deployment of agentic AI solutions.\\n\\n## Setup\\n\\nFirst, let\'s install the required dependencies. Note that pyautogen requires `python>=3.9`.\\n\\n### Environment Setup\\n\\n```shell\\n%pip install pyautogen mlflow -U -q\\n```\\n\\nWe must also get API credentials to use an LLM. For this tutorial, we\'ll be using OpenAI. Note that a great way to securely pass tokens to your interactive python environment is via the [getpass](https://docs.python.org/3/library/getpass.html) package.\\n\\n```python\\nimport os\\nfrom getpass import getpass\\n\\nos.environ[\\"OPENAI_API_KEY\\"] = getpass(\\"OPENAI_API_KEY:\\")\\n\\nassert os.getenv(\\"OPENAI_API_KEY\\"), \\"Please set an OPENAI_API_KEY environment variable.\\"\\n```\\n\\nGreat! We\'ve setup our authentication configuration and are ready to start building an agent framework.\\n\\n## Create Our Agent Framework with AutoGen and MLflow\\n\\nIn this tutorial we will be creating an image generation agent framework. There is a lot of code copied and modified from the [autogen tutorial](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_dalle_and_gpt4v.ipynb), but the core agent functionality remains the same.\\n\\n### Agent Code\\n\\nYou don\'t have to worry about the specifics of the implementation. At a high level, we are creating an agent framework that...\\n\\n1. Takes a prompt.\\n2. Leverages [OpenAI\'s DALLE](https://openai.com/index/dall-e-3/) to create an image based on that prompt.\\n3. Iteratively \\"catifies\\" e.g. adds fluffy cats to the image.\\n\\nStep 3 is where AutoGen shines. We\'re able to leverage AutoGen\'s [MultimodalConversableAgent](https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent#multimodalconversableagent) to create a critic agent that observes the images and, based on a system prompt provided by the user to \\"add fluffy cats\\", gives feedback on how the prompt should be improved.\\n\\n```python\\nimport os\\nimport re\\nfrom typing import Dict, List, Optional, Union\\n\\nimport matplotlib.pyplot as plt\\nimport PIL\\nfrom diskcache import Cache\\nfrom openai import OpenAI\\nfrom PIL import Image\\n\\nfrom autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\\nfrom autogen.agentchat.contrib.img_utils import _to_pil, get_image_data, get_pil_image\\nfrom autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\\n\\n# Define our prompt of interest\\nCRITIC_PROMPT = \\"\\"\\"Add fluffy cats. Like a lot of cats. If there\'s less than 100 cats I\'ll be mad.\\"\\"\\"\\n\\n# Define our LLM configurations\\n\\ndef dalle_call(client: OpenAI, model: str, prompt: str, size: str, quality: str, n: int) -> str:\\n    \\"\\"\\"\\n    Generate an image using OpenAI\'s DALL-E model and cache the result.\\n\\n    This function takes a prompt and other parameters to generate an image using OpenAI\'s DALL-E model.\\n    It checks if the result is already cached; if so, it returns the cached image data. Otherwise,\\n    it calls the DALL-E API to generate the image, stores the result in the cache, and then returns it.\\n\\n    Args:\\n        client (OpenAI): The OpenAI client instance for making API calls.\\n        model (str): The specific DALL-E model to use for image generation.\\n        prompt (str): The text prompt based on which the image is generated.\\n        size (str): The size specification of the image.\\n        quality (str): The quality setting for the image generation.\\n        n (int): The number of images to generate.\\n\\n    Returns:\\n    str: The image data as a string, either retrieved from the cache or newly generated.\\n\\n    Note:\\n    - The cache is stored in a directory named \'.cache/\'.\\n    - The function uses a tuple of (model, prompt, size, quality, n) as the key for caching.\\n    - The image data is obtained by making a secondary request to the URL provided by the DALL-E API response.\\n    \\"\\"\\"\\n    # Function implementation...\\n    cache = Cache(\\".cache/\\")  # Create a cache directory\\n    key = (model, prompt, size, quality, n)\\n    if key in cache:\\n        return cache[key]\\n\\n    # If not in cache, compute and store the result\\n    response = client.images.generate(\\n        model=model,\\n        prompt=prompt,\\n        size=size,\\n        quality=quality,\\n        n=n,\\n    )\\n    image_url = response.data[0].url\\n    img_data = get_image_data(image_url)\\n    cache[key] = img_data\\n\\n    return img_data\\n\\ndef extract_img(agent: Agent) -> PIL.Image:\\n    \\"\\"\\"\\n    Extracts an image from the last message of an agent and converts it to a PIL image.\\n\\n    This function searches the last message sent by the given agent for an image tag,\\n    extracts the image data, and then converts this data into a PIL (Python Imaging Library) image object.\\n\\n    Parameters:\\n        agent (Agent): An instance of an agent from which the last message will be retrieved.\\n\\n    Returns:\\n        PIL.Image: A PIL image object created from the extracted image data.\\n\\n    Note:\\n    - The function assumes that the last message contains an <img> tag with image data.\\n    - The image data is extracted using a regular expression that searches for <img> tags.\\n    - It\'s important that the agent\'s last message contains properly formatted image data for successful extraction.\\n    - The `_to_pil` function is used to convert the extracted image data into a PIL image.\\n    - If no <img> tag is found, or if the image data is not correctly formatted, the function may raise an error.\\n    \\"\\"\\"\\n    last_message = agent.last_message()[\\"content\\"]\\n\\n    if isinstance(last_message, str):\\n        img_data = re.findall(\\"<img (.*)>\\", last_message)[0]\\n    elif isinstance(last_message, list):\\n        # The GPT-4V format, where the content is an array of data\\n        assert isinstance(last_message[0], dict)\\n        img_data = last_message[0][\\"image_url\\"][\\"url\\"]\\n\\n    pil_img = get_pil_image(img_data)\\n    return pil_img\\n\\nclass DALLEAgent(ConversableAgent):\\n    def __init__(self, name, llm_config: dict, **kwargs):\\n        super().__init__(name, llm_config=llm_config, **kwargs)\\n\\n        api_key = os.getenv(\\"OPENAI_API_KEY\\")\\n        self._dalle_client = OpenAI(api_key=api_key)\\n        self.register_reply([Agent, None], DALLEAgent.generate_dalle_reply)\\n\\n    def send(\\n        self,\\n        message: Union[Dict, str],\\n        recipient: Agent,\\n        request_reply: Optional[bool] = None,\\n        silent: Optional[bool] = False,\\n    ):\\n        # override and always \\"silent\\" the send out message;\\n        # otherwise, the print log would be super long!\\n        super().send(message, recipient, request_reply, silent=True)\\n\\n    def generate_dalle_reply(self, messages: Optional[List[Dict]], sender: \\"Agent\\", config):\\n        \\"\\"\\"Generate a reply using OpenAI DALLE call.\\"\\"\\"\\n        client = self._dalle_client if config is None else config\\n        if client is None:\\n            return False, None\\n        if messages is None:\\n            messages = self._oai_messages[sender]\\n\\n        prompt = messages[-1][\\"content\\"]\\n        img_data = dalle_call(\\n            client=client,\\n            model=\\"dall-e-3\\",\\n            prompt=prompt,\\n            size=\\"1024x1024\\",\\n            quality=\\"standard\\",\\n            n=1,\\n        )\\n\\n        img_data = _to_pil(img_data)  # Convert to PIL image\\n\\n        # Return the OpenAI message format\\n        return True, {\\"content\\": [{\\"type\\": \\"image_url\\", \\"image_url\\": {\\"url\\": img_data}}]}\\n\\nclass CatifyWithDalle(AssistantAgent):\\n    def __init__(self, n_iters=2, **kwargs):\\n        \\"\\"\\"\\n        Initializes a CatifyWithDalle instance.\\n\\n        This agent facilitates the creation of visualizations through a collaborative effort among\\n        its child agents: dalle and critics.\\n\\n        Parameters:\\n            - n_iters (int, optional): The number of \\"improvement\\" iterations to run. Defaults to 2.\\n            - **kwargs: keyword arguments for the parent AssistantAgent.\\n        \\"\\"\\"\\n        super().__init__(**kwargs)\\n        self.register_reply([Agent, None], reply_func=CatifyWithDalle._reply_user, position=0)\\n        self._n_iters = n_iters\\n\\n    def _reply_user(self, messages=None, sender=None, config=None):\\n        if all((messages is None, sender is None)):\\n            error_msg = f\\"Either {messages=} or {sender=} must be provided.\\"\\n            raise AssertionError(error_msg)\\n\\n        if messages is None:\\n            messages = self._oai_messages[sender]\\n\\n        img_prompt = messages[-1][\\"content\\"]\\n\\n        ## Define the agents\\n        self.critics = MultimodalConversableAgent(\\n            name=\\"Critics\\",\\n            system_message=f\\"\\"\\"You need to improve the prompt of the figures you saw.\\n{CRITIC_PROMPT}\\nReply with the following format:\\n\\nCRITICS: the image needs to improve...\\nPROMPT: here is the updated prompt!\\n\\n\\"\\"\\",\\n            llm_config={\\"max_tokens\\": 1000, \\"model\\": \\"gpt-4o\\"},\\n            human_input_mode=\\"NEVER\\",\\n            max_consecutive_auto_reply=3,\\n        )\\n\\n        self.dalle = DALLEAgent(\\n            name=\\"Dalle\\", llm_config={\\"model\\": \\"dalle\\"}, max_consecutive_auto_reply=0\\n        )\\n\\n        # Data flow begins\\n        self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n        img = extract_img(self.dalle)\\n        plt.imshow(img)\\n        plt.axis(\\"off\\")  # Turn off axis numbers\\n        plt.show()\\n        print(\\"Image PLOTTED\\")\\n\\n        for i in range(self._n_iters):\\n            # Downsample the image s.t. GPT-4V can take\\n            img = extract_img(self.dalle)\\n            smaller_image = img.resize((128, 128), Image.Resampling.LANCZOS)\\n            smaller_image.save(\\"result.png\\")\\n\\n            self.msg_to_critics = f\\"\\"\\"Here is the prompt: {img_prompt}.\\n            Here is the figure <img result.png>.\\n            Now, critique and create a prompt so that DALLE can give me a better image.\\n            Show me both \\"CRITICS\\" and \\"PROMPT\\"!\\n            \\"\\"\\"\\n            self.send(message=self.msg_to_critics, recipient=self.critics, request_reply=True)\\n            feedback = self._oai_messages[self.critics][-1][\\"content\\"]\\n            img_prompt = re.findall(\\"PROMPT: (.*)\\", feedback)[0]\\n\\n            self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n            img = extract_img(self.dalle)\\n            plt.imshow(img)\\n            plt.axis(\\"off\\")  # Turn off axis numbers\\n            plt.show()\\n            print(f\\"Image {i} PLOTTED\\")\\n\\n        return True, \\"result.jpg\\"\\n\\n```\\n\\nGreat! We have an agent framework. To quickly show how it works, let\'s instantiate our agent and give it a prompt.\\n\\n```python\\ncreator = CatifyWithDalle(\\n    name=\\"creator\\",\\n    max_consecutive_auto_reply=0,\\n    system_message=\\"Help me coordinate generating image\\",\\n    llm_config={\\"model\\": \\"gpt-4\\"},\\n)\\n\\nuser_proxy = UserProxyAgent(\\n    name=\\"User\\",\\n    human_input_mode=\\"NEVER\\",\\n    max_consecutive_auto_reply=0,\\n    code_execution_config={\\n        \\"work_dir\\": \\"output\\", # Location where code will be written\\n        \\"use_docker\\": False # Use local jupyter execution environment instead of docker\\n    }\\n)\\n\\n_ = user_proxy.initiate_chat(\\n    creator, message=\\"Show me something boring\\"\\n)\\n```\\n\\nThe initial result from the first iteration from the user prompt:\\n\\n```text\\nUser (to creator):\\n\\nShow me something boring\\n\\ncreator (to Dalle):\\n\\nShow me something boring\\n\\n```\\n\\n![An uninspired image](_img/boring_0.png)\\n\\nThis is definitely a boring room. Notice the responses of the critics and how the critics enhance the submission prompt in the following iterations.\\n\\n```text\\nImage PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: Show me something boring.\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image is simple and mundane, with a plain room and basic furniture, which accomplishes the task of showing something boring. However, it can be improved by adding an element of whimsy or interest, juxtaposing the boring scene with something unexpected. Let\'s add a lot of cats to make it more engaging.\\n\\nPROMPT: Show me a boring living room with plain furniture, but add 100 cats in various places around the room.\\n\\ncreator (to Dalle):\\n\\nShow me a boring living room with plain furniture, but add 100 cats in various places around the room.\\n```\\n\\n![A mild improvement](_img/boring_1.png)\\n\\nOn the final iteration, we can see a more refined instruction set to add additional details.\\n\\n```text\\n\\nImage 0 PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: Show me a boring living room with plain furniture, but add 100 cats in various places around the room..\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image has successfully incorporated cats into a boring living room, bringing in an element of surprise and quirkiness. However, it is in black and white, which can make the image feel duller and less lively. Additionally, while there are many cats, they could be positioned in more playful and unexpected ways to create more interest.\\n\\nPROMPT: Show me a colorful, boring living room with plain furniture, but add 100 cats in various imaginative and playful positions around the room.\\n\\ncreator (to Dalle):\\n\\nShow me a colorful, boring living room with plain furniture, but add 100 cats in various imaginative and playful positions around the room.\\n\\n```\\n\\n![Final cat room](_img/boring_2.png)\\n\\nWithout any direct intervention, we now have an image that is remarkably different in style than the original user instruction. The agent has successfully\\nintroduced elements of whimsy into the original instruction set.\\n\\n### MLflow Model From Code\\n\\nNow that we\'ve proven the concept, it\'s time to leverage MLflow to manage our ML modeling lifecycle. For instance, it\'s highly likely that we\'d want to take this model to production, so strong dependency management, model versioning, and support for tracking between development cycles would all be useful.\\n\\nIn this blog we will leverage the [Model from Code](https://mlflow.org/docs/latest/models.html#models-from-code) feature to achieve the above functionality. MLflow Model from Code allows you to define and log models directly from a stand-alone python script. This feature is particularly useful when you want to log models that can be effectively stored as a code representation (models that do not need optimized weights through training) or applications that rely on external services (e.g., LangChain chains). Another benefit is that this approach entirely bypasses the use of the `pickle` or `cloudpickle` modules within Python, which can carry security\\n\\nTo leverage Model from Code, we must perform the following steps:\\n\\n1. Declare a [custom PyFunc](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n2. Leverage [mlflow.models.set_model](https://mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=set_model#mlflow.models.set_model) to indicate which python object is our model.\\n\\nTo achieve these steps, we simply copy the above and below code to a python file. For simplicity, you can just create a single Python file with both code snippets, but MLflow also supports specifying local dependencies when logging our model via the `code_paths` parameter in [mlflow.pyfunc.lod_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc%20log_model#mlflow.pyfunc.log_model)\\n\\n**This step was omitted for brevity and must be done manually.**\\n\\n```python\\nimport mlflow\\n\\nclass CatifyPyfunc(mlflow.pyfunc.PythonModel):\\n  def predict(self, context, model_input, params):\\n    import mlflow\\n    mlflow.autogen.autolog()\\n\\n    creator = CatifyWithDalle(\\n            name=\\"creator\\",\\n            max_consecutive_auto_reply=0,\\n            system_message=\\"Help me coordinate generating image\\",\\n            llm_config={\\"model\\":\\"gpt-4\\"},\\n        )\\n\\n    user_proxy = UserProxyAgent(name=\\"User\\", human_input_mode=\\"NEVER\\", max_consecutive_auto_reply=0, code_execution_config={\\n                \\"work_dir\\": \\"output\\", # Location where code will be written\\n                \\"use_docker\\": False # Use local jupyter execution environment instead of docker\\n            })\\n\\n    return user_proxy.initiate_chat(\\n        creator, message=model_input\\n    )\\nmlflow.models.set_model(CatifyPyfunc())\\n```\\n\\nAt the end of this step, you should have a Python file that has both code snippets. The name is up to the user, but for this blog we will use \\"catify_model.py\\".\\n\\n## Use Our Agent Framework\\n\\nWe are now positioned to leverage MLflow to interact with our powerful \\"catify\\" agent.\\n\\n### Log and Load\\n\\nFirst, let\'s demonstrate the standard user journey of logging model to MLflow\'s tracking server. We will then load it back and perform inference.\\n\\n```python\\nimport mlflow\\nmlflow.autogen.autolog() # Enable logging of traces\\n\\nwith mlflow.start_run() as run:\\n    mlflow.pyfunc.log_model(\\n        artifact_path=\\"autogen_pyfunc\\",\\n        python_model=\\"catify_model.py\\", # Our model from code python file\\n\\n    )\\n\\n    run_id = run.info.run_id\\n```\\n\\nWith our model logged, let\'s reload it and perform inference, this time with a more cool prompt.\\n\\n```python\\nloaded = mlflow.pyfunc.load_model(f\\"runs:/{run_id}/autogen_pyfunc\\")\\nout = loaded.predict(\\"The matrix with a cat\\")\\n```\\n\\nThe initial stage\'s results:\\n\\n```text\\nUser (to creator):\\n\\nThe matrix with a cat\\n\\ncreator (to Dalle):\\n\\nThe matrix with a cat\\n```\\n\\n![Initial Matrix Cat](_img/cool_0.png)\\n\\nOn the next stage, the generation prompt is greatly enhanced by the critic agent.\\n\\n```text\\nImage PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: The matrix with a cat.\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image effectively captures the Matrix-themed aesthetic with a cat, combining a cyberpunk atmosphere with digital elements. However, to improve the image:\\n\\n- Increase the number of cats to align with the requirement of having lots of cats (aim for around 100).\\n- Enhance the digital and neon elements to make the Matrix theme more pronounced.\\n- Add more movement or dynamic elements to the scene for a more immersive feel.\\n- Ensure diversity in cat appearances, sizes, and positions to make the scene more complex and interesting.\\n\\nPROMPT: \\"Create a Matrix-themed scene set in a cyberpunk alleyway, with digital and neon elements filling the atmosphere. The scene should feature around 100 cats of various sizes, colors, and positions\u2014some sitting, some walking, and some interacting with the digital elements. Make the digital grid and floating code more prominent, and add dynamic elements such as digital rain or floating holograms to create a more immersive and lively environment.\\"\\n\\ncreator (to Dalle):\\n\\n\\"Create a Matrix-themed scene set in a cyberpunk alleyway, with digital and neon elements filling the atmosphere. The scene should feature around 100 cats of various sizes, colors, and positions\u2014some sitting, some walking, and some interacting with the digital elements. Make the digital grid and floating code more prominent, and add dynamic elements such as digital rain or floating holograms to create a more immersive and lively environment.\\"\\n```\\n\\n![First Matrix Iteration](_img/cool_1.png)\\n\\nThis is definitely an improvement, show casing the power of multi-turn agents.\\n\\nThe final stage enhances the instruction set even further.\\n\\n```text\\nImage 0 PLOTTED\\ncreator (to Critics):\\n\\nHere is the prompt: \\"Create a Matrix-themed scene set in a cyberpunk alleyway, with digital and neon elements filling the atmosphere. The scene should feature around 100 cats of various sizes, colors, and positions\u2014some sitting, some walking, and some interacting with the digital elements. Make the digital grid and floating code more prominent, and add dynamic elements such as digital rain or floating holograms to create a more immersive and lively environment.\\".\\nHere is the figure `<image>`.\\nNow, critique and create a prompt so that DALLE can give me a better image.\\nShow me both \\"CRITICS\\" and \\"PROMPT\\"!\\n\\nCritics (to creator):\\n\\nCRITICS: The image significantly improves the Matrix-themed atmosphere with a cyberpunk alley and an abundance of cats. However, there are a few areas for improvement:\\n\\n- Increase the variety of the digital elements (e.g., different shapes of holograms, varied colors and intensities of neon signs).\\n- Make the cats more dynamic by showing more interactions such as jumping, playing, or chasing digital elements.\\n- Enhance the depth and perspective of the scene to create a more three-dimensional and immersive look.\\n- Add more detail to the surrounding environment, like futuristic posters or graffiti to intensify the cyberpunk feel.\\n\\nPROMPT: \\"Craft a highly detailed, Matrix-themed scene set in a cyberpunk alleyway. The atmosphere should be rich with diverse digital and neon elements, including various shapes of holograms and a range of vivid colors. Populate the scene with around 100 dynamic cats of different sizes, colors, and actions\u2014some sitting, some walking, some jumping, playing, or chasing digital elements. Enhance the depth and perspective of the scene to create a more immersive three-dimensional experience. Include detailed futuristic environment elements like posters, graffiti, and neon signs to intensify the cyberpunk feel.\\"\\n\\ncreator (to Dalle):\\n\\n\\"Craft a highly detailed, Matrix-themed scene set in a cyberpunk alleyway. The atmosphere should be rich with diverse digital and neon elements, including various shapes of holograms and a range of vivid colors. Populate the scene with around 100 dynamic cats of different sizes, colors, and actions\u2014some sitting, some walking, some jumping, playing, or chasing digital elements. Enhance the depth and perspective of the scene to create a more immersive three-dimensional experience. Include detailed futuristic environment elements like posters, graffiti, and neon signs to intensify the cyberpunk feel.\\"\\n```\\n\\n![2nd cool image](_img/cool_2.png)\\n\\nA little dystopian, but we\'ll take it!\\n\\nWe have successfully demonstrated that we can log and load our model, then perform inference from the loaded model.\\n\\n### Show MLflow Traces\\n\\n[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) provides a thread-safe API to track the execution of complex applications. The MLflow AutoGen flavor has tracing built in as an autologging feature. So, simply by running `mlflow.autogen.autolog()` prior to doing inference, we will get traces logged automatically.\\n\\nTraces can be accessed via the fluent APIs, MLflow client, and manually via the MLflow UI. For more, please visit the documentation linked above.\\n\\n```python\\n# Example with fluent APIs\\nlast_active_trace = mlflow.get_last_active_trace()\\nprint(last_active_trace)\\n\\n# Output: Trace(request_id=71ffcf92785b4dfc965760a43193095c)\\n```\\n\\nIn the meantime, we will display the MLFlow UI here. If you are running in an interactive context, such as jupyter, run the following command.\\n\\n```python\\nimport subprocess\\nfrom IPython.display import IFrame\\n\\n# Start MLflow server in the background\\nmlflow_ui_server = subprocess.Popen([\\"mlflow\\", \\"ui\\", \\"--host\\", \\"127.0.0.1\\", \\"--port\\", \\"5000\\"])\\nIFrame(src=\\"http://127.0.0.1:5000\\", width=\\"100%\\", height=\\"600\\")\\n\\n# Run the below command to stop the server\\n# mlflow_ui_server.terminate()\\n```\\n\\nIf you\'re not running interactively, you can simply run the follow shell command and navigate to the associated host and port in your web browser.\\n\\n```bash\\nmlflow ui\\n```\\n\\nIf we navigate to the tracing tab, as shown in the image below, we can see our logged trace.\\n\\n![The MLflow Tracing UI](./_img/tracing_main_page.png)\\n\\nBy clicking on that trace ID, we can see a detailed execution plan. At the bottom, we can see our prompt `\\"The matrix with a cat\\"` which kicked off the chat session. From there, many agents interacted to create images and provide feedback to \\"catify\\" them. Also, note that the trace ID is the same as the one returned by `mlflow.get_last_active_trace()` above.\\n\\n![The MLflow Tracing UI](./_img/tracing_detail.png)\\n\\nFinally, let\'s dig a bit deeper on the tracing LLM call. As you can see, we have lots of valuable information about the execution, such as the model and usage statistics. Tracing helps you monitor not just performance, but cost, usage patterns, and much more! You can also leverage custom metadata to get even more granular insights.\\n\\n![The MLflow Tracing UI](./_img/tracing_chat_completion_1.png)\\n\\n### Logging Artifacts with MLflow\\n\\nTracing\'s primary purpose is to provide robust lightweight summaries of complex agent executions. For larger or custom payloads, MLflow exposes a variety of artifact-logging APIs that can store images, text, tables, and more in the MLflow tracking server. Let\'s quickly demonstrate this functionality by logging the prompts and their associated images.\\n\\nWithin our `CatifyWithDalle` class, we will make 4 modifications...\\n\\n1. Create an instance variable in the class `__init__` to save metadata about our objects.\\n2. Create a private utility to increment our metadata and log and images with [mlflow.log_image](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=log_image#mlflow.log_image).\\n3. Call the above utility after new images have been generated.\\n4. Finally, log our metadata object as JSON with [mlflow.log_dict](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=log_image#mlflow.log_dict).\\n\\n```python\\nimport uuid  # Add to generate artifact file names and indeces for prompt mapping to generated images\\n\\nclass CatifyWithDalle(AssistantAgent):\\n    def __init__(self, n_iters=2, **kwargs):\\n        \\"\\"\\"\\n        Initializes a CatifyWithDalle instance.\\n\\n        This agent facilitates the creation of visualizations through a collaborative effort among\\n        its child agents: dalle and critics.\\n\\n        Parameters:\\n            - n_iters (int, optional): The number of \\"improvement\\" iterations to run. Defaults to 2.\\n            - **kwargs: keyword arguments for the parent AssistantAgent.\\n        \\"\\"\\"\\n        super().__init__(**kwargs)\\n        self.register_reply([Agent, None], reply_func=CatifyWithDalle._reply_user, position=0)\\n        self._n_iters = n_iters\\n        self.dict_to_log = {}  # Add a buffer for storing mapping information\\n\\n    # Adding this method to log the generated images and the prompt-to-image mapping file\\n    def _log_image_and_append_to_dict(self, img: Image, img_prompt: str, image_index: int)-> None:\\n        \\"\\"\\" Method for logging generated images to MLflow and building a prompt mapping file \\"\\"\\"\\n        # Generate a unique ID\\n        _id = str(uuid.uuid1())\\n\\n        # Append to class variable to log once at the end of all inference\\n        self.dict_to_log[_id] = {\\"prompt\\": img_prompt, \\"index\\": image_index}\\n\\n        # Log image to MLflow tracking server\\n        mlflow.log_image(img, f\\"{_id}.png\\")\\n\\n    def _reply_user(self, messages=None, sender=None, config=None):\\n        if all((messages is None, sender is None)):\\n            error_msg = f\\"Either {messages=} or {sender=} must be provided.\\"\\n            raise AssertionError(error_msg)\\n\\n        if messages is None:\\n            messages = self._oai_messages[sender]\\n\\n        img_prompt = messages[-1][\\"content\\"]\\n\\n        ## Define the agents\\n        self.critics = MultimodalConversableAgent(\\n            name=\\"Critics\\",\\n            system_message=f\\"\\"\\"You need to improve the prompt of the figures you saw.\\n{CRITIC_PROMPT}\\nReply with the following format:\\n\\nCRITICS: the image needs to improve...\\nPROMPT: here is the updated prompt!\\n\\n\\"\\"\\",\\n            llm_config={\\"max_tokens\\": 1000, \\"model\\": \\"gpt-4o\\"},\\n            human_input_mode=\\"NEVER\\",\\n            max_consecutive_auto_reply=3,\\n        )\\n\\n        self.dalle = DALLEAgent(\\n            name=\\"Dalle\\", llm_config={\\"model\\": \\"dalle\\"}, max_consecutive_auto_reply=0\\n        )\\n\\n        # Data flow begins\\n        self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n        img = extract_img(self.dalle)\\n        plt.imshow(img)\\n        plt.axis(\\"off\\")  # Turn off axis numbers\\n        plt.show()\\n        print(\\"Image PLOTTED\\")\\n\\n        self._log_image_and_append_to_dict(img, img_prompt, -1)  # Add image logging and buffer updates\\n\\n        for i in range(self._n_iters):\\n            # Downsample the image s.t. GPT-4V can take\\n            img = extract_img(self.dalle)\\n            smaller_image = img.resize((128, 128), Image.Resampling.LANCZOS)\\n            smaller_image.save(\\"result.png\\")\\n\\n            self.msg_to_critics = f\\"\\"\\"Here is the prompt: {img_prompt}.\\n            Here is the figure <img result.png>.\\n            Now, critic and create a prompt so that DALLE can give me a better image.\\n            Show me both \\"CRITICS\\" and \\"PROMPT\\"!\\n            \\"\\"\\"\\n            self.send(message=self.msg_to_critics, recipient=self.critics, request_reply=True)\\n            feedback = self._oai_messages[self.critics][-1][\\"content\\"]\\n            img_prompt = re.findall(\\"PROMPT: (.*)\\", feedback)[0]\\n\\n            self.send(message=img_prompt, recipient=self.dalle, request_reply=True)\\n            img = extract_img(self.dalle)\\n            plt.imshow(img)\\n            plt.axis(\\"off\\")  # Turn off axis numbers\\n            plt.show()\\n            print(f\\"Image {i} PLOTTED\\")\\n            self._log_image_and_append_to_dict(img, img_prompt, i)  # Log the image in the iteration\\n\\n\\n\\n        mlflow.log_dict(self.dict_to_log, \\"image_lookup.json\\")  # Log the prompt-to-image mapping buffer\\n        return True, \\"result.jpg\\"\\n```\\n\\nNow, if we rerun the above model logging code, every time we load the newest version of our model, images generated by our agent will be logged and a JSON object with all prompts, indexes of the prompts, and image names (for lookup purposes) will be logged.\\n\\nLet\'s demonstrate this and wrap infernce in a single MLflow run for easy aggregation. Also note that we will be leveraging Autogen\'s [caching](https://microsoft.github.io/autogen/docs/reference/cache/) functionality, so given we\'ve already done inference with this prompt, we won\'t actually be making LLM calls again; we\'re just reading from cache and logging with our new MLflow code.\\n\\n```python\\n# Be sure to re-log the model by rerunning the above code\\nwith mlflow.start_run(run_name=\\"log_image_during_inferfence\\"):\\n    loaded = mlflow.pyfunc.load_model(f\\"runs:/{run_id}/autogen_pyfunc\\")\\n    loaded.predict(\\"The matrix with a cat\\")\\n```\\n\\n![Logged Images and JSON Artifacts](./_img/logged_images.png)\\n\\nAs you can see, we have logged three images of interest and a lookup dict. The keys of the dict correspond to the image names and the values correspond to additional information for how the image was generated. With these artifacts we can perform detailed analyses on prompt quality and make iterative improvements to our \\"catify\\" agent!\\n\\n### Additional Benefits of MLflow\\n\\nThere is a lot more happening behind the scenes that is out of the scope of this tutorial, but here\'s a quick list of additional MLflow features that are useful when building agentic frameworks.\\n\\n- **Dependency management**: when you log a model, MLflow will automatically try to infer your pip requirements. These requirements are written in several formats that makes remote serving of your model much simpler. If you have local dependencies, as noted above, you can specify additional paths for MLflow to serialize via the `code_paths` argument when logging your model.\\n- **Model aliasing**: when iteratively building your agentic framework, you want an easy way to compare models. MLflow model [aliases and tags](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags) facilitate lookups to the MLflow model registry and allow you to easily load and deploy an specific model version.\\n- **Nested Runs**: with agentic frameworks, especially when training underlying LLM components, you will often have complex nested structures. MLflow supports [nested runs](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs.html) to facilitate aggregating your run information. This can be especially useful with LLM training and fine tuning.\\n\\n## Summary\\n\\nIn this blog we outlined how to create a complex agent with AutoGen. We also showed how to leverage the MLflow [Model from Code](https://mlflow.org/docs/latest/models.html#models-from-code) feature to log and load our model. Finally, we leveraged the MLflow AutoGen\'s autologging capabilities to automatically leverage MLflow tracing to get fine-grained and thread-safe agent execution information.\\n\\nHappy coding!"},{"id":"langgraph-model-from-code","metadata":{"permalink":"/mlflow-website/blog/langgraph-model-from-code","source":"@site/blog/2024-08-06-langgraph-model-from-code/index.md","title":"LangGraph with Model From Code","description":"In this blog, we\'ll guide you through creating a LangGraph chatbot using MLflow. By combining MLflow with LangGraph\'s ability to create and manage cyclical graphs, you can create powerful stateful, multi-actor applications in a scalable fashion.","date":"2024-08-06T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":7.255,"hasTruncateMarker":false,"authors":[{"name":"Michael Berk","title":"Sr. Resident Solutions Architect at Databricks","url":"https://www.linkedin.com/in/-michael-berk/","imageURL":"/mlflow-website/img/authors/michael_berk.png","key":"michael-berk","page":null},{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"LangGraph with Model From Code","tags":["genai","mlops"],"slug":"langgraph-model-from-code","authors":["michael-berk","mlflow-maintainers"],"thumbnail":"/img/blog/release-candidates.png"},"unlisted":false,"prevItem":{"title":"AutoGen with Custom PyFunc","permalink":"/mlflow-website/blog/autogen-image-agent"},"nextItem":{"title":"PyFunc in Practice","permalink":"/mlflow-website/blog/pyfunc-in-practice"}},"content":"In this blog, we\'ll guide you through creating a LangGraph chatbot using MLflow. By combining MLflow with LangGraph\'s ability to create and manage cyclical graphs, you can create powerful stateful, multi-actor applications in a scalable fashion.\\n\\nThroughout this post we will demonstrate how to leverage MLflow\'s capabilities to create a serializable and servable MLflow model which can easily be tracked, versioned, and deployed on a variety of servers. We\'ll be using the [langchain flavor](https://mlflow.org/docs/latest/llms/langchain/index.html) combined with MLflow\'s [model from code](https://mlflow.org/docs/latest/models.html#models-from-code) feature.\\n\\n### What is LangGraph?\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits:\\n\\n- **Cycles and Branching**: Implement loops and conditionals in your apps.\\n- **Persistence**: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\\n- **Human-in-the-Loop**: Interrupt graph execution to approve or edit next action planned by the agent.\\n- **Streaming Support**: Stream outputs as they are produced by each node (including token streaming).\\n- **Integration with LangChain**: LangGraph integrates seamlessly with LangChain.\\n\\nLangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\\n\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nFor a full walkthrough, check out the [LangGraph Quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/) and for more on the fundamentals of design with LangGraph, check out the [conceptual guides](https://langchain-ai.github.io/langgraph/concepts/#human-in-the-loop).\\n\\n## 1 - Setup\\n\\nFirst, we must install the required dependencies. We will use OpenAI for our LLM in this example, but using LangChain with LangGraph makes it easy to substitute any alternative supported LLM or LLM provider.\\n\\n```python\\n%%capture\\n%pip install langchain_openai==0.2.0 langchain==0.3.0 langgraph==0.2.27\\n%pip install -U mlflow\\n```\\n\\nNext, let\'s get our relevant secrets. `getpass`, as demonstrated in the [LangGraph quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/#setup) is a great way to insert your keys into an interactive jupyter environment.\\n\\n```python\\nimport os\\n\\n# Set required environment variables for authenticating to OpenAI\\n# Check additional MLflow tutorials for examples of authentication if needed\\n# https://mlflow.org/docs/latest/llms/openai/guide/index.html#direct-openai-service-usage\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set the OPENAI_API_KEY environment variable.\\"\\n```\\n\\n## 2 - Custom Utilities\\n\\nWhile this is a demo, it\'s good practice to separate reusable utilities into a separate file/directory. Below we create three general utilities that theoretically would valuable when building additional MLflow + LangGraph implementations.\\n\\nNote that we use the magic `%%writefile` command to create a new file in a jupyter notebook context. If you\'re running this outside of an interactive notebook, simply create the file below, omitting the `%%writefile {FILE_NAME}.py` line.\\n\\n```python\\n%%writefile langgraph_utils.py\\n# omit this line if directly creating this file; this command is purely for running within Jupyter\\n\\nimport os\\nfrom typing import Union\\nfrom langgraph.pregel.io import AddableValuesDict\\n\\ndef _langgraph_message_to_mlflow_message(\\n    langgraph_message: AddableValuesDict,\\n) -> dict:\\n    langgraph_type_to_mlflow_role = {\\n        \\"human\\": \\"user\\",\\n        \\"ai\\": \\"assistant\\",\\n        \\"system\\": \\"system\\",\\n    }\\n\\n    if type_clean := langgraph_type_to_mlflow_role.get(langgraph_message.type):\\n        return {\\"role\\": type_clean, \\"content\\": langgraph_message.content}\\n    else:\\n        raise ValueError(f\\"Incorrect role specified: {langgraph_message.type}\\")\\n\\n\\ndef get_most_recent_message(response: AddableValuesDict) -> dict:\\n    most_recent_message = response.get(\\"messages\\")[-1]\\n    return _langgraph_message_to_mlflow_message(most_recent_message)[\\"content\\"]\\n\\n\\ndef increment_message_history(\\n    response: AddableValuesDict, new_message: Union[dict, AddableValuesDict]\\n) -> list[dict]:\\n    if isinstance(new_message, AddableValuesDict):\\n        new_message = _langgraph_message_to_mlflow_message(new_message)\\n\\n    message_history = [\\n        _langgraph_message_to_mlflow_message(message)\\n        for message in response.get(\\"messages\\")\\n    ]\\n\\n    return message_history + [new_message]\\n```\\n\\nBy the end of this step, you should see a new file in your current directory with the name `langgraph_utils.py`.\\n\\nNote that it\'s best practice to add unit tests and properly organize your project into logically structured directories.\\n\\n## 3 - Log the LangGraph Model\\n\\nGreat! Now that we have some reusable utilities located in `./langgraph_utils.py`, we are ready to log the model with MLflow\'s official LangGraph flavor.\\n\\n### 3.1 - Create our Model-From-Code File\\n\\nQuickly, some background. MLflow looks to serialize model artifacts to the MLflow tracking server. Many popular ML packages don\'t have robust serialization and deserialization support, so MLflow looks to augment this functionality via the [models from code](https://mlflow.org/docs/latest/models.html#models-from-code) feature. With models from code, we\'re able to leverage Python as the serialization format, instead of popular alternatives such as JSON or pkl. This opens up tons of flexibility and stability.\\n\\nTo create a Python file with models from code, we must perform the following steps:\\n\\n1. Create a new python file. Let\'s call it `graph.py`.\\n2. Define our langgraph graph.\\n3. Leverage [mlflow.models.set_model](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.set_model) to indicate to MLflow which object in the Python script is our model of interest.\\n\\nThat\'s it!\\n\\n```python\\n%%writefile graph.py\\n# omit this line if directly creating this file; this command is purely for running within Jupyter\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.graph.state import CompiledStateGraph\\n\\nimport mlflow\\n\\nimport os\\nfrom typing import TypedDict, Annotated\\n\\ndef load_graph() -> CompiledStateGraph:\\n    \\"\\"\\"Create example chatbot from LangGraph Quickstart.\\"\\"\\"\\n\\n    assert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set the OPENAI_API_KEY environment variable.\\"\\n\\n    class State(TypedDict):\\n        messages: Annotated[list, add_messages]\\n\\n    graph_builder = StateGraph(State)\\n    llm = ChatOpenAI()\\n\\n    def chatbot(state: State):\\n        return {\\"messages\\": [llm.invoke(state[\\"messages\\"])]}\\n\\n    graph_builder.add_node(\\"chatbot\\", chatbot)\\n    graph_builder.add_edge(START, \\"chatbot\\")\\n    graph_builder.add_edge(\\"chatbot\\", END)\\n    graph = graph_builder.compile()\\n    return graph\\n\\n# Set are model to be leveraged via model from code\\nmlflow.models.set_model(load_graph())\\n```\\n\\n### 3.2 - Log with \\"Model from Code\\"\\n\\nAfter creating this implementation, we can leverage the standard MLflow APIs to log the model.\\n\\n```python\\nimport mlflow\\n\\nwith mlflow.start_run() as run_id:\\n    model_info = mlflow.langchain.log_model(\\n        lc_model=\\"graph.py\\", # Path to our model Python file\\n        artifact_path=\\"langgraph\\",\\n    )\\n\\n    model_uri = model_info.model_uri\\n```\\n\\n## 4 - Use the Logged Model\\n\\nNow that we have successfully logged a model, we can load it and leverage it for inference.\\n\\nIn the code below, we demonstrate that our chain has chatbot functionality!\\n\\n```python\\nimport mlflow\\n\\n# Custom utilities for handling chat history\\nfrom langgraph_utils import (\\n    increment_message_history,\\n    get_most_recent_message,\\n)\\n\\n# Enable tracing\\nmlflow.set_experiment(\\"Tracing example\\") # In Databricks, use an absolute path. Visit Databricks docs for more.\\nmlflow.langchain.autolog()\\n\\n# Load the model\\nloaded_model = mlflow.langchain.load_model(model_uri)\\n\\n# Show inference and message history functionality\\nprint(\\"-------- Message 1 -----------\\")\\nmessage = \\"What\'s my name?\\"\\npayload = {\\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": message}]}\\nresponse = loaded_model.invoke(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {get_most_recent_message(response)}\\")\\n\\nprint(\\"\\\\n-------- Message 2 -----------\\")\\nmessage = \\"My name is Morpheus.\\"\\nnew_messages = increment_message_history(response, {\\"role\\": \\"user\\", \\"content\\": message})\\npayload = {\\"messages\\": new_messages}\\nresponse = loaded_model.invoke(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {get_most_recent_message(response)}\\")\\n\\nprint(\\"\\\\n-------- Message 3 -----------\\")\\nmessage = \\"What is my name?\\"\\nnew_messages = increment_message_history(response, {\\"role\\": \\"user\\", \\"content\\": message})\\npayload = {\\"messages\\": new_messages}\\nresponse = loaded_model.invoke(payload)\\n\\nprint(f\\"User: {message}\\")\\nprint(f\\"Agent: {get_most_recent_message(response)}\\")\\n```\\n\\nOuput:\\n\\n```text\\n-------- Message 1 -----------\\nUser: What\'s my name?\\nAgent: I\'m sorry, I cannot guess your name as I do not have access to that information. If you would like to share your name with me, feel free to do so.\\n\\n-------- Message 2 -----------\\nUser: My name is Morpheus.\\nAgent: Nice to meet you, Morpheus! How can I assist you today?\\n\\n-------- Message 3 -----------\\nUser: What is my name?\\nAgent: Your name is Morpheus.\\n```\\n\\n### 4.1 - MLflow Tracing\\n\\nBefore concluding, let\'s demonstrate [MLflow tracing](https://mlflow.org/docs/latest/llms/tracing/index.html).\\n\\nMLflow Tracing is a feature that enhances LLM observability in your Generative AI (GenAI) applications by capturing detailed information about the execution of your application\u2019s services. Tracing provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors.\\n\\nStart the MLflow server as outlined in the [tracking server docs](https://mlflow.org/docs/latest/tracking/server.html). After entering the MLflow UI, we can see our experiment and corresponding traces.\\n\\n![MLflow UI Experiment Traces](_img/mlflow_ui_experiment_traces.png)\\n\\nAs you can see, we\'ve logged our traces and can easily see them by clicking our experiment of interest and the then the \\"Tracing\\" tab.\\n\\n![MLflow UI Trace](_img/mlflow_ui_trace.png)\\n\\nAfter clicking on one of the traces, we can now see run execution for a single query. Notice that we log inputs, outputs, and lots of great metadata such as usage and invocation parameters. As we scale our application both from a usage and complexity perspective, this thread-safe and highly-performant tracking system will ensure robust monitoring of the app.\\n\\n## 5 - Summary\\n\\nThere are many logical extensions of the this tutorial, however the MLflow components can remain largely unchanged. Some examples include persisting chat history to a database, implementing a more complex langgraph object, productionizing this solution, and much more!\\n\\nTo summarize, here\'s what was covered in this tutorial:\\n\\n- Creating a simple LangGraph chain.\\n- Leveraging MLflow [model from code](https://mlflow.org/docs/latest/models.html#models-from-code) functionality to log our graph.\\n- Loading the model via the standard MLflow APIs.\\n- Leveraging [MLflow tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) to view graph execution.\\n\\nHappy coding!"},{"id":"pyfunc-in-practice","metadata":{"permalink":"/mlflow-website/blog/pyfunc-in-practice","source":"@site/blog/2024-07-26-pyfunc-in-practice/index.md","title":"PyFunc in Practice","description":"Creative Applications of MLflow Pyfunc in Machine Learning Projects","date":"2024-07-26T00:00:00.000Z","tags":[{"inline":true,"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"},{"inline":true,"label":"ensemble-models","permalink":"/mlflow-website/blog/tags/ensemble-models"}],"readingTime":22.055,"hasTruncateMarker":true,"authors":[{"name":"Hugo Carvalho","title":"Machine Learning Analyst at adidas","url":"https://www.linkedin.com/in/hugodscarvalho/","imageURL":"/mlflow-website/img/authors/hugo_carvalho.png","key":"hugo-carvalho","page":null},{"name":"Joana Ferreira","title":"Machine Learning Engineer at adidas","url":"https://www.linkedin.com/in/joanaferreira96/","imageURL":"/mlflow-website/img/authors/joana_ferreira.png","key":"joana-ferreira","page":null},{"name":"Rahul Pandey","title":"Sr. Solutions Architect at adidas","url":"https://www.linkedin.com/in/rahulpandey1901/","imageURL":"/mlflow-website/img/ambassadors/Rahul_Pandey.png","key":"rahul-pandey","page":null},{"name":"Filipe Miranda","title":"Sr. Data Engineer at adidas","url":"https://www.linkedin.com/in/filipe-miranda-b576b186/","imageURL":"/mlflow-website/img/authors/filipe_miranda.png","key":"filipe-miranda","page":null}],"frontMatter":{"title":"PyFunc in Practice","description":"Creative Applications of MLflow Pyfunc in Machine Learning Projects","tags":["pyfunc","mlflow","ensemble-models"],"slug":"pyfunc-in-practice","authors":["hugo-carvalho","joana-ferreira","rahul-pandey","filipe-miranda"],"thumbnail":"/img/blog/pyfunc-in-practice.png"},"unlisted":false,"prevItem":{"title":"LangGraph with Model From Code","permalink":"/mlflow-website/blog/langgraph-model-from-code"},"nextItem":{"title":"Introducing MLflow Tracing","permalink":"/mlflow-website/blog/mlflow-tracing"}},"content":"If you\'re looking to fully leverage the capabilities of `mlflow.pyfunc` and understand how it can be utilized in a Machine Learning project, this blog post will guide you through the process. MLflow PyFunc offers creative freedom and flexibility, allowing the development of complex systems encapsulated as models in MLflow that follow the same lifecycle as traditional ones. This blog will showcase how to create multi-model setups, seamlessly connect to databases, and implement your own custom fit method in your MLflow PyFunc model.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nThis blog post demonstrates the capabilities of [MLflow PyFunc](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and how it can be utilized to build a multi-model setup encapsulated as a PyFunc flavor model in MLflow. This approach allows ensemble models to follow the same lifecycle as traditional [Built-In Model Flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors) in MLflow.\\n\\nBut first, let\'s use an analogy to get you familiarized with the concept of ensemble models and why you should consider this solution in your next Machine Learning project.\\n\\nImagine you are in the market to buy a house. Would you make a decision based solely on the first house you visit and the advice of a single real estate agent? Of course not! The process of buying a house involves considering multiple factors and gathering information from various sources to make an informed decision.\\n\\nThe house buying process explained:\\n\\n- **Identify Your Needs**: Determine whether you want a new or used house, the type of house, the model, and the year of construction.\\n- **Research**: Look for a list of available houses, check for discounts and offers, read customer reviews, and seek opinions from friends and family.\\n- **Evaluate**: Consider the performance, location, neighborhood amenities, and price range.\\n- **Compare**: Compare multiple houses to find the best fit for your needs and budget.\\n\\nIn short, you wouldn\u2019t directly reach a conclusion but would instead make a decision considering all the aforementioned factors before deciding on the best choice.\\n\\nEnsemble models in Machine Learning operate on a similar idea. Ensemble learning helps improve Machine Learning results by combining several models to improve predictive performance compared to a single model. The performance increase can be due to several factors such as the reduction in variance by averaging multiple models or reducing bias by focusing on errors of previous models. There are several types of ensemble learning techniques exists such as:\\n\\n- **Averaging**\\n- **Weighted Averaging**\\n- **Bagging**\\n- **Boosting**\\n- **Stacking**\\n\\nHowever, developing such systems requires careful management of the lifecycle of ensemble models, as integrating diverse models can be highly complex. This is where MLflow PyFunc becomes invaluable. It offers the flexibility to build complex systems, treating the entire ensemble as a model that adheres to the same lifecycle processes as traditional models. Essentially, MLflow PyFunc allows the creation of custom methods tailored to ensemble models, serving as an alternative to the built-in MLflow flavors available for popular frameworks such as scikit-learn, PyTorch, and LangChain.\\n\\nThis blog utilizes the house price dataset from [Kaggle](https://www.kaggle.com/) to demonstrate the development and management of ensemble models through MLflow.\\n\\nWe will leverage various tools and technologies to highlight the capabilities of MLflow PyFunc models. Before delving into the ensemble model itself, we will explore how these components integrate to create a robust and efficient Machine Learning pipeline.\\n\\n### Components of the Project\\n\\n**DuckDB**  \\nDuckDB is a high-performance analytical database system designed to be fast, reliable, portable, and easy to use. In this project, it showcases the integration of a database connection within the model context, facilitating efficient data handling directly within the model. [Learn more about DuckDB](https://duckdb.org/).\\n\\n**scikit-learn (sklearn)**  \\nscikit-learn is a Machine Learning library for Python that provides efficient tools for data analysis and modelling. In this project, it is used to develop and evaluate various Machine Learning models that are integrated into our ensemble model. [Learn more about scikit-learn](https://scikit-learn.org/).\\n\\n**MLflow**  \\nMLflow is an open-source platform for managing the end-to-end Machine Learning lifecycle, including experimentation, reproducibility, and deployment. In this project, it tracks experiments, manages model versions, and facilitates the deployment of MLflow PyFunc models in a similar manner to how we are familiar with individual flavors. [Learn more about MLflow](https://mlflow.org/).\\n\\n> **Note:** To reproduce this project, please refer to the official MLflow documentation for more details on setting up a simple local [MLflow Tracking Server](https://mlflow.org/docs/latest/tracking/server.html).\\n\\n## Creating the Ensemble Model\\n\\nCreating a MLflow PyFunc ensemble model requires additional steps compared to using the built-in flavors for logging and working with popular Machine Learning frameworks.\\n\\nTo implement an ensemble model, you need to define an `mlflow.pyfunc` model, which involves creating a Python class that inherits from the `PythonModel` class and implementing its constructor and class methods. While the basic creation of a PyFunc model only requires implementing the `predict` method, an ensemble model requires additional methods to manage the models and obtain multi-model predictions. After instantiating the ensemble model, you must use the custom `fit` method to train the ensemble model\'s sub-models. Similar to an out-of-the-box MLflow model, you need to log the model along with its artifacts during the training run and then register the model in the MLflow Model Registry. A model alias `production` will also be added to the model to streamline both model updates and inference. Model aliases allow you to assign a mutable, named reference to a specific version of a registered model. By assigning the alias to a particular model version, it can be easily referenced via a model URI or the model registry API. This setup allows for seamless updates to the model version used for inference without changing the serving workload code. For more details, refer to [Deploy and Organize Models with Aliases and Tags](https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags).\\n\\nThe following sections, as depicted in the diagram, detail the implementation of each method for the ensemble model, providing a comprehensive understanding of defining, managing, and utilizing an ensemble model with MLflow PyFunc.\\n\\n![Ensemble Model Architecture](ensemble-model-architecture.png)\\n\\nBefore delving into the detailed implementation of each method, let\'s first review the skeleton of our `EnsembleModel` class. This skeleton serves as a blueprint for understanding the structure of the ensemble model. The subsequent sections will provide an overview and code for both the default methods provided by MLflow PyFunc and the custom methods implemented for the ensemble model.\\n\\nHere is the skeleton of the `EnsembleModel` class:\\n\\n```python\\nimport mlflow\\n\\nclass EnsembleModel(mlflow.pyfunc.PythonModel):\\n    \\"\\"\\"Ensemble model class leveraging Pyfunc for multi-model integration in MLflow.\\"\\"\\"\\n\\n    def __init__(self):\\n        \\"\\"\\"Initialize the EnsembleModel instance.\\"\\"\\"\\n        ...\\n\\n    def add_strategy_and_save_to_db(self):\\n        \\"\\"\\"Add strategies to the DuckDB database.\\"\\"\\"\\n        ...\\n\\n    def feature_engineering(self):\\n        \\"\\"\\"Perform feature engineering on input data.\\"\\"\\"\\n        ...\\n\\n    def initialize_models(self):\\n        \\"\\"\\"Initialize models and their hyperparameter grids.\\"\\"\\"\\n        ...\\n\\n    def fit(self):\\n        \\"\\"\\"Train the ensemble of models.\\"\\"\\"\\n        ...\\n\\n    def predict(self):\\n        \\"\\"\\"Predict using the ensemble of models.\\"\\"\\"\\n        ...\\n\\n    def load_context(self):\\n        \\"\\"\\"Load the preprocessor and models from the MLflow context.\\"\\"\\"\\n        ...\\n```\\n\\n### Initializing the EnsembleModel\\n\\nThe constructor method in the ensemble model is crucial for setting up its essential elements. It establishes key attributes such as the preprocessor, a dictionary to store trained models, the path to a DuckDB database, and a pandas DataFrame for managing different ensemble strategies. Additionally, it takes advantage of the `initialize_models` method to define the sub-models integrated into the ensemble.\\n\\n```python\\nimport pandas as pd\\n\\ndef __init__(self):\\n    \\"\\"\\"\\n    Initializes the EnsembleModel instance.\\n\\n    Sets up an empty preprocessing pipeline, a dictionary for fitted models,\\n    and a DataFrame to store strategies. Also calls the method to initialize sub-models.\\n    \\"\\"\\"\\n    self.preprocessor = None\\n    self.fitted_models = {}\\n    self.db_path = None\\n    self.strategies = pd.DataFrame(columns=[\\"strategy\\", \\"model_list\\", \\"weights\\"])\\n    self.initialize_models()\\n```\\n\\n### Adding Strategies and Saving to the Database\\n\\nThe custom-defined `add_strategy_and_save_to_db` method enables the addition of new ensemble strategies to the model and their storage in a DuckDB database. This method accepts a pandas DataFrame containing the strategies and the database path as inputs. It appends the new strategies to the existing ones and saves them in the database specified during the initialization of the ensemble model. This method facilitates the management of various ensemble strategies and ensures their persistent storage for future use.\\n\\n```python\\nimport duckdb\\nimport pandas as pd\\n\\ndef add_strategy_and_save_to_db(self, strategy_df: pd.DataFrame, db_path: str) -> None:\\n    \\"\\"\\"Add strategies from a DataFrame and save them to the DuckDB database.\\n\\n    Args:\\n        strategy_df (pd.DataFrame): DataFrame containing strategies.\\n        db_path (str): Path to the DuckDB database.\\n    \\"\\"\\"\\n    # Update the instance-level database path for the current object\\n    self.db_path = db_path\\n\\n    # Attempt to concatenate new strategies with the existing DataFrame\\n    try:\\n        self.strategies = pd.concat([self.strategies, strategy_df], ignore_index=True)\\n    except Exception as e:\\n        # Print an error message if any exceptions occur during concatenation\\n        print(f\\"Error concatenating DataFrames: {e}\\")\\n        return  # Exit early to prevent further errors\\n\\n    # Use context manager for the database connection\\n    try:\\n        with duckdb.connect(self.db_path) as con:\\n            # Register the strategies DataFrame as a temporary table in DuckDB\\n            con.register(\\"strategy_df\\", self.strategies)\\n\\n            # Drop any existing strategies table and create a new one with updated strategies\\n            con.execute(\\"DROP TABLE IF EXISTS strategies\\")\\n            con.execute(\\"CREATE TABLE strategies AS SELECT * FROM strategy_df\\")\\n    except Exception as e:\\n        # Print an error message if any exceptions occur during database operations\\n        print(f\\"Error executing database operations: {e}\\")\\n```\\n\\nThe following example demonstrates how to use this method to add strategies to the database.\\n\\n```python\\nimport pandas as pd\\n\\n# Initialize ensemble model\\nensemble_model = EnsembleModel()\\n\\n# Define strategies for the ensemble model\\nstrategy_data = {\\n    \\"strategy\\": [\\"average_1\\"],\\n    \\"model_list\\": [\\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\"],\\n    \\"weights\\": [\\"1\\"],\\n}\\n\\n# Create a DataFrame to hold the strategy information\\nstrategies_df = pd.DataFrame(strategy_data)\\n\\n# Add strategies to the database\\nensemble_model.add_strategy_and_save_to_db(strategies_df, \\"models/strategies.db\\")\\n```\\n\\nThe DataFrame `strategy_data` includes:\\n\\n- **strategy**: The name of the strategy for model predictions.\\n- **model_list**: A comma-separated list of model names included in the strategy.\\n- **weights**: A comma-separated list of weights assigned to each model in the `model_list`. If not provided, implies equal weights or default values.\\n\\n| strategy  | model_list                                                     | weights |\\n| --------- | -------------------------------------------------------------- | ------- |\\n| average_1 | random_forest,xgboost,decision_tree,gradient_boosting,adaboost | 1       |\\n\\n### Feature Engineering\\n\\nThe `feature_engineering` method preprocesses input data by handling missing values, scaling numerical features, and encoding categorical features. It applies different transformations to both numerical and categorical features, and returns the processed features as a NumPy array. This method is crucial for preparing data in a suitable format for model training, ensuring consistency and enhancing model performance.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ndef feature_engineering(self, X: pd.DataFrame) -> np.ndarray:\\n    \\"\\"\\"\\n    Applies feature engineering to the input data X, including imputation, scaling, and encoding.\\n\\n    Args:\\n        X (pd.DataFrame): Input features with potential categorical and numerical columns.\\n\\n    Returns:\\n        np.ndarray: Processed feature array after transformations.\\n    \\"\\"\\"\\n    # Convert columns with \'object\' dtype to \'category\' dtype for proper handling of categorical features\\n    X = X.apply(\\n        lambda col: col.astype(\\"category\\") if col.dtypes == \\"object\\" else col\\n    )\\n\\n    # Identify categorical and numerical features from the DataFrame\\n    categorical_features = X.select_dtypes(include=[\\"category\\"]).columns\\n    numerical_features = X.select_dtypes(include=[\\"number\\"]).columns\\n\\n    # Define the pipeline for numerical features: imputation followed by scaling\\n    numeric_transformer = Pipeline(\\n        steps=[\\n            (\\n                \\"imputer\\",\\n                SimpleImputer(strategy=\\"median\\"),\\n            ),  # Replace missing values with the median\\n            (\\n                \\"scaler\\",\\n                StandardScaler(),\\n            ),  # Standardize features by removing the mean and scaling to unit variance\\n        ]\\n    )\\n\\n    # Define the pipeline for categorical features: imputation followed by one-hot encoding\\n    categorical_transformer = Pipeline(\\n        steps=[\\n            (\\n                \\"imputer\\",\\n                SimpleImputer(strategy=\\"most_frequent\\"),\\n            ),  # Replace missing values with the most frequent value\\n            (\\n                \\"onehot\\",\\n                OneHotEncoder(handle_unknown=\\"ignore\\"),\\n            ),  # Encode categorical features as a one-hot numeric array\\n        ]\\n    )\\n\\n    # Create a ColumnTransformer to apply the appropriate pipelines to the respective feature types\\n    preprocessor = ColumnTransformer(\\n        transformers=[\\n            (\\n                \\"num\\",\\n                numeric_transformer,\\n                numerical_features,\\n            ),  # Apply the numeric pipeline to numerical features\\n            (\\n                \\"cat\\",\\n                categorical_transformer,\\n                categorical_features,\\n            ),  # Apply the categorical pipeline to categorical features\\n        ]\\n    )\\n\\n    # Fit and transform the input data using the preprocessor\\n    X_processed = preprocessor.fit_transform(X)\\n\\n    # Store the preprocessor for future use in the predict method\\n    self.preprocessor = preprocessor\\n    return X_processed\\n```\\n\\n### Initializing Models\\n\\nThe `initialize_models` method sets up a dictionary of various Machine Learning models along with their hyperparameter grids. This includes models such as `RandomForest`, `XGBoost`, `DecisionTree`, `GradientBoosting`, and `AdaBoost`. This step is crucial for preparing the ensemble\u2019s sub-models and specifying the hyperparameters to adjust during training, ensuring that each model is configured correctly and ready for training.\\n\\n```python\\nfrom sklearn.ensemble import (\\n    AdaBoostRegressor,\\n    GradientBoostingRegressor,\\n    RandomForestRegressor,\\n)\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom xgboost import XGBRegressor\\n\\ndef initialize_models(self) -> None:\\n    \\"\\"\\"\\n    Initializes a dictionary of models along with their hyperparameter grids for grid search.\\n    \\"\\"\\"\\n    # Define various regression models with their respective hyperparameter grids for tuning\\n    self.models = {\\n        \\"random_forest\\": (\\n            RandomForestRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [None, 10, 20]},\\n        ),\\n        \\"xgboost\\": (\\n            XGBRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [3, 6, 10]},\\n        ),\\n        \\"decision_tree\\": (\\n            DecisionTreeRegressor(random_state=42),\\n            {\\"max_depth\\": [None, 10, 20]},\\n        ),\\n        \\"gradient_boosting\\": (\\n            GradientBoostingRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"max_depth\\": [3, 5, 7]},\\n        ),\\n        \\"adaboost\\": (\\n            AdaBoostRegressor(random_state=42),\\n            {\\"n_estimators\\": [50, 100, 200], \\"learning_rate\\": [0.01, 0.1, 1.0]},\\n        ),\\n    }\\n```\\n\\n### Defining a Custom `fit` Method to Train and Save Multi-Models\\n\\nAs already highlighted in the previous method, a key feature of MLflow PyFunc models is the ability to define custom methods, providing significant flexibility and customization for various tasks. In the multi-model PyFunc setup, the `fit` method is essential for customizing and optimizing multiple sub-models. It manages the training and fine-tuning of algorithms such as `RandomForestRegressor`, `XGBRegressor`, `DecisionTreeRegressor`, `GradientBoostingRegressor`, and `AdaBoostRegressor`. For demonstration purposes, Grid Search is used, which, while straightforward, can be computationally intensive and time-consuming, especially for ensemble models. To enhance efficiency, advanced optimization methods such as Bayesian optimization are recommended. Tools like [Optuna](https://optuna.org/) and [Hyperopt](https://hyperopt.github.io/hyperopt/) leverage probabilistic models to intelligently navigate the search space, significantly reducing the number of evaluations needed to identify optimal configurations.\\n\\n```python\\nimport os\\n\\nimport joblib\\nimport pandas as pd\\nfrom sklearn.model_selection import GridSearchCV\\n\\ndef fit(\\n        self, X_train_processed: pd.DataFrame, y_train: pd.Series, save_path: str\\n    ) -> None:\\n    \\"\\"\\"\\n    Trains the ensemble of models using the provided preprocessed training data.\\n\\n    Args:\\n        X_train_processed (pd.DataFrame): Preprocessed feature matrix for training.\\n        y_train (pd.Series): Target variable for training.\\n        save_path (str): Directory path where trained models will be saved.\\n    \\"\\"\\"\\n    # Create the directory for saving models if it does not exist\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    # Iterate over each model and its parameter grid\\n    for model_name, (model, param_grid) in self.models.items():\\n        # Perform GridSearchCV to find the best hyperparameters for the current model\\n        grid_search = GridSearchCV(\\n            model, param_grid, cv=5, n_jobs=-1, scoring=\\"neg_mean_squared_error\\"\\n        )\\n        grid_search.fit(\\n            X_train_processed, y_train\\n        )  # Fit the model with the training data\\n\\n        # Save the best estimator from GridSearchCV\\n        best_model = grid_search.best_estimator_\\n        self.fitted_models[model_name] = best_model\\n\\n        # Save the trained model to disk\\n        joblib.dump(best_model, os.path.join(save_path, f\\"{model_name}.pkl\\"))\\n```\\n\\n### Defining a Custom `predict` Method to Aggregate Multi-model Predictions\\n\\nTo streamline the inference process, every PyFunc model should define a custom `predict` method as the single entry point for inference. This approach abstracts the model\'s internal workings at inference time, whether dealing with a custom PyFunc model or an out-of-the-box MLflow built-in flavor for popular ML frameworks.\\n\\nThe custom `predict` method for the ensemble model is designed to collect and combine predictions from the sub-models, supporting various aggregation strategies (e.g., average, weighted). The process involves the following steps:\\n\\n1. Load the sub-model predictions aggregation strategy based on the user-defined approach.\\n2. Load the models to be used for inference.\\n3. Preprocess the input data.\\n4. Collect predictions from individual models.\\n5. Aggregate the model predictions according to the specified strategy.\\n\\n```python\\nimport duckdb\\nimport joblib\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ndef predict(self, context, model_input: pd.DataFrame) -> np.ndarray:\\n    \\"\\"\\"\\n    Predicts the target variable using the ensemble of models based on the selected strategy.\\n\\n    Args:\\n        context: MLflow context object.\\n        model_input (pd.DataFrame): Input features for prediction.\\n\\n    Returns:\\n        np.ndarray: Array of predicted values.\\n\\n    Raises:\\n        ValueError: If the strategy is unknown or no models are fitted.\\n    \\"\\"\\"\\n    # Check if the \'strategy\' column is present in the input DataFrame\\n    if \\"strategy\\" in model_input.columns:\\n        # Extract the strategy and drop it from the input features\\n        print(f\\"Strategy: {model_input[\'strategy\'].iloc[0]}\\")\\n        strategy = model_input[\\"strategy\\"].iloc[0]\\n        model_input.drop(columns=[\\"strategy\\"], inplace=True)\\n    else:\\n        # Default to \'average\' strategy if none is provided\\n        strategy = \\"average\\"\\n\\n    # Load the strategy details from the pre-loaded strategies DataFrame\\n    loaded_strategy = self.strategies[self.strategies[\\"strategy\\"] == strategy]\\n\\n    if loaded_strategy.empty:\\n        # Raise an error if the specified strategy is not found\\n        raise ValueError(\\n            f\\"Strategy \'{strategy}\' not found in the pre-loaded strategies.\\"\\n        )\\n\\n    # Parse the list of models to be used for prediction\\n    model_list = loaded_strategy[\\"model_list\\"].iloc[0].split(\\",\\")\\n\\n    # Transform input features using the preprocessor, if available\\n    if self.preprocessor is None:\\n        # Feature engineering is required if the preprocessor is not set\\n        X_processed = self.feature_engineering(model_input)\\n    else:\\n        # Use the existing preprocessor to transform the features\\n        X_processed = self.preprocessor.transform(model_input)\\n\\n    if not self.fitted_models:\\n        # Raise an error if no models are fitted\\n        raise ValueError(\\"No fitted models found. Please fit the models first.\\")\\n\\n    # Collect predictions from all models specified in the strategy\\n    predictions = np.array(\\n        [self.fitted_models[model].predict(X_processed) for model in model_list]\\n    )\\n\\n    # Apply the specified strategy to combine the model predictions\\n    if \\"average\\" in strategy:\\n        # Calculate the average of predictions from all models\\n        return np.mean(predictions, axis=0)\\n    elif \\"weighted\\" in strategy:\\n        # Extract weights from the strategy and normalize them\\n        weights = [float(w) for w in loaded_strategy[\\"weights\\"].iloc[0].split(\\",\\")]\\n        weights = np.array(weights)\\n        weights /= np.sum(weights)  # Ensure weights sum to 1\\n\\n        # Compute the weighted average of predictions\\n        return np.average(predictions, axis=0, weights=weights)\\n    else:\\n        # Raise an error if an unknown strategy is encountered\\n        raise ValueError(f\\"Unknown strategy: {strategy}\\")\\n```\\n\\n### Defining a `load context` custom method to initialize the Ensemble Model\\n\\nWhen loading the ensemble model using `mlflow.pyfunc.load_model`, the custom `load_context` method is executed to handle the required model initialization steps before inference.\\n\\nThis initialization process includes:\\n\\n1. Loading model artifacts, including both the pre-trained models and the preprocessor, using the context object that contains the artifacts references.\\n2. Fetching strategies definitions from DuckDB Database.\\n\\n```python\\nimport duckdb\\nimport joblib\\nimport pandas as pd\\n\\ndef load_context(self, context) -> None:\\n    \\"\\"\\"\\n    Loads the preprocessor and models from the MLflow context.\\n\\n    Args:\\n        context: MLflow context object which provides access to saved artifacts.\\n    \\"\\"\\"\\n    # Load the preprocessor if its path is specified in the context artifacts\\n    preprocessor_path = context.artifacts.get(\\"preprocessor\\", None)\\n    if preprocessor_path:\\n        self.preprocessor = joblib.load(preprocessor_path)\\n\\n    # Load each model from the context artifacts and store it in the fitted_models dictionary\\n    for model_name in self.models.keys():\\n        model_path = context.artifacts.get(model_name, None)\\n        if model_path:\\n            self.fitted_models[model_name] = joblib.load(model_path)\\n        else:\\n            # Print a warning if a model is not found in the context artifacts\\n            print(\\n                f\\"Warning: {model_name} model not found in artifacts. Initialized but not fitted.\\"\\n            )\\n\\n    # Reconnect to the DuckDB database to load the strategies\\n    conn = duckdb.connect(self.db_path)\\n    # Fetch strategies from the DuckDB database into the strategies DataFrame\\n    self.strategies = conn.execute(\\"SELECT * FROM strategies\\").fetchdf()\\n    # Close the database connection\\n    conn.close()\\n```\\n\\n### Bringing It All Together\\n\\nHaving explored each method in detail, the next step is to integrate them to observe the complete implementation in action. This will offer a comprehensive view of how the components interact to achieve the project\'s objectives.\\n\\nYou can use the skeleton provided in the [Creating the Ensemble Model](#creating-the-ensemble-model) section to assemble the entire `EnsembleModel` class. Each method was demonstrated with its specific dependencies included. Now, you just need to combine these methods into the class definition, following the outline given. Feel free to add any custom logic that fits your specific use case or enhances the functionality of the ensemble model.\\n\\nAfter everything has been encapsulated in a PyFunc model, the lifecycle of the ensemble model closely mirrors that of a traditional MLflow model. The following diagram depicts the lifecycle of the model.\\n\\n![Ensemble Model Lifecycle](ensemble-model-lifecycle.png)\\n\\n## MLflow Tracking\\n\\n### Using the `fit` Method to Train Sub-Models\\n\\nOnce the data is preprocessed, we use the custom `fit` method to train all the sub-models in our Ensemble Model. This method applies grid search to find the best hyperparameters for each sub-model, fits them to the training data, and saves the trained models for future use.\\n\\n> **Note:** For the following block of code, you might need to set the MLflow Tracking Server if you\'re not using Managed MLflow. In the [Components of the Project](#components-of-the-project), there\'s a note about setting up a simple local MLflow Tracking Server. For this step of the project, you\'ll need to point MLflow to the server\u2019s URI that has been configured and is currently running. Don\'t forget to set the server URI variable `remote_server_uri`. You can refer to the official MLflow documentation for more details on [Logging to a Tracking Server](https://mlflow.org/docs/latest/tracking/server.html#logging-to-a-tracking-server).\\n\\n```python\\nimport datetime\\nimport os\\n\\nimport joblib\\nimport mlflow\\nimport pandas as pd\\nfrom mlflow.models.signature import infer_signature\\nfrom sklearn.model_selection import train_test_split\\n\\n# Initialize the MLflow client\\nclient = mlflow.MlflowClient()\\n\\n# Set the URI of your MLflow Tracking Server\\nremote_server_uri = \\"...\\"  # Replace with your server URI\\n\\n# Point MLflow to your MLflow Tracking Server\\nmlflow.set_tracking_uri(remote_server_uri)\\n\\n# Set the experiment name for organizing runs in MLflow\\nmlflow.set_experiment(\\"Ensemble Model\\")\\n\\n# Load dataset from the provided URL\\ndata = pd.read_csv(\\n    \\"https://github.com/zobi123/Machine-Learning-project-with-Python/blob/master/Housing.csv?raw=true\\"\\n)\\n\\n# Separate features and target variable\\nX = data.drop(\\"price\\", axis=1)\\ny = data[\\"price\\"]\\n\\n# Split dataset into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)\\n\\n# Create a directory to save the models and related files\\nos.makedirs(\\"models\\", exist_ok=True)\\n\\n# Initialize and train the EnsembleModel\\nensemble_model = EnsembleModel()\\n\\n# Preprocess the training data using the defined feature engineering method\\nX_train_processed = ensemble_model.feature_engineering(X_train)\\n\\n# Fit the models with the preprocessed training data and save them\\nensemble_model.fit(X_train_processed, y_train, save_path=\\"models\\")\\n\\n# Infer the model signature using a small example from the training data\\nexample_input = X_train[:1]  # Use a single sample for signature inference\\nexample_input[\\"strategy\\"] = \\"average\\"\\nexample_output = y_train[:1]\\nsignature = infer_signature(example_input, example_output)\\n\\n# Save the preprocessing pipeline to disk\\njoblib.dump(ensemble_model.preprocessor, \\"models/preprocessor.pkl\\")\\n\\n# Define strategies for the ensemble model\\nstrategy_data = {\\n    \\"strategy\\": [\\n        \\"average_1\\",\\n        \\"average_2\\",\\n        \\"weighted_1\\",\\n        \\"weighted_2\\",\\n        \\"weighted_3\\",\\n        \\"weighted_4\\",\\n    ],\\n    \\"model_list\\": [\\n        \\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\",\\n        \\"decision_tree\\",\\n        \\"random_forest,xgboost,decision_tree,gradient_boosting,adaboost\\",\\n        \\"random_forest,xgboost,gradient_boosting\\",\\n        \\"decision_tree,adaboost\\",\\n        \\"xgboost,gradient_boosting\\",\\n    ],\\n    \\"weights\\": [\\"1\\", \\"1\\", \\"0.2,0.3,0.1,0.2,0.2\\", \\"0.4,0.4,0.2\\", \\"0.5,0.5\\", \\"0.7,0.3\\"],\\n}\\n\\n# Create a DataFrame to hold the strategy information\\nstrategies_df = pd.DataFrame(strategy_data)\\n\\n# Add strategies to the database\\nensemble_model.add_strategy_and_save_to_db(strategies_df, \\"models/strategies.db\\")\\n\\n# Define the Conda environment configuration for the MLflow model\\nconda_env = {\\n    \\"name\\": \\"mlflow-env\\",\\n    \\"channels\\": [\\"conda-forge\\"],\\n    \\"dependencies\\": [\\n        \\"python=3.8\\",\\n        \\"scikit-learn=1.3.0\\",\\n        \\"xgboost=2.0.3\\",\\n        \\"joblib=1.2.0\\",\\n        \\"pandas=1.5.3\\",\\n        \\"numpy=1.23.5\\",\\n        \\"duckdb=1.0.0\\",\\n        {\\n            \\"pip\\": [\\n                \\"mlflow==2.14.1\\",\\n            ]\\n        },\\n    ],\\n}\\n\\n# Get current timestamp\\ntimestamp = datetime.datetime.now().isoformat()\\n\\n# Log the model using MLflow\\nwith mlflow.start_run(run_name=timestamp) as run:\\n    # Log parameters, artifacts, and model signature\\n    mlflow.log_param(\\"model_type\\", \\"EnsembleModel\\")\\n\\n    artifacts = {\\n        model_name: os.path.join(\\"models\\", f\\"{model_name}.pkl\\")\\n        for model_name in ensemble_model.models.keys()\\n    }\\n    artifacts[\\"preprocessor\\"] = os.path.join(\\"models\\", \\"preprocessor.pkl\\")\\n    artifacts[\\"strategies_db\\"] = os.path.join(\\"models\\", \\"strategies.db\\")\\n\\n    mlflow.pyfunc.log_model(\\n        artifact_path=\\"ensemble_model\\",\\n        python_model=ensemble_model,\\n        artifacts=artifacts,\\n        conda_env=conda_env,\\n        signature=signature,\\n    )\\n\\n    print(f\\"Model logged in run {run.info.run_id}\\")\\n```\\n\\n### Registering the Model with MLflow\\n\\nFollowing the completion of model training, the subsequent step involves registering the ensemble model with MLflow. This process entails logging the trained models, preprocessing pipelines, and associated strategies into the MLflow Tracking Server. This ensures that all components of the ensemble model are systematically saved and versioned, facilitating reproducibility and traceability.\\n\\nMoreover, we will assign to this initial version of the model a production alias. This designation establishes a baseline model against which future iterations can be assessed. By marking this version as the `production` model, we can effectively benchmark improvements and confirm that subsequent versions yield measurable advancements over this established baseline.\\n\\n```python\\n# Register the model in MLflow and assign a production alias\\nmodel_uri = f\\"runs:/{run.info.run_id}/ensemble_model\\"\\nmodel_details = mlflow.register_model(model_uri=model_uri, name=\\"ensemble_model\\")\\n\\nclient.set_registered_model_alias(\\n\\tname=\\"ensemble_model\\", alias=\\"production\\", version=model_details.version\\n)\\n```\\n\\nThe following illustration demonstrates the complete lifecycle of our ensemble model within the MLflow UI up until this step.\\n\\n![Ensemble Model within MLflow UI](ensemble-model-mlflow-ui.gif)\\n\\n### Using the `predict` Method to Perform Inference\\n\\nWith the ensemble model registered in the MLflow Model Registry, it can now be utilized to predict house prices by aggregating the predictions from the various sub-models within the ensemble.\\n\\n```python\\nimport pandas as pd\\n\\nimport mlflow\\nfrom sklearn.metrics import r2_score\\n\\n# Load the registered model using its alias\\nloaded_model = mlflow.pyfunc.load_model(\\n\\tmodel_uri=f\\"models:/ensemble_model@production\\"\\n)\\n\\n# Define the different strategies for evaluation\\nstrategies = [\\n    \\"average_1\\",\\n    \\"average_2\\",\\n    \\"weighted_1\\",\\n    \\"weighted_2\\",\\n    \\"weighted_3\\",\\n    \\"weighted_4\\",\\n]\\n\\n# Initialize a DataFrame to store the results of predictions\\nresults_df = pd.DataFrame()\\n\\n# Iterate over each strategy, make predictions, and calculate R^2 scores\\nfor strategy in strategies:\\n    # Create a test DataFrame with the current strategy\\n    X_test_with_params = X_test.copy()\\n    X_test_with_params[\\"strategy\\"] = strategy\\n\\n    # Use the loaded model to make predictions\\n    y_pred = loaded_model.predict(X_test_with_params)\\n\\n    # Calculate R^2 score for the predictions\\n    r2 = r2_score(y_test, y_pred)\\n\\n    # Store the results and R^2 score in the results DataFrame\\n    results_df[strategy] = y_pred\\n    results_df[f\\"r2_{strategy}\\"] = r2\\n\\n# Add the actual target values to the results DataFrame\\nresults_df[\\"y_test\\"] = y_test.values\\n```\\n\\nSimilar to out-of-the-box MLflow models, you begin by loading the ensemble model using `mlflow.pyfunc.load_model` to generate the house price predictions. After defining the different strategies for aggregating sub-model predictions and creating the model input containing both the housing data features and aggregation strategy, simply call the ensemble model\'s `predict` method to get the aggregated house price prediction.\\n\\n### Evaluating Model Performance with Different Strategies\\n\\nTo evaluate the performance of our ensemble model, we calculated the average R\xb2 scores for different aggregation strategies. These strategies include both simple averaging and weighted combinations of sub-models, with varying configurations of models and their respective weights. By comparing the R\xb2 scores, we can assess which strategies provide the most accurate predictions.\\n\\nThe bar graph below illustrates the average R\xb2 scores for each strategy. Higher values indicate better predictive performance. As shown in the graph, the ensemble strategies generally outperform individual models as depicted in our second strategy that is relying on a single `DecisionTree` (average_2), demonstrating the effectiveness of aggregating predictions from multiple sub-models. This visual comparison highlights the benefits of using an ensemble approach, particularly with weighted strategies that optimize the contribution of each sub-model.\\n\\n![Ensemble Model Evaluation](ensemble-model-evaluation.png)\\n\\n## Summary\\n\\nThis blog post highlights the capabilities of mlflow.pyfunc and its application in a Machine Learning project. This powerful feature of MLflow provides creative freedom and flexibility, enabling teams to build complex systems encapsulated as models within MLflow, following the same lifecycle as traditional models. The post showcases the creation of ensemble model setups, seamless integration with DuckDB, and the implementation of custom methods using this versatile module.\\n\\nBeyond offering a structured approach to achieving desired outcomes, this blog demonstrates practical possibilities based on hands-on experience, discussing potential challenges and their solutions.\\n\\n## Additional resources\\n\\nExplore the following resources for a deeper understanding of MLflow PyFunc models:\\n\\n- [Custom MLflow Models with mlflow.pyfunc](https://mlflow.org/blog/custom-pyfunc)\\n- [Understanding PyFunc in MLflow](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part2-pyfunc-components.html)\\n- [Building Custom Python Function Models with MLflow](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n- [Deploy an MLflow PyFunc model with Model Serving](https://mlflow.org/docs/latest/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.html)"},{"id":"mlflow-tracing","metadata":{"permalink":"/mlflow-website/blog/mlflow-tracing","source":"@site/blog/2024-06-10-mlflow-tracing/index.md","title":"Introducing MLflow Tracing","description":"We\'re excited to announce the release of a powerful new feature in MLflow: MLflow Tracing.","date":"2024-06-10T00:00:00.000Z","tags":[{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"},{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"mlops","permalink":"/mlflow-website/blog/tags/mlops"}],"readingTime":3.825,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"Introducing MLflow Tracing","tags":["tracing","genai","mlops"],"slug":"mlflow-tracing","authors":["mlflow-maintainers"],"thumbnail":"/img/blog/trace-intro.gif"},"unlisted":false,"prevItem":{"title":"PyFunc in Practice","permalink":"/mlflow-website/blog/pyfunc-in-practice"},"nextItem":{"title":"Deep Learning with MLflow (Part 2)","permalink":"/mlflow-website/blog/deep-learning-part-2"}},"content":"We\'re excited to announce the release of a powerful new feature in MLflow: [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html).\\nThis feature brings comprehensive instrumentation capabilities to your GenAI applications, enabling you to gain deep insights into the execution of your\\nmodels and workflows, from simple chat interfaces to complex multi-stage Retrieval Augmented Generation (RAG) applications.\\n\\n> NOTE: MLflow Tracing has been released in MLflow 2.14.0 and is not available in previous versions.\\n\\n## Introducing MLflow Tracing\\n\\nTracing is a critical aspect of understanding and optimizing complex applications, especially in the realm of machine learning and artificial intelligence.\\nWith the release of MLflow Tracing, you can now easily capture, visualize, and analyze detailed execution traces of your GenAI applications.\\nThis new feature aims to provide greater visibility and control over your applications\' performance and behavior, aiding in everything from fine-tuning to debugging.\\n\\n## What is MLflow Tracing?\\n\\nMLflow Tracing offers a variety of methods to enable [tracing](https://mlflow.org/docs/latest/llms/tracing/overview.html) in your applications:\\n\\n- **Automated Tracing with LangChain**: A fully automated integration with [LangChain](https://www.langchain.com/) allows you to activate tracing simply by enabling `mlflow.langchain.autolog()`.\\n- **Manual Trace Instrumentation with High-Level Fluent APIs**: Use decorators, function wrappers, and context managers via the fluent API to add tracing functionality with minimal code modifications.\\n- **Low-Level Client APIs for Tracing**: The MLflow client API provides a thread-safe way to handle trace implementations for fine-grained control of what and when data is recorded.\\n\\n## Getting Started with MLflow Tracing\\n\\n### LangChain Automatic Tracing\\n\\nThe easiest way to get started with MLflow Tracing is through the built-in integration with LangChain. By enabling autologging, traces are automatically logged to the active MLflow experiment when calling invocation APIs on chains. Here\u2019s a quick example:\\n\\n```python\\nimport os\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI\\nimport mlflow\\n\\nassert \\"OPENAI_API_KEY\\" in os.environ, \\"Please set your OPENAI_API_KEY environment variable.\\"\\n\\nmlflow.set_experiment(\\"LangChain Tracing\\")\\nmlflow.langchain.autolog(log_models=True, log_input_examples=True)\\n\\nllm = OpenAI(temperature=0.7, max_tokens=1000)\\nprompt_template = \\"Imagine you are {person}, and you are answering a question: {question}\\"\\nchain = prompt_template | llm\\n\\nchain.invoke({\\"person\\": \\"Richard Feynman\\", \\"question\\": \\"Why should we colonize Mars?\\"})\\nchain.invoke({\\"person\\": \\"Linus Torvalds\\", \\"question\\": \\"Can I set everyone\'s access to sudo?\\"})\\n\\n```\\n\\nAnd this is what you will see after invoking the chains when navigating to the **LangChain Tracing** experiment in the MLflow UI:\\n\\n![Traces in UI](tracing-ui.gif)\\n\\n### Fluent APIs for Manual Tracing\\n\\nFor more control, you can use MLflow\u2019s fluent APIs to manually instrument your code. This approach allows you to capture detailed trace data with minimal changes to your existing code.\\n\\n#### Trace Decorator\\n\\nThe trace decorator captures the inputs and outputs of a function:\\n\\n```python\\nimport mlflow\\n\\nmlflow.set_experiment(\\"Tracing Demo\\")\\n\\n@mlflow.trace\\ndef some_function(x, y, z=2):\\n    return x + (y - z)\\n\\nsome_function(2, 4)\\n```\\n\\n#### Context Handler\\n\\nThe context handler is ideal for supplementing span information with additional data at the point of information generation:\\n\\n```python\\nimport mlflow\\n\\n@mlflow.trace\\ndef first_func(x, y=2):\\n    return x + y\\n\\n@mlflow.trace\\ndef second_func(a, b=3):\\n    return a * b\\n\\ndef do_math(a, x, operation=\\"add\\"):\\n    with mlflow.start_span(name=\\"Math\\") as span:\\n        span.set_inputs({\\"a\\": a, \\"x\\": x})\\n        span.set_attributes({\\"mode\\": operation})\\n        first = first_func(x)\\n        second = second_func(a)\\n        result = first + second if operation == \\"add\\" else first - second\\n        span.set_outputs({\\"result\\": result})\\n        return result\\n\\ndo_math(8, 3, \\"add\\")\\n```\\n\\n### Comprehensive Tracing with Client APIs\\n\\nFor advanced use cases, the MLflow client API offers fine-grained control over trace management. These APIs allows you to create, manipulate, and retrieve traces programmatically, albeit with additional complexity throughout the implementation.\\n\\n#### Starting and Managing Traces with the Client APIs\\n\\n```python\\nfrom mlflow import MlflowClient\\n\\nclient = MlflowClient()\\n\\n# Start a new trace\\nroot_span = client.start_trace(\\"my_trace\\")\\nrequest_id = root_span.request_id\\n\\n# Create a child span\\nchild_span = client.start_span(\\n    name=\\"child_span\\",\\n    request_id=request_id,\\n    parent_id=root_span.span_id,\\n    inputs={\\"input_key\\": \\"input_value\\"},\\n    attributes={\\"attribute_key\\": \\"attribute_value\\"},\\n)\\n\\n# End the child span\\nclient.end_span(\\n    request_id=child_span.request_id,\\n    span_id=child_span.span_id,\\n    outputs={\\"output_key\\": \\"output_value\\"},\\n    attributes={\\"custom_attribute\\": \\"value\\"},\\n)\\n\\n# End the root span (trace)\\nclient.end_trace(\\n    request_id=request_id,\\n    outputs={\\"final_output_key\\": \\"final_output_value\\"},\\n    attributes={\\"token_usage\\": \\"1174\\"},\\n)\\n```\\n\\n## Diving Deeper into Tracing\\n\\nMLflow Tracing is designed to be flexible and powerful, supporting various use cases from simple function tracing to complex, asynchronous workflows.\\n\\nTo learn more about this feature, [read the guide](https://mlflow.org/docs/latest/llms/tracing/index.html), [review the API Docs](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-fluent-python-apis) and [get started with the LangChain integration](https://mlflow.org/docs/latest/llms/tracing/index.html#langchain-automatic-tracing) today!\\n\\n## Join Us on This Journey\\n\\nThe introduction of MLflow Tracing marks a significant milestone in our mission to provide comprehensive tools for managing machine learning workflows. We\u2019re excited about the possibilities this new feature opens up and look forward to your [feedback](https://github.com/mlflow/mlflow/issues) and [contributions](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).\\n\\nFor those in our community with a passion for sharing knowledge, we invite you to [collaborate](https://github.com/mlflow/mlflow-website/blob/main/CONTRIBUTING.md). Whether it\u2019s writing tutorials, sharing use-cases, or providing feedback, every contribution enriches the MLflow community.\\n\\nStay tuned for more updates, and as always, happy coding!"},{"id":"deep-learning-part-2","metadata":{"permalink":"/mlflow-website/blog/deep-learning-part-2","source":"@site/blog/2024-04-26-deep-learning-part-2/index.md","title":"Deep Learning with MLflow (Part 2)","description":"Using MLflow\'s Deep Learning tracking features for fine tuning an LLM","date":"2024-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"Deep Learning","permalink":"/mlflow-website/blog/tags/deep-learning"}],"readingTime":11.64,"hasTruncateMarker":true,"authors":[{"name":"Puneet Jain","title":"Sr. Specialist Solutions Architect at Databricks","url":"https://www.linkedin.com/in/puneetjain159/","imageURL":"/mlflow-website/img/authors/puneet.png","key":"puneet-jain","page":null},{"name":"Avinash Sooriyarachchi","title":"Sr. Solutions Architect at Databricks","url":"https://www.linkedin.com/in/avi-data-ml/","imageURL":"/mlflow-website/img/authors/avinash.png","key":"avinash-sooriyarachchi","page":null},{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/mlflow-website/img/authors/abe.png","key":"abe-omorogbe","page":null},{"name":"Ben Wilson","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/benjamin-wilson-arch/","imageURL":"/mlflow-website/img/authors/ben.png","key":"ben","page":null}],"frontMatter":{"title":"Deep Learning with MLflow (Part 2)","description":"Using MLflow\'s Deep Learning tracking features for fine tuning an LLM","slug":"deep-learning-part-2","authors":["puneet-jain","avinash-sooriyarachchi","abe-omorogbe","ben"],"tags":["Deep Learning"],"thumbnail":"/img/blog/dl-blog-2.png"},"unlisted":false,"prevItem":{"title":"Introducing MLflow Tracing","permalink":"/mlflow-website/blog/mlflow-tracing"},"nextItem":{"title":"MLflow Release Candidates","permalink":"/mlflow-website/blog/release-candidates"}},"content":"In the realm of deep learning, finetuning of pre-trained Large Language Models (LLMs) on private datasets is an excellent customization\\noption to increase a model\u2019s relevancy for a specific task. This practice is not only common, but also essential for developing specialized\\nmodels, particularly for tasks like text classification and summarization.\\n\\nIn such scenarios, tools like MLflow are invaluable. Tracking tools like MLflow help to ensure that every aspect of the training\\nprocess - metrics, parameters, and artifacts - are reproducibly tracked and logged, allowing for the analysis, comparison, and sharing of tuning iterations.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this blog post, we are going to be using [MLflow 2.12](https://mlflow.org/releases/2.12.1) and the\\n[recently introduced MLflow Deep Learning features](https://mlflow.org/blog/deep-learning-part-1) to track all the important aspects of fine\\ntuning a large language model for text classification, including the use of automated logging of training checkpoints in order to simplify\\nthe process of resumption of training.\\n\\n## Use Case: Fine Tuning a Transformer Model for Text Classification\\n\\nThe example scenario that we\'re using within this blog utilizes the [unfair-TOS](https://huggingface.co/datasets/coastalcph/lex_glue/viewer/unfair_tos) dataset.\\n\\nIn today\u2019s world, it\u2019s hard to find a service, platform, or even a consumer good that doesn\u2019t have a legally-binding terms of service connected\\nwith it. These encyclopedic size agreements, filled with dense legal jargon and sometimes baffling levels of specificity, are so large that\\nmost people simply accept them without reading them. However, reports have indicated over time that occasionally, some suspiciously unfair\\nterms are embedded within them.\\n\\nAddressing unfair clauses in Terms of Service (TOS) agreements through machine learning (ML) is particularly relevant due to the pressing\\nneed for transparency and fairness in legal agreements that affect consumers. Consider the following clause from an example TOS\\nagreement: **\\"We may revise these Terms from time to time. The changes will not be retroactive, and the most current version of the Terms, which will always...\\"**\\nThis clause stipulates that the service provider may suspend or terminate the service at any time for any reason,\\nwith or without notice. Most people would consider this to be quite unfair.\\n\\nWhile this sentence is buried quite deeply within a fairly dense document, an ML algorithm is not burdened by the exhaustion that a human\\nwould have for combing through the text and identifying clauses that might seem a bit unfair. By automatically identifying potentially\\nunfair clauses, a transformers-based Deep Learning (DL) model can help protect consumers from exploitative practices, ensure greater compliance with legal standards,\\nand foster trust between service providers and users.\\n\\nA base pre-trained transformer model, without specialized fine-tuning, faces several challenges in accurately identifying unfair Terms of Service clauses.\\nFirstly, it lacks the domain-specific knowledge essential for understanding complex legal language. Secondly, its training objectives are\\ntoo general to capture the nuanced interpretation required for legal analysis. Lastly, it may not effectively recognize the subtle\\ncontextual meanings that determine the fairness of contractual terms, making it less effective for this specialized task.\\n\\nUsing prompt engineering to address the identification of unfair Terms of Service clauses with a closed-source Large language model\\ncan be prohibitively expensive. This approach requires extensive trial and error to refine prompts without the ability to tweak\\nthe underlying model mechanics. Each iteration can consume significant computational resources , especially when using\\n[few-shot prompting](https://www.promptingguide.ai/techniques/fewshot), leading to escalating costs without guaranteeing a corresponding\\nincrease in accuracy or effectiveness.\\n\\nIn this context, the use of the **RoBERTa-base** model is particularly effective, provided that it is fine-tuned. This model is robust\\nenough to handle complex tasks like discerning embedded instructions within texts, yet it is sufficiently compact to be fine-tuned\\non modest hardware, such as an Nvidia T4 GPU.\\n\\n### What is PEFT?\\n\\n[Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/en/index) approaches are advantageous as they involve\\nkeeping the bulk of the pre-trained model parameters fixed while either only training a few additional layers or modifying the parameters used\\nwhen interacting with the model\'s weights. This methodology not only conserves memory during training, but also significantly reduces the overall training time. When\\ncompared with the alternative of fine-tuning a base model\'s weights in order to customize its performance for a specific targeted task, the PEFT\\napproach can save significant cost in both time and money, while providing an equivalent or better performance results with less data than is required\\nfor a comprehensive fine-tuning training task.\\n\\n## Integrating Hugging-Face models and the PyTorch Lightning framework\\n\\n[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) integrates seamlessly with\\n[Hugging Face\'s Transformers library](https://huggingface.co/docs/transformers/en/index), enabling streamlined model training workflows\\nthat capitalize on Lightning\'s easy-to-use Higher level API\u2019s and HF\'s state-of-the-art pre-trained models. The combination of Lightning with transformers\u2019\\n[PEFT module](https://huggingface.co/blog/peft) enhances productivity and scalability by reducing code complexity and enabling the use of\\nhigh-quality pre-optimized models for a range of diverse NLP tasks.\\n\\nBelow is an example of configuring the PEFT-based fine tuning of a base model using PyTorch Lightning and HuggingFace\'s `peft` module.\\n\\n```python\\nfrom typing import List\\nfrom lightning import LightningModule\\nfrom peft import get_peft_model, LoraConfig, TaskType\\nfrom transformers import AutoModelForSequenceClassification\\n\\n\\nclass TransformerModule(LightningModule):\\n    def __init__(\\n        self,\\n        pretrained_model: str,\\n        num_classes: int = 2,\\n        lora_alpha: int = 32,\\n        lora_dropout: float = 0.1,\\n        r: int = 8,\\n        lr: float = 2e-4\\n    ):\\n        super().__init__()\\n        self.model = self.create_model(pretrained_model, num_classes, lora_alpha, lora_dropout, r)\\n        self.lr = lr\\n        self.save_hyperparameters(\\"pretrained_model\\")\\n\\n    def create_model(self, pretrained_model, num_classes, lora_alpha, lora_dropout, r):\\n        \\"\\"\\"Create and return the PEFT model with the given configuration.\\n\\n        Args:\\n            pretrained_model: The path or identifier for the pretrained model.\\n            num_classes: The number of classes for the sequence classification.\\n            lora_alpha: The alpha parameter for LoRA.\\n            lora_dropout: The dropout rate for LoRA.\\n            r: The rank of LoRA adaptations.\\n\\n        Returns:\\n            Model: A model configured with PEFT.\\n        \\"\\"\\"\\n        model = AutoModelForSequenceClassification.from_pretrained(\\n            pretrained_model_name_or_path=pretrained_model,\\n            num_labels=num_classes\\n        )\\n        peft_config = LoraConfig(\\n            task_type=TaskType.SEQ_CLS,\\n            inference_mode=False,\\n            r=r,\\n            lora_alpha=lora_alpha,\\n            lora_dropout=lora_dropout\\n        )\\n        return get_peft_model(model, peft_config)\\n\\n    def forward(self, input_ids: List[int], attention_mask: List[int], label: List[int]):\\n        \\"\\"\\"Calculate the loss by passing inputs to the model and comparing against ground truth labels.\\n\\n        Args:\\n            input_ids: List of token indices to be fed to the model.\\n            attention_mask: List to indicate to the model which tokens should be attended to, and which should not.\\n            label: List of ground truth labels associated with the input data.\\n\\n        Returns:\\n            torch.Tensor: The computed loss from the model as a tensor.\\n        \\"\\"\\"\\n        return self.model(\\n            input_ids=input_ids,\\n            attention_mask=attention_mask,\\n            labels=label\\n        )\\n```\\n\\nAdditional references for the full implementation can be [seen within the companion repository here](https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/custom_module/fine_tune_clsify_head.py)\\n\\n## Configuring MLflow for PEFT-based fine-tuning\\n\\nBefore initiating the training process, it\'s crucial to configure MLFlow so that all system metrics, loss metrics, and parameters are logged for the training run.\\nAs of MLFlow 2.12, auto-logging for TensorFlow and PyTorch now includes support for checkpointing model weights during training, giving a snapshot of the model\\nweights at defined epoch frequencies in order to provide for training resumption in the case of an error or loss of the compute environment.\\nBelow is an example of how to enable this feature:\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.enable_system_metrics_logging()\\nmlflow.pytorch.autolog(checkpoint_save_best_only = False, checkpoint_save_freq=\'epoch\')\\n```\\n\\nIn the code above we are doing the following:\\n\\n- **Enabling System Metrics Logging**: The system resources will be logged to MLflow in order to understand where bottlenecks in memory, CPU, GPU, disk usage, and network traffic are throughout the training process.\\n\\n![MLflow UI System Metrics](sys_metrics.png)\\n\\n- **Configuring Auto Logging to log parameters, metrics and checkpoints for all epochs**: Deep learning involves experimenting with various model architectures and hyperparameter settings. Auto logging plays a crucial role in systematically recording these experiments, making it easier to compare different runs and determine which configurations yield the best results. Checkpoints are logged at every epoch, enabling detailed evaluations of all intermediate epochs during the initial exploration phase of the project. However, it is generally not advisable to log all epochs during late-stage development to avoid excessive data writes and latency in the final training stages.\\n\\n![System Metrics Logged](epoch_logging.png)\\n\\nThe auto-logged checkpoint metrics and model artifacts will be viewable in the MLflow UI as the model trains, as shown below:\\n\\n![Metrics logged with each epoch](checkpoint_metrics.png)\\n\\n## The Importance of Logging and Early-stopping\\n\\nThe integration of the Pytorch Lightning `Trainer` callback with MLflow is crucial within this training exercise. The integration allows for comprehensive\\ntracking and logging of metrics, parameters, and artifacts during model finetuning without having to explicitly call MLflow logging APIs. Additionally,\\nthe autologging API allows for modifying the default logging behavior, permitting changes to the logging frequency, allowing for logging to occur at each\\nepoch, after a specified number of epochs, or at explicitly defined steps.\\n\\n### Early stopping\\n\\nEarly stopping is a critical regularization technique in neural network training, designed to assist in preventing overfitting through the act of\\nhalting training when validation performance plateaus. Pytorch Lightning includes APIs that allow for an easy high-level control of training cessation,\\nas demonstrated below.\\n\\n### Configuring Pytorch Trainer Callback with Early stopping\\n\\nThe example below shows the configuration of the `Trainer` object within `Lightning` to leverage early stopping to prevent overfitting. Once configured, the\\ntraining is executed by calling `fit` on the `Trainer` object. By providing the `EarlyStopping` callback, in conjunction with MLflow\'s autologging, the\\nappropriate number of epochs will be used, logged, and tracked without any additional effort.\\n\\n```python\\nfrom dataclasses import dataclass, field\\nimport os\\n\\nfrom data import LexGlueDataModule\\nfrom lightning import Trainer\\nfrom lightning.pytorch.callbacks import EarlyStopping\\nimport mlflow\\n\\n\\n@dataclass\\nclass TrainConfig:\\n    pretrained_model: str = \\"bert-base-uncased\\"\\n    num_classes: int = 2\\n    lr: float = 2e-4\\n    max_length: int = 128\\n    batch_size: int = 256\\n    num_workers: int = os.cpu_count()\\n    max_epochs: int = 10\\n    debug_mode_sample: int | None = None\\n    max_time: dict[str, float] = field(default_factory=lambda: {\\"hours\\": 3})\\n    model_checkpoint_dir: str = \\"/local_disk0/tmp/model-checkpoints\\"\\n    min_delta: float = 0.005\\n    patience: int = 4\\n\\ntrain_config = TrainConfig()\\n\\n# Instantiate the custom Transformer class for PEFT training\\nnlp_model = TransformerModule(\\n        pretrained_model=train_config.pretrained_model,\\n        num_classes=train_config.num_classes,\\n        lr=train_config.lr,\\n    )\\n\\ndatamodule = LexGlueDataModule(\\n        pretrained_model=train_config.pretrained_model,\\n        max_length=train_config.max_length,\\n        batch_size=train_config.batch_size,\\n        num_workers=train_config.num_workers,\\n        debug_mode_sample=train_config.debug_mode_sample,\\n    )\\n\\n# Log system metrics while training loop is running\\nmlflow.enable_system_metrics_logging()\\n\\n# Automatically log per-epoch parameters, metrics, and checkpoint weights\\nmlflow.pytorch.autolog(checkpoint_save_best_only = False)\\n\\n# Define the Trainer configuration\\ntrainer = Trainer(\\n   callbacks=[\\n       EarlyStopping(\\n           monitor=\\"Val_F1_Score\\",\\n           min_delta=train_config.min_delta,\\n           patience=train_config.patience,\\n           verbose=True,\\n           mode=\\"max\\",\\n       )\\n   ],\\n   default_root_dir=train_config.model_checkpoint_dir,\\n   fast_dev_run=bool(train_config.debug_mode_sample),\\n   max_epochs=train_config.max_epochs,\\n   max_time=train_config.max_time,\\n   precision=\\"32-true\\"\\n)\\n\\n# Execute the training run\\ntrainer.fit(model=nlp_model, datamodule=datamodule)\\n```\\n\\n## Visualization and Sharing Capabilities within MLflow\\n\\nThe newly introduced DL-specific visualization capabilities introduced in MLflow 2.12 enable you to make comparisons between different runs and artifacts over epochs.\\nWhen comparing training runs, MLflow is capable of generating useful visualization that can be integrated into dashboards, facilitating\\neasy sharing. Additionally, the centralized storage of metrics, in conjunction with parameters, allows for effective analysis of the training\\nefficacy, as shown in the image below.\\n\\n![Epoch Run Compare](compare.png)\\n\\n## When to stop training?\\n\\nWhen training DL models, it is important to understand when to stop. Efficient training (for minimizing the overall cost incurred for\\nconducting training) and optimal model performance rely heavily on preventing a model from overfitting on the training data. A model\\nthat trains for too long will invariably become quite good at effectively \u2018memorizing\u2019 the training data, resulting in a reduction in\\nthe performance of the model when presented with novel data. A straightforward way to evaluate this behavior is to ensure that\\nvalidation data set metrics (scoring loss metrics on data that is not in the training data set) are captured during the training\\nloop. Integrating the MLflow callback into the PyTorch Lightning Trainer allows for iterative logging of loss metrics at\\nconfigurable iterations, enabling an easily debuggable evaluation of the training performance, ensuring that stopping criteria\\ncan be enforced at the appropriate time to prevent overfitting.\\n\\n### Evaluating epoch checkpoints of Fine Tuned Models with MLflow\\n\\nWith your training process meticulously tracked and logged by MLflow, you have the flexibility to retrieve and test your model at\\nany arbitrary checkpoint. To do this, you can use the mlflow.pytorch.load_model() API to load the model from a specific run\\nand use the `predict()` method for evaluation.\\n\\nIn the example below, we will load the model checkpoint from the 3rd epoch and use the `Lightning` train module to generate predictions based on the\\ncheckpoint state of the saved training epoch.\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.pytorch.autolog(disable = True)\\n\\nrun_id = \'<Add the run ID>\'\\n\\nmodel = mlflow.pytorch.load_checkpoint(TransformerModule, run_id, 3)\\n\\nexamples_to_test = [\\"We reserve the right to modify the service price at any time and retroactively apply the adjusted price to historical service usage.\\"]\\n\\ntrain_module = Trainer()\\ntokenizer = AutoTokenizer.from_pretrained(train_config.pretrained_model)\\ntokens = tokenizer(examples_to_test,\\n                  max_length=train_config.max_length,\\n                  padding=\\"max_length\\",\\n                  truncation=True)\\nds = Dataset.from_dict(dict(tokens))\\nds.set_format(\\n            type=\\"torch\\", columns=[\\"input_ids\\", \\"attention_mask\\"]\\n        )\\n\\ntrain_module.predict(model, dataloaders = DataLoader(ds))\\n```\\n\\n## Summary\\n\\nThe integration of MLflow into the finetuning process of pre-trained language models, particularly for applications like custom\\nnamed entity recognition, text classification and instruction-following represents a significant advancement in managing and\\noptimizing deep learning workflows. Leveraging the autologging and tracking capabilities of MLflow in these workstreams not only\\nenhances the reproducibility and efficiency of model development, but also fosters a collaborative environment where insights\\nand improvements can be easily shared and implemented.\\n\\nAs we continue to push the boundaries of what these models can achieve, tools like MLflow will be instrumental in harnessing their full potential.\\n\\nIf you\'re interested in seeing the full example in its entirety, feel free to [see the full example implementation](https://github.com/puneet-jain159/deeplearning_with_mlfow)\\n\\n### Check out the code\\n\\nThe code we provide will delve into additional aspects such as training from a checkpoint, integrating MLflow and TensorBoard, and utilizing Pyfunc for model wrapping, among others. These resources are specifically tailored for implementation on [Databricks Community Edition](https://mlflow.org/blog/databricks-ce). The main runner notebook\\nwithin the full example repository [can be found here](https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/train.ipynb).\\n\\n## Get Started with MLflow 2.12 Today\\n\\nDive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\\nMLflow is here to elevate your ML workflow to new heights. [Get started now with MLflow\'s cutting-edge tools and features](https://mlflow.org/releases/2.12.1).\\n\\n## Feedback\\n\\nWe value your input! Our feature prioritization is guided by feedback from the MLflow late 2023 survey. Please fill out our\\n[Spring 2024 survey](https://surveys.training.databricks.com/jfe/form/SV_3jGIliwGC0g5xTU), and by participating, you can help ensure that the features\\nyou want most are implemented in MLflow."},{"id":"release-candidates","metadata":{"permalink":"/mlflow-website/blog/release-candidates","source":"@site/blog/2024-04-17-release-candidates.md","title":"MLflow Release Candidates","description":"We are excited to announce the implementation of a release candidate process for MLflow!","date":"2024-04-17T00:00:00.000Z","tags":[{"inline":true,"label":"mlflow","permalink":"/mlflow-website/blog/tags/mlflow"}],"readingTime":2.82,"hasTruncateMarker":false,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"MLflow Release Candidates","tags":["mlflow"],"slug":"release-candidates","authors":["mlflow-maintainers"],"thumbnail":"/img/blog/release-candidates.png"},"unlisted":false,"prevItem":{"title":"Deep Learning with MLflow (Part 2)","permalink":"/mlflow-website/blog/deep-learning-part-2"},"nextItem":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","permalink":"/mlflow-website/blog/deep-learning-part-1"}},"content":"We are excited to announce the implementation of a release candidate process for MLflow!\\nThe pace of feature development in MLflow is faster now than ever before and the core maintainer team has even more exciting things planned in the near future! However, with an increased velocity on major feature development comes with a risk of breaking things. As the maintainers of such a widely used project, we are cognizant of the disruptive nature of regressions and we strive to avoid them as much as we can. Aside from new feature development work, our primary goal is in ensuring the stability of production systems. While we do have the aspirational goal of moving fast(er), we certainly don\'t want to move fast and break things. With that goal in mind, we\'ve decided to introduce a release candidate (RC) process. The RC process allows us to introduce new features and fixes in a controlled environment before they become part of the official release.\\n\\n## How It Works\\n\\nStarting from MLflow 2.13.0, new MLflow major and minor releases will be tagged as release candidates (e.g., `2.13.0rc0`) in PyPI two weeks before they are officially released.\\n\\nThe release candidate process involves several key stages:\\n\\n- Feature Development Freeze: Prior to cutting the RC branch and announcing its availability, we will freeze the RC branch from feature commits. Once the branch is cut, only bug fix and stability PRs will be permitted to be merged, ensuring that unexpected, late-arriving, potentially regression-causing merges are not permitted to destabilize the forthcoming release.\\n- Pre-Release Announcement: We will announce upcoming features and improvements, providing our community with a roadmap of what to expect.\\n- Release Candidate Rollout: A release candidate version will be made available for testing, accompanied by detailed release notes outlining the changes.\\n- Community Testing and Feedback: We encourage our users to test the release candidate in their environments and share their feedback with us by filing issue reports on the MLflow Github repository. This feedback is invaluable for identifying issues and ensuring the final release aligns with user needs (i.e., we didn\'t break your workflows).\\n- Final Release: After incorporating feedback and making necessary adjustments, we will proceed with the final release. This version will include all updates tested in the RC phase, offering a polished and stable experience for all users.\\n\\nThis approach provides several benefits:\\n\\n- Enhanced Stability: By rigorously testing release candidates, we can identify and address potential issues early, reducing the likelihood of disruptions in production environments.\\n- Community Feedback: The RC phase offers you, a member of the MLflow community, the opportunity to provide feedback on upcoming changes. This collaborative approach ensures that the final release aligns with the needs and expectations of our users.\\n- Gradual Adoption: Users can choose to experiment with new features in a release candidate without committing to a full upgrade. This flexibility supports cautious integration and thorough evaluation in various environments.\\n\\n## Get Involved\\n\\nYour participation is crucial to the success of this process. We invite you to join us in testing upcoming release candidates and sharing your insights. Together, we can ensure that MLflow continues to serve as a reliable foundation for your machine learning projects.\\nStay tuned for announcements regarding our first release candidate. We look forward to your contributions and feedback as we take this important step toward a more stable and dependable MLflow."},{"id":"deep-learning-part-1","metadata":{"permalink":"/mlflow-website/blog/deep-learning-part-1","source":"@site/blog/2024-03-05-deep-learning-part-1/index.md","title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","description":"Highlighting the recent improvements in MLflow for Deep Learning workflows","date":"2024-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Deep Learning","permalink":"/mlflow-website/blog/tags/deep-learning"}],"readingTime":5.175,"hasTruncateMarker":true,"authors":[{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/mlflow-website/img/authors/abe.png","key":"abe-omorogbe","page":null},{"name":"Hubert Zub","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/hubert-zub/","imageURL":"/mlflow-website/img/authors/hubert.png","key":"hubert-zub","page":null},{"name":"Yun Park","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/yunpark93/","imageURL":"/mlflow-website/img/authors/yun.png","key":"yun-park","page":null},{"name":"Chen Qian","title":"Software Engineer at Databricks","url":"https://www.linkedin.com/in/thomas-chen-qian/","imageURL":"/mlflow-website/img/authors/chen.png","key":"chen-qian","page":null},{"name":"Jesse Chan","title":"Software Engineer at Databricks","key":"jesse-chan","page":null}],"frontMatter":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","description":"Highlighting the recent improvements in MLflow for Deep Learning workflows","slug":"deep-learning-part-1","authors":["abe-omorogbe","hubert-zub","yun-park","chen-qian","jesse-chan"],"tags":["Deep Learning"],"thumbnail":"/img/blog/dl-chart-grouping.gif"},"unlisted":false,"prevItem":{"title":"MLflow Release Candidates","permalink":"/mlflow-website/blog/release-candidates"},"nextItem":{"title":"2023 Year in Review","permalink":"/mlflow-website/blog/mlflow-year-in-review"}},"content":"In the quickly evolving world of artificial intelligence, where generative AI has taken center stage, the landscape of machine learning is\\nevolving at an unprecedented pace. There has been a surge in the use of cutting-edge deep learning (DL) libraries like\\n[Transformers](https://huggingface.co/docs/transformers/index), [Tensorflow](https://www.tensorflow.org/),\\nand [PyTorch](https://pytorch.org/) to fine-tune these generative AI models for enhanced performance.\\nAs this trend accelerates, it\'s become clear that the tools used to build these models must rapidly evolve as well, particularly when it comes\\nto managing and optimizing these deep learning workloads. MLflow offers a practical solution for managing the complexities of these machine learning projects.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn collaboration with [MosaicML](https://www.mosaicml.com/) and the broader ML community, MLflow is thrilled to unveil a set of eagerly awaited enhancements.\\nThis latest release ([MLflow 2.11](https://www.mlflow.org/releases/2.11.0)) introduces updated tracking UI capabilities in direct response to\\n[the feedback](https://www.linkedin.com/posts/mlflow-org_qualtrics-survey-qualtrics-experience-management-activity-7128154257924513793-RCDG?utm_source=share&utm_medium=member_desktop)\\nand needs of MLflow enthusiasts. These updates are not just incremental; they represent a leap forward in addressing the needs of MLflow users doing Deep Learning.\\n\\nThe evolution of enhanced Deep Learning capabilities is a testament to MLflow\'s commitment to serving the open-source community, ensuring that its offerings\\nare not just keeping pace, but setting the pace in the rapidly evolving domain of machine learning.\\n\\n## Deep Learning API Improvements\\n\\nLeveraging valuable insights from our user community, we\'ve implemented critical enhancements to the effective scale of metrics logging and the inclusion of\\nsystem-related metric logging within our platform. These improvements encompass expanded scalability options, support for logging more iterations and the\\nlogging of system metrics.\\n\\n### System Metrics\\n\\nThis feature allows you to [monitor system metrics](https://mlflow.org/docs/latest/system-metrics/index.html?highlight=system) and identify any hardware issues that might be impacting performance.\\nMetrics such as CPU utilization, Memory usage, disk usage etc., from all nodes in your cluster can now be logged and visualized within the MLflow UI.\\n\\n![System Metrics](system-metrics.png)\\n\\n### Improved Logging Performance\\n\\nWe recently introduced both asynchronous and batch logging, making it easier to log both\\n[parallel and distributed](https://mlflow.org/docs/latest/tracking/tracking-api.html#parallel-runs) DL training sessions. Additionally, the MLflow Client\\nnow supports up to **1 million** steps (iterations) when logging metrics, allowing users to log more steps during long-running DL jobs.\\n\\n![Parallel Runs](parallel-runs.png)\\n\\n### Checkpointing for Deep Learning\\n\\n[TensorFlow](https://mlflow.org/releases/2.11.0#autologging-for-tensorflow-and-pytorch-now-supports-checkpointing-of-model-weights:~:text=both%20PyTorch%20and-,TensorFlow,-for%20automatic%20model)\\nand [PyTorch](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) now support model weight checkpointing when\\nusing autologging.\\n\\n![DL Checkpointing](dl-checkpointing.png)\\n\\n## User Experience and Productivity Enhancements\\n\\nWe have introduced substantial improvements to user experience and feature organization within our platform. These enhancements include more\\nsophisticated user interfaces and an intuitive redesign of the run details page, the addition of chart groups and metric aggregation, all\\naimed at simplifying navigation and enhancing productivity especially for Deep Learning use cases.\\n\\n### Metric Aggregation\\n\\nWe\'ve enhanced the UI with metric aggregation, enabling you to aggregate metrics across multiple runs based on\\ndatasets, tags, or parameters. These improvements significantly improve the time it takes to understand training results when working\\nwith large DL models, enabling more nuanced and comprehensive analysis of overarching trends in model performance across multiple dimensions.\\n\\n![DL Metric Aggregation](dl-metric-aggregation.gif)\\n\\n### Chart Grouping Functionality\\n\\nYou can now easily categorize and organize your metrics, such as training, testing, and system metrics into\\nnamed groups within the MLflow UI. This organization allows for a comprehensive overview of all metrics, enabling quicker access and\\nbetter management, particularly when handling experiments with many metrics.\\n\\n![DL Chart Grouping](dl-chart-grouping.gif)\\n\\n### Slash (\\"/\\") Logging Syntax\\n\\nTo further streamline metric organization, we\'ve implemented a new logging syntax that uses slashes\\n(\\"/\\") to group metrics. For example, using mlflow.log_metric(\\"x/y/score\\", 100) helps in structuring and segregating different types\\nof data or metrics into hierarchical groups, making it easier to navigate and interpret the logs, especially when dealing with complex\\nmodels and experiments.\\n\\n```python\\n\\nmlflow.log_metric(\'SVR/val_MAPE\', mean_absolute_percentage_error(test_y, pred_y))\\n\\n```\\n\\n![DL Slash Logging](dl-slash-logging.png)\\n\\n### Chart Searching\\n\\nWe\'ve significantly enhanced the search functionality within our platform, enabling more robust and intuitive searching\\nacross charts, parameters, and metrics. This upgrade allows for quicker and more precise retrieval of specific data points, streamlining the\\nprocess of analyzing and comparing different aspects of your experiments.\\n\\n![DL Chart Searching](dl-chart-searching.gif)\\n\\n### Run Details Redesign\\n\\nWe reorganized the Run Details UI to a modular tabbed layout, added new drag and drop UI functionality so that you can\\ncan now render logged tables. This enhancement will make it easier to organize your runs and experiments.\\n\\n![DL Run Details Redesign](dl-run-details.gif)\\n\\n## Getting Started Updates\\n\\nFollowing extensive feedback from our user community, we\'ve introduced significant updates to enhance the\\n[getting started](https://www.mlflow.org/docs/latest/getting-started/index.html) documentation within MLflow. These updates include a\\n[comprehensive overhaul](https://www.mlflow.org/docs/latest/deep-learning/index.html) of our documentation for easier navigation and\\n[enriched guidance](https://www.mlflow.org/docs/latest/deep-learning/pytorch/quickstart/pytorch_quickstart.html), along with a streamlined\\n[login API](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login). These enhancements, reflecting our commitment to improving the\\nuser experience and workflow, aim to empower our users to achieve more with greater speed and ease.\\n\\n### New Tutorials and Docs\\n\\nWe\'ve overhauled our documentation to offer a more comprehensive, user-friendly experience with practical examples\\nto support both newcomers and experienced practitioners with the information they need to start a Deep Learning project.\\n\\n![Deep Learning Docs](dl-docs.png)\\n\\n### Seamless login with mlflow.login()\\n\\nWe\'ve streamlined our authentication processes.\\n[This method](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#method-2-use-free-hosted-tracking-server-databricks-community-edition)\\nprovides a simple way to connect MLflow to your tracking server without having to leave your development environment.\\n[Try it out today](https://mlflow.org/blog/databricks-ce)\\n\\n![Login Update](login-update.png)\\n\\n## Get Started Today\\n\\nDive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\\nMLflow is here to elevate your data science workflow to new heights.\\n[Get started now with MLflow\'s cutting-edge tools and features](https://mlflow.org/releases/2.11.0).\\n\\n```bash\\npip install mlflow==2.11\\n\\nmlflow ui --port 8080\\n```\\n\\n```python\\nimport mlflow\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n# Set our tracking server uri for logging\\nmlflow.set_tracking_uri(uri=\\"http://127.0.0.1:8080\\")\\n\\nmlflow.autolog()\\n\\ndb = load_diabetes()\\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\\n\\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\\n# MLflow triggers logging automatically upon model fitting\\nrf.fit(X_train, y_train)\\n```\\n\\n## Feedback\\n\\nWe value your input! Our [feature roadmap](https://github.com/orgs/mlflow/projects/4) prioritization is guided by feedback from the [MLflow late 2023 survey](https://www.linkedin.com/feed/update/urn:li:activity:7128154257924513793), [GitHub Issues](https://github.com/mlflow/mlflow) and [Slack](https://mlflow.org/slack). Look out for our next survey later this year, by participating you can help ensure that the features you want are implemented in MLflow. You can also create an [issue on GitHub](https://github.com/mlflow/mlflow) or join our [Slack](https://mlflow.org/slack)."},{"id":"mlflow-year-in-review","metadata":{"permalink":"/mlflow-website/blog/mlflow-year-in-review","source":"@site/blog/2024-01-26-mlflow-year-in-review/index.md","title":"2023 Year in Review","description":"MLflow year-end recap","date":"2024-01-26T00:00:00.000Z","tags":[{"inline":true,"label":"MLflow","permalink":"/mlflow-website/blog/tags/m-lflow"},{"inline":true,"label":"2023","permalink":"/mlflow-website/blog/tags/2023"},{"inline":true,"label":"Linux Foundation","permalink":"/mlflow-website/blog/tags/linux-foundation"}],"readingTime":6.625,"hasTruncateMarker":true,"authors":[{"name":"Carly Akerly","title":"OSS Marketing Consultant at The Linux Foundation","url":"https://www.linkedin.com/in/carlyakerly/","imageURL":"/mlflow-website/img/authors/carly.png","key":"carly-akerly","page":null}],"frontMatter":{"title":"2023 Year in Review","description":"MLflow year-end recap","slug":"mlflow-year-in-review","authors":["carly-akerly"],"tags":["MLflow","2023","Linux Foundation"],"thumbnail":"/img/blog/2023-year-in-review.png"},"unlisted":false,"prevItem":{"title":"Announcing MLflow Enhancements - Deep Learning with MLflow (Part 1)","permalink":"/mlflow-website/blog/deep-learning-part-1"},"nextItem":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","permalink":"/mlflow-website/blog/databricks-ce"}},"content":"With more than **16 million** monthly downloads, MLflow has established itself as a leading open-source MLOps platform worldwide.\\nThis achievement underscores the robustness of MLflow and the active community that consistently refines and improves it.\\n\\nThe past year marked a significant milestone for MLflow, particularly in Generative AI. Its integration and support for Large Language Models\\n(LLMs) stood out. This strategic decision has propelled MLflow to the forefront of the AI revolution, establishing itself as the premier GenAI\\nplatform that enables users to create more intelligent, efficient, and adaptable AI models and applications.\\n\\n\x3c!-- truncate --\x3e\\n\\n![16 Million Downloads!](download-graph.png)\\n\\n## 2023: A Year of GenAI and Innovation\\n\\nLast year was remarkable for MLflow, particularly in integrating LLMs and other generative AI tools. MLflow has evolved significantly by offering\\na unified platform and workflow for traditional ML, deep learning, and GenAI applications. This integration ensures unparalleled efficiency and\\ninnovation. MLflow\'s dedication to improving LLM support has revolutionized how users create and oversee AI workflows, establishing it as an\\nindispensable tool for building advanced machine learning applications.\\n\\n### Integrations with Leading AI Tools\\n\\nMLflow has successfully incorporated support for popular AI services/frameworks such as [Hugging Face](https://huggingface.co/),\\n[LangChain](https://www.langchain.com/), and [OpenAI](https://openai.com/), while offering a unified and framework-agnostic interface for\\npackaging, evaluating, and deploying them. These integrations have opened new horizons for MLflow users, allowing them to leverage the capabilities\\nof these advanced AI tools seamlessly within their MLflow workflows.\\n\\n![GenAI Integrations](integrations.png)\\n\\n#### Model Packaging for LLMs\\n\\nRecognizing the surge in LLM popularity and utility, MLflow has focused on enhancing packaging support for these models. With MLflow\u2019s new built-in\\nmodel flavors for [Hugging Face](https://www.mlflow.org/docs/latest/llms/transformers/index.html), [LangChain](https://www.mlflow.org/docs/latest/llms/langchain/index.html)\\n, and [OpenAI](https://www.mlflow.org/docs/latest/llms/openai/index.html), users can log and deploy their LLMs and generative AI applications within minutes.\\n\\n#### Retrieval Augmented Generation (RAG) and MLflow Integration\\n\\n[Retrieval Augmented Generation (RAG)](https://mlflow.org/docs/latest/llms/rag/index.html) represents an impactful method in natural language processing.\\nIt combines pre-trained models with retrieval mechanisms to access a dataset of documents that fetch validated and curated content as opposed to relying\\non pure generation. This approach significantly improves generated responses\' contextual relevance and factual accuracy. With\\n[mlflow.evaluate()](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html), users can compare RAG systems across prompts, models, vector\\ndatabases, and more. See further details in the blog post:\\n[\\"Evaluating Retrieval Augmented Generation (RAG) Systems with MLflow\\"](https://medium.com/@dliden/evaluating-retrieval-augmented-generation-rag-systems-with-mlflow-cf09a74faadb).\\n\\n![RAG with MLflow](rag.webp)\\n\\n#### MLflow Deployment Server in MLflow 2.9.0\\n\\nThe [MLflow Deployment Server](https://www.mlflow.org/docs/latest/llms/deployments/index.html) simplifies LLM usage and management from various providers\\nlike OpenAI, MosaicML, Anthropic, Hugging Face, Cohere, MLflow models, and more. Besides supporting popular SaaS LLM providers, the MLflow Deployment Server\\nintegrates with MLflow model serving, enabling users to serve their own LLM or fine-tuned foundation models within their serving infrastructure.\\nThe MLflow Deployment Server also provides a unified inference API across different providers and services, making it much easier to query and compose\\nthem together. It uses securely stored keys from a centralized location, so users no longer need to share sensitive API keys with each member of their\\norganization. This simplifies how we interact with language models, adding an extra layer of security for managing API keys.\\n\\n#### Enhanced MLflow Evaluate API in MLflow 2.8.0\\n\\nThe [MLflow Evaluate API](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) underwent significant feature enhancements to support LLM\\nworkflows better and incorporate multiple new evaluation modes, including support for\\n[LLM-as-a-judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge). This upgraded API enables a more refined\\nand thorough analysis of LLM performance.\\n\\n#### Prompt Engineering UI in MLflow 2.7.0\\n\\nMLflow introduced the [Prompt Engineering UI](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html), a tool specifically designed for efficient prompt\\ndevelopment, testing, and assessment in Large Language Models (LLMs). This user-friendly interface and comprehensive toolkit have notably improved the\\naccessibility and efficiency of prompt engineering within LLM workflows.\\n\\n![Prompt Engineering UI](prompt-engineering.png)\\n\\n## Community Growth and Engagement\\n\\nThe introduction of the MLflow blog in 2023 was a new addition to the MLflow website. This fresh section signifies a crucial stride toward boosting\\ncommunity involvement and fostering knowledge exchange within the MLflow ecosystem. The blog serves as a direct avenue for sharing updates about new\\nfeatures, improvements, and the future trajectory of the MLflow project.\\n\\nMLflow surpassed 45,000 followers in 2023! Not only this, across [X](https://twitter.com/MLflow?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\\nand [LinkedIn](https://www.linkedin.com/company/mlflow-org/), MLflow had over 1 million impressions, the number of times our\\ncontent was displayed to users. When it came to MLflow contributor growth, the MLflow contributor count grew from 530 to 690 in 2023.\\n\\n### MLflow Docs Overhaul\\n\\nWe have undertaken a massive initiative to reimagine how our users interact with our content. The primary goal is to enhance clarity, improve navigation,\\nand provide more in-depth resources for our community, in addition to refreshing the look and feel. The overhaul of the MLflow documentation is a significant\\nmilestone, but it\u2019s just the beginning. We have a roadmap full of exciting updates, new content, and features. Whether it\u2019s writing tutorials, sharing use cases,\\nor providing feedback, every contribution enriches the MLflow community.\\n\\n![Docs Overhaul](docs-overhaul.png)\\n\\n### 2023 Events\\n\\nMLflow made a substantial impact at two significant events: **NeurIPS 2023** and the **Data+AI Summit 2023**. These events underscored MLflow\'s commitment\\nto contributing to the evolving discourse in machine learning and AI, emphasizing its pivotal role in shaping the future of these dynamic fields.\\nThe Data+AI Summit occurred in June 2023 and featured various MLflow-related sessions. Notably, two sessions stood out:\\n\\n- [Advancements in Open Source LLM Tooling, Including MLflow](https://www.youtube.com/watch?v=WpudXKAZQNI): Explored MLflow\'s seamless integration\\n  with leading generative AI tools like Hugging Face, LangChain, and OpenAI. It highlighted how these integrations enable effortless construction of AI workflows.\\n- [How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse](https://www.youtube.com/watch?v=MYqXfMqEUq4): Offered a\\n  comprehensive insight into how the Texas Rangers baseball team leveraged MLflow and Databricks to revolutionize their approach to data analytics.\\n\\n![Big Data Baseball](baseball.png)\\n\\nIn December 2023, MLflow participated in the 37th Annual Conference of Neural Information Processing Systems (NeurIPS) held in New Orleans, LA. NeurIPS\\nstands as one of the most prestigious conferences in machine learning and computational neuroscience.\\nFor those seeking guidance on fine-tuning a Large Language Model for general-purpose instruction following, the session\\n[\\"LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms\\"](https://arxiv.org/abs/2311.13133) at NeurIPS presented valuable insights.\\n\\n### Stay Plugged In\\n\\nIf you are interested in joining the MLflow community, we\u2019d love to connect! Join us on\\n[Slack](https://mlflow-users.slack.com/ssb/redirect), [Google Groups](https://groups.google.com/g/mlflow-users), and [GitHub](https://github.com/mlflow/mlflow/).\\nWe have a roadmap full of exciting updates, new content, and features. Whether it\u2019s writing tutorials, developing code, sharing use-cases, or providing feedback, let\u2019s work together!\\nAre you already an MLflow contributor? The newly launched MLflow Ambassador Program is a great way to boost your involvement. As an MLflow Ambassador,\\nyou will serve as one of our esteemed global ambassadors, pivotal in propelling the adoption and amplifying awareness of MLflow within the global data\\ncommunity. We invite you to submit an application [here](https://forms.gle/adAPNvH6aVq4diPF9).\\n\\n![Ambassador Program](ambassador-program.png)\\n\\n### Looking Forward\\n\\n\u201cIn 2024, we\'re launching new initiatives to engage, support, and expand our community. MLflow is thrilled to broaden its horizons this year through strategic\\ncollaboration and partnership\u201d, says Ben Wilson, Software Engineer at Databricks. \u201cThis collaboration will unlock fresh opportunities for our users and\\nsignificantly contribute to MLflow\'s evolution. Stay tuned for an announcement about this exciting effort.\u201d\\n\\nThe year 2023 marked a transformative period for MLflow. By embracing the latest ML and GenAI advancements, MLflow improved its platform and made substantial\\ncontributions to the wider AI and machine learning community. To our MLflow community, we extend our deepest gratitude.\\n\\nYou have been instrumental in driving MLflow\'s success over the past year. Whether it\'s enhancing existing features, exploring new integrations, or sharing\\nyour expertise, your contributions are the lifeblood of the MLflow community. If you\'re interested in contributing to MLflow,\\n[this guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md) is an excellent starting point. Looking ahead, we\'re excited about the myriad\\npossibilities and new frontiers we can explore together.\\n\\nMLflow is poised to continue its path of growth and innovation, cementing its role as a leader in managing machine learning and GenAI workflows across the\\nentire lifecycle. We\'re eager to keep pushing the boundaries of what\'s achievable in AI and strive to create an innovative, inclusive, and open future."},{"id":"databricks-ce","metadata":{"permalink":"/mlflow-website/blog/databricks-ce","source":"@site/blog/2024-01-25-databricks-ce/index.md","title":"Streamline your MLflow Projects with Free Hosted MLflow","description":"A guide to using Databricks Community Edition with integrated managed MLflow","date":"2024-01-25T00:00:00.000Z","tags":[{"inline":true,"label":"managed mlflow","permalink":"/mlflow-website/blog/tags/managed-mlflow"},{"inline":true,"label":"getting started","permalink":"/mlflow-website/blog/tags/getting-started"}],"readingTime":5.27,"hasTruncateMarker":true,"authors":[{"name":"Abe Omorogbe","title":"Product Manager, ML at Databricks","url":"https://www.linkedin.com/in/abeomor/","imageURL":"/mlflow-website/img/authors/abe.png","key":"abe-omorogbe","page":null}],"frontMatter":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","description":"A guide to using Databricks Community Edition with integrated managed MLflow","slug":"databricks-ce","thumbnail":"/img/blog/databricks-ce.png","authors":["abe-omorogbe"],"tags":["managed mlflow","getting started"]},"unlisted":false,"prevItem":{"title":"2023 Year in Review","permalink":"/mlflow-website/blog/mlflow-year-in-review"},"nextItem":{"title":"Custom MLflow Models with mlflow.pyfunc","permalink":"/mlflow-website/blog/custom-pyfunc"}},"content":"If you\'re new to MLflow and want to get started with a fully-managed and completely free deployment of MLflow, this blog will show you how to get started using MLflow in minutes.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Streamline Your ML Projects: Get Started with Hosted MLflow for Free\\n\\nExplore the world of big data and machine learning with [Databricks Community Edition (CE)](https://community.cloud.databricks.com/), a free, limited[^1] version of the Databricks platform.\\nIdeal for beginners and those new to Databricks and MLflow, this edition streamlines the learning curve by offering a managed environment. It eliminates the complexity of manually\\nsetting up a tracking server. Databricks CE includes hosted MLflow, enabling efficient management and visualization of your MLflow experiments. This makes it a prime choice for\\ndeveloping machine learning projects in a user-friendly interface, allowing you to connect from your favorite IDE, notebook environment, or even from within Databricks CE\'s notebooks.\\n\\n[^1]: The Model Registry and Model Deployment features are not available in the Databricks Community Edition.\\n\\n### Benefits of Using Databricks CE for MLflow\\n\\nMLflow is an open-source framework compatible with any platform, yet it offers distinct benefits when used on Databricks (including the Community Edition, CE) compared to other platforms. These advantages include:\\n\\n1. **Cost-Effective**: Free of charge, MLflow on Databricks CE is perfect for educational purposes and small-scale projects.\\n\\n2. **Simple Setup**: Gain access to a fully managed tracking server and user interface from any location. To connect to Databricks CE, just execute `mlflow.login()`.\\n\\n3. **Easy Sharing**: In the Databricks ecosystem, sharing your notebooks is straightforward and hassle-free.\\n\\n4. **Seamless Integration**: Databricks CE allows for direct storage and visualization of MLflow experiments, runs, and models.\\n\\n5. **Scalability**: MLflow on Databricks CE provides an easy path to scale your projects. It also integrates seamlessly with a wide range of data tools available on the Databricks platform.\\n\\n### Scenario\\n\\nIn this blog, we will walk through running ML experiments on your local device and tracking them on an [MLflow tracking server hosted on Databricks CE](https://mlflow.org/docs/latest/tracking.html#common-setups)\\n\\nTo give you an idea of the options available for running MLflow, the figure below shows what is possible for common setup configurations.\\n\\n![Remote Tracking Server](remote-tracking-server.png)\\n\\nFor this blog, we\'re showing #3, using a remote (fully managed) tracking server.\\n\\n### Step-by-Step Guide\\n\\n#### 1. Creating a Databricks CE Account\\n\\nIf you haven\'t already, you can [sign up for a free account](https://www.databricks.com/try-databricks#account). The process is quick, typically taking no more than 3 minutes.\\n\\nFill out the signup form and select \u201cGet started with Community Edition.\u201d\\n\\n![Databricks CE Signup Page](ce-signup.png)\\n\\nOnce signed up, you\'ll get information on how to set a password that you can use to login to CE with[^2].\\n\\n[^2]: Databricks CE only supports basic authorization signin (username / password). For more advanced and secure authorization setups, only the full Databricks product supports those.\\n\\n#### 2. Installing Dependencies\\n\\nBefore you start, ensure that you have the necessary packages installed. Run the following command in your favorite IDE or notebook from your device:\\n\\n```bash\\n%pip install -q mlflow databricks-sdk\\n```\\n\\n#### 3. Setting Up Databricks CE Authentication\\n\\nThe main advantage of Databricks Community Edition (CE) is its convenience: it offers an MLflow tracking server without requiring\\n[local infrastructure setup](https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server.html). You can easily access this server through the\\n[mlflow.login()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login) function after creating your CE account, streamlining the process for MLflow experiment tracking.\\n\\nTo authenticate with Databricks CE, use the [mlflow.login()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.login) function. This will prompt you for:\\n\\n- **Databricks Host**: `https://community.cloud.databricks.com/`\\n\\n- **Username**: Your Databricks CE email address.\\n\\n- **Password**: Your Databricks CE password.\\n\\nUpon successful authentication, you will see a confirmation message.\\n\\n```python\\nimport mlflow\\n\\nmlflow.login()\\n\\n# Follow the prompts for authentication\\n```\\n\\n#### 4. Connect to Hosted MLflow and Track Experiments with Databricks CE\\n\\nAfter you login from your local machine, start an experiment with [mlflow.set_experiment()](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=mlflow%20set_experiment#mlflow.set_experiment) and log some metrics. For instance:\\n\\n```python\\nmlflow.set_experiment(\\"/Users/\\\\<email>/check-databricks-ce-connection\\")\\n\\nwith mlflow.start_run():\\n\\n\xa0\xa0\xa0\xa0mlflow.log_metric(\\"foo\\", 1)\\n\\n\xa0\xa0\xa0\xa0mlflow.log_metric(\\"bar\\", 2)\\n```\\n\\n> **Note**: The Databricks environment requires you to set experiments with the directory (from root)\\n\\n    `/Users/{your email address for your account}/{name of your experiment}`, which is different from the behavior in self-hosted MLflow (and when running MLFlow locally).\\n\\n#### 5. Viewing Your Experiment in Databricks CE\\n\\nNow let\u2019s navigate to Databricks CE to view the experiment result. Log in to your [Databricks CE](https://community.cloud.databricks.com/)\\naccount, and click on the top left to select machine learning in the drop down list. Finally, click on the experiment icon. See the screenshots below:\\n\\nNavigate to the Machine Learning Section\\n\\n![Navigate to ML Section of Databricks CE](navigate-to-experiments.png)\\n\\nNavigate to the MLflow UI\\n\\n![Navigate to the MLflow UI on Databricks CE](navigate-to-mlflow-ui.png)\\n\\nIn the \u201cExperiments\u201d view, you should be able to find the experiment `/Users/{your email}/check-databricks-ce-connection`, similar to:\\n\\n![Experiment view of Databricks MLflow server](view-experiment.png)\\n\\nClicking on the run name, which in this example is \'youthful-lamb-287\' (note that you will see a different, randomly generated name in your CE console),\\nwill take you to the run view that looks similar to the following:\\n\\n![Run view of Databricks MLflow server](view-run.png)\\n\\nIn the run view, you will see our dummy metrics `\u201cfoo\u201d` and `\u201cbar\u201d` have been logged successfully.\\n\\n#### 6. Run any MLflow tutorial in Databricks CE\\n\\nIf you want to try a tutorial from the MLflow website, you can use Databricks CE to quickly test (and modify, if you\'re inclined) the tutorial. For example, if you wanted to test\\nthe [Creating Custom Pyfunc tutorial](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.html):\\n\\n1. Click Workspace and\xa0 select \u201cImport notebook\u201d\\n\\n![Import a Notebook](import-notebook.png)\\n\\n2. Use the `URL` option to import the notebook directly from the MLflow documentation website. For this example, to import, replace the last element of the url\\n   from `html` to `ipynb`. This can be done with any of the tutorial or guide notebooks that are hosted on the MLflow website.\\n\\n   .../notebooks/basic-pyfunc.~~html~~ &rarr; .../notebooks/basic-pyfunc.**ipynb**[^3]\\n\\n[^3]: Or you can [download the notebook](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb) and manually load it in the UI by selecting `File` instead of `URL`.\\n\\n![Select the Notebook for Importing](import-notebook-2.png)\\n\\n### Conclusion\\n\\nDatabricks Community Edition (CE) offers an accessible and collaborative platform for MLflow experiment tracking, presenting several advantages. Its setup process is effortless\\nand quick, providing a user-friendly experience. Additionally, it\'s free to use, making it an ideal choice for beginners, learners, and small-scale projects.\\n\\n### Getting started\\n\\nTry out the notebook on [Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2830662238121329/3266358972198675/8538262732615206/latest.html)\\n\\n### Further Reading\\n\\n- Learn more about [different methods to setup your tracking server](https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#minute-tracking-server-overv)\\n\\n- Learn more about running [Tutorial Notebooks ](https://mlflow.org/docs/latest/getting-started/running-notebooks/index.html)with Databricks CE"},{"id":"custom-pyfunc","metadata":{"permalink":"/mlflow-website/blog/custom-pyfunc","source":"@site/blog/2024-01-23-custom-pyfunc/index.md","title":"Custom MLflow Models with mlflow.pyfunc","description":"A guide for creating custom MLflow models","date":"2024-01-23T00:00:00.000Z","tags":[{"inline":true,"label":"pyfunc","permalink":"/mlflow-website/blog/tags/pyfunc"},{"inline":true,"label":"models","permalink":"/mlflow-website/blog/tags/models"}],"readingTime":15.29,"hasTruncateMarker":true,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/mlflow-website/img/authors/daniel_liden.png","key":"daniel-liden","page":null}],"frontMatter":{"title":"Custom MLflow Models with mlflow.pyfunc","description":"A guide for creating custom MLflow models","slug":"custom-pyfunc","authors":["daniel-liden"],"tags":["pyfunc","models"],"thumbnail":"/img/blog/custom-pyfunc.png"},"unlisted":false,"prevItem":{"title":"Streamline your MLflow Projects with Free Hosted MLflow","permalink":"/mlflow-website/blog/databricks-ce"},"nextItem":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","permalink":"/mlflow-website/blog/mlflow-autolog"}},"content":"If you\'re looking to learn about all of the flexibility and customization that is possible within\\nMLflow\'s custom models, this blog will help you on your journey in understanding more about how to\\nleverage this powerful and highly customizable model storage format.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Welcome](./header.png)\\n\\nMLflow offers built-in methods for logging and working with models from many popular machine\\nlearning and generative AI frameworks and model providers, such as scikit-learn, PyTorch,\\nHuggingFace transformers, and LangChain. For example,\\n[mlflow.sklearn.log_model](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)\\nwill log a scikit-learn model as an MLflow artifact without requiring you to define custom methods for\\nprediction or for handling artifacts.\\n\\nIn some cases, however, you might be working in a framework for which MLflow does not have\\nbuilt-in methods, or you might want something different than the model\u2019s default prediction\\noutputs. In those cases, MLflow allows you to create custom models to work with essentially\\nany framework and to integrate custom logic to existing supported frameworks.\\n\\nIn its simplest form, all that\u2019s required is to define a custom predict method and log the model.\\nThe following example defines a simple pyfunc model that just returns the square of its input:\\n\\n```python\\nimport mlflow\\n\\n# Define a custom model\\nclass MyModel(mlflow.pyfunc.PythonModel):\\n    def predict(self, context, model_input):\\n        # Directly return the square of the input\\n        return model_input**2\\n\\n\\n# Save the model\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"model\\",\\n        python_model=MyModel()\\n    )\\n\\n# Load the model\\nloaded_model = mlflow.pyfunc.load_model(\\n    model_uri=model_info.model_uri\\n)\\n\\n# Predict\\nloaded_model.predict(2)\\n```\\n\\nLet\u2019s dig into how this works, starting with some basic concepts.\\n\\n## Models and Model Flavors\\n\\n![Models and Flavors](models-and-flavors.png)\\n\\nAn MLflow model is a directory that includes everything needed to reproduce a machine learning model\\nacross different environments. Aside from the stored model itself, the most important component\\nstored is an `MLmodel` YAML file that specifies the model\u2019s supported model flavors.\\nA [model flavor](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part1-named-flavors.html#components-of-a-model-in-mlflow)\\nis a set of rules specifying how MLflow can interact with the model (i.e., save it, load it, and\\nget predictions from it).\\n\\nWhen you create a custom model in MLflow, it has the `python_function` or pyfunc model flavor,\\nwhich is a kind of \u201cuniversal translator\u201d across formats in MLflow. When you save a model in MLflow\\nusing a built-in model flavor, e.g. with [mlflow.sklearn.log_model](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model),\\nthat model also has the pyfunc model flavor in addition to its framework-specific flavor.\\nHaving both framework-specific and pyfunc model flavors allows you to use the model via the\\nframework\u2019s native API (e.g., `scikit-learn`) or via the pyfunc flavor\u2019s framework-agnostic inference API.\\n\\nModels with the pyfunc flavor are loaded as instances of the [mlflow.pyfunc.PyfuncModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.PyFuncModel)\\nclass, which exposes a standardized predict method. This enables straightforward inference through a single\\nfunction call, regardless of the underlying model\'s implementation details.\\n\\n## Defining Custom MLflow Pyfunc Models\\n\\nSaving a model from any supported machine learning framework as an MLflow model results in the\\ncreation of a pyfunc model flavor that provides a framework-agnostic interface for managing and\\nusing the model. But what if you\u2019re using a framework without an MLflow integration, or you\u2019re\\ntrying to elicit some custom behavior from a model? [Custom pyfunc models](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models)\\nallow you to work with essentially any framework and to integrate custom logic.\\n\\nTo implement a custom pyfunc model, define a new Python class inheriting from the PythonModel class\\nand implement the necessary methods. Minimally, this will involve implementing a custom predict\\nmethod. Next, create an instance of your model and log or save the model. Once you\u2019ve loaded the\\nsaved or logged model, you can use it for predictions.\\n\\n![Creating a custom model](custom-model-creation.png)\\n\\nLet\u2019s work through a few examples, each adding a little more complexity and highlighting different\\naspects of defining a custom pyfunc model. We\u2019ll cover four main techniques for implementing custom\\nbehaviors in pyfunc models:\\n\\n1. Implementing a custom `predict` method\\n2. Implementing a custom `__init__` method\\n3. Implementing a custom `load_context` method\\n4. Implementing user-defined custom methods\\n\\n![Pyfunc model customization](custom-pyfunc-types.png)\\n\\n### Defining a custom `predict` method\\n\\nAt a minimum, a pyfunc model should specify a custom predict method that defines what happens when\\nwe call `model.predict`. Here\u2019s an example of a custom model that applies a simple linear\\ntransformation to the model inputs, multiplying each input by two and adding three:\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass SimpleLinearModel(PythonModel):\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(2 * model_input + 3)\\n\\n\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"linear_model\\",\\n        python_model=SimpleLinearModel(),\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nNote that you can (and should) also include a [signature](https://mlflow.org/docs/latest/models.html#model-signature)\\nand an [input example](https://mlflow.org/docs/latest/models.html#model-input-example) when saving/logging a\\nmodel. If you pass an input example, the signature will be inferred automatically. The model\\nsignature provides a way for MLflow to enforce correct usage of your model.\\n\\nOnce we\u2019ve defined the model path and saved an instance of the model, we can load the saved model\\nand use it to generate predictions:\\n\\n```python\\n# Now the model can be loaded and used for predictions\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\nmodel_input = pd.DataFrame([1, 2, 3])  # Example input data\\nprint(loaded_model.predict(model_input))  # Outputs transformed data\\n```\\n\\nWhich will return:\\n\\n```text\\n:    0\\n: 0  5\\n: 1  7\\n: 2  9\\n```\\n\\nNote that if a custom `predict` method is all you need\u2014that is, if your model does not have any\\nartifacts that require special handling\u2014you can save or log the `predict` method directly without\\nneeding to wrap it in a Python class:\\n\\n```python\\nimport mlflow\\nimport pandas as pd\\n\\n\\ndef predict(model_input):\\n    \\"\\"\\"\\n    Applies a simple linear transformation\\n    to the input data. For example, y = 2x + 3.\\n    \\"\\"\\"\\n    # Assuming model_input is a pandas DataFrame with one column\\n    return pd.DataFrame(2 * model_input + 3)\\n\\n\\n# Pass predict method as python_model argument to save/log model\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"simple_function\\",\\n        python_model=predict,\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nNote that with this approach, we **must include** an input example along with the custom predict\\nmethod. We also have to modify the predict method such that it takes only one input (i.e., no self or context).\\nRunning this example and then loading with the same code as the preceding code block will retain the same output as\\nthe example using a class definiton.\\n\\n### Parameterizing the custom model\\n\\nNow suppose we want to parameterize the custom linear function model so that it can be used with\\ndifferent slopes and intercepts. We can define the `__init__` method to set up custom parameters,\\nas in the following example. Note that the custom model class\u2019s `__init__` method should not be used\\nto load external resources like data files or pretrained models; these are handled in the\\n`load_context` method, which we\u2019ll discuss shortly.\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass ParameterizedLinearModel(PythonModel):\\n    def __init__(self, slope, intercept):\\n        \\"\\"\\"\\n        Initialize the parameters of the model. Note that we are not loading\\n        any external resources here, just setting up the parameters.\\n        \\"\\"\\"\\n        self.slope = slope\\n        self.intercept = intercept\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(self.slope * model_input + self.intercept)\\n\\n\\nlinear_model = ParameterizedLinearModel(10, 20)\\n\\n# Saving the model with mlflow\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"parameter_model\\",\\n        python_model=linear_model,\\n        input_example=pd.DataFrame([10, 20, 30]),\\n    )\\n```\\n\\nAgain, we can load this model and make some predictions:\\n\\n```python\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\nmodel_input = pd.DataFrame([1, 2, 3])  # Example input data\\nprint(loaded_model.predict(model_input))  # Outputs transformed data\\n```\\n\\n```text\\n:     0\\n: 0  30\\n: 1  40\\n: 2  50\\n```\\n\\nThere are many cases where we might want to parameterize a model in this manner. We can define\\nvariables in the `__init__` method to:\\n\\n- Set model hyperparameters.\\n- A/B test models with different parameter sets.\\n- Set user-specific customizations.\\n- Toggle features.\\n- Set, e.g., access credentials and endpoints for models that access external APIs.\\n\\nIn some cases, we may want to be able to pass parameters at inference time rather than when we\\ninitialize the model. This can be accomplished with\\n[model inference params](https://mlflow.org/docs/latest/models.html#model-inference-params). To use\\ninference params, we must pass a valid model signature including `params`. Here\u2019s how to adapt the\\npreceding example to use inference params:\\n\\n```python\\nimport pandas as pd\\nimport mlflow\\nfrom mlflow.models import infer_signature\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\n# Custom PythonModel class\\nclass LinearFunctionInferenceParams(PythonModel):\\n    def predict(self, context, model_input, params):\\n        \\"\\"\\"\\n        Applies a simple linear transformation\\n        to the input data. For example, y = 2x + 3.\\n        \\"\\"\\"\\n        slope = params[\\"slope\\"]\\n        # Assuming model_input is a pandas DataFrame with one column\\n        return pd.DataFrame(slope * model_input + params[\\"intercept\\"])\\n\\n\\n# Set default params\\nparams = {\\"slope\\": 2, \\"intercept\\": 3}\\n\\n# Define model signature\\nsignature = infer_signature(model_input=pd.DataFrame([10, 20, 30]), params=params)\\n\\n# Save the model with mlflow\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"model_with_params\\",\\n        python_model=LinearFunctionInferenceParams(),\\n        signature=signature,\\n    )\\n```\\n\\nAfter loading the model as before, you can now pass a `params` argument to the `predict` method,\\nenabling you to use the same loaded model for different combinations of parameters:\\n\\n```python\\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\\n\\nparameterized_predictions = loaded_model.predict(\\n    pd.DataFrame([10, 20, 30]), params={\\"slope\\": 2, \\"intercept\\": 10}\\n)\\nprint(parameterized_predictions)\\n```\\n\\n```text\\n:     0\\n: 0  30\\n: 1  50\\n: 2  70\\n```\\n\\n### Loading external resources with `load_context`\\n\\nCustom models often require external files such as model weights in order to perform inference.\\nThese files, or artifacts, must be handled carefully to avoid unnecessarily loading files into\\nmemory or errors during model serialization. When building custom pyfunc models in MLflow, you can\\nuse the `load_context` method to handle model artifacts correctly.\\n\\nThe `load_context` method receives a `context` object containing artifacts the model can use during\\ninference. You can specify these artifacts using the `artifacts` argument when saving or logging\\nmodels, making them accessible to the `load_context` method via the `context.artifacts` dictionary.\\n\\nIn practice, the `load_context` method often initializes the model called by the `predict` method by\\nhandling the loading of model artifacts.\\n\\nThis raises an important question: why do we load artifacts and define the model in the `load_context`\\nmethod and not in `__init__` or directly in `predict`? Correct usage of `load_context` is essential\\nfor the maintainability, efficiency, scalability, and portability of MLflow pyfunc models. This is because:\\n\\n- The `load_context` method is executed once when the model is loaded via `mlflow.pyfunc.load_model`.\\n  This setup ensures that resource-intensive processes defined within this method, such as loading\\n  large model files, are not repeated unnecessarily. If artifact loading is done in the predict\\n  method, it will occur every single time a prediction is made. This is highly inefficient for\\n  resource-intensive models.\\n- Saving or logging an MLflow `pyfunc` model involves serializing the Python model class (the subclass\\n  of [mlflow.pyfunc.PythonModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel)\\n  you created) and its attributes. Complex ML models are not always compatible with the methods used\\n  to serialize the Python object, which can lead to errors if they are created as attributes of the Python object.\\n\\nAs an example, suppose we want to load a large language model (LLM) in the `gguf` model format\\n(a file format designed for storing models for inference) and run it with the\\n[ctransformers library](https://pypi.org/project/ctransformers). At the time of writing, there is\\nno built-in model flavor that lets us use `gguf` models for inference, so we\u2019ll create a custom\\npyfunc model that loads the required libraries and model files in the `load_context` method.\\nSpecifically, we\u2019re going to load a quantized version of the [AWQ version of the Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ).\\n\\nFirst, we\u2019ll download the model snapshot using the huggingface hub cli:\\n\\n```bash\\nhuggingface-cli download TheBloke/Mistral-7B-v0.1-GGUF \\\\\\n                mistral-7b-v0.1.Q4_K_M.gguf \\\\\\n                --local-dir /path/to/mistralfiles/ \\\\\\n                --local-dir-use-symlinks False\\n```\\n\\nAnd then we\u2019ll define our custom `pyfunc` model. Note the addition of the `load_context` method:\\n\\n```python\\nimport ctransformers\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\nclass CTransformersModel(PythonModel):\\n    def __init__(self, gpu_layers):\\n        \\"\\"\\"\\n        Initialize with GPU layer configuration.\\n        \\"\\"\\"\\n        self.gpu_layers = gpu_layers\\n        self.model = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Load the model from the specified artifacts directory.\\n        \\"\\"\\"\\n        model_file_path = context.artifacts[\\"model_file\\"]\\n\\n        # Load the model\\n        self.model = ctransformers.AutoModelForCausalLM.from_pretrained(\\n            model_path_or_repo_id=model_file_path,\\n            gpu_layers=self.gpu_layers,\\n        )\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Perform prediction using the loaded model.\\n        \\"\\"\\"\\n        if self.model is None:\\n            raise ValueError(\\n                \\"The model has not been loaded. \\"\\n                \\"Ensure that \'load_context\' is properly executed.\\"\\n            )\\n        return self.model(model_input)\\n```\\n\\nThere\u2019s a lot going on here, so let\u2019s break it down. Here are the key points:\\n\\n- As before, we use the `__init__` method to parameterize the model (in this case, to set the\\n  `gpu_layers` argument for the model).\\n- The purpose of the `load_context` method is to load the artifacts required for use in the\\n  `predict` method. In this case, we need to load the model and its weights.\\n- You\u2019ll notice that we reference `context.artifacts[\\"model_file\\"]`. This comes from the artifacts\\n  argument to `mlflow.pyfunc.save_model` or `mlflow.pyfunc.load_model`, as shown in the following\\n  code snippet. This is an important part of working with `pyfunc` models. The `predict` and\\n  `load_context` methods can access the artifacts defined in the artifacts argument to the\\n  `save_model` or `log_model` method via the `context.artifacts` object. `load_context` is executed\\n  when the model is loaded via `load_model`; as described earlier, this provides a way to ensure that\\n  the potentially time-consuming initialization of a model does not occur each time the model is used\\n  for prediction.\\n\\nNow we can initialize and save an instance of the model. Note the artifacts argument to the\\n`save_model` function:\\n\\n```python\\n# Create an instance of the model\\nmistral_model = CTransformersModel(gpu_layers=50)\\n\\n# Log the model using mlflow with the model file as an artifact\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.log_model(\\n        artifact_path=\\"mistral_model\\",\\n        python_model=mistral_model,\\n        artifacts={\\"model_file\\": model_file_path},\\n        pip_requirements=[\\n            \\"ctransformers==0.2.27\\",\\n        ],\\n    )\\n\\n# Load the saved model\\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\\n\\n# Make a prediction with the model\\nloaded_model.predict(\\"Question: What is the MLflow Pyfunc model flavor?\\")\\n```\\n\\nTo recap: correct use of the `load_context` method helps to ensure efficient handling of model\\nartifacts and prevents errors in serialization that could result from attempting to define artifacts\\nas model attributes.\\n\\n### Defining custom methods\\n\\nYou can define your own methods in the custom `pyfunc` model to handle tasks like preprocessing\\ninputs or post-processing outputs. These custom methods can then be called by the predict method.\\nKeep in mind that these custom methods, just like `__init__` and `predict`, should **not be used** for\\nloading artifacts. Loading artifacts is the exclusive role of the `load_context` method.\\n\\nFor example, we can modify the `CTransformersModel` to incorporate some prompt formatting as follows:\\n\\n```python\\nimport ctransformers\\nfrom mlflow.pyfunc import PythonModel\\n\\n\\nclass CTransformersModel(PythonModel):\\n    def __init__(self, gpu_layers):\\n        \\"\\"\\"\\n        Initialize with GPU layer configuration.\\n        \\"\\"\\"\\n        self.gpu_layers = gpu_layers\\n        self.model = None\\n\\n    def load_context(self, context):\\n        \\"\\"\\"\\n        Load the model from the specified artifacts directory.\\n        \\"\\"\\"\\n        model_file_path = context.artifacts[\\"model_file\\"]\\n\\n        # Load the model\\n        self.model = ctransformers.AutoModelForCausalLM.from_pretrained(\\n            model_path_or_repo_id=model_file_path,\\n            gpu_layers=self.gpu_layers,\\n        )\\n\\n    @staticmethod\\n    def _format_prompt(prompt):\\n        \\"\\"\\"\\n        Formats the user\'s prompt\\n        \\"\\"\\"\\n        formatted_prompt = (\\n            \\"Question: What is an MLflow Model?\\\\n\\\\n\\"\\n            \\"Answer: An MLflow Model is a directory that includes \\"\\n            \\"everything that is needed to reproduce a machine \\"\\n            \\"learning model across different environments. \\"\\n            \\"It is essentially a container holding the trained model \\"\\n            \\"files, dependencies, environment details, input examples, \\"\\n            \\"and additional metadata. The directory also includes an \\"\\n            \\"MLmodel YAML file, which describes the different \\"\\n            f\\"flavors of the model.\\\\n\\\\nQuestion: {prompt}\\\\nAnswer: \\"\\n        )\\n\\n        return formatted_prompt\\n\\n    def predict(self, context, model_input):\\n        \\"\\"\\"\\n        Perform prediction using the loaded model.\\n        \\"\\"\\"\\n        if self.model is None:\\n            raise ValueError(\\n                \\"Model was not loaded. Ensure that \'load_context\' \\"\\n                \\"is properly executed.\\"\\n            )\\n        return self.model(self._format_prompt(model_input))\\n```\\n\\nNow the `predict` method can access the private `_format_prompt` static method to apply custom formatting to the prompts.\\n\\n### Dependencies and Source Code\\n\\nThe custom `CTransformersModel` defined above uses the `ctransformers` library. There are a few\\ndifferent approaches for making sure this library (and any other source code, including from your\\nlocal device) is correctly loaded with your model. Correctly specifying dependencies is essential\\nfor ensuring that custom models work as expected across environments.\\n\\nThere are three main approaches to be aware of for specifying dependencies:\\n\\n- Define pip requirements explicitly with the `pip_requirements` argument to `save_model` or `log_model`.\\n- Add extra pip requirements to an automatically generated set of requirements with the\\n  `extra_pip_requirements` argument to `save_model` or `log_model`.\\n- Define a Conda environment with the `conda_env` argument to `save_model` or `log_model`.\\n\\nEarlier, we used the first approach to specify that the `ctransformers` library was needed:\\n\\n```python\\n# Log the model using mlflow with the model file as an artifact\\nwith mlflow.start_run():\\n    model_info = mlflow.pyfunc.save_model(\\n        artifact_path=\\"mistralmodel\\",\\n        python_model=mistral_model,\\n        artifacts={\\"model_file\\": \\"path/to/mistral/model/on/local/filesystem\\"},\\n        pip_requirements=[\\n            \\"ctransformers==0.2.27\\",\\n        ],\\n    )\\n```\\n\\nIf you do not specify dependencies explicitly, MLflow will attempt to infer the correct set of\\nrequirements and environment details. To enable greater accuracy, it is **strongly recommended** to\\ninclude an `input_example` when saving or logging your model due to the internal execution of a\\nsample inference step that will capture any loaded library references associated with the inference\\nexecution, enabling a higher probability that the correct dependencies will be recorded.\\n\\nYou can also work with custom code on your own filesystem with the `code_path` argument.\\n`code_path` takes a list of paths to Python file dependencies and prepends them to the system\\npath before the model is loaded, so the custom pyfunc model can import from these modules.\\n\\nSee the documentation for the [log_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.log_model) and\\n[save_model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?highlight=pyfunc#mlflow.pyfunc.save_model)\\nfunctions for more details on the accepted formats for `pip`, `Conda`, and local code requirements.\\n\\n### Summary: Custom Pyfunc Models in MLflow\\n\\nMLflow has built-in methods for working with models from many popular machine learning frameworks,\\nsuch as [scikit-learn](https://www.mlflow.org/docs/latest/models.html#scikit-learn-sklearn),\\n[PyTorch](https://www.mlflow.org/docs/latest/models.html#pytorch-pytorch), and\\n[Transformers](https://www.mlflow.org/docs/latest/llms/transformers/index.html). You can define your own custom\\n`mlflow.pyfunc` model when you want to work with models that do not yet have built-in model\\nflavors, or when you want to implement a custom predict method for models with built-in model flavors.\\n\\nThere are several ways to customize `pyfunc` models to get the desired behavior. Minimally, you can\\nimplement a custom `predict` method. If your model requires saving or loading artifacts, you should also\\nimplement a `load_context` method. For further customization, you can use the `__init__` method for\\nsetting custom attributes and define your own custom methods for pre- and post-processing.\\nCombining these approaches gives you the ability to flexibly define custom logic for your machine\\nlearning models.\\n\\n### Further Learning\\n\\nInterested in learning more about custom `pyfunc` implementations? You can visit:\\n\\n- [Custom Pyfuncs for Advanced LLMs with MLflow](https://www.mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html)\\n- [Build Custom Python Function Models for traditional ML](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html)\\n- [Custom PyFunc notebook examples](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/notebooks/index.html)"},{"id":"mlflow-autolog","metadata":{"permalink":"/mlflow-website/blog/mlflow-autolog","source":"@site/blog/2023-11-30-mlflow-autolog/index.md","title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","description":"Looking to learn more about the autologging functionality included in MLflow? Look no further than this primer on the basics of using this powerful and time-saving feature!","date":"2023-11-30T00:00:00.000Z","tags":[{"inline":true,"label":"autolog","permalink":"/mlflow-website/blog/tags/autolog"}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/mlflow-website/img/authors/daniel_liden.png","key":"daniel-liden","page":null}],"frontMatter":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","slug":"mlflow-autolog","tags":["autolog"],"authors":["daniel-liden"],"thumbnail":"/img/blog/mlflow-autolog.png"},"unlisted":false,"prevItem":{"title":"Custom MLflow Models with mlflow.pyfunc","permalink":"/mlflow-website/blog/custom-pyfunc"},"nextItem":{"title":"MLflow Docs Overhaul","permalink":"/mlflow-website/blog/mlflow-docs-overhaul"}},"content":"Looking to learn more about the autologging functionality included in MLflow? Look no further than this primer on the basics of using this powerful and time-saving feature!\\n\\n# Introduction to [mlflow.autolog](https://www.mlflow.org/docs/latest/tracking/autolog.html)\\n\\nRobust logging practices are central to the iterative development and improvement of machine learning models. Carefully tracking metrics, parameters, and artifacts can be challenging when working with complex machine learning libraries or when experimenting with multiple different frameworks with varying APIs and selections of different objects and values to track.\\n\\n\x3c!-- truncate --\x3e\\n\\nMLflow\u2019s **automatic logging functionality** offers a simple solution that is compatible with many widely-used machine learning libraries, such as [PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html), [Scikit-learn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog), and [XGBoost](https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog). Using `mlflow.autolog()` instructs MLflow to capture essential data without requiring the user to specify what to capture manually. It is an accessible and powerful entrypoint for MLflow\u2019s logging capabilities.\\n\\nTo enable automatic logging, simply add the following line to your machine learning scripts/notebooks, before initiating activities like model training or evaluation that may include information or artifacts you would like to log:\\n\\n```python\\nimport mlflow\\n\\n\\nmlflow.autolog()\\n```\\n\\n## Autolog features\\n\\nWhen a data science workflow includes `mlflow.autolog()`, MLflow will automatically log:\\n\\n- **Metrics**: standard training and evaluation measures such as accuracy and F1 score;\\n- **Parameters**: hyperparameters, such as learning rate and number of estimators; and\\n- **Artifacts**: important files, such as trained models.\\n\\nMLflow\u2019s automatic logging captures details tailored to the specific activities of the library being used: different libraries will result in different logged objects and data. In addition, MLflow logs key metadata such as software versions, a git commit hash, and the file name from which the run was initiated. By documenting the system\'s state during model training, MLflow aims to facilitate environment reproducibility and provide audit lineage, minimizing the possibility of inference issues that could arise from package regressions or deprecations in newer library versions.\\n\\nThe specifics of what is captured through automatic logging depend on the libraries used. Additionally, MLflow captures contextual metadata such as software versions, git commit hash, and the name of the file from which the run was launched. By logging as much detail as possible about the state of the system that trained the model, MLflow can offer environment reproducibility and audit lineage, minimizing the possibility of inference issues resulting from, for example, package regressions or deprecations.\\n\\n## Basic Usage of `mlflow.autolog`\\n\\nYou can access auto logging functionality in two different ways:\\n\\n1. Via the `mlflow.autolog()` function, which enables and configures automatic logging across all supported libraries. This provides a broad, one-size-fits-all approach when working with multiple libraries and is ideal for prototyping and exploratory analysis of a machine learning pipeline.\\n2. Via the library-specific autolog functions, such as `mlflow.sklearn.autolog()`, which enable finer-grained logging configuration for individual libraries. For example, `mlflow.pytorch.autolog()` includes the `log_every_n_epoch` and `log_every_n_step` arguments for specifying how often to log metrics.\\n\\nRegardless of which of these two approaches you use, you do not need to manually initialize an MLflow run with [start_run()](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run) in order to have a run created and for your model, parameters, and metrics to be captured in MLflow.\\n\\n### Example\\n\\n```python\\nimport mlflow\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Generate a 3-class classification problem\\nX, y = datasets.make_classification(\\n    n_samples=1000,\\n    class_sep=0.5,\\n    random_state=42,\\n    n_classes=3,\\n    n_informative=3,\\n)\\n\\n# Split the data into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Enable autolog\\nmlflow.autolog()  # or mlflow.sklearn.autolog()\\n\\n# Initialize the classifier with n_estimators=200 and max_depth=10\\nclf = RandomForestClassifier(n_estimators=200, max_depth=10)\\n\\n# Fit the classifier to the data.\\n# The `fit` method is patched to perform autologging. When engaged in training, a\\n# run is created and the parameters are logged.\\n# After the fit is complete, the model artifact is logged to the run.\\nclf.fit(X_train, y_train)\\n\\n# Score the model on the data\\n# The current active run is retrieved during calling `score` and the loss metrics are logged\\n# to the active MLflow run.\\nclf.score(X_val, y_val)\\n\\n# Visualize the automatically logged run results to validate what we recorded\\nmlflow.last_active_run()\\n```\\n\\nThe above logs model parameters, metrics, and the model to an MLflow run. The output result of the final statement ([mlflow.last_active_run()](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.last_active_run)) in the above example, which will return data from the run on model metrics, parameters, and logged artifacts (results truncated) is as shown below:\\n\\n```text\\n<Run: data=<RunData:\\nmetrics={\'RandomForestClassifier_score_X_val\': 0.72,\\n         \'training_accuracy_score\': 0.99625,\\n         \'training_f1_score\': 0.9962547564333545,\\n         \'training_log_loss\': 0.3354604497935824,\\n         \'training_precision_score\': 0.9962921348314606,\\n         \'training_recall_score\': 0.99625,\\n         \'training_roc_auc\': 0.9998943433719795,\\n         \'training_score\': 0.99625\\n         },\\n params={\'bootstrap\': \'True\',\\n         \'ccp_alpha\': \'0.0\',\\n         \'class_weight\': \'None\',\\n         \'criterion\': \'gini\',\\n         \'max_depth\': \'10\',\\n         \'max_features\': \'sqrt\',\\n         \'max_leaf_nodes\': \'None\',\\n         [...],\\n         },\\ntags={\'estimator_class\': \'sklearn.ensemble._forest.RandomForestClassifier\',\\n      \'estimator_name\': \'RandomForestClassifier\',\\n      \'mlflow.autologging\': \'sklearn\',\\n      [...]\\n}, [...]>>\\n```\\n\\nYou can also access these in the mlflow ui by executing [mlflow ui](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) on a command line terminal.\\n\\n![The MLflow Tracking UI](./autolog_in_ui.png)\\n\\nThe MLflow UI also allows you to graphically compare different metrics and parameters across multiple runs, including those generated via `mlflow.autolog`.\\n\\n![Run comparison of autologged runs in the MLflow UI](./autolog_compare_runs.png)\\n\\n## Configuration and Customization\\n\\nThe automatic logging functions take many arguments that give the user greater control over logging behavior. For example, `mlflow.autolog()` includes `log_models` and `log_datasets` arguments (both `True` by default) that specify whether models and dataset information should be saved to the MLflow run, enabling you to specify what is actually logged. To disable automatic logging of datasets while continuing to log all the usual elements, simply disable the autologging of datasets feature by setting `mlflow.autolog(log_datasets=False)` before fitting a model. You can also adjust the behavior of library-specific autolog functions: for example, the `mlflow.sklearn.autolog()` function includes a `max_tuning_runs` argument that specifies how many nested runs to capture when performing hyperparameter searches.\\n\\n`mlflow.autolog()` can be used in combination with the library-specific autolog functions to control the logging behavior for specific libraries. The library-specific autolog call will always supersede `mlflow.autolog()`, regardless of the order in which they are called. For example, combining `mlflow.autolog()` with `mlflow.sklearn.autolog(disable=True)` will result in automatic logging for all supported libraries except for `scikit-learn`.\\n\\nIt is important to consult the documentation for the specific framework(s) you are using in order to understand what is logged automatically and what configuration options are available. See the [further reading section below](#further-reading) for links.\\n\\n## Conclusion and Next Steps\\n\\nMLflow\'s autologging capabilities and library-specific automatic logging functions provide a straightforward starting point for MLflow tracking with little or no required configuration. They log key metrics, parameters, and artifacts from many popular machine learning libraries, allowing users to track their machine learning workflows without writing custom tracking code.\\n\\nThey are not, however, the right solution for all use cases. If you only need to track a handful of specific metrics, enabling automatic logging may be inefficient, resulting in much more generated data and stored artifacts than needed. Furthermore, automatic logging is not available for every possible framework and custom values one might want to track. In such cases, it might be necessary to [manually specify what to track](https://mlflow.org/docs/latest/tracking/tracking-api.html#logging-functions).\\n\\n## Further Reading\\n\\n- [MLflow Documentation on Automatic Logging](https://mlflow.org/docs/latest/tracking/autolog.html)\\n- [Python API reference for mlflow.autolog](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.autolog)\\n- Python API references for library-specific autolog functions\\n  - [PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\\n  - [Tensorflow](https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.autolog)\\n  - [Scikit-learn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog)\\n  - [XGBoost](https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog)\\n  - [PySpark](https://mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog)\\n  - [Statsmodels](https://mlflow.org/docs/latest/python_api/mlflow.statsmodels.html#mlflow.statsmodels.autolog)\\n  - [LightGBM](https://mlflow.org/docs/latest/python_api/mlflow.lightgbm.html#mlflow.lightgbm.autolog)\\n  - [Paddle](https://mlflow.org/docs/latest/python_api/mlflow.paddle.html#mlflow.paddle.autolog)\\n  - [Fastai](https://mlflow.org/docs/latest/python_api/mlflow.fastai.html#mlflow.fastai.autolog)"},{"id":"mlflow-docs-overhaul","metadata":{"permalink":"/mlflow-website/blog/mlflow-docs-overhaul","source":"@site/blog/2023-10-31-mlflow-docs-overhaul.md","title":"MLflow Docs Overhaul","description":"The MLflow Documentation is getting an upgrade.","date":"2023-10-31T00:00:00.000Z","tags":[{"inline":true,"label":"docs","permalink":"/mlflow-website/blog/tags/docs"}],"readingTime":5.11,"hasTruncateMarker":true,"authors":[{"name":"MLflow maintainers","title":"MLflow maintainers","url":"https://github.com/mlflow/mlflow.git","imageURL":"https://github.com/mlflow-automation.png","key":"mlflow-maintainers","page":null}],"frontMatter":{"title":"MLflow Docs Overhaul","tags":["docs"],"slug":"mlflow-docs-overhaul","authors":["mlflow-maintainers"],"thumbnail":"/img/blog/docs-overhaul.png"},"unlisted":false,"prevItem":{"title":"Automatic Metric, Parameter, and Artifact Logging with mlflow.autolog","permalink":"/mlflow-website/blog/mlflow-autolog"}},"content":"The MLflow Documentation is getting an upgrade.\\n\\n## Overhauling the MLflow Docs\\n\\nWe\'re thrilled to announce a comprehensive overhaul of the MLflow Docs. This initiative is not just about refreshing the look and feel but about reimagining how our users interact with our content. Our primary goal is to enhance clarity, improve navigation, and provide more in-depth resources for our community.\\n\\n## A Renewed Focus on User Experience\\n\\nThe MLflow documentation has always been an essential resource for our users. Over time, we\'ve received invaluable feedback, and we\'ve listened. The modernization effort is a direct response to the needs and preferences of our community.\\n\\n\x3c!-- truncate --\x3e\\n\\nAlong with working on covering new cutting-edge features as part of this documentation overhaul, we\'re working on addressing the complexity of getting started. As the first part of a series of tutorials and guides focusing on the initial learning phase, we\'ve created a new [getting started guide](https://www.mlflow.org/docs/latest/getting-started/logging-first-model/index.html), the first of many in a new series we\'re working on in an effort to teach the fundamentals of using MLflow. We feel that more in-depth instructional tutorials for learning the concepts and tools of MLflow will help to enhance the user experience for not only new users, but experienced users who need a refresher of how to do certain tasks.\\n\\nThere are more of these coming in the future!\\n\\n### **Easier Navigation**\\n\\nOur first order of business is to declutter and reorganize. This is going to be a process, though. With some of the monolithic pages ([Mlflow Models](https://www.mlflow.org/docs/2.7.1/models.html)), this will be more of a marathon than a sprint.\\n\\nWe\'ve introduced a [new main navigation page](https://www.mlflow.org/docs/latest/index.html) in an effort to help steer you to the content that you\'re looking for based on end-use domain, rather than component of MLflow. We\'re hoping that this helps to bring new feature content and useful examples to your awareness, limiting the amount of exploratory discovery needed to understand how to use these new features.\\n\\nAnother priority for us was to make major new features easier to discover. While the [release notes](https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md) are useful, particularly for Engineers who are maintaining integrations with, or are managing a deployment of, MLflow, they\'re not particularly user-friendly for an end-user of MLflow. We felt that a curated list of major new features would help to distill the information in our release notes, so we built the [new features](https://www.mlflow.org/docs/latest/new-features/index.html) page. We sincerely hope it helps to reduce the amount of effort needed to know what new major features have been released.\\n\\n### **Interactive Learning with Notebooks**\\n\\nIn today\'s fast-paced tech world, interactive learning is becoming the norm. Recognizing this trend, we\'re embedding viewable notebooks directly within the docs. But we\'re not stopping there. These notebooks are downloadable, allowing you to run, modify, and experiment with them locally. It\'s a hands-on approach to learning, bridging the gap between theory and practice.\\n\\n### **In-depth Tutorials and Guides**\\n\\nWhile our previous documentation provided a solid foundation, we felt there was room for more detailed explorations. We\'re introducing comprehensive [tutorials](https://www.mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html) and [guides](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html) that delve deep into MLflow\'s features, showing how to solve actual problems. These first new tutorials and guides are just the start. We\'re going to be spending a lot of time and effort on making much more of MLflow documented in this way, helping to dramatically reduce the amount of time you have to spend figuring out how to leverage features in MLflow.\\n\\n## Diving Deeper: Expanding on Guides and Tutorials\\n\\nOur dedication to simplifying the usage of MLflow shines through in our revamped tutorials and guides. We\'re not just providing instructions; we\'re offering [deep dives](https://www.mlflow.org/docs/latest/llms/custom-pyfunc-for-llms/notebooks/index.html), [best practices](https://www.mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/index.html), and real-world applications. What you see in the MLflow 2.8.0 release is just the beginning. We\'re going to be heavily focusing on creating more content, showing the best way to leverage the many features and services within MLflow, all the while endeavoring to make it easier than ever to manage any ML project you\'re working on.\\n\\n- **LLMs**: With all of the [new LLM-focused features](https://www.mlflow.org/docs/latest/llms/llm-evaluate/notebooks/rag-evaluation.html) we\'ve been releasing in the past year, we feel the need to create easier getting started guides,\\n  [in-depth tutorials](https://www.mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html), runnable examples, and more teaching-oriented step-by-step introductions to these features.\\n\\n- **Tracking and the MLflow UI**: Our expanded section on tracking will cover everything from setting up your first experiment to advanced tracking techniques. The MLflow UI, an integral part of the platform, will also get its spotlight, ensuring you can make the most of its features.\\n\\n- **Model Registry**: The model registry is where MLflow truly shines, and our new guides will ensure you can harness its full power. From organizing models to version control, we\'ll cover it all.\\n\\n- **Recipes and LLM-focused Features**: MLflow\'s versatility is one of its strengths. Our new content will explore the breadth of features available, from recipes to LLM-focused tools like the AI Gateway, LLM Evaluation, and the PromptLab UI.\\n\\n## The Transformative Power of Interactive Notebooks\\n\\nInteractive notebooks have revolutionized data science and machine learning. By integrating them into our documentation, we aim to provide a holistic learning experience. You can see code in action, understand its impact, and then experiment on their own. It\'s a dynamic way to grasp complex concepts, ensuring that you not only understand but can also apply your knowledge in your actual project code.\\n\\n## Join Us on This Journey\\n\\nThe overhaul of the MLflow documentation is a significant milestone, but it\'s just the beginning. We have a roadmap full of exciting updates, new content, and features. And for those in our community with a passion for sharing knowledge, we have a message: We\'d love to [collaborate](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md)! Whether it\'s writing tutorials, sharing use-cases, or providing feedback, every contribution enriches the MLflow community.\\n\\nIn conclusion, our commitment to providing top-notch documentation is a new primary focus of the maintainer group. We believe that well-documented features, combined with interactive learning tools, can significantly enhance the experience of using any tool. We want to put in the effort and time to make sure that your journey with using MLflow is as simple and powerful as it can be.\\n\\nStay tuned for more updates, and as always, happy coding!"}]'),me=()=>{const e=ue.slice(0,3),n=(0,i.jsx)(u.A,{href:"/blog",className:"",children:(0,i.jsx)(r,{variant:"outline",size:"small",children:"View all"})});return(0,i.jsxs)("div",{className:"flex flex-col gap-4 w-full items-center md:item-start",children:[(0,i.jsxs)("div",{className:"flex flex-row justify-between items-center gap-4 flex-wrap w-full",children:[(0,i.jsxs)("div",{className:"flex flex-col gap-6 items-start",children:[(0,i.jsx)(F,{label:"Blog"}),(0,i.jsx)(se,{level:2,children:"Latest news"})]}),(0,i.jsx)("div",{className:"hidden md:block",children:n})]}),(0,i.jsx)(_,{children:e.map((e=>(0,i.jsx)(v,{children:(0,i.jsxs)(u.A,{href:e.metadata.permalink,className:"flex flex-col gap-6 h-full justify-between",children:[(0,i.jsxs)("div",{className:"flex flex-col gap-6",children:[(0,i.jsx)("span",{className:"text-gray-500",children:new Date(e.metadata.date).toLocaleDateString("en-us",{month:"short",day:"numeric",year:"numeric"})}),(0,i.jsx)("h3",{className:"overflow-hidden text-ellipsis text-pretty",children:e.metadata.title})]}),(0,i.jsx)("img",{src:(0,k.Ay)(e.metadata.frontMatter.thumbnail),alt:e.metadata.title,className:"rounded-2xl md:max-h-[210px] object-cover max-w-full"})]})},e.metadata.permalink)))}),(0,i.jsx)("div",{className:"block md:hidden",children:n})]})};var he,pe;function ge(){return ge=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},ge.apply(null,arguments)}const fe=({title:e,titleId:n,...t})=>a.createElement("svg",ge({xmlns:"http://www.w3.org/2000/svg",width:126,height:24,fill:"none",viewBox:"0 0 126 24","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,he||(he=a.createElement("g",{clipPath:"url(#a)"},a.createElement("path",{fill:"#EE3D2C",d:"m17.485 8.853-8.238 4.639L.425 8.535 0 8.765v3.598l9.247 5.184 8.237-4.62v1.905L9.247 19.47.425 14.515 0 14.744v.617l9.247 5.185 9.228-5.185v-3.598l-.425-.23-8.803 4.939-8.257-4.62V9.946l8.257 4.62 9.228-5.183V5.836l-.46-.264-8.768 4.92L1.415 6.12l7.832-4.391 6.434 3.615.566-.317v-.441l-7-3.932L0 5.837v.565l9.247 5.185 8.237-4.639z"}),a.createElement("path",{fill:"#fff",d:"M35.747 17.605V1.768h-2.44v5.925c0 .088-.053.16-.141.194a.22.22 0 0 1-.23-.053c-.83-.97-2.121-1.517-3.536-1.517-3.023 0-5.393 2.54-5.393 5.785 0 1.587.549 3.051 1.556 4.127a5.2 5.2 0 0 0 3.837 1.657c1.397 0 2.687-.582 3.536-1.586.053-.07.16-.089.229-.07.089.035.143.104.143.193v1.182zm-5.816-1.923c-1.946 0-3.465-1.569-3.465-3.58 0-2.01 1.52-3.58 3.465-3.58s3.465 1.57 3.465 3.58-1.52 3.58-3.465 3.58m18.864 1.924V6.583h-2.422v1.11c0 .09-.054.16-.142.194a.19.19 0 0 1-.23-.07c-.813-.97-2.086-1.517-3.536-1.517-3.023 0-5.392 2.54-5.392 5.785s2.369 5.784 5.393 5.784c1.396 0 2.687-.582 3.535-1.604.053-.071.159-.088.23-.071.088.035.142.106.142.195v1.198h2.422zm-5.781-1.923c-1.946 0-3.465-1.57-3.465-3.58s1.52-3.58 3.465-3.58 3.465 1.57 3.465 3.58-1.52 3.58-3.465 3.58m26.518 1.923V6.583h-2.423v1.11c0 .09-.052.16-.14.194a.19.19 0 0 1-.23-.07c-.814-.97-2.087-1.517-3.537-1.517-3.04 0-5.392 2.54-5.392 5.804 0 3.261 2.37 5.784 5.392 5.784 1.397 0 2.687-.582 3.536-1.605.053-.071.16-.09.23-.071.089.035.141.105.141.194v1.199zm-5.782-1.923c-1.944 0-3.465-1.57-3.465-3.58s1.52-3.58 3.465-3.58 3.465 1.57 3.465 3.58-1.52 3.58-3.465 3.58m10.22.617c.018 0 .053-.018.07-.018.054 0 .124.035.16.07.83.97 2.12 1.517 3.535 1.517 3.023 0 5.394-2.539 5.394-5.784 0-1.588-.55-3.051-1.556-4.127A5.2 5.2 0 0 0 77.735 6.3c-1.396 0-2.687.582-3.535 1.587-.054.071-.142.088-.23.071-.088-.035-.142-.106-.142-.195V1.768h-2.44v15.836h2.44v-1.11c0-.088.054-.159.142-.194m-.248-4.197c0-2.01 1.521-3.58 3.465-3.58 1.945 0 3.465 1.57 3.465 3.58s-1.52 3.58-3.465 3.58-3.465-1.587-3.465-3.58m16.46-3.261c.23 0 .443.018.583.053V6.391a2.3 2.3 0 0 0-.406-.036c-1.273 0-2.44.653-3.058 1.694-.054.088-.142.124-.23.088a.21.21 0 0 1-.16-.194V6.584H84.49v11.04h2.44v-4.867c0-2.415 1.237-3.914 3.253-3.914m4.596-2.26h-2.475v11.04h2.475zm-1.274-4.798a1.5 1.5 0 1 0 0 2.998 1.5 1.5 0 1 0 0-2.997m8.523 4.514c-3.395 0-5.853 2.434-5.853 5.804 0 1.639.584 3.102 1.627 4.162 1.061 1.057 2.546 1.64 4.207 1.64 1.38 0 2.44-.265 4.456-1.747l-1.396-1.464c-.991.653-1.91.97-2.811.97-2.052 0-3.59-1.534-3.59-3.562 0-2.029 1.538-3.563 3.59-3.563.971 0 1.873.318 2.775.97l1.555-1.463c-1.82-1.552-3.465-1.747-4.56-1.747m8.733 6.455a.2.2 0 0 1 .142-.053h.017c.054 0 .106.036.16.07l3.906 4.833h3.006l-5.056-6.103c-.071-.088-.071-.212.017-.281l4.65-4.64h-2.988l-4.013 4.022c-.053.053-.141.07-.23.053a.22.22 0 0 1-.124-.194V1.785h-2.457v15.837h2.44v-4.303c0-.053.017-.124.071-.159z"}),a.createElement("path",{fill:"#fff",d:"M121.968 17.887c1.998 0 4.031-1.217 4.031-3.527 0-1.516-.955-2.558-2.899-3.192l-1.327-.44c-.901-.3-1.326-.724-1.326-1.306 0-.67.602-1.129 1.45-1.129.813 0 1.538.53 1.998 1.446l1.963-1.057c-.725-1.482-2.228-2.4-3.961-2.4-2.192 0-3.783 1.412-3.783 3.334 0 1.534.92 2.557 2.81 3.157l1.363.44c.954.3 1.361.688 1.361 1.306 0 .935-.867 1.27-1.61 1.27-.989 0-1.873-.636-2.297-1.676l-1.998 1.059c.654 1.675 2.262 2.715 4.225 2.715m-66.385-.106c.778 0 1.468-.07 1.857-.124v-2.115q-.61.063-1.22.07c-.99 0-1.75-.176-1.75-2.31V8.769c0-.124.088-.211.212-.211h2.386V6.564h-2.386a.204.204 0 0 1-.212-.21V3.179h-2.44V6.37a.204.204 0 0 1-.213.212H50.12v1.993h1.697c.124 0 .213.088.213.21v5.133c0 3.862 2.581 3.862 3.553 3.862"}))),pe||(pe=a.createElement("defs",null,a.createElement("clipPath",{id:"a"},a.createElement("path",{fill:"#fff",d:"M0 .5h126v23H0z"})))));var we;function ye(){return ye=Object.assign?Object.assign.bind():function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)({}).hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},ye.apply(null,arguments)}const _e=({title:e,titleId:n,...t})=>a.createElement("svg",ye({xmlns:"http://www.w3.org/2000/svg",width:16,height:16,fill:"none",viewBox:"0 0 16 16","aria-labelledby":n},t),e?a.createElement("title",{id:n},e):null,we||(we=a.createElement("path",{stroke:"#fff",strokeLinecap:"round",strokeLinejoin:"round",strokeOpacity:.6,strokeWidth:1.333,d:"m2 10 4.196 3.333L14 2.667"}))),be=({link:e=h.OU,size:n="medium",width:t="default",variant:a="primary"})=>(0,i.jsx)(u.A,{to:e,children:(0,i.jsx)(r,{size:n,width:t,variant:a,children:"Get started"})}),ve=()=>{const e=j();return(0,i.jsxs)("div",{className:S("grid grid-cols-1 lg:grid-cols-2 gap-8"),children:[(0,i.jsxs)("div",{className:S("flex flex-col gap-6 items-start","blue"!==e?"lg:col-span-2":""),children:[(0,i.jsx)(se,{level:2,children:"Get started with MLflow"}),"blue"!==e?(0,i.jsx)(le,{size:"l",children:"Choose from two options depending on your needs"}):null]}),(0,i.jsxs)("div",{className:"flex flex-col gap-8 p-8 bg-[#fff]/4 rounded-2xl justify-between",children:[(0,i.jsxs)("div",{className:"flex flex-col gap-8",children:[(0,i.jsx)("h3",{className:"m-0 text-white",children:"Self-hosted Open Source"}),(0,i.jsx)("div",{className:"flex flex-col gap-4",children:["Apache-2.0 license","Full control over your own infrastructure","Community support"].map(((e,n)=>(0,i.jsxs)("div",{className:"flex flex-row items-center gap-4",children:[(0,i.jsx)(_e,{className:"shrink-0"}),(0,i.jsx)("span",{className:"text-md font-light text-gray-600",children:e})]},n)))})]}),(0,i.jsx)(be,{size:"large",width:"full",variant:"primary",link:h.OU})]}),"blue"!==e?(0,i.jsxs)("div",{className:"flex flex-col gap-8 p-8 bg-[#fff]/4 rounded-2xl @container justify-between",children:[(0,i.jsxs)("div",{className:"flex flex-col gap-8",children:[(0,i.jsx)("div",{className:"flex flex-row justify-between items-center gap-4",children:(0,i.jsxs)("div",{className:"flex flex-row justify-center items-end gap-3 flex-wrap",children:[(0,i.jsx)("h3",{className:"m-0 text-white",children:"Free hosted "}),(0,i.jsx)("span",{className:"text-gray-500 text-sm",children:"ON"}),(0,i.jsx)(fe,{})]})}),(0,i.jsx)("div",{className:"flex flex-col gap-4",children:["Explore MLflow with a fully managed experience","Built and maintained by the creators of MLflow","Full OSS compatability"].map(((e,n)=>(0,i.jsxs)("div",{className:"flex flex-row items-center gap-4",children:[(0,i.jsx)(_e,{className:"shrink-0"}),(0,i.jsx)("span",{className:"text-md font-light text-gray-600",children:e})]},n)))})]}),(0,i.jsx)(be,{size:"large",width:"full",variant:"dark",link:h.Q5})]}):null]})};function ke({children:e,title:n,body:t,sectionLabel:a,hasGetStartedButton:o,bodyColor:s}){const r=Array.isArray(t)?t:[t];return(0,i.jsxs)("div",{className:"flex flex-col min-h-screen",children:[(0,i.jsxs)("div",{className:"flex flex-col justify-center items-center flex-1 gap-6 w-full max-w-5xl mx-auto px-4",children:[a&&(0,i.jsx)(F,{label:a}),(0,i.jsx)("div",{className:"max-w-4xl mx-auto",children:(0,i.jsx)(se,{level:1,children:n})}),(0,i.jsx)("div",{className:"mx-auto text-pretty",children:r.map(((e,n)=>(0,i.jsx)(le,{size:"l",align:"center",color:s,children:e},n)))}),o&&(0,i.jsx)(be,{link:!0===o?void 0:o})]}),e]})}function xe({children:e}){return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(ve,{}),e,(0,i.jsx)(de,{})]})}const Le=({children:e})=>(0,i.jsx)("div",{className:"flex flex-col rounded-4xl overflow-hidden bg-white/16 backdrop-blur-[8px] w-full justify-between border-white/20 border shadow-2xl",children:e});function Me({children:e}){return(0,i.jsx)("div",{className:"flex flex-col md:flex-row gap-10",children:e})}const Te=()=>(0,i.jsxs)("div",{className:"flex flex-col md:flex-row justify-center items-center gap-6 md:gap-16",children:[(0,i.jsx)("span",{className:"text-lg text-gray-600 text-center",children:"Join the industry leading companies building with MLflow"}),(0,i.jsx)(be,{variant:"primary",link:h.OU})]}),Ie=()=>(0,i.jsx)(W,{title:"Trusted by thousands of organizations and research teams",children:(0,i.jsx)(M,{})}),Ae=((0,o.F)("uppercase leading-[120%] tracking-[8%]",{variants:{size:{l:"font-medium text-[14px]",s:"font-semibold text-[12px]"}}}),(0,o.F)("flex flex-col gap-4",{variants:{padded:{true:"p-6",false:""}}})),Ee=(0,o.F)("w-full relative",{variants:{rounded:{true:"rounded-[16px] overflow-hidden",false:""}}});function Se({title:e,body:n,bodySize:t="l",cta:a,image:o,padded:s=!1,rounded:l=!0}){const c=Array.isArray(n)?n:[n];return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)("div",{className:Ae({padded:s}),children:[(0,i.jsx)(se,{level:3,children:e}),(0,i.jsx)("div",{children:c.map(((e,n)=>(0,i.jsx)(le,{size:t,margin:"tight",children:e},n)))}),a&&(0,i.jsx)(u.A,{href:a.href,children:(0,i.jsx)(r,{variant:a.prominent?"primary":"outline",size:a.prominent?"medium":"small",children:a.text})})]}),o&&(0,i.jsxs)("div",{className:Ee({rounded:l}),children:[o,(0,i.jsx)("div",{className:"absolute inset-0 bg-black/5 pointer-events-none"})]})]})}function qe(e){return(0,i.jsxs)("div",{className:"relative w-full max-w-[800px] rounded-[16px] overflow-hidden mx-auto",children:[(0,i.jsx)("img",{...e}),(0,i.jsx)("div",{className:"absolute inset-0 bg-black/5 pointer-events-none"})]})}const Ce=()=>(0,i.jsx)(W,{label:"Why us?",title:"Why MLflow is unique",children:(0,i.jsxs)(_,{columns:2,children:[(0,i.jsx)(v,{children:(0,i.jsx)(Se,{title:"Open, Flexible, and Extensible",body:"Open-source and extensible, MLflow prevents vendor lock-in by integrating with the GenAI/ML ecosystem and using open protocols for data ownership, adapting to your existing and future stacks."})}),(0,i.jsx)(v,{children:(0,i.jsx)(Se,{title:"Unified, End-to-End MLOps and AI Observability",body:"MLflow offers a unified platform for the entire GenAI and ML model lifecycle, simplifying the experience and boosting collaboration by reducing tool integration friction."})}),(0,i.jsx)(v,{children:(0,i.jsx)(Se,{title:"Framework neutrality",body:"MLflow's framework-agnostic design is one of its strongest differentiators. Unlike proprietary solutions that lock you into specific ecosystems, MLflow works seamlessly with all popular ML and GenAI frameworks."})}),(0,i.jsx)(v,{children:(0,i.jsx)(Se,{title:"Enterprise adoption",body:"MLflow's impact extends beyond its technical capabilities. Created by Databricks, it has become one of the most widely adopted MLOps tools in the industry, with integration support from major cloud providers."})})]})});var Pe=t(35363);const De={grid:"grid_QZER",fadeOverlay:"fadeOverlay_uhV4",buttonContainer:"buttonContainer_NARt",toggleButton:"toggleButton_Rjzo"},Ne=({items:e,defaultVisibleCount:n,renderItem:t})=>{const[o,s]=(0,a.useState)(!1),r=o?e:e.slice(0,n);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("div",{className:`${De.grid} ${o?"":De.fadeOverlay}`,children:r.map(((e,n)=>(0,i.jsx)("div",{className:De.gridItem,children:t(e,n)},n)))}),e.length>n&&(0,i.jsx)("div",{className:De.buttonContainer,children:(0,i.jsx)("button",{onClick:()=>s(!o),className:De.toggleButton,children:o?"See Less \u2227":"See All \u2228"})})]})},Oe="card_TKUl",je="container_liof",Re="img_kVo8",Fe="title_HEBx",Ge="a_Wouj",We=({title:e,src:n,href:t})=>(0,i.jsx)("div",{className:(0,E.A)("card",Oe),children:(0,i.jsx)("a",{href:t,className:Ge,children:(0,i.jsx)("div",{className:"card__body",children:(0,i.jsxs)("div",{className:je,children:[(0,i.jsx)("img",{src:n,alt:e,className:Re}),(0,i.jsx)("h4",{className:Fe,children:e})]})})})}),ze=()=>(0,i.jsx)("div",{className:"w-full px-4 md:px-8 lg:px-16",children:(0,i.jsx)(W,{title:"Integrates with 25+ apps and frameworks",children:(0,i.jsx)(Pe.A,{children:()=>(0,i.jsx)(Ne,{items:[{title:"PyTorch",src:"img/pytorch.svg",href:"https://mlflow.org/docs/latest/deep-learning/pytorch/index.html"},{title:"OpenAI",src:"img/openai.svg",href:"https://mlflow.org/docs/latest/llms/openai/index.html"},{title:"HuggingFace",src:"img/huggingface.svg",href:"https://mlflow.org/docs/latest/llms/transformers/index.html"},{title:"LangChain",src:"img/langchain.svg",href:"https://mlflow.org/docs/latest/llms/langchain/index.html"},{title:"Anthropic",src:"img/anthropic.svg",href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing"},{title:"Gemini",src:"img/google-gemini.svg",href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing"},{title:"AutoGen",src:"img/autogen.jpeg",href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing"},{title:"LlamaIndex",src:"img/llamaindex.svg",href:"https://mlflow.org/docs/latest/llms/llama-index/index.html"},{title:"Spark",src:"img/spark.svg",href:"https://mlflow.org/docs/latest/python_api/mlflow.spark.html"},{title:"Keras",src:"img/keras.svg",href:"https://mlflow.org/docs/latest/deep-learning/keras/index.html"},{title:"TensorFlow",src:"img/tensorflow.svg",href:"https://mlflow.org/docs/latest/deep-learning/tensorflow/index.html"},{title:"scikit-learn",src:"img/scikit-learn.svg",href:"https://mlflow.org/docs/latest/models.html#scikit-learn-sklearn"},{title:"XGBoost",src:"img/xgboost.svg",href:"https://mlflow.org/docs/latest/models.html#xgboost-xgboost"},{title:"LightGBM",src:"img/lightgbm.svg",href:"https://mlflow.org/docs/latest/models.html#lightgbm-lightgbm"},{title:"CatBoost",src:"img/catboost.svg",href:"https://mlflow.org/docs/latest/models.html#catboost-catboost"},{title:"CrewAI",src:"img/crewai.svg",href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing"},{title:"LiteLLM",src:"img/litellm.png",href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing"},{title:"Sentence Transformers",src:"img/sentence-transformers.svg",href:"https://mlflow.org/docs/latest/llms/sentence-transformers/index.html"},{title:"ONNX",src:"img/onnx.svg",href:"https://mlflow.org/docs/latest/models.html#onnx-onnx"},{title:"Spacy",src:"img/spacy.svg",href:"https://mlflow.org/docs/latest/models.html#spacy-spacy"},{title:"FastAI",src:"img/fastai.png",href:"https://mlflow.org/docs/latest/models.html#fastai-fastai"},{title:"StatsModels",src:"img/statsmodels.svg",href:"https://mlflow.org/docs/latest/models.html#statsmodels-statsmodels"},{title:"Prompt flow",src:"img/promptflow.svg",href:"https://mlflow.org/docs/latest/models.html#promptflow-promptflow-experimental"},{title:"JohnSnowLabs",src:"img/johnsnowlab.svg",href:"https://mlflow.org/docs/latest/models.html#john-snow-labs-johnsnowlabs-experimental"},{title:"H2O",src:"img/h2o.svg",href:"https://mlflow.org/docs/latest/models.html#h2o-h2o"},{title:"Prophet",src:"img/prophet.svg",href:"https://mlflow.org/docs/latest/python_api/mlflow.prophet.html"}],defaultVisibleCount:window.innerWidth>996?16:8,renderItem:({title:e,src:n,href:t})=>(0,i.jsx)(We,{title:e,src:n,href:t})})})})})}}]);