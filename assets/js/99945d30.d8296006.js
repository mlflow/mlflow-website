"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8762],{7386:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var t=i(5893),a=i(1151);const o={title:"Deep Learning with MLflow (Part 2)",description:"Using MLflow's Deep Learning tracking features for fine tuning an LLM",slug:"deep-learning-part-2",authors:["puneet-jain","avinash-sooriyarachchi","abe-omorogbe","ben"],tags:["Deep Learning"],thumbnail:"img/blog/dl-blog-2.png"},r=void 0,s={permalink:"/mlflow-website/blog/deep-learning-part-2",source:"@site/blog/2024-04-26-deep-learning-part-2/index.md",title:"Deep Learning with MLflow (Part 2)",description:"Using MLflow's Deep Learning tracking features for fine tuning an LLM",date:"2024-04-26T00:00:00.000Z",formattedDate:"April 26, 2024",tags:[{label:"Deep Learning",permalink:"/mlflow-website/blog/tags/deep-learning"}],readingTime:11.64,hasTruncateMarker:!0,authors:[{name:"Puneet Jain",title:"Sr. Specialist Solutions Architect at Databricks",url:"https://www.linkedin.com/in/puneetjain159/",imageURL:"/img/authors/puneet.png",key:"puneet-jain"},{name:"Avinash Sooriyarachchi",title:"Sr. Solutions Architect at Databricks",url:"https://www.linkedin.com/in/avi-data-ml/",imageURL:"/img/authors/avinash.png",key:"avinash-sooriyarachchi"},{name:"Abe Omorogbe",title:"Product Manager, ML at Databricks",url:"https://www.linkedin.com/in/abeomor/",imageURL:"/img/authors/abe.png",key:"abe-omorogbe"},{name:"Ben Wilson",title:"Software Engineer at Databricks",url:"https://www.linkedin.com/in/benjamin-wilson-arch/",imageURL:"/img/authors/ben.png",key:"ben"}],frontMatter:{title:"Deep Learning with MLflow (Part 2)",description:"Using MLflow's Deep Learning tracking features for fine tuning an LLM",slug:"deep-learning-part-2",authors:["puneet-jain","avinash-sooriyarachchi","abe-omorogbe","ben"],tags:["Deep Learning"],thumbnail:"img/blog/dl-blog-2.png"},unlisted:!1,prevItem:{title:"Introducing MLflow Tracing",permalink:"/mlflow-website/blog/mlflow-tracing"},nextItem:{title:"MLflow Release Candidates",permalink:"/mlflow-website/blog/release-candidates"}},l={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[{value:"Use Case: Fine Tuning a Transformer Model for Text Classification",id:"use-case-fine-tuning-a-transformer-model-for-text-classification",level:2},{value:"What is PEFT?",id:"what-is-peft",level:3},{value:"Integrating Hugging-Face models and the PyTorch Lightning framework",id:"integrating-hugging-face-models-and-the-pytorch-lightning-framework",level:2},{value:"Configuring MLflow for PEFT-based fine-tuning",id:"configuring-mlflow-for-peft-based-fine-tuning",level:2},{value:"The Importance of Logging and Early-stopping",id:"the-importance-of-logging-and-early-stopping",level:2},{value:"Early stopping",id:"early-stopping",level:3},{value:"Configuring Pytorch Trainer Callback with Early stopping",id:"configuring-pytorch-trainer-callback-with-early-stopping",level:3},{value:"Visualization and Sharing Capabilities within MLflow",id:"visualization-and-sharing-capabilities-within-mlflow",level:2},{value:"When to stop training?",id:"when-to-stop-training",level:2},{value:"Evaluating epoch checkpoints of Fine Tuned Models with MLflow",id:"evaluating-epoch-checkpoints-of-fine-tuned-models-with-mlflow",level:3},{value:"Summary",id:"summary",level:2},{value:"Check out the code",id:"check-out-the-code",level:3},{value:"Get Started with MLflow 2.12 Today",id:"get-started-with-mlflow-212-today",level:2},{value:"Feedback",id:"feedback",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"In the realm of deep learning, finetuning of pre-trained Large Language Models (LLMs) on private datasets is an excellent customization\noption to increase a model\u2019s relevancy for a specific task. This practice is not only common, but also essential for developing specialized\nmodels, particularly for tasks like text classification and summarization."}),"\n",(0,t.jsx)(n.p,{children:"In such scenarios, tools like MLflow are invaluable. Tracking tools like MLflow help to ensure that every aspect of the training\nprocess - metrics, parameters, and artifacts - are reproducibly tracked and logged, allowing for the analysis, comparison, and sharing of tuning iterations."}),"\n",(0,t.jsxs)(n.p,{children:["In this blog post, we are going to be using ",(0,t.jsx)(n.a,{href:"https://mlflow.org/releases/2.12.1",children:"MLflow 2.12"})," and the\n",(0,t.jsx)(n.a,{href:"https://mlflow.org/blog/deep-learning-part-1",children:"recently introduced MLflow Deep Learning features"})," to track all the important aspects of fine\ntuning a large language model for text classification, including the use of automated logging of training checkpoints in order to simplify\nthe process of resumption of training."]}),"\n",(0,t.jsx)(n.h2,{id:"use-case-fine-tuning-a-transformer-model-for-text-classification",children:"Use Case: Fine Tuning a Transformer Model for Text Classification"}),"\n",(0,t.jsxs)(n.p,{children:["The example scenario that we're using within this blog utilizes the ",(0,t.jsx)(n.a,{href:"https://huggingface.co/datasets/coastalcph/lex_glue/viewer/unfair_tos",children:"unfair-TOS"})," dataset."]}),"\n",(0,t.jsx)(n.p,{children:"In today\u2019s world, it\u2019s hard to find a service, platform, or even a consumer good that doesn\u2019t have a legally-binding terms of service connected\nwith it. These encyclopedic size agreements, filled with dense legal jargon and sometimes baffling levels of specificity, are so large that\nmost people simply accept them without reading them. However, reports have indicated over time that occasionally, some suspiciously unfair\nterms are embedded within them."}),"\n",(0,t.jsxs)(n.p,{children:["Addressing unfair clauses in Terms of Service (TOS) agreements through machine learning (ML) is particularly relevant due to the pressing\nneed for transparency and fairness in legal agreements that affect consumers. Consider the following clause from an example TOS\nagreement: ",(0,t.jsx)(n.strong,{children:'"We may revise these Terms from time to time. The changes will not be retroactive, and the most current version of the Terms, which will always..."'}),"\nThis clause stipulates that the service provider may suspend or terminate the service at any time for any reason,\nwith or without notice. Most people would consider this to be quite unfair."]}),"\n",(0,t.jsx)(n.p,{children:"While this sentence is buried quite deeply within a fairly dense document, an ML algorithm is not burdened by the exhaustion that a human\nwould have for combing through the text and identifying clauses that might seem a bit unfair. By automatically identifying potentially\nunfair clauses, a transformers-based Deep Learning (DL) model can help protect consumers from exploitative practices, ensure greater compliance with legal standards,\nand foster trust between service providers and users."}),"\n",(0,t.jsx)(n.p,{children:"A base pre-trained transformer model, without specialized fine-tuning, faces several challenges in accurately identifying unfair Terms of Service clauses.\nFirstly, it lacks the domain-specific knowledge essential for understanding complex legal language. Secondly, its training objectives are\ntoo general to capture the nuanced interpretation required for legal analysis. Lastly, it may not effectively recognize the subtle\ncontextual meanings that determine the fairness of contractual terms, making it less effective for this specialized task."}),"\n",(0,t.jsxs)(n.p,{children:["Using prompt engineering to address the identification of unfair Terms of Service clauses with a closed-source Large language model\ncan be prohibitively expensive. This approach requires extensive trial and error to refine prompts without the ability to tweak\nthe underlying model mechanics. Each iteration can consume significant computational resources , especially when using\n",(0,t.jsx)(n.a,{href:"https://www.promptingguide.ai/techniques/fewshot",children:"few-shot prompting"}),", leading to escalating costs without guaranteeing a corresponding\nincrease in accuracy or effectiveness."]}),"\n",(0,t.jsxs)(n.p,{children:["In this context, the use of the ",(0,t.jsx)(n.strong,{children:"RoBERTa-base"})," model is particularly effective, provided that it is fine-tuned. This model is robust\nenough to handle complex tasks like discerning embedded instructions within texts, yet it is sufficiently compact to be fine-tuned\non modest hardware, such as an Nvidia T4 GPU."]}),"\n",(0,t.jsx)(n.h3,{id:"what-is-peft",children:"What is PEFT?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/peft/en/index",children:"Parameter-Efficient Fine-Tuning (PEFT)"})," approaches are advantageous as they involve\nkeeping the bulk of the pre-trained model parameters fixed while either only training a few additional layers or modifying the parameters used\nwhen interacting with the model's weights. This methodology not only conserves memory during training, but also significantly reduces the overall training time. When\ncompared with the alternative of fine-tuning a base model's weights in order to customize its performance for a specific targeted task, the PEFT\napproach can save significant cost in both time and money, while providing an equivalent or better performance results with less data than is required\nfor a comprehensive fine-tuning training task."]}),"\n",(0,t.jsx)(n.h2,{id:"integrating-hugging-face-models-and-the-pytorch-lightning-framework",children:"Integrating Hugging-Face models and the PyTorch Lightning framework"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://lightning.ai/docs/pytorch/stable/",children:"PyTorch Lightning"})," integrates seamlessly with\n",(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/index",children:"Hugging Face's Transformers library"}),", enabling streamlined model training workflows\nthat capitalize on Lightning's easy-to-use Higher level API\u2019s and HF's state-of-the-art pre-trained models. The combination of Lightning with transformers\u2019\n",(0,t.jsx)(n.a,{href:"https://huggingface.co/blog/peft",children:"PEFT module"})," enhances productivity and scalability by reducing code complexity and enabling the use of\nhigh-quality pre-optimized models for a range of diverse NLP tasks."]}),"\n",(0,t.jsxs)(n.p,{children:["Below is an example of configuring the PEFT-based fine tuning of a base model using PyTorch Lightning and HuggingFace's ",(0,t.jsx)(n.code,{children:"peft"})," module."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from typing import List\nfrom lightning import LightningModule\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForSequenceClassification\n\n\nclass TransformerModule(LightningModule):\n    def __init__(\n        self,\n        pretrained_model: str,\n        num_classes: int = 2,\n        lora_alpha: int = 32,\n        lora_dropout: float = 0.1,\n        r: int = 8,\n        lr: float = 2e-4\n    ):\n        super().__init__()\n        self.model = self.create_model(pretrained_model, num_classes, lora_alpha, lora_dropout, r)\n        self.lr = lr\n        self.save_hyperparameters("pretrained_model")\n\n    def create_model(self, pretrained_model, num_classes, lora_alpha, lora_dropout, r):\n        """Create and return the PEFT model with the given configuration.\n\n        Args:\n            pretrained_model: The path or identifier for the pretrained model.\n            num_classes: The number of classes for the sequence classification.\n            lora_alpha: The alpha parameter for LoRA.\n            lora_dropout: The dropout rate for LoRA.\n            r: The rank of LoRA adaptations.\n\n        Returns:\n            Model: A model configured with PEFT.\n        """\n        model = AutoModelForSequenceClassification.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model,\n            num_labels=num_classes\n        )\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_CLS,\n            inference_mode=False,\n            r=r,\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout\n        )\n        return get_peft_model(model, peft_config)\n\n    def forward(self, input_ids: List[int], attention_mask: List[int], label: List[int]):\n        """Calculate the loss by passing inputs to the model and comparing against ground truth labels.\n\n        Args:\n            input_ids: List of token indices to be fed to the model.\n            attention_mask: List to indicate to the model which tokens should be attended to, and which should not.\n            label: List of ground truth labels associated with the input data.\n\n        Returns:\n            torch.Tensor: The computed loss from the model as a tensor.\n        """\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=label\n        )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Additional references for the full implementation can be ",(0,t.jsx)(n.a,{href:"https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/custom_module/fine_tune_clsify_head.py",children:"seen within the companion repository here"})]}),"\n",(0,t.jsx)(n.h2,{id:"configuring-mlflow-for-peft-based-fine-tuning",children:"Configuring MLflow for PEFT-based fine-tuning"}),"\n",(0,t.jsx)(n.p,{children:"Before initiating the training process, it's crucial to configure MLFlow so that all system metrics, loss metrics, and parameters are logged for the training run.\nAs of MLFlow 2.12, auto-logging for TensorFlow and PyTorch now includes support for checkpointing model weights during training, giving a snapshot of the model\nweights at defined epoch frequencies in order to provide for training resumption in the case of an error or loss of the compute environment.\nBelow is an example of how to enable this feature:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import mlflow\n\n\nmlflow.enable_system_metrics_logging()\nmlflow.pytorch.autolog(checkpoint_save_best_only = False, checkpoint_save_freq='epoch')\n"})}),"\n",(0,t.jsx)(n.p,{children:"In the code above we are doing the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Enabling System Metrics Logging"}),": The system resources will be logged to MLflow in order to understand where bottlenecks in memory, CPU, GPU, disk usage, and network traffic are throughout the training process."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLflow UI System Metrics",src:i(1329).Z+"",width:"1600",height:"866"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configuring Auto Logging to log parameters, metrics and checkpoints for all epochs"}),": Deep learning involves experimenting with various model architectures and hyperparameter settings. Auto logging plays a crucial role in systematically recording these experiments, making it easier to compare different runs and determine which configurations yield the best results. Checkpoints are logged at every epoch, enabling detailed evaluations of all intermediate epochs during the initial exploration phase of the project. However, it is generally not advisable to log all epochs during late-stage development to avoid excessive data writes and latency in the final training stages."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"System Metrics Logged",src:i(5700).Z+"",width:"1600",height:"864"})}),"\n",(0,t.jsx)(n.p,{children:"The auto-logged checkpoint metrics and model artifacts will be viewable in the MLflow UI as the model trains, as shown below:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Metrics logged with each epoch",src:i(7339).Z+"",width:"1600",height:"731"})}),"\n",(0,t.jsx)(n.h2,{id:"the-importance-of-logging-and-early-stopping",children:"The Importance of Logging and Early-stopping"}),"\n",(0,t.jsxs)(n.p,{children:["The integration of the Pytorch Lightning ",(0,t.jsx)(n.code,{children:"Trainer"})," callback with MLflow is crucial within this training exercise. The integration allows for comprehensive\ntracking and logging of metrics, parameters, and artifacts during model finetuning without having to explicitly call MLflow logging APIs. Additionally,\nthe autologging API allows for modifying the default logging behavior, permitting changes to the logging frequency, allowing for logging to occur at each\nepoch, after a specified number of epochs, or at explicitly defined steps."]}),"\n",(0,t.jsx)(n.h3,{id:"early-stopping",children:"Early stopping"}),"\n",(0,t.jsx)(n.p,{children:"Early stopping is a critical regularization technique in neural network training, designed to assist in preventing overfitting through the act of\nhalting training when validation performance plateaus. Pytorch Lightning includes APIs that allow for an easy high-level control of training cessation,\nas demonstrated below."}),"\n",(0,t.jsx)(n.h3,{id:"configuring-pytorch-trainer-callback-with-early-stopping",children:"Configuring Pytorch Trainer Callback with Early stopping"}),"\n",(0,t.jsxs)(n.p,{children:["The example below shows the configuration of the ",(0,t.jsx)(n.code,{children:"Trainer"})," object within ",(0,t.jsx)(n.code,{children:"Lightning"})," to leverage early stopping to prevent overfitting. Once configured, the\ntraining is executed by calling ",(0,t.jsx)(n.code,{children:"fit"})," on the ",(0,t.jsx)(n.code,{children:"Trainer"})," object. By providing the ",(0,t.jsx)(n.code,{children:"EarlyStopping"})," callback, in conjunction with MLflow's autologging, the\nappropriate number of epochs will be used, logged, and tracked without any additional effort."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass, field\nimport os\n\nfrom data import LexGlueDataModule\nfrom lightning import Trainer\nfrom lightning.pytorch.callbacks import EarlyStopping\nimport mlflow\n\n\n@dataclass\nclass TrainConfig:\n    pretrained_model: str = "bert-base-uncased"\n    num_classes: int = 2\n    lr: float = 2e-4\n    max_length: int = 128\n    batch_size: int = 256\n    num_workers: int = os.cpu_count()\n    max_epochs: int = 10\n    debug_mode_sample: int | None = None\n    max_time: dict[str, float] = field(default_factory=lambda: {"hours": 3})\n    model_checkpoint_dir: str = "/local_disk0/tmp/model-checkpoints"\n    min_delta: float = 0.005\n    patience: int = 4\n\ntrain_config = TrainConfig()\n\n# Instantiate the custom Transformer class for PEFT training\nnlp_model = TransformerModule(\n        pretrained_model=train_config.pretrained_model,\n        num_classes=train_config.num_classes,\n        lr=train_config.lr,\n    )\n\ndatamodule = LexGlueDataModule(\n        pretrained_model=train_config.pretrained_model,\n        max_length=train_config.max_length,\n        batch_size=train_config.batch_size,\n        num_workers=train_config.num_workers,\n        debug_mode_sample=train_config.debug_mode_sample,\n    )\n\n# Log system metrics while training loop is running\nmlflow.enable_system_metrics_logging()\n\n# Automatically log per-epoch parameters, metrics, and checkpoint weights\nmlflow.pytorch.autolog(checkpoint_save_best_only = False)\n\n# Define the Trainer configuration\ntrainer = Trainer(\n   callbacks=[\n       EarlyStopping(\n           monitor="Val_F1_Score",\n           min_delta=train_config.min_delta,\n           patience=train_config.patience,\n           verbose=True,\n           mode="max",\n       )\n   ],\n   default_root_dir=train_config.model_checkpoint_dir,\n   fast_dev_run=bool(train_config.debug_mode_sample),\n   max_epochs=train_config.max_epochs,\n   max_time=train_config.max_time,\n   precision="32-true"\n)\n\n# Execute the training run\ntrainer.fit(model=nlp_model, datamodule=datamodule)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visualization-and-sharing-capabilities-within-mlflow",children:"Visualization and Sharing Capabilities within MLflow"}),"\n",(0,t.jsx)(n.p,{children:"The newly introduced DL-specific visualization capabilities introduced in MLflow 2.12 enable you to make comparisons between different runs and artifacts over epochs.\nWhen comparing training runs, MLflow is capable of generating useful visualization that can be integrated into dashboards, facilitating\neasy sharing. Additionally, the centralized storage of metrics, in conjunction with parameters, allows for effective analysis of the training\nefficacy, as shown in the image below."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Epoch Run Compare",src:i(5690).Z+"",width:"1600",height:"820"})}),"\n",(0,t.jsx)(n.h2,{id:"when-to-stop-training",children:"When to stop training?"}),"\n",(0,t.jsx)(n.p,{children:"When training DL models, it is important to understand when to stop. Efficient training (for minimizing the overall cost incurred for\nconducting training) and optimal model performance rely heavily on preventing a model from overfitting on the training data. A model\nthat trains for too long will invariably become quite good at effectively \u2018memorizing\u2019 the training data, resulting in a reduction in\nthe performance of the model when presented with novel data. A straightforward way to evaluate this behavior is to ensure that\nvalidation data set metrics (scoring loss metrics on data that is not in the training data set) are captured during the training\nloop. Integrating the MLflow callback into the PyTorch Lightning Trainer allows for iterative logging of loss metrics at\nconfigurable iterations, enabling an easily debuggable evaluation of the training performance, ensuring that stopping criteria\ncan be enforced at the appropriate time to prevent overfitting."}),"\n",(0,t.jsx)(n.h3,{id:"evaluating-epoch-checkpoints-of-fine-tuned-models-with-mlflow",children:"Evaluating epoch checkpoints of Fine Tuned Models with MLflow"}),"\n",(0,t.jsxs)(n.p,{children:["With your training process meticulously tracked and logged by MLflow, you have the flexibility to retrieve and test your model at\nany arbitrary checkpoint. To do this, you can use the mlflow.pytorch.load_model() API to load the model from a specific run\nand use the ",(0,t.jsx)(n.code,{children:"predict()"})," method for evaluation."]}),"\n",(0,t.jsxs)(n.p,{children:["In the example below, we will load the model checkpoint from the 3rd epoch and use the ",(0,t.jsx)(n.code,{children:"Lightning"})," train module to generate predictions based on the\ncheckpoint state of the saved training epoch."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n\nmlflow.pytorch.autolog(disable = True)\n\nrun_id = \'<Add the run ID>\'\n\nmodel = mlflow.pytorch.load_checkpoint(TransformerModule, run_id, 3)\n\nexamples_to_test = ["We reserve the right to modify the service price at any time and retroactively apply the adjusted price to historical service usage."]\n\ntrain_module = Trainer()\ntokenizer = AutoTokenizer.from_pretrained(train_config.pretrained_model)\ntokens = tokenizer(examples_to_test,\n                  max_length=train_config.max_length,\n                  padding="max_length",\n                  truncation=True)\nds = Dataset.from_dict(dict(tokens))\nds.set_format(\n            type="torch", columns=["input_ids", "attention_mask"]\n        )\n\ntrain_module.predict(model, dataloaders = DataLoader(ds))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"The integration of MLflow into the finetuning process of pre-trained language models, particularly for applications like custom\nnamed entity recognition, text classification and instruction-following represents a significant advancement in managing and\noptimizing deep learning workflows. Leveraging the autologging and tracking capabilities of MLflow in these workstreams not only\nenhances the reproducibility and efficiency of model development, but also fosters a collaborative environment where insights\nand improvements can be easily shared and implemented."}),"\n",(0,t.jsx)(n.p,{children:"As we continue to push the boundaries of what these models can achieve, tools like MLflow will be instrumental in harnessing their full potential."}),"\n",(0,t.jsxs)(n.p,{children:["If you're interested in seeing the full example in its entirety, feel free to ",(0,t.jsx)(n.a,{href:"https://github.com/puneet-jain159/deeplearning_with_mlfow",children:"see the full example implementation"})]}),"\n",(0,t.jsx)(n.h3,{id:"check-out-the-code",children:"Check out the code"}),"\n",(0,t.jsxs)(n.p,{children:["The code we provide will delve into additional aspects such as training from a checkpoint, integrating MLflow and TensorBoard, and utilizing Pyfunc for model wrapping, among others. These resources are specifically tailored for implementation on ",(0,t.jsx)(n.a,{href:"https://mlflow.org/blog/databricks-ce",children:"Databricks Community Edition"}),". The main runner notebook\nwithin the full example repository ",(0,t.jsx)(n.a,{href:"https://github.com/puneet-jain159/deeplearning_with_mlfow/blob/master/train.ipynb",children:"can be found here"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"get-started-with-mlflow-212-today",children:"Get Started with MLflow 2.12 Today"}),"\n",(0,t.jsxs)(n.p,{children:["Dive into the latest MLflow updates today and enhance the way you manage your machine learning projects! With our newest enhancements,\nincluding advanced metric aggregation, automatic capturing of system metrics, intuitive feature grouping, and streamlined search capabilities,\nMLflow is here to elevate your ML workflow to new heights. ",(0,t.jsx)(n.a,{href:"https://mlflow.org/releases/2.12.1",children:"Get started now with MLflow's cutting-edge tools and features"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"feedback",children:"Feedback"}),"\n",(0,t.jsxs)(n.p,{children:["We value your input! Our feature prioritization is guided by feedback from the MLflow late 2023 survey. Please fill out our\n",(0,t.jsx)(n.a,{href:"https://surveys.training.databricks.com/jfe/form/SV_3jGIliwGC0g5xTU",children:"Spring 2024 survey"}),", and by participating, you can help ensure that the features\nyou want most are implemented in MLflow."]})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},7339:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/checkpoint_metrics-ad7d10db8f81555e3ab4d04c8fb63240.png"},5690:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/compare-ea0bead7c1244ca8868d1f039c791ac4.png"},5700:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/epoch_logging-c21dd79a1004d20b4804923690472a18.png"},1329:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/sys_metrics-9423a86b67b9aeeac27ee5aea7be01e2.png"},1151:(e,n,i)=>{i.d(n,{Z:()=>s,a:()=>r});var t=i(7294);const a={},o=t.createContext(a);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);