"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4439],{82005:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>c,toc:()=>d});var a=n(85893),i=n(11151),r=n(74866),l=n(85162);const o={title:"Practical AI Observability: Getting Started with MLflow Tracing",description:"A practical guide to implementing AI tracing in your GenAI applications",slug:"ai-observability-mlflow-tracing",authors:["daniel-liden"],tags:["genai","observability","tracing"],thumbnail:"/img/blog/tracing-intro-thumbnail.png"},s=void 0,c={permalink:"/mlflow-website/blog/ai-observability-mlflow-tracing",source:"@site/blog/2025-03-06-tracing-introduction/index.mdx",title:"Practical AI Observability: Getting Started with MLflow Tracing",description:"A practical guide to implementing AI tracing in your GenAI applications",date:"2025-03-06T00:00:00.000Z",formattedDate:"March 6, 2025",tags:[{label:"genai",permalink:"/mlflow-website/blog/tags/genai"},{label:"observability",permalink:"/mlflow-website/blog/tags/observability"},{label:"tracing",permalink:"/mlflow-website/blog/tags/tracing"}],readingTime:6.95,hasTruncateMarker:!1,authors:[{name:"Daniel Liden",title:"Developer Advocate at Databricks",url:"https://www.linkedin.com/in/danielliden",imageURL:"/img/authors/daniel_liden.png",key:"daniel-liden"}],frontMatter:{title:"Practical AI Observability: Getting Started with MLflow Tracing",description:"A practical guide to implementing AI tracing in your GenAI applications",slug:"ai-observability-mlflow-tracing",authors:["daniel-liden"],tags:["genai","observability","tracing"],thumbnail:"/img/blog/tracing-intro-thumbnail.png"},unlisted:!1,prevItem:{title:"Automatically find the bad LLM responses in your LLM Evals with Cleanlab",permalink:"/mlflow-website/blog/tlm-tracing"},nextItem:{title:"Beyond Autolog: Add MLflow Tracing to a New LLM Provider",permalink:"/mlflow-website/blog/custom-tracing"}},h={authorsImageUrls:[void 0]},d=[{value:"MLflow Tracing: Observability for GenAI",id:"mlflow-tracing-observability-for-genai",level:2},{value:"Quickstart",id:"quickstart",level:2},{value:"Collecting Traces with Autologging",id:"collecting-traces-with-autologging",level:3},{value:"Viewing your LLM Traces",id:"viewing-your-llm-traces",level:3},{value:"Organizing your Traces",id:"organizing-your-traces",level:2},{value:"Tracing Other Providers",id:"tracing-other-providers",level:2},{value:"Conclusion: Effective LLM tracing with one line of code",id:"conclusion-effective-llm-tracing-with-one-line-of-code",level:2},{value:"Next Steps",id:"next-steps",level:3}];function u(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"mlflow-tracing-observability-for-genai",children:"MLflow Tracing: Observability for GenAI"}),"\n",(0,a.jsx)(t.p,{children:"GenAI providers and frameworks often respond with complicated and hard-to-read data structures or with simple responses that hide intermediate steps. Furthermore, it can be hard to keep track of and compare GenAI model/framework calls over time, especially if you are moving between frameworks and scripts."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html",children:"MLflow's LLM tracing"})," solves these issues by recording all of your GenAI calls, including both individual LLM calls and multi-step agentic workflows, and providing an easy-to-read interface for browsing and comparing them. You can enable this functionality for most GenAI providers with a single line of code: ",(0,a.jsx)(t.code,{children:"mlflow.<provider>.autolog()"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"This blog will show how to get started with MLflow tracing\u2014in about five minutes. It assumes some familiarity with GenAI APIs (e.g. the OpenAI API), but does not assume any prior familiarity with MLflow."}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"/img/blog/tracing-intro/05_langchain.png",alt:"Alt text"}),(0,a.jsx)("figcaption",{children:(0,a.jsx)("i",{children:"Tracing a LangChain Application with MLflow Tracing"})})]}),"\n",(0,a.jsx)(t.h2,{id:"quickstart",children:"Quickstart"}),"\n",(0,a.jsxs)(t.p,{children:["We'll start by showing how to use MLflow autologging to automatically trace calls to OpenAI models, though MLflow supports automatic tracing for an ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing",children:"ever-growing number of providers and frameworks"})," including Anthropic, Ollama, Langchain, LlamaIndex, and may others. To get started, install the MLflow and OpenAI Python packages with:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"pip install mlflow openai\n"})}),"\n",(0,a.jsx)(t.h3,{id:"collecting-traces-with-autologging",children:"Collecting Traces with Autologging"}),"\n",(0,a.jsxs)(t.p,{children:["In a Python script or notebook, import MLflow and the GenAI provider you're working with, and enable tracing with ",(0,a.jsx)(t.code,{children:"mlflow.<provider>.autolog"}),". Here's how to set up automatic tracing for OpenAI:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"import mlflow\nfrom openai import OpenAI\n\nmlflow.openai.autolog()\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Make sure to ",(0,a.jsx)(t.a,{href:"https://platform.openai.com/docs/quickstart?language=python#create-and-export-an-api-key",children:"create and set your OpenAI API key"}),"! You can set it in your environment with"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:'export OPENAI_API_KEY="your_api_key_here"\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Now, when you use the OpenAI library, MLflow will capture ",(0,a.jsx)(t.em,{children:"traces"})," of your model calls. For example, MLflow will log a trace of the following OpenAI invocation because we have enabled autologging."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'client = OpenAI()\n\ncompletion = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {\n            "role": "user",\n            "content": "What is an MLflow tracking server?"\n        }\n    ]\n)\n'})}),"\n",(0,a.jsx)(t.h3,{id:"viewing-your-llm-traces",children:"Viewing your LLM Traces"}),"\n",(0,a.jsx)(t.p,{children:"The MLflow UI provides an AI observability dashboard for viewing your traces. Start the MLflow UI from your terminal with:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"mlflow ui\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Navigate to the UI. the output of the ",(0,a.jsx)(t.code,{children:"mlflow ui"})," command will tell you where to go (",(0,a.jsx)(t.code,{children:"http://localhost:5000"}),' by default). In the UI, navigate to the "Traces" tab. This will list all of the collected traces. Click on a trace\'s Trace ID to open up a new pane with more details.']}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Traces in the MLflow UI",src:n(4257).Z+"",width:"1280",height:"738"})}),"\n",(0,a.jsx)(t.admonition,{type:"tip",children:(0,a.jsxs)(t.p,{children:["By default, the MLflow server will listen on ",(0,a.jsx)(t.code,{children:"http://localhost:5000"}),". You can choose a different port with ",(0,a.jsx)(t.code,{children:"mlflow ui -p <port>"}),". For example, to listen on port 5001, use ",(0,a.jsx)(t.code,{children:"mlflow ui -p 5001"}),"."]})}),"\n",(0,a.jsxs)(t.p,{children:["Starting the MLflow tracking server with ",(0,a.jsx)(t.code,{children:"mlflow ui"})," also enables you to ",(0,a.jsx)(t.a,{href:"https://mlflow.org/blog/mlflow-tracing-in-jupyter",children:"view traces right in a Jupyter notebook"}),"! You just have to set the tracking uri to the location specified above:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'mlflow.set_tracking_uri("http://localhost:5000")\n'})}),"\n",(0,a.jsx)(t.p,{children:"Then, when you invoke an AI model/framework with tracing enabled, the generated trace(s) will appear right in the notebook outputs."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Tracing in Jupyter Notebooks",src:n(60726).Z+"",width:"1280",height:"746"})}),"\n",(0,a.jsxs)(t.p,{children:["You can disable this functionality with ",(0,a.jsx)(t.code,{children:"mlflow.tracing.disable_notebook_display()"}),"."]}),"\n",(0,a.jsx)(t.h2,{id:"organizing-your-traces",children:"Organizing your Traces"}),"\n",(0,a.jsx)(t.p,{children:"If you use tracing across multiple different projects and tasks, you might want to organize the traces into separate groups."}),"\n",(0,a.jsxs)(t.p,{children:["The easiest way to organize your traces is to separate them into ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracking.html#experiments",children:(0,a.jsx)(t.em,{children:"experiments"})}),". Each experiment has its own traces tab, which displays the traces for that experiment."]}),"\n",(0,a.jsx)(t.p,{children:'You can create an experiment in the UI (With the "+" button next to "Experiments"), with the MLflow CLI, or with Python. Let\'s create a new experiment called "Quickstart" and log a trace.'}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'mlflow.set_experiment("quickstart")\n\ncompletion = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {\n            "role": "user",\n            "content": "What is an MLflow tracking server?"\n        }\n    ]\n)\n'})}),"\n",(0,a.jsx)(t.p,{children:'We can now find this trace in the "Traces" tab in the "quickstart" experiment.'}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Trace in Experiment",src:n(70930).Z+"",width:"2444",height:"1438"})}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"set_experiment"}),' function specifies which experiment traces should be logged to, creating it if it does not exist, so the code snippet above created a new "quickstart" experiment.']}),"\n",(0,a.jsxs)(t.p,{children:["You can also organize your traces with ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing#trace-tags",children:"tags"})," and ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing#q-how-can-i-associate-a-trace-with-an-mlflow-run",children:"runs"}),"."]}),"\n",(0,a.jsx)(t.h2,{id:"tracing-other-providers",children:"Tracing Other Providers"}),"\n",(0,a.jsxs)(t.p,{children:["Our quickstart example focused on OpenAI, but MLflow supports automatic tracing of ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing#automatic-tracing",children:"many different AI providers and frameworks"}),". The approach is the same: just add the line ",(0,a.jsx)(t.code,{children:"mlflow.<provider>.autolog"})," to your notebook or script."]}),"\n",(0,a.jsxs)(t.p,{children:["Here are a few examples. See ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing#automatic-tracing",children:"here"})," for the full list of supported providers."]}),"\n",(0,a.jsxs)(r.Z,{children:[(0,a.jsxs)(l.Z,{value:"Anthropic",label:"Anthropic",default:!0,children:[(0,a.jsxs)(t.p,{children:["Enable automatic tracing for Anthropic model calls with ",(0,a.jsx)(t.code,{children:"mlflow.anthropic.autolog()"}),"."]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import anthropic\nimport mlflow\n\nmlflow.anthropic.autolog()\n\nclient = anthropic.Anthropic()\n\nmessage = client.messages.create(\n    model="claude-3-7-sonnet-20250219",\n    max_tokens=1000,\n    temperature=1,\n    messages=[\n        {\n            "role": "user",\n            "content": "What is an MLflow tracking server?"\n        }\n    ]\n)\n\n'})}),(0,a.jsx)(t.p,{children:"This returns the following in the MLflow UI:"}),(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Anthropic tracing",src:n(15202).Z+"",width:"2516",height:"1936"})})]}),(0,a.jsxs)(l.Z,{value:"langchain",label:"LangChain",children:[(0,a.jsxs)(t.p,{children:["Enable automatic tracing for LangChain and LangGraph with ",(0,a.jsx)(t.code,{children:"mlflow.langchain.autolog()"}),". MLflow automatic tracing captures all LangChain component executions, including chains, LLMs, agents, tools, prompts, and retrievers."]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import mlflow\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\nmlflow.set_experiment("quickstart")\n\nmlflow.langchain.autolog()\n\nllm = ChatOpenAI(model="gpt-4o-mini", temperature=1, max_tokens=500)\n\nprompt_template = PromptTemplate.from_template(\n    "Explain the following MLflow concept at the specified technical level. "\n    "For \'beginner\', use simple analogies and avoid complex terms. "\n    "For \'intermediate\', include more technical details and some code examples. "\n    "For \'advanced\', go deep into implementation details and provide comprehensive explanations. "\n    "Technical level: {level}. Question: {question}"\n)\n\nchain = prompt_template | llm | StrOutputParser()\n\nchain.invoke(\n    {\n        "level": "beginner",\n        "question": "How do MLflow tracking servers help with experiment management?",\n    }\n)\n'})}),(0,a.jsx)(t.p,{children:"This example LangChain chain includes multiple components:"}),(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["The ",(0,a.jsx)(t.code,{children:"PromptTemplate"}),", which assembles the prompt based on the user input"]}),"\n",(0,a.jsxs)(t.li,{children:["The ",(0,a.jsx)(t.code,{children:"ChatOpenAI"})," model, which is used to call the OpenAI ",(0,a.jsx)(t.code,{children:"gpt-4o-mini"})," model"]}),"\n",(0,a.jsxs)(t.li,{children:["The ",(0,a.jsx)(t.code,{children:"StrOutputParser"}),", which returns the final answer to the user's query as a string"]}),"\n"]}),(0,a.jsxs)(t.p,{children:["We can see each of these components in the MLflow UI, nested under the parent ",(0,a.jsx)(t.code,{children:"RunnableSequence"})," chain."]}),(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"LangChain Tracing",src:n(63535).Z+"",width:"2738",height:"2078"})})]}),(0,a.jsxs)(l.Z,{value:"ollama",label:"Ollama",children:[(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://ollama.com/",children:"Ollama"})," is a tool for running open source AI models locally. You can enable automatic tracing of Ollama models via Ollama's ",(0,a.jsx)(t.a,{href:"https://github.com/ollama/ollama/blob/main/docs/openai.md",children:"OpenAI-compatible API"})," and MLflow's OpenAI autologging. You just need to set the base URL to your Ollama REST endpoint."]}),(0,a.jsx)(t.p,{children:"This pattern should work with any provider that offers an OpenAI-compatible endpoint, even those that are not explicitly referenced in the docs."}),(0,a.jsx)(t.p,{children:"Here's how it works for Ollama:"}),(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsx)(t.li,{children:"First, run the Ollama server with your desired model."}),"\n"]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"ollama run phi3:latest\n"})}),(0,a.jsxs)(t.ol,{start:"2",children:["\n",(0,a.jsxs)(t.li,{children:["Configure the OpenAI client, setting the ",(0,a.jsx)(t.code,{children:"base_url"})," to the Ollama OpenAI-compatible endpoint."]}),"\n"]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:11434/v1",  # The local Ollama REST endpoint\n    api_key="dummy",  # Required to instantiate OpenAI client, it can be a random string\n)\n'})}),(0,a.jsxs)(t.ol,{start:"3",children:["\n",(0,a.jsx)(t.li,{children:"Enable MLflow OpenAI autologging and query the model"}),"\n"]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'mlflow.openai.autolog()\n\ncompletion = client.chat.completions.create(\n    model="phi3:latest",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is an MLflow tracking server?"}\n    ]\n)\n'})}),(0,a.jsx)(t.p,{children:"Here is the trace of the Ollama model call in the MLflow UI."}),(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Ollama Tracing",src:n(49916).Z+"",width:"2738",height:"2078"})})]})]}),"\n",(0,a.jsx)(t.h2,{id:"conclusion-effective-llm-tracing-with-one-line-of-code",children:"Conclusion: Effective LLM tracing with one line of code"}),"\n",(0,a.jsxs)(t.p,{children:["In this guide, you have learned how to use MLflow's autologging capabilities to get a complete AI observability solution with a single line of code. If you are using one of the ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing#automatic-tracing",children:"many GenAI frameworks/providers"})," for which MLflow offers automatic tracing\u2014including any providers with OpenAI-compatible endpoints\u2014automatic logging is the easiest way to visualize and debug your AI application behavior. All you need is ",(0,a.jsx)(t.code,{children:"mlflow.<provider>.autolog()"}),"."]}),"\n",(0,a.jsx)(t.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(t.p,{children:"Autologging is a great place to start with MLflow tracing, but you may need more flexibility in how you collect and use traces as you develop more complex GenAI applications. Furthermore, MLflow includes many tools for working with GenAI applications beyond tracing."}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["For a longer conceptual introduction to tracing, read ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing/",children:"this guide"})," on tracing concepts."]}),"\n",(0,a.jsxs)(t.li,{children:["MLflow traces can provide an excellent source of data for evaluation, SME review, fine-tuning, and more. Learn about searching and retrieving trace data ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing/api/search",children:"here"}),"."]}),"\n",(0,a.jsxs)(t.li,{children:["MLflow provides ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/llm-evaluate/index.html",children:"LLM evaluation functionality"})," for running structured experiments with your AI models and applications."]}),"\n",(0,a.jsxs)(t.li,{children:["You can add tracing to your own AI applications with the tracing ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing/api/manual-instrumentation",children:"fluent APIs"})," and ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/tracing/api/client",children:"client APIs"}),". You can also ",(0,a.jsx)(t.a,{href:"https://mlflow.org/blog/custom-tracing",children:"add tracing to libraries and frameworks"})," that do not (yet) have autologging support."]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},85162:(e,t,n)=>{n.d(t,{Z:()=>l});n(67294);var a=n(36905);const i={tabItem:"tabItem_Ymn6"};var r=n(85893);function l(e){let{children:t,hidden:n,className:l}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,a.Z)(i.tabItem,l),hidden:n,children:t})}},74866:(e,t,n)=>{n.d(t,{Z:()=>y});var a=n(67294),i=n(36905),r=n(12466),l=n(16550),o=n(20469),s=n(91980),c=n(67392),h=n(50012);function d(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return d(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:i}}=e;return{value:t,label:n,attributes:a,default:i}}))}(n);return function(e){const t=(0,c.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function p(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:n}=e;const i=(0,l.k6)(),r=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,s._X)(r),(0,a.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(i.location.search);t.set(r,e),i.replace({...i.location,search:t.toString()})}),[r,i])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:i}=e,r=u(e),[l,s]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!p({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:r}))),[c,d]=m({queryString:n,groupId:i}),[g,f]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[i,r]=(0,h.Nk)(n);return[i,(0,a.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:i}),w=(()=>{const e=c??g;return p({value:e,tabValues:r})?e:null})();(0,o.Z)((()=>{w&&s(w)}),[w]);return{selectedValue:l,selectValue:(0,a.useCallback)((e=>{if(!p({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),f(e)}),[d,f,r]),tabValues:r}}var f=n(72389);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=n(85893);function b(e){let{className:t,block:n,selectedValue:a,selectValue:l,tabValues:o}=e;const s=[],{blockElementScrollPositionUntilNextRender:c}=(0,r.o5)(),h=e=>{const t=e.currentTarget,n=s.indexOf(t),i=o[n].value;i!==a&&(c(t),l(i))},d=e=>{let t=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const n=s.indexOf(e.currentTarget)+1;t=s[n]??s[0];break}case"ArrowLeft":{const n=s.indexOf(e.currentTarget)-1;t=s[n]??s[s.length-1];break}}t?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":n},t),children:o.map((e=>{let{value:t,label:n,attributes:r}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>s.push(e),onKeyDown:d,onClick:h,...r,className:(0,i.Z)("tabs__item",w.tabItem,r?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function v(e){let{lazy:t,children:n,selectedValue:i}=e;const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=r.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:r.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==i})))})}function j(e){const t=g(e);return(0,x.jsxs)("div",{className:(0,i.Z)("tabs-container",w.tabList),children:[(0,x.jsx)(b,{...e,...t}),(0,x.jsx)(v,{...e,...t})]})}function y(e){const t=(0,f.Z)();return(0,x.jsx)(j,{...e,children:d(e.children)},String(t))}},4257:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/01_tracing_ui-8e76a8ca87a7fe97bf0131dafe98b57d.gif"},60726:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/02_jupyter-5d2a220fed5d83148404a27d50b2c3af.gif"},70930:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/03_experiment-bc7a7c3bfd2722df1c738a478d29b1a1.png"},15202:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/04_anthropic-6794542fdcd30d9d698a3b6ea186855a.png"},63535:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/05_langchain-1dfddf4fb18ed0a80bf6b419fca3532f.png"},49916:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/06_ollama-8e0d6bf2dac8ef784677e50d09a3fa19.png"},11151:(e,t,n)=>{n.d(t,{Z:()=>o,a:()=>l});var a=n(67294);const i={},r=a.createContext(i);function l(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),a.createElement(r.Provider,{value:t},e.children)}}}]);