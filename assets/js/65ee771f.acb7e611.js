"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2293],{24237:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/3_log_outputs-653a54208f3f6c1e5ffc1efac431fa99.png"},28453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>i});var a=n(96540);const s={},o=a.createContext(s);function l(e){const t=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),a.createElement(o.Provider,{value:t},e.children)}},42052:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/6_tool_call-292eaae4c2bcc07d0215b3bcdc349714.png"},63407:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>a,toc:()=>c});var a=n(96580),s=n(74848),o=n(28453);const l={title:"Beyond Autolog: Add MLflow Tracing to a New LLM Provider",tags:["genai","tracing","ollama"],slug:"custom-tracing",authors:["daniel-liden"],thumbnail:"/img/blog/tracing-new-provider.png"},i=void 0,r={authorsImageUrls:[void 0]},c=[{value:"Adding MLflow Tracing to a New Provider: General Principles",id:"adding-mlflow-tracing-to-a-new-provider-general-principles",level:2},{value:"Adding tracing to the Ollama Python SDK",id:"adding-tracing-to-the-ollama-python-sdk",level:2},{value:"Step 1: Install and Test the Ollama Python SDK",id:"step-1-install-and-test-the-ollama-python-sdk",level:3},{value:"Step 2: Write a Tracing Decorator",id:"step-2-write-a-tracing-decorator",level:3},{value:"Step 3: Patch the <code>chat</code> method and try it out",id:"step-3-patch-the-chat-method-and-try-it-out",level:3},{value:"Tracing Tools and Tool Calls",id:"tracing-tools-and-tool-calls",level:2},{value:"Orchestration: Building a tool calling loop",id:"orchestration-building-a-tool-calling-loop",level:2},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["In this post, we will show how to add MLflow Tracing to a new LLM provider by adding tracing support to the ",(0,s.jsx)(t.code,{children:"chat"})," method of the Ollama Python SDK."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html",children:"MLflow Tracing"})," is an observability tool in MLflow that captures detailed execution traces for GenAI applications and workflows. In addition to inputs, outputs, and metadata for individual calls, MLflow tracing can also capture intermediate steps such as tool calls, reasoning steps, retrieval steps, or other custom steps."]}),"\n",(0,s.jsxs)(t.p,{children:["MLflow provides ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing",children:"built-in Tracing support"})," for many popular LLM providers and orchestration frameworks. If you are using one of these providers, you can enable tracing with a single line of code: ",(0,s.jsx)(t.code,{children:"mlflow.<provider>.autolog()"}),". While MLflow's autologging capabilities cover many of the most widely-used LLM providers and orchestration frameworks, there may be times when you need to add tracing to an unsupported provider or customize tracing beyond what autologging provides. This post demonstrates how flexible and extensible MLflow Tracing can be by:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Adding basic tracing support to an unsupported provider (the Ollama Python SDK)"}),"\n",(0,s.jsx)(t.li,{children:"Showing how to capture both simple completions and more complex tool-calling workflows"}),"\n",(0,s.jsx)(t.li,{children:"Illustrating how tracing can be added with minimal changes to existing code"}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["We'll use ",(0,s.jsx)(t.a,{href:"https://github.com/ollama/ollama-python",children:"the Ollama Python SDK"}),", an open-source Python SDK for the ",(0,s.jsx)(t.a,{href:"https://ollama.ai/",children:"Ollama"})," LLM platform, as our example. We'll work through the process step-by-step, showing how to capture the key information with MLflow tracing while maintaining a clean integration with the provider's SDK. Note that MLflow ",(0,s.jsx)(t.em,{children:"does"})," have autologging support for Ollama, but currently only for use via the OpenAI client, not directly with the Ollama Python SDK."]}),"\n",(0,s.jsx)(t.h2,{id:"adding-mlflow-tracing-to-a-new-provider-general-principles",children:"Adding MLflow Tracing to a New Provider: General Principles"}),"\n",(0,s.jsxs)(t.p,{children:["The MLflow docs have an ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/contribute.html",children:"excellent guide"})," to contributing to MLflow tracing. Though we will not be contributing to MLflow itself in this example, we will follow the same general principles."]}),"\n",(0,s.jsxs)(t.p,{children:["This post assumes that you have a basic understanding of what MLflow Tracing is and how it works. If you are just learning, or you need a refresher, take a look at the ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/overview.html",children:"Tracing Concepts"})," guide."]}),"\n",(0,s.jsx)(t.p,{children:"Adding tracing to a new provider involves a few key considerations:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Understand the Provider's key functionality:"})," We first need to understand what API methods need to be traced in order to get the tracing information we want. For LLM inference providers, this typically involves operations such as chat completions, tool calls, or embedding generation. In orchestration frameworks, this may involve operations such as retrieval, reasoning, routing, or any of a wide range of custom steps. In our Ollama example, we will focus on the chat completions API. This step will vary significantly depending on the provider."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Map operations to spans:"})," MLflow tracing uses different ",(0,s.jsx)(t.em,{children:"span types"})," to represent different types of operations. You can find descriptions of built-in span types ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html#span-type",children:"here"}),". Different span types are displayed differently in the MLflow UI and can enable specific functionality. Within spans, we also want to map the provider's inputs and outputs to the formats expected by MLflow. MLflow offers utilities for recording chat and tool inputs and outputs, which are then displayed as formatted messages in the MLflow UI."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Chat Messages",src:n(97675).A+"",width:"1384",height:"1106"})}),"\n",(0,s.jsx)(t.p,{children:"When adding tracing to a new provider, our main task is to map the provider's API methods to MLflow Tracing spans with appropriate span types."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Structure and preserve key data:"})," For each operation we want to trace, we need to identify the key information we want to preserve and make sure it is captured and displayed in a useful way. For example, we may want to capture the input and configuration data that control the operation's behavior, the outputs and metadata that explain the results, errors that terminated the operation prematurely, etc. Looking at traces and tracing implementations for similar providers can provide a good starting point for how to structure and preserve these data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"adding-tracing-to-the-ollama-python-sdk",children:"Adding tracing to the Ollama Python SDK"}),"\n",(0,s.jsx)(t.p,{children:"Now that we have a high-level understanding of the key step of adding tracing to a new provider, let's work through the process and add tracing to the Ollama Python SDK."}),"\n",(0,s.jsx)(t.h3,{id:"step-1-install-and-test-the-ollama-python-sdk",children:"Step 1: Install and Test the Ollama Python SDK"}),"\n",(0,s.jsxs)(t.p,{children:["First, we need to install the Ollama Python SDK and figure out what methods we need to pay attention to when adding tracing support. You can install the Ollama Python SDK with ",(0,s.jsx)(t.code,{children:"pip install ollama-python"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"If you have used the OpenAI Python SDK, the Ollama Python SDK will feel quite familiar. Here's how we use it to make a chat completion call:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from ollama import chat\nfrom rich import print\n\nresponse = chat(model="llama3.2",\n     messages = [\n         {"role": "user", "content": "Briefly describe the components of an MLflow model"}\n     ]\n)\n\nprint(response)\n'})}),"\n",(0,s.jsx)(t.p,{children:"Which will return:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"ChatResponse(\n    model='llama3.2',\n    created_at='2025-01-30T15:57:39.097119Z',\n    done=True,\n    done_reason='stop',\n    total_duration=7687553708,\n    load_duration=823704250,\n    prompt_eval_count=35,\n    prompt_eval_duration=3414000000,\n    eval_count=215,\n    eval_duration=3447000000,\n    message=Message(\n        role='assistant',\n        content=\"In MLflow, a model consists of several key components:\\n\\n1. **Model Registry**: A centralized\nstorage for models, containing metadata such as the model's name, version, and description.\\n2. **Model Version**:\nA specific iteration of a model, represented by a unique version number. This can be thought of as a snapshot of\nthe model at a particular point in time.\\n3. **Model Artifacts**: The actual model code, parameters, and data used\nto train the model. These artifacts are stored in the Model Registry and can be easily deployed or reused.\\n4.\n**Experiment**: A collection of runs that use the same hyperparameters and model version to train and evaluate a\nmodel. Experiments help track progress, provide reproducibility, and facilitate collaboration.\\n5. **Run**: An\nindividual instance of training or testing a model using a specific experiment. Runs capture the output of each\nrun, including metrics such as accuracy, loss, and more.\\n\\nThese components work together to enable efficient\nmodel management, version control, and reproducibility in machine learning workflows.\",\n        images=None,\n        tool_calls=None\n    )\n)\n"})}),"\n",(0,s.jsxs)(t.p,{children:["We have verified that the Ollama Python SDK is set up and working. We also know what method we need to focus on when adding tracing support: ",(0,s.jsx)(t.code,{children:"ollama.chat"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"step-2-write-a-tracing-decorator",children:"Step 2: Write a Tracing Decorator"}),"\n",(0,s.jsxs)(t.p,{children:["There are several ways we could add tracing to Ollama's SDK\u2014we could modify the SDK code directly, create a wrapper class, or use Python's method patching capabilities. For this example, we'll use a decorator to patch the SDK's ",(0,s.jsx)(t.code,{children:"chat"})," method. This approach lets us add tracing without modifying the SDK code or creating additional wrapper classes, though it does require understanding both Python's decorator pattern and how MLflow tracing works."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import SpanType\nfrom mlflow.tracing.utils import set_span_chat_messages\nfrom functools import wraps\nfrom ollama import chat as ollama_chat\n\ndef _get_span_type(task_name: str) -> str:\n    span_type_mapping = {\n        "chat": SpanType.CHAT_MODEL,\n    }\n    return span_type_mapping.get(task_name, SpanType.UNKNOWN)\n\ndef trace_ollama_chat(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        with mlflow.start_span(\n            name="ollama.chat",\n            span_type=_get_span_type("chat"),\n        ) as span:\n            # Set model name as a span attribute\n            model_name = kwargs.get("model", "")\n            span.set_attribute("model_name", model_name)\n\n            # Log the inputs\n            input_messages = kwargs.get("messages", [])\n            span.set_inputs({\n                "messages": input_messages,\n                "model": model_name,\n            })\n\n            # Set input messages\n            set_span_chat_messages(span, input_messages)\n\n            # Make the API call\n            response = func(*args, **kwargs)\n\n            # Log the outputs\n            if hasattr(response, \'to_dict\'):\n                output = response.to_dict()\n            else:\n                output = response\n            span.set_outputs(output)\n\n            output_message = response.message\n\n            # Append the output message\n            set_span_chat_messages(span, [{"role": output_message.role, "content": output_message.content}], append=True)\n\n            return response\n    return wrapper\n'})}),"\n",(0,s.jsx)(t.p,{children:"Let's break down the code and see how it works."}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We start by defining a helper function, ",(0,s.jsx)(t.code,{children:"_get_span_type"}),", that maps Ollama methods to MLflow span types. This isn't strictly necessary as we are currently only tracing the ",(0,s.jsx)(t.code,{children:"chat"})," function, but it shows a pattern that could be applied to other methods. This follows the reference implementation for the ",(0,s.jsx)(t.a,{href:"https://github.com/mlflow/mlflow/blob/master/mlflow/anthropic/autolog.py",children:"Anthropic provider"}),", as recommended in the tracing contribution guide."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We define a decorator, ",(0,s.jsx)(t.code,{children:"trace_ollama_chat"}),", using ",(0,s.jsx)(t.a,{href:"https://docs.python.org/3/library/functools.html#functools.wraps",children:(0,s.jsx)(t.code,{children:"functools.wraps"})}),", that patches the ",(0,s.jsx)(t.code,{children:"chat"})," function. There are a few key steps here:"]}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We start a new span with ",(0,s.jsx)(t.code,{children:"mlflow.start_span"}),'. The span name is set to "ollama.chat" and the span type is set to the value returned by ',(0,s.jsx)(t.code,{children:"_get_span_type"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We set ",(0,s.jsx)(t.code,{children:"model_name"})," as an attribute on the span with ",(0,s.jsx)(t.code,{children:"span.set_attribute"}),". This isn't strictly necessary as model name will be captured in the inputs, but it illustrates how to set arbitrary attributes on a span."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We log the messages as inputs to the span with ",(0,s.jsx)(t.code,{children:"span.set_inputs"}),". We get these from the ",(0,s.jsx)(t.code,{children:"messages"})," argument by accessing the ",(0,s.jsx)(t.code,{children:"kwargs"}),' dictionary. These messages will be logged to the "inputs" section of the span in the MLflow UI. We also log the model name as an input, again to illustrate how to record arbitrary inputs.']}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Inputs",src:n(88615).A+"",width:"1446",height:"792"})}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We use MLflow's ",(0,s.jsx)(t.code,{children:"set_span_chat_messages"})," utility function to format the input messages in a way that will be displayed nicely in the MLflow UI's Chat panel. This helper ensures that the messages are properly formatted and displayed with appropriate styling for each message role."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We call the original function with ",(0,s.jsx)(t.code,{children:"func(*args, **kwargs)"}),". This is the Ollama ",(0,s.jsx)(t.code,{children:"chat"})," function."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We log the outputs of the function as a span attribute with ",(0,s.jsx)(t.code,{children:"span.set_outputs"}),'. This takes the response from the Ollama API and sets it as an attribute on the span. These outputs will be logged to the "outputs" section of the span in the MLflow UI.']}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Outputs",src:n(24237).A+"",width:"1434",height:"1294"})}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["We extract the output message from the response and use ",(0,s.jsx)(t.code,{children:"set_span_chat_messages"})," again to append it to the chat history, ensuring it appears in the Chat panel of the MLflow UI."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Messages Panel",src:n(68102).A+"",width:"1434",height:"860"})}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Finally, we return the response from the API call, without any changes. Now, when we patch the chat function with ",(0,s.jsx)(t.code,{children:"trace_ollama_chat"}),", the function will be traced, but will otherwise behave as normal."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"A few points to note:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"This implementation uses a simple decorator pattern that adds tracing without modifying the underlying Ollama SDK code. This makes it a lightweight and maintainable approach."}),"\n",(0,s.jsxs)(t.li,{children:["The use of ",(0,s.jsx)(t.code,{children:"set_span_chat_messages"})," ensures that both input and output messages are displayed in a user-friendly way in the MLflow UI's Chat panel, making it easy to follow the conversation flow."]}),"\n",(0,s.jsxs)(t.li,{children:["There are several other ways we could have implemented this tracing behavior. We could have written a wrapper class or used a simple wrapper function that decorates the ",(0,s.jsx)(t.code,{children:"chat"})," function with ",(0,s.jsx)(t.code,{children:"@mlflow.trace"}),". Some orchestration frameworks may require a more complex approach, such as callbacks or API hooks. See the ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/tracing/contribute.html",children:"MLflow Tracing Contribution Guide"})," for more details."]}),"\n"]}),"\n",(0,s.jsxs)(t.h3,{id:"step-3-patch-the-chat-method-and-try-it-out",children:["Step 3: Patch the ",(0,s.jsx)(t.code,{children:"chat"})," method and try it out"]}),"\n",(0,s.jsxs)(t.p,{children:["Now that we have a tracing decorator, we can patch Ollama's ",(0,s.jsx)(t.code,{children:"chat"})," method and try it out."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"original_chat = ollama_chat\nchat = trace_ollama_chat(ollama_chat)\n"})}),"\n",(0,s.jsxs)(t.p,{children:["This code effectively patches the ",(0,s.jsx)(t.code,{children:"ollama.chat"})," function in the current scope. We first store the original function in ",(0,s.jsx)(t.code,{children:"original_chat"})," for safekeeping, then reassign ",(0,s.jsx)(t.code,{children:"chat"})," to the decorated version. This means that any subsequent calls to ",(0,s.jsx)(t.code,{children:"chat()"})," in our code will use the traced version, while still preserving the original functionality."]}),"\n",(0,s.jsxs)(t.p,{children:["Now, when we call ",(0,s.jsx)(t.code,{children:"chat()"}),", the method will be traced and the results will be logged to the MLflow UI:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'mlflow.set_experiment("ollama-tracing")\n\nresponse = chat(model="llama3.2",\n     messages = [\n         {"role": "user", "content": "Briefly describe the components of an MLflow model"}\n     ]\n)\n'})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Tracing results",src:n(74738).A+"",width:"2468",height:"1844"})}),"\n",(0,s.jsx)(t.h2,{id:"tracing-tools-and-tool-calls",children:"Tracing Tools and Tool Calls"}),"\n",(0,s.jsx)(t.p,{children:"The Ollama Python SDK supports tool calls. We want to record two main things:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"The tools that are available to the LLM"}),"\n",(0,s.jsx)(t.li,{children:"The actual tool calls, including the specific tool and the arguments passed to it."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:'Note that a "tool call" refers to the LLM\'s specification of which tool to use and what arguments to pass to it\u2014not the actual execution of that tool. When an LLM makes a tool call, it\'s essentially saying "this tool should be run with these parameters" rather than running the tool itself. The actual execution of the tool happens separately, typically in the application code.'}),"\n",(0,s.jsx)(t.p,{children:"Here is an updated version of the tracing code, patching the Ollama chat method, that records the available tools and captures tool calls:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from mlflow.entities import SpanType\nfrom mlflow.tracing.utils import set_span_chat_messages, set_span_chat_tools\nfrom functools import wraps\nfrom ollama import chat as ollama_chat\nimport json\nfrom uuid import uuid4\n\ndef _get_span_type(task_name: str) -> str:\n    span_type_mapping = {\n        "chat": SpanType.CHAT_MODEL,\n    }\n    return span_type_mapping.get(task_name, SpanType.UNKNOWN)\n\ndef trace_ollama_chat(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        with mlflow.start_span(\n            name="ollama.chat",\n            span_type=_get_span_type("chat"),\n        ) as span:\n            # Set model name as a span attribute\n            model_name = kwargs.get("model", "")\n            span.set_attribute("model_name", model_name)\n\n            # Log the inputs\n            input_messages = kwargs.get("messages", [])\n            tools = kwargs.get("tools", [])\n            span.set_inputs({\n                "messages": input_messages,\n                "model": model_name,\n                "tools": tools,\n            })\n\n            # Set input messages and tools\n            set_span_chat_messages(span, input_messages)\n            if tools:\n                set_span_chat_tools(span, tools)\n\n            # Make the API call\n            response = func(*args, **kwargs)\n\n            # Log the outputs\n            if hasattr(response, "to_dict"):\n                output = response.to_dict()\n            else:\n                output = response\n            span.set_outputs(output)\n\n            output_message = response.message\n\n            # Prepare the output message for span\n            output_span_message = {\n                "role": output_message.role,\n                "content": output_message.content,\n            }\n\n            # Handle tool calls if present\n            if output_message.tool_calls:\n                tool_calls = []\n                for tool_call in output_message.tool_calls:\n                    tool_calls.append({\n                        "id": str(uuid4()),\n                        "type": "function",\n                        "function": {\n                            "name": tool_call.function.name,\n                            "arguments": json.dumps(tool_call.function.arguments),\n                        }\n                    })\n                output_span_message["tool_calls"] = tool_calls\n\n            # Append the output message\n            set_span_chat_messages(span, [output_span_message], append=True)\n\n            return response\n\n    return wrapper\n'})}),"\n",(0,s.jsx)(t.p,{children:"The key changes here are:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["We extracted the list of available tools from the ",(0,s.jsx)(t.code,{children:"tools"})," argument with ",(0,s.jsx)(t.code,{children:'tools = kwargs.get("tools", [])'}),", logged them as inputs, and use ",(0,s.jsx)(t.code,{children:"set_span_chat_tools"})," to capture them for inclusion in the Chat panel."]}),"\n",(0,s.jsxs)(t.li,{children:["We added a specific handling for tool calls in the output message, making sure to format them according to the ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ToolCall",children:"ToolCall"})," specification."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["Now let's test this with a simple tip calculation tool. Tools are defined according to the ",(0,s.jsx)(t.a,{href:"https://platform.openai.com/docs/guides/function-calling#defining-functions",children:"OpenAI specification"})," for tool calls."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'chat = trace_ollama_chat(ollama_chat)\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "calculate_tip",\n            "description": "Calculate the tip amount based on the bill amount and tip percentage",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "bill_amount": {\n                        "type": "number",\n                        "description": "The total bill amount"\n                    },\n                    "tip_percentage": {\n                        "type": "number",\n                        "description": "The percentage of the bill to be given as a tip, given as a whole number."\n                    }\n                },\n                "required": ["bill_amount", "tip_percentage"]\n            }\n        }\n    }\n]\n\nresponse = chat(\n    model="llama3.2",\n    messages=[\n        {"role": "user", "content": "What is the tip for a $187.32 bill with a 22% tip?"}\n    ],\n    tools=tools,\n)\n\n'})}),"\n",(0,s.jsx)(t.p,{children:"We can inspect the trace in the MLflow UI, now with both the available tools and the tool call results displayed:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Tool Call Results",src:n(42052).A+"",width:"2024",height:"2236"})}),"\n",(0,s.jsx)(t.h2,{id:"orchestration-building-a-tool-calling-loop",children:"Orchestration: Building a tool calling loop"}),"\n",(0,s.jsx)(t.p,{children:"So far, the Ollama example just generates a single span whenever a chat completion is made. But many GenAI applications include multiple LLM calls, retrieval steps, tool executions, and other custom steps. While we won't go into detail on adding tracing to orchestration frameworks here, we will illustrate some of the key concepts by defining a tool calling loop based on the tool we defined earlier."}),"\n",(0,s.jsx)(t.p,{children:"The tool calling loop will follow this pattern:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Take user prompt as input"}),"\n",(0,s.jsx)(t.li,{children:"Respond with a tool call or calls"}),"\n",(0,s.jsx)(t.li,{children:"For each tool call, execute the tool and store the results"}),"\n",(0,s.jsxs)(t.li,{children:["Append the tool call results to the message history with the ",(0,s.jsx)(t.code,{children:"tool"})," role"]}),"\n",(0,s.jsx)(t.li,{children:"Call the LLM again with the tool call results, prompting it for a final answer to the user's prompt"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Here's an implementation with just one tool call."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'class ToolExecutor:\n    def __init__(self):\n        self.tools = [\n            {\n                "type": "function",\n                "function": {\n                    "name": "calculate_tip",\n                    "description": "Calculate the tip amount based on the bill amount and tip percentage",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "bill_amount": {\n                                "type": "number",\n                                "description": "The total bill amount"\n                            },\n                            "tip_percentage": {\n                                "type": "number",\n                                "description": "The percentage of the bill to be given as a tip, represented as a whole number."\n                            }\n                        },\n                        "required": ["bill_amount", "tip_percentage"]\n                    }\n                }\n            }\n        ]\n\n        # Map tool names to their Python implementations\n        self.tool_implementations = {\n            "calculate_tip": self._calculate_tip\n        }\n\n    def _calculate_tip(self, bill_amount: float, tip_percentage: float) -> float:\n        """Calculate the tip amount based on the bill amount and tip percentage."""\n        bill_amount = float(bill_amount)\n        tip_percentage = float(tip_percentage)\n        return round(bill_amount * (tip_percentage / 100), 2)\n    def execute_tool_calling_loop(self, messages):\n        """Execute a complete tool calling loop with tracing."""\n        with mlflow.start_span(\n            name="ToolCallingLoop",\n            span_type="CHAIN",\n        ) as parent_span:\n            # Set initial inputs\n            parent_span.set_inputs({\n                "initial_messages": messages,\n                "available_tools": self.tools\n            })\n\n            # Set input messages\n            set_span_chat_messages(parent_span, messages)\n\n            # First LLM call (already traced by our chat method patch)\n            response = chat(\n                messages=messages,\n                model="llama3.2",\n                tools=self.tools,\n            )\n\n            messages.append(response.message)\n\n            tool_calls = response.message.tool_calls\n            tool_results = []\n\n            # Execute tool calls\n            for tool_call in tool_calls:\n                with mlflow.start_span(\n                    name=f"ToolExecution_{tool_call.function.name}",\n                    span_type="TOOL",\n                ) as tool_span:\n                    # Parse tool inputs\n                    tool_inputs = tool_call.function.arguments\n                    tool_span.set_inputs(tool_inputs)\n\n                    # Execute tool\n                    func = self.tool_implementations.get(tool_call.function.name)\n                    if func is None:\n                        raise ValueError(f"No implementation for tool: {tool_call.function.name}")\n\n                    result = func(**tool_inputs)\n                    tool_span.set_outputs({"result": result})\n\n                    tool_results.append({\n                        "tool_call_id": str(uuid4()),\n                        "output": str(result)\n                    })\n\n                    messages.append({\n                        "role": "tool",\n                        "tool_call_id": str(uuid4()),\n                        "content": str(result)\n                    })\n\n            # Prepare messages for final response\n            messages.append({\n                "role": "user",\n                "content": "Answer the initial question based on the tool call results. Do not refer to the tool call results in your response. Just give a direct answer."\n            })\n\n            # Final LLM call (already traced by our chat method patch)\n            final_response = chat(\n                messages=messages,\n                model="llama3.2"\n            )\n\n            # Set the final output for the parent span\n            parent_span.set_outputs({\n                "final_response": final_response.message.content,\n                "tool_results": tool_results\n            })\n\n            print(final_response)\n\n            # set output messages\n            set_span_chat_messages(parent_span, [final_response.message.model_dump()], append=True)\n\n            return final_response\n'})}),"\n",(0,s.jsx)(t.p,{children:"Here's how we handled tracing in this tool calling loop:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["We first set up a parent span for the tool calling loop with ",(0,s.jsx)(t.code,{children:"mlflow.start_span"}),'. We set the span name to "ToolCallingLoop" and the span type to "CHAIN", representing a chain of operations.']}),"\n",(0,s.jsx)(t.li,{children:"We record the initial messages and available tools as inputs to the span. This could be helpful for future debugging by allowing us to verify that tools are made available and configured correctly."}),"\n",(0,s.jsxs)(t.li,{children:["We make the first LLM call with our patched ",(0,s.jsx)(t.code,{children:"chat"})," function. This call is already traced by our decorator, so we don't need to do anything special to trace it."]}),"\n",(0,s.jsx)(t.li,{children:"We iterate over the tool calls, executing each tool and storing the results. Each tool execution is traced with a new span, named after the tool function name. The inputs and outputs are logged as attributes on the span."}),"\n",(0,s.jsxs)(t.li,{children:["We append the tool call results to the message history with the ",(0,s.jsx)(t.code,{children:"tool"})," role. This allows the LLM to see the results of the tool calls in subsequent requests. It also allows us to see the tool call results in the MLflow UI."]}),"\n",(0,s.jsx)(t.li,{children:"We prepare messages for the final response, including a prompt to answer the initial question based on the tool call results."}),"\n",(0,s.jsxs)(t.li,{children:["We make the final LLM call with our patched ",(0,s.jsx)(t.code,{children:"chat"})," function. Again, because we are using the patched function, this call is already traced."]}),"\n",(0,s.jsx)(t.li,{children:"We set the final output for the parent span, including both the final response from the LLM and the tool results."}),"\n",(0,s.jsxs)(t.li,{children:["Finally, we use ",(0,s.jsx)(t.code,{children:"set_span_chat_messages"})," to append the final response to the chat history in the MLflow UI. Note that, to keep things clean and simple, we only record the user's initial query and the final response to the parent span with ",(0,s.jsx)(t.code,{children:"set_span_chat_messages"}),". We can click into the nested spans to see the tool call results and other details."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"This process creates a comprehensive trace of the entire tool calling loop, from the initial request through tool executions and the final response."}),"\n",(0,s.jsxs)(t.p,{children:["We can execute this as follows. However, note that you should ",(0,s.jsx)(t.em,{children:"not"})," run arbitrary code generated or invoked by LLMs without fully understanding what it will do on your system."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'executor = ToolExecutor()\nresponse = executor.execute_tool_calling_loop(\n    messages=[\n        {"role": "user", "content": "What is the tip for a $235.32 bill with a 22% tip?"}\n    ]\n)\n'})}),"\n",(0,s.jsx)(t.p,{children:"Resulting in the following trace:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Tool Calling Loop",src:n(75895).A+"",width:"2286",height:"1986"})}),"\n",(0,s.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsxs)(t.p,{children:["This post has shown how to extend MLflow Tracing beyond its built-in provider support. We started with a simple example\u2014adding tracing to the Ollama Python SDK's ",(0,s.jsx)(t.code,{children:"chat"})," method\u2014and saw how, with a lightweight patch, we could capture detailed information about each chat completion. We then built on this foundation to trace a more complex tool execution loop."]}),"\n",(0,s.jsx)(t.p,{children:"The key takeaways are:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"MLflow Tracing is highly customizable and can be adapted to providers for which autologging is not available"}),"\n",(0,s.jsxs)(t.li,{children:["Adding basic tracing support can often be done with minimal code changes. In this case, we patched the Ollama Python SDK's ",(0,s.jsx)(t.code,{children:"chat"})," method and wrote a few lines of code to add tracing support."]}),"\n",(0,s.jsx)(t.li,{children:"The same principles used for simple API calls can be extended to complex workflows with multiple steps. In this case, we traced a tool calling loop that included multiple steps and tool calls."}),"\n"]})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},68102:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/4_chat_panel-33af7b0d1baf67d6c7767d7d12ac07a0.png"},74738:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/5_trace_results-f6a33f3cc529287c67ac845ab6d4d278.png"},75895:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/7_tool_loop-db29a3ec8eee8a67b8c7013e23cf3154.png"},88615:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/2_log_inputs-10584fdd103f701729749620de7d5f20.png"},96580:e=>{e.exports=JSON.parse('{"permalink":"/mlflow-website/blog/custom-tracing","source":"@site/blog/2025-01-30-custom-tracing-provider/index.md","title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","description":"In this post, we will show how to add MLflow Tracing to a new LLM provider by adding tracing support to the chat method of the Ollama Python SDK.","date":"2025-01-30T00:00:00.000Z","tags":[{"inline":true,"label":"genai","permalink":"/mlflow-website/blog/tags/genai"},{"inline":true,"label":"tracing","permalink":"/mlflow-website/blog/tags/tracing"},{"inline":true,"label":"ollama","permalink":"/mlflow-website/blog/tags/ollama"}],"readingTime":16.82,"hasTruncateMarker":false,"authors":[{"name":"Daniel Liden","title":"Developer Advocate at Databricks","url":"https://www.linkedin.com/in/danielliden","imageURL":"/mlflow-website/img/authors/daniel_liden.png","key":"daniel-liden","page":null}],"frontMatter":{"title":"Beyond Autolog: Add MLflow Tracing to a New LLM Provider","tags":["genai","tracing","ollama"],"slug":"custom-tracing","authors":["daniel-liden"],"thumbnail":"/img/blog/tracing-new-provider.png"},"unlisted":false,"prevItem":{"title":"Practical AI Observability: Getting Started with MLflow Tracing","permalink":"/mlflow-website/blog/ai-observability-mlflow-tracing"},"nextItem":{"title":"From Natural Language to SQL: Building and Tracking a Multi-Lingual Query Engine","permalink":"/mlflow-website/blog/from-natural-language-to-sql"}}')},97675:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/1_chat_type-31c3db50724a332d9bd61791e84338f7.png"}}]);