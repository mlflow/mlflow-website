
  

<!DOCTYPE html>
<!-- source: docs/source/llms/transformers/guide/index.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ü§ó Transformers within MLflow &mdash; MLflow 2.13.1.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/transformers/guide/index.html">
  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="MLflow 2.13.1.dev0 documentation" href="../../../index.html"/>
        <link rel="up" title="MLflow Transformers Flavor" href="../index.html"/>
        <link rel="next" title="MLflow OpenAI Flavor" href="/../../openai/index.html"/>
        <link rel="prev" title="Prompt Templating with MLflow and Transformers" href="/../tutorials/prompt-templating/prompt-templating.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.13.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../index.html" class="main-navigation-home"><img src="../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id1">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id2">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id3">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html">MLflow Transformers Flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../langchain/index.html">MLflow LangChain Flavor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id4">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">MLflow Transformers Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>ü§ó Transformers within MLflow</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/guide/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="transformers-within-mlflow">
<h1>ü§ó Transformers within MLflow<a class="headerlink" href="#transformers-within-mlflow" title="Permalink to this headline"> </a></h1>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor is in active development and is marked as Experimental. Public APIs may change and new features are
subject to be added as additional functionality is brought to the flavor.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model flavor enables logging of
<a class="reference external" href="https://huggingface.co/docs/transformers/index">transformers models, components, and pipelines</a> in MLflow format via
the <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> and <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> functions. Use of these
functions also adds the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the model to be
interpreted as a generic Python function for inference via <a class="reference internal" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
You can also use the <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a> function to load a saved or logged MLflow
Model with the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor in the native transformers formats.</p>
<p>This page explains the detailed features and configurations of the MLflow <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor. For the general introduction about the MLflow‚Äôs Transformer integration, please refer to the <a class="reference external" href="../index.html">MLflow Transformers Flavor</a> page.</p>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#loading-a-transformers-model-as-a-python-function" id="id2">Loading a Transformers Model as a Python Function</a></p></li>
<li><p><a class="reference internal" href="#saving-transformer-pipelines-with-an-openai-compatible-inference-interface" id="id3">Saving Transformer Pipelines with an OpenAI-Compatible Inference Interface</a></p></li>
<li><p><a class="reference internal" href="#saving-prompt-templates-with-transformer-pipelines" id="id4">Saving Prompt Templates with Transformer Pipelines</a></p></li>
<li><p><a class="reference internal" href="#using-model-config-and-model-signature-params-for-inference" id="id5">Using model_config and Model Signature Params for Inference</a></p></li>
<li><p><a class="reference internal" href="#pipelines-vs-component-logging" id="id6">Pipelines vs. Component Logging</a></p></li>
<li><p><a class="reference internal" href="#automatic-metadata-and-modelcard-logging" id="id7">Automatic Metadata and ModelCard logging</a></p></li>
<li><p><a class="reference internal" href="#automatic-signature-inference" id="id8">Automatic Signature inference</a></p></li>
<li><p><a class="reference internal" href="#scale-inference-with-overriding-pytorch-dtype" id="id9">Scale Inference with Overriding Pytorch dtype</a></p></li>
<li><p><a class="reference internal" href="#input-data-types-for-audio-pipelines" id="id10">Input Data Types for Audio Pipelines</a></p></li>
<li><p><a class="reference internal" href="#storage-efficient-model-logging-with-save-pretrained-option" id="id11">Storage-Efficient Model Logging with <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> Option</a></p></li>
<li><p><a class="reference internal" href="#peft-models-in-mlflow-transformers-flavor" id="id12">PEFT Models in MLflow Transformers flavor</a></p></li>
</ul>
</div>
<div class="section" id="loading-a-transformers-model-as-a-python-function">
<h2><a class="toc-backref" href="#id2">Loading a Transformers Model as a Python Function</a><a class="headerlink" href="#loading-a-transformers-model-as-a-python-function" title="Permalink to this headline"> </a></h2>
<div class="section" id="supported-transformers-pipeline-types">
<h3>Supported Transformers Pipeline types<a class="headerlink" href="#supported-transformers-pipeline-types" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> <a class="reference internal" href="../../../models.html#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> simplifies
and standardizes both the inputs and outputs of pipeline inference. This conformity allows for serving
and batch inference by coercing the data structures that are required for <code class="docutils literal notranslate"><span class="pre">transformers</span></code> inference pipelines
to formats that are compatible with json serialization and casting to Pandas DataFrames.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain <cite>TextGenerationPipeline</cite> types, particularly instructional-based ones, may return the original
prompt and included line-formatting carriage returns <cite>‚Äún‚Äù</cite> in their outputs. For these pipeline types,
if you would like to disable the prompt return, you can set the following in the <cite>model_config</cite> dictionary when
saving or logging the model: <cite>‚Äúinclude_prompt‚Äù: False</cite>. To remove the newline characters from within the body
of the generated text output, you can add the <cite>‚Äúcollapse_whitespace‚Äù: True</cite> option to the <cite>model_config</cite> dictionary.
If the pipeline type being saved does not inherit from <cite>TextGenerationPipeline</cite>, these options will not perform
any modification to the output returned from pipeline inference.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Not all <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipeline types are supported. See the table below for the list of currently supported Pipeline
types that can be loaded as <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>.</p>
<p>In the current version, audio and text-based large language
models are supported for use with <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>, while computer vision, multi-modal, timeseries,
reinforcement learning, and graph models are only supported for native type loading via <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a></p>
<p>Future releases of MLflow will introduce <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> support for these additional types.</p>
</div>
<p>The table below shows the mapping of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipeline types to the <a class="reference internal" href="../../../models.html#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a>
data type inputs and outputs.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The inputs and outputs of the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> implementation of these pipelines <em>are not guaranteed to match</em> the input types and output types that would
return from a native use of a given pipeline type. If your use case requires access to scores, top_k results, or other additional references within
the output from a pipeline inference call, please use the native implementation by loading via <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a> to
receive the full output.</p>
<p>Similarly, if your use case requires the use of raw tensor outputs or processing of outputs through an external <code class="docutils literal notranslate"><span class="pre">processor</span></code> module, load the
model components directly as a <code class="docutils literal notranslate"><span class="pre">dict</span></code> by calling <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a> and specify the <code class="docutils literal notranslate"><span class="pre">return_type</span></code> argument as ‚Äòcomponents‚Äô.</p>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 22%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Pipeline Type</p></th>
<th class="head"><p>Input Type</p></th>
<th class="head"><p>Output Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Instructional Text Generation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Conversational</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Summarization</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Text Classification</p></td>
<td><p>str or List[str]</p></td>
<td><p>pd.DataFrame (dtypes: {‚Äòlabel‚Äô: str, ‚Äòscore‚Äô: double})</p></td>
</tr>
<tr class="row-even"><td><p>Text Generation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Text2Text Generation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Token Classification</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Translation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>ZeroShot Classification*</p></td>
<td><p>Dict[str, [List[str] | str]*</p></td>
<td><p>pd.DataFrame (dtypes: {‚Äòsequence‚Äô: str, ‚Äòlabels‚Äô: str, ‚Äòscores‚Äô: double})</p></td>
</tr>
<tr class="row-odd"><td><p>Table Question Answering**</p></td>
<td><p>Dict[str, [List[str] | str]**</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Question Answering***</p></td>
<td><p>Dict[str, str]***</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Fill Mask****</p></td>
<td><p>str or List[str]****</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Feature Extraction</p></td>
<td><p>str or List[str]</p></td>
<td><p>np.ndarray</p></td>
</tr>
<tr class="row-odd"><td><p>AutomaticSpeechRecognition</p></td>
<td><p>bytes*****, str, or np.ndarray</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>AudioClassification</p></td>
<td><p>bytes*****, str, or np.ndarray</p></td>
<td><p>pd.DataFrame (dtypes: {‚Äòlabel‚Äô: str, ‚Äòscore‚Äô: double})</p></td>
</tr>
</tbody>
</table>
<p>* A collection of these inputs can also be passed. The standard required key names are ‚Äòsequences‚Äô and ‚Äòcandidate_labels‚Äô, but these may vary.
Check the input requirments for the architecture that you‚Äôre using to ensure that the correct dictionary key names are provided.</p>
<p>** A collection of these inputs can also be passed. The reference table must be a json encoded dict (i.e. {‚Äòquery‚Äô: ‚Äòwhat did we sell most of?‚Äô, ‚Äòtable‚Äô: json.dumps(table_as_dict)})</p>
<p>*** A collection of these inputs can also be passed. The standard required key names are ‚Äòquestion‚Äô and ‚Äòcontext‚Äô. Verify the expected input key names match the
expected input to the model to ensure your inference request can be read properly.</p>
<p>**** The mask syntax for the model that you‚Äôve chosen is going to be specific to that model‚Äôs implementation. Some are ‚Äò[MASK]‚Äô, while others are ‚Äò&lt;mask&gt;‚Äô. Verify the expected syntax to
avoid failed inference requests.</p>
<p>***** If using <cite>pyfunc</cite> in MLflow Model Serving for realtime inference, the raw audio in bytes format must be base64 encoded prior to submitting to the endpoint. String inputs will be interpreted as uri locations.</p>
</div>
<div class="section" id="example-of-loading-a-transformers-model-as-a-python-function">
<h3>Example of loading a transformers model as a python function<a class="headerlink" href="#example-of-loading-a-transformers-model-as-a-python-function" title="Permalink to this headline"> </a></h3>
<p>In the below example, a simple pre-trained model is used within a pipeline. After logging to MLflow, the pipeline is
loaded as a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> and used to generate a response from a passed-in list of strings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="c1"># Read a pre-trained conversation pipeline from HuggingFace hub</span>
<span class="n">conversational_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;microsoft/DialoGPT-medium&quot;</span><span class="p">)</span>

<span class="c1"># Define the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_signature</span><span class="p">(</span>
    <span class="s2">&quot;Hi there, chatbot!&quot;</span><span class="p">,</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generate_signature_output</span><span class="p">(</span>
        <span class="n">conversational_pipeline</span><span class="p">,</span> <span class="s2">&quot;Hi there, chatbot!&quot;</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Log the pipeline</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">conversational_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="s2">&quot;conversational&quot;</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="s2">&quot;A clever and witty question&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load the saved pipeline as pyfunc</span>
<span class="n">chatbot</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Ask the chatbot a question</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">&quot;What is machine learning?&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="c1"># &gt;&gt; [It&#39;s a new thing that&#39;s been around for a while.]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-transformer-pipelines-with-an-openai-compatible-inference-interface">
<h2><a class="toc-backref" href="#id3">Saving Transformer Pipelines with an OpenAI-Compatible Inference Interface</a><a class="headerlink" href="#saving-transformer-pipelines-with-an-openai-compatible-inference-interface" title="Permalink to this headline"> </a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is only available in MLflow 2.11.0 and above. Also, the <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> task type is only available for models saved with <code class="docutils literal notranslate"><span class="pre">transformers</span> <span class="pre">&gt;=</span> <span class="pre">4.34.0</span></code>.</p>
</div>
<p>MLflow‚Äôs native transformers integration allows you to pass in the <code class="docutils literal notranslate"><span class="pre">task</span></code> param when saving a model
with <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> and <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a>. Originally, this param
accepts any of the <a class="reference external" href="https://huggingface.co/tasks">Transformers pipeline task types</a>, but in MLflow 2.11.0
and above, we‚Äôve added a few more MLflow-specific keys for <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipelines.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipelines, instead of specifying <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> as the task type, you can provide
one of two string literals conforming to the <a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#general-configuration-parameters">MLflow Deployments Server‚Äôs endpoint_type specification</a>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/chat&quot;</span></code> for chat-style applications</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/completions&quot;</span></code> for generic completions</p></li>
<li><p>(The last <code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/embeddings&quot;</span></code> can be specified as a task on models saved with <a class="reference internal" href="../../../python_api/mlflow.sentence_transformers.html#mlflow.sentence_transformers.save_model" title="mlflow.sentence_transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sentence_transformers.save_model()</span></code></a>)</p></li>
</ul>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;tinyllama-chat&quot;</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;llm/v1/chat&quot;</span><span class="p">,</span>
    <span class="n">transformers_model</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>When one of these keys is specified, MLflow will automatically handle everything required to serve a chat
or completions model. This includes:</p>
<ul class="simple">
<li><p>Setting a chat/completions compatible signature on the model</p></li>
<li><p>Performing data pre- and post-processing to ensure the inputs and outputs conform to
the <a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#chat">Chat/Completions API spec</a>,
which is compatible with OpenAI‚Äôs API spec.</p></li>
</ul>
<p>Note that these modifications only apply when the model is loaded with <a class="reference internal" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> (e.g. when
serving the model with the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span> <span class="pre">serve</span></code> CLI tool). If you want to load just the base pipeline, you can
always do so via <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a>.</p>
<p>Check out the <a class="reference external" href="../tutorials/conversational/pyfunc-chat-model.html">notebook tutorial</a> to see this feature in action!</p>
</div>
<div class="section" id="saving-prompt-templates-with-transformer-pipelines">
<h2><a class="toc-backref" href="#id4">Saving Prompt Templates with Transformer Pipelines</a><a class="headerlink" href="#saving-prompt-templates-with-transformer-pipelines" title="Permalink to this headline"> </a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is only available in MLflow 2.10.0 and above.</p>
</div>
<p>MLflow supports specifying prompt templates for certain pipeline types:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.FeatureExtractionPipeline">feature-extraction</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.FillMaskPipeline">fill-mask</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.SummarizationPipeline">summarization</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.Text2TextGenerationPipeline">text2text-generation</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TextGenerationPipeline">text-generation</a></p></li>
</ul>
<p>Prompt templates are strings that are used to format user inputs prior to <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> inference. To specify a prompt template,
use the <code class="docutils literal notranslate"><span class="pre">prompt_template</span></code> argument when calling <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> or <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a>.
The prompt template must be a string with a single format placeholder, <code class="docutils literal notranslate"><span class="pre">{prompt}</span></code>.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize a pipeline. `distilgpt2` uses a &quot;text-generation&quot; pipeline</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilgpt2&quot;</span><span class="p">)</span>

<span class="c1"># Define a prompt template</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;Answer the following question: </span><span class="si">{prompt}</span><span class="s2">&quot;</span>

<span class="c1"># Save the model</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">transformers_model</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;path/to/model&quot;</span><span class="p">,</span>
    <span class="n">prompt_template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>When the model is then loaded with <a class="reference internal" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>, the prompt
template will be used to format user inputs before passing them into the pipeline:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Load the model with pyfunc</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;path/to/model&quot;</span><span class="p">)</span>

<span class="c1"># The prompt template will be used to format this input, so the</span>
<span class="c1"># string that is passed to the text-generation pipeline will be:</span>
<span class="c1"># &quot;Answer the following question: What is MLflow?&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipelines with a prompt template will have the <a class="reference external" href="https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationParameters.return_full_text">return_full_text pipeline argument</a>
set to <code class="docutils literal notranslate"><span class="pre">False</span></code> by default. This is to prevent the template from being shown to the users,
which could potentially cause confusion as it was not part of their original input. To
override this behaviour, either set <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> via <code class="docutils literal notranslate"><span class="pre">params</span></code>, or by
including it in a <code class="docutils literal notranslate"><span class="pre">model_config</span></code> dict in <code class="docutils literal notranslate"><span class="pre">log_model()</span></code>. See <a class="reference external" href="#using-model-config-and-model-signature-params-for-transformers-inference">this section</a>
for more details on how to do this.</p>
</div>
<p>For a more in-depth guide, check out the <a class="reference internal" href="../tutorials/prompt-templating/prompt-templating.html"><span class="doc">Prompt Templating notebook</span></a>!</p>
</div>
<div class="section" id="using-model-config-and-model-signature-params-for-inference">
<h2><a class="toc-backref" href="#id5">Using model_config and Model Signature Params for Inference</a><a class="headerlink" href="#using-model-config-and-model-signature-params-for-inference" title="Permalink to this headline"> </a></h2>
<p>For <cite>transformers</cite> inference, there are two ways to pass in additional arguments to the pipeline.</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">model_config</span></code> when saving/logging the model. Optionally, specify <code class="docutils literal notranslate"><span class="pre">model_config</span></code> when calling <code class="docutils literal notranslate"><span class="pre">load_model</span></code>.</p></li>
<li><p>Specify params at inference time when calling <code class="docutils literal notranslate"><span class="pre">predict()</span></code></p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">model_config</span></code> to control how the model is loaded and inference performed for all input samples. Configuration in
<code class="docutils literal notranslate"><span class="pre">model_config</span></code> is not overridable at <code class="docutils literal notranslate"><span class="pre">predict()</span></code> time unless a <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> is indicated with the same parameters.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> with params schema, on the other hand, to allow downstream consumers to provide additional inference
params that may be needed to compute the predictions for their specific samples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If both <code class="docutils literal notranslate"><span class="pre">model_config</span></code> and <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> with parameters are saved when logging model, both of them
will be used for inference. The default parameters in <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> will override the params in <code class="docutils literal notranslate"><span class="pre">model_config</span></code>.
If extra <code class="docutils literal notranslate"><span class="pre">params</span></code> are provided at inference time, they take precedence over all params. We recommend using
<code class="docutils literal notranslate"><span class="pre">model_config</span></code> for those parameters needed to run the model in general for all the samples. Then, add
<code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> with parameters for those extra parameters that you want downstream consumers to indicated at
per each of the samples.</p>
</div>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">model_config</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">generate_signature_output</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;mrm8488/t5-base-finetuned-common_gen&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text2text-generation&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;pencil draw paper&quot;</span>

<span class="c1"># Infer the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">generate_signature_output</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Define an model_config</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;remove_invalid_values&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Saving model_config with the model</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;text2text&quot;</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;text2text&quot;</span><span class="p">)</span>
<span class="c1"># model_config will be applied</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pyfunc_loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># overriding some inference configuration with diferent values</span>
<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="s2">&quot;text2text&quot;</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that in the previous example, the user can‚Äôt override the configuration <code class="docutils literal notranslate"><span class="pre">do_sample</span></code>
when calling <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p>
</div>
<ul class="simple">
<li><p>Specifying params at inference time</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">generate_signature_output</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;mrm8488/t5-base-finetuned-common_gen&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text2text-generation&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;pencil draw paper&quot;</span>

<span class="c1"># Define an model_config</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;remove_invalid_values&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Define the inference parameters params</span>
<span class="n">inference_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Infer the signature including params</span>
<span class="n">signature_with_params</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">generate_signature_output</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">),</span>
    <span class="n">params</span><span class="o">=</span><span class="n">inference_params</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Saving model with signature and model config</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;text2text&quot;</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="n">signature_with_params</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;text2text&quot;</span><span class="p">)</span>

<span class="c1"># Pass params at inference time</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># In this case we only override max_length and do_sample,</span>
<span class="c1"># other params will use the default one saved on ModelSignature</span>
<span class="c1"># or in the model configuration.</span>
<span class="c1"># The final params used for prediction is as follows:</span>
<span class="c1"># {</span>
<span class="c1">#    &quot;num_beams&quot;: 5,</span>
<span class="c1">#    &quot;max_length&quot;: 20,</span>
<span class="c1">#    &quot;do_sample&quot;: False,</span>
<span class="c1">#    &quot;remove_invalid_values&quot;: True,</span>
<span class="c1"># }</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pyfunc_loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="pipelines-vs-component-logging">
<h2><a class="toc-backref" href="#id6">Pipelines vs. Component Logging</a><a class="headerlink" href="#pipelines-vs-component-logging" title="Permalink to this headline"> </a></h2>
<p>The transformers flavor has two different primary mechanisms for saving and loading models: pipelines and components.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Saving transformers models with custom code (i.e. models that require <code class="docutils literal notranslate"><span class="pre">trust_remote_code=True</span></code>) requires <code class="docutils literal notranslate"><span class="pre">transformers</span> <span class="pre">&gt;=</span> <span class="pre">4.26.0</span></code>.</p>
</div>
<p><strong>Pipelines</strong></p>
<p>Pipelines, in the context of the Transformers library, are high-level objects that combine pre-trained models and tokenizers
(as well as other components, depending on the task type) to perform a specific task. They abstract away much of the preprocessing
and postprocessing work involved in using the models.</p>
<p>For example, a text classification pipeline would handle the tokenization of text, passing the tokens through a model, and then interpret the logits to produce a human-readable classification.</p>
<p>When logging a pipeline with MLflow, you‚Äôre essentially saving this high-level abstraction, which can be loaded and used directly
for inference with minimal setup. This is ideal for end-to-end tasks where the preprocessing and postprocessing steps are standard
for the task at hand.</p>
<p><strong>Components</strong></p>
<p>Components refer to the individual parts that can make up a pipeline, such as the model itself, the tokenizer, and any additional
processors, extractors, or configuration needed for a specific task. Logging components with MLflow allows for more flexibility and
customization. You can log individual components when your project needs to have more control over the preprocessing and postprocessing
steps or when you need to access the individual components in a bespoke manner that diverges from how the pipeline abstraction would call them.</p>
<p>For example, you might log the components separately if you have a custom tokenizer or if you want to apply some special postprocessing
to the model outputs. When loading the components, you can then reconstruct the pipeline with your custom components or use the components
individually as needed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MLflow by default uses a 500 MB <cite>max_shard_size</cite> to save the model object in <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> or <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> APIs. You can use the environment variable <cite>MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE</cite> to override the value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For component-based logging, the only requirement that must be met in the submitted <code class="docutils literal notranslate"><span class="pre">dict</span></code> is that a model is provided. All other elements of the <code class="docutils literal notranslate"><span class="pre">dict</span></code> are optional.</p>
</div>
<div class="section" id="logging-a-components-based-model">
<h3>Logging a components-based model<a class="headerlink" href="#logging-a-components-based-model" title="Permalink to this headline"> </a></h3>
<p>The example below shows logging components of a <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model via a dictionary mapping of specific named components. The names of the keys within the submitted dictionary
must be in the set: <code class="docutils literal notranslate"><span class="pre">{&quot;model&quot;,</span> <span class="pre">&quot;tokenizer&quot;,</span> <span class="pre">&quot;feature_extractor&quot;,</span> <span class="pre">&quot;image_processor&quot;}</span></code>. Processor type objects (some image processors, audio processors, and multi-modal processors)
must be saved explicitly with the <code class="docutils literal notranslate"><span class="pre">processor</span></code> argument in the <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> or <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> APIs.</p>
<p>After logging, the components are automatically inserted into the appropriate <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> type for the task being performed and are returned, ready for inference.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The components that are logged can be retrieved in their original structure (a dictionary) by setting the attribute <code class="docutils literal notranslate"><span class="pre">return_type</span></code> to ‚Äúcomponents‚Äù in the <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> API.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Not all model types are compatible with the pipeline API constructor via component elements. Incompatible models will raise an
<code class="docutils literal notranslate"><span class="pre">MLflowException</span></code> error stating that the model is missing the <cite>name_or_path</cite> attribute. In
the event that this occurs, please construct the model directly via the <code class="docutils literal notranslate"><span class="pre">transformers.pipeline(&lt;repo</span> <span class="pre">name&gt;)</span></code> API and save the pipeline object directly.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;text-classification&quot;</span>
<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>

<span class="c1"># Define the components of the model in a dictionary</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">}</span>

<span class="c1"># Log the model components</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">transformers_model</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;text_classifier&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load the components as a pipeline</span>
<span class="n">loaded_pipeline</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;pipeline&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">loaded_pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="c1"># &gt;&gt; TextClassificationPipeline</span>

<span class="n">loaded_pipeline</span><span class="p">([</span><span class="s2">&quot;MLflow is awesome!&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformers is a great library!&quot;</span><span class="p">])</span>

<span class="c1"># &gt;&gt; [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998478889465332},</span>
<span class="c1"># &gt;&gt;  {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998030066490173}]</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-a-pipeline-and-loading-components">
<h3>Saving a pipeline and loading components<a class="headerlink" href="#saving-a-pipeline-and-loading-components" title="Permalink to this headline"> </a></h3>
<p>Some use cases can benefit from the simplicity of defining a solution as a pipeline, but need the component-level access for performing a micro-services based deployment strategy
where pre / post-processing is performed on containers that do not house the model itself. For this paradigm, a pipeline can be loaded as its constituent parts, as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">translation_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;translation_en_to_fr&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">translation_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;french_translator&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">translation_components</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;components&quot;</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">translation_components</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># &gt;&gt; task -&gt; str</span>
<span class="c1"># &gt;&gt; model -&gt; T5ForConditionalGeneration</span>
<span class="c1"># &gt;&gt; tokenizer -&gt; T5TokenizerFast</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">translation_pipeline</span><span class="p">(</span><span class="s2">&quot;MLflow is great!&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="c1"># &gt;&gt; [{&#39;translation_text&#39;: &#39;MLflow est formidable!&#39;}]</span>

<span class="n">reconstructed_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="o">**</span><span class="n">translation_components</span><span class="p">)</span>

<span class="n">reconstructed_response</span> <span class="o">=</span> <span class="n">reconstructed_pipeline</span><span class="p">(</span>
    <span class="s2">&quot;transformers makes using Deep Learning models easy and fun!&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">reconstructed_response</span><span class="p">)</span>

<span class="c1"># &gt;&gt; [{&#39;translation_text&#39;: &quot;Les transformateurs rendent l&#39;utilisation de mod√®les Deep Learning facile et amusante!&quot;}]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="automatic-metadata-and-modelcard-logging">
<h2><a class="toc-backref" href="#id7">Automatic Metadata and ModelCard logging</a><a class="headerlink" href="#automatic-metadata-and-modelcard-logging" title="Permalink to this headline"> </a></h2>
<p>In order to provide as much information as possible for saved models, the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor will automatically fetch the <code class="docutils literal notranslate"><span class="pre">ModelCard</span></code> for any model or pipeline that
is saved that has a stored card on the HuggingFace hub. This card will be logged as part of the model artifact, viewable at the same directory level as the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file and
the stored model object.</p>
<p>In addition to the <code class="docutils literal notranslate"><span class="pre">ModelCard</span></code>, the components that comprise any Pipeline (or the individual components if saving a dictionary of named components) will have their source types
stored. The model type, pipeline type, task, and classes of any supplementary component (such as a <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> or <code class="docutils literal notranslate"><span class="pre">ImageProcessor</span></code>) will be stored in the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file as well.</p>
<p>In order to preserve any attached legal requirements to the usage of any  model that is hosted on the huggingface hub, a ‚Äúbest effort‚Äù attempt
is made when logging a transformers model to retrieve and persist any license information. A file will be generated (<code class="docutils literal notranslate"><span class="pre">LICENSE.txt</span></code>) within the root of
the model directory. Within this file you will either find a copy of a declared license, the name of a common license type that applies to the model‚Äôs use (i.e., ‚Äòapache-2.0‚Äô, ‚Äòmit‚Äô),
or, in the event that license information was never submitted to the huggingface hub when uploading a model repository, a link to the repository for you to use
in order to determine what restrictions exist regarding the use of the model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Model license information was introduced in <strong>MLflow 2.10.0</strong>. Previous versions do not include license information for models.</p>
</div>
</div>
<div class="section" id="automatic-signature-inference">
<h2><a class="toc-backref" href="#id8">Automatic Signature inference</a><a class="headerlink" href="#automatic-signature-inference" title="Permalink to this headline"> </a></h2>
<p>For pipelines that support <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>, there are 3 means of attaching a model signature to the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file.</p>
<ul class="simple">
<li><p>Provide a model signature explicitly via setting a valid <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> to the <code class="docutils literal notranslate"><span class="pre">signature</span></code> attribute. This can be generated via the helper utility <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.generate_signature_output" title="mlflow.transformers.generate_signature_output"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.generate_signature_output()</span></code></a></p></li>
<li><p>Provide an <code class="docutils literal notranslate"><span class="pre">input_example</span></code>. The signature will be inferred and validated that it matches the appropriate input type. The output type will be validated by performing inference automatically (if the model is a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> supported type).</p></li>
<li><p>Do nothing. The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor will automatically apply the appropriate general signature that the pipeline type supports (only for a single-entity; collections will not be inferred).</p></li>
</ul>
</div>
<div class="section" id="scale-inference-with-overriding-pytorch-dtype">
<h2><a class="toc-backref" href="#id9">Scale Inference with Overriding Pytorch dtype</a><a class="headerlink" href="#scale-inference-with-overriding-pytorch-dtype" title="Permalink to this headline"> </a></h2>
<p>A common configuration for lowering the total memory pressure for pytorch models within <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipelines is to modify the
processing data type. This is achieved through setting the <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> argument when creating a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>.
For a full reference of these tunable arguments for configuration of pipelines, see the <a class="reference external" href="https://huggingface.co/docs/transformers/v4.28.1/en/perf_train_gpu_one#floating-data-types">training docs</a> .</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature does not exist in versions of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> &lt; 4.26.x</p>
</div>
<p>In order to apply these configurations to a saved or logged run, there are two options:</p>
<ul class="simple">
<li><p>Save a pipeline with the <cite>torch_dtype</cite> argument set to the encoding type of your choice.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;translation_en_to_fr&quot;</span>

<span class="n">my_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">),</span>
    <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">my_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_pipeline&quot;</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Illustrate that the torch data type is recorded in the flavor configuration</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="s2">&quot;transformers&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span><span class="s1">&#39;transformers_version&#39;</span>:<span class="w"> </span><span class="s1">&#39;4.28.1&#39;</span>,
<span class="w"> </span><span class="s1">&#39;code&#39;</span>:<span class="w"> </span>None,
<span class="w"> </span><span class="s1">&#39;task&#39;</span>:<span class="w"> </span><span class="s1">&#39;translation_en_to_fr&#39;</span>,
<span class="w"> </span><span class="s1">&#39;instance_type&#39;</span>:<span class="w"> </span><span class="s1">&#39;TranslationPipeline&#39;</span>,
<span class="w"> </span><span class="s1">&#39;source_model_name&#39;</span>:<span class="w"> </span><span class="s1">&#39;t5-small&#39;</span>,
<span class="w"> </span><span class="s1">&#39;pipeline_model_type&#39;</span>:<span class="w"> </span><span class="s1">&#39;T5ForConditionalGeneration&#39;</span>,
<span class="w"> </span><span class="s1">&#39;framework&#39;</span>:<span class="w"> </span><span class="s1">&#39;pt&#39;</span>,
<span class="w"> </span><span class="s1">&#39;torch_dtype&#39;</span>:<span class="w"> </span><span class="s1">&#39;torch.bfloat16&#39;</span>,
<span class="w"> </span><span class="s1">&#39;tokenizer_type&#39;</span>:<span class="w"> </span><span class="s1">&#39;T5TokenizerFast&#39;</span>,
<span class="w"> </span><span class="s1">&#39;components&#39;</span>:<span class="w"> </span><span class="o">[</span><span class="s1">&#39;tokenizer&#39;</span><span class="o">]</span>,
<span class="w"> </span><span class="s1">&#39;pipeline&#39;</span>:<span class="w"> </span><span class="s1">&#39;pipeline&#39;</span><span class="o">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Specify the <cite>torch_dtype</cite> argument when loading the model to override any values set during logging or saving.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;translation_en_to_fr&quot;</span>

<span class="n">my_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">),</span>
    <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">my_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_pipeline&quot;</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">loaded_pipeline</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;pipeline&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loaded_pipeline</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torch.float64
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MLflow 2.12.1 slightly changed the <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> extraction logic.
Previously it depended on the <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> attribute of the pipeline instance, but now it is extracted from the underlying model via <code class="docutils literal notranslate"><span class="pre">dtype</span></code> property. This enables MLflow to capture the dtype change of the model after pipeline instantiation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Logging or saving a model in ‚Äòcomponents‚Äô mode (using a dictionary to declare components) does not support setting the data type for a constructed pipeline.
If you need to override the default behavior of how data is encoded, please save or log a <cite>pipeline</cite> object.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Overriding the data type for a pipeline when loading as a <a class="reference internal" href="../../../models.html#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> is not supported.
The value set for <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> during <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> or <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> will persist when loading as <cite>pyfunc</cite>.</p>
</div>
</div>
<div class="section" id="input-data-types-for-audio-pipelines">
<h2><a class="toc-backref" href="#id10">Input Data Types for Audio Pipelines</a><a class="headerlink" href="#input-data-types-for-audio-pipelines" title="Permalink to this headline"> </a></h2>
<p>Note that passing raw data to an audio pipeline (raw bytes) requires two separate elements of the same effective library.
In order to use the bitrate transposition and conversion of the audio bytes data into numpy nd.array format, the library <cite>ffmpeg</cite> is required.
Installing this package directly from pypi (<cite>pip install ffmpeg</cite>) does not install the underlying <cite>c</cite> dll‚Äôs that are required to make <cite>ffmpeg</cite> function.
Please consult with the documentation at <a class="reference external" href="https://ffmpeg.org/download.html">the ffmpeg website</a> for guidance on your given operating system.</p>
<p>The Audio Pipeline types, when loaded as a <a class="reference internal" href="../../../models.html#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> have three input types available:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></li>
</ul>
<p>The string input type is meant for blob references (uri locations) that are accessible to the instance of the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model.
This input mode is useful when doing large batch processing of audio inference in Spark due to the inherent limitations of handling large <code class="docutils literal notranslate"><span class="pre">bytes</span></code>
data in <code class="docutils literal notranslate"><span class="pre">Spark</span></code> <code class="docutils literal notranslate"><span class="pre">DataFrames</span></code>. Ensure that you have <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> installed in the environment that the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model is running in order
to use <code class="docutils literal notranslate"><span class="pre">str</span></code> input uri-based inference. If this package is not properly installed (both from <code class="docutils literal notranslate"><span class="pre">pypi</span></code> and from the <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> binaries), an Exception
will be thrown at inference time.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If using a uri (<cite>str</cite>) as an input type for a <cite>pyfunc</cite> model that you are intending to host for realtime inference through the <cite>MLflow Model Server</cite>,
you <em>must</em> specify a custom model signature when logging or saving the model.
The default signature input value type of <code class="docutils literal notranslate"><span class="pre">bytes</span></code> will, in <cite>MLflow Model serving</cite>, force the conversion of the uri string to <code class="docutils literal notranslate"><span class="pre">bytes</span></code>, which will cause an Exception
to be thrown from the serving process stating that the soundfile is corrupt.</p>
</div>
<p>An example of specifying an appropriate uri-based input model signature for an audio model is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">generate_signature_output</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.mywebsite.com/sound/files/for/transcription/file111.mp3&quot;</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">generate_signature_output</span><span class="p">(</span><span class="n">my_audio_pipeline</span><span class="p">,</span> <span class="n">url</span><span class="p">))</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">my_audio_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_transcriber&quot;</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">bytes</span></code></p></li>
</ul>
<p>This is the default serialization format of audio files. It is the easiest format to utilize due to the fact that
Pipeline implementations will automatically convert the audio bitrate from the file with the use of <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> (a required dependency if using this format) to the bitrate required by the underlying model within the <cite>Pipeline</cite>.
When using the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> representation of the pipeline directly (not through serving), the sound file can be passed directly as <code class="docutils literal notranslate"><span class="pre">bytes</span></code> without any
modification. When used through serving, the <code class="docutils literal notranslate"><span class="pre">bytes</span></code> data <em>must be</em> base64 encoded.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code></p></li>
</ul>
<p>This input format requires that both the bitrate has been set prior to conversion to <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> (i.e., through the use of a package like
<code class="docutils literal notranslate"><span class="pre">librosa</span></code> or <code class="docutils literal notranslate"><span class="pre">pydub</span></code>) and that the model has been saved with a signature that uses the <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format for the input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Audio models being used for serving that intend to utilize pre-formatted audio in <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format
must have the model saved with a signature configuration that reflects this schema. Failure to do so will result in type casting errors due to the default signature for
audio transformers pipelines being set as expecting <code class="docutils literal notranslate"><span class="pre">binary</span></code> (<code class="docutils literal notranslate"><span class="pre">bytes</span></code>) data. The serving endpoint cannot accept a union of types, so a particular model instance must choose one
or the other as an allowed input type.</p>
</div>
</div>
<div class="section" id="storage-efficient-model-logging-with-save-pretrained-option">
<span id="transformers-save-pretrained-guide"></span><h2><a class="toc-backref" href="#id11">Storage-Efficient Model Logging with <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> Option</a><a class="headerlink" href="#storage-efficient-model-logging-with-save-pretrained-option" title="Permalink to this headline"> </a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> argument is only available in MLflow 2.11.0 and above, and still in experimental stage. The API and behavior may change in future releases. Moreover, this feature is intended for advanced users who are familiar with Transformers and MLflow, understanding <a class="reference internal" href="#caveats-of-save-pretrained"><span class="std std-ref">the potential risks</span></a> of using this feature.</p>
</div>
<div class="section" id="avoiding-redundant-model-copy-by-setting-save-pretrained-false">
<h3>Avoiding Redundant Model Copy by Setting <code class="docutils literal notranslate"><span class="pre">save_pretrained=False</span></code><a class="headerlink" href="#avoiding-redundant-model-copy-by-setting-save-pretrained-false" title="Permalink to this headline"> </a></h3>
<p>Typically, when MLflow logs an ML model, it saves a copy of the model weight to the artifact store.
However, this is not optimal when you use a pretrained model from HuggingFace Hub and have no intention of fine-tuning or otherwise manipulating the model or its weights before logging it. For this very common case, copying the (typically very large) model weights is redundant while developing prompts, testing inference parameters, and otherwise is little more than an unnecessary waste of storage space.</p>
<p>To address this issue, MLflow 2.11.0 introduced a new argument <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> in the <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> and <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> APIs. When with argument is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, MLflow will forego saving the pretrained model weights, opting instead to store a reference to the underlying repository entry on the HuggingFace Hub; specifically, the  repository name and the unique commit hash of the model weights are stored when your components or pipeline are logged. When loading back such a <em>refernce-only</em> model, MLflow will check the repository name and commit hash from the saved metadata, and either download the model weight from the HuggingFace Hub or use the locally cached model from your HuggingFace local cache directory.</p>
<p>A good analogy for this feature is the comparison between a file <em>copy</em> and a <em>symlink</em> operation. The default behavior for the transformers flavor is to perform a copy, materializing the model weight files in your artifact store that is associated with the run that the model is logged to. By setting <code class="docutils literal notranslate"><span class="pre">save_pretrained=False</span></code>, MLflow will log a link to the HuggingFace Hub repository, effectively building in symlink functionality to the run. This will save storage space and reduce the logging latency significantly, particularly for large models like LLMs.</p>
</div>
<div class="section" id="example-usage">
<h3>Example Usage<a class="headerlink" href="#example-usage" title="Permalink to this headline"> </a></h3>
<p>Here is the example of using <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> argument for logging a model</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;databricks/dolly-v2-7b&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;torch.float16&quot;</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;dolly&quot;</span><span class="p">,</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>In the above example, MLflow will not save a copy of the <strong>Dolly-v2-7B</strong> model‚Äôs weights and will instead log the following metadata as a reference to the HuggingFace Hub model. This will save roughly 15GB of storage space and reduce the logging latency significantly as well for each run that you initiate during development.
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">source_model_name:</span> <span class="pre">&quot;databricks/dolly-v2-7b&quot;</span>
<span class="pre">source_model_revision:</span> <span class="pre">&quot;d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f&quot;</span>
<span class="pre">`</span></code></p>
</div>
<div class="section" id="caveats-of-reference-only-models">
<span id="caveats-of-save-pretrained"></span><h3>Caveats of Reference-Only Models<a class="headerlink" href="#caveats-of-reference-only-models" title="Permalink to this headline"> </a></h3>
<p>While the <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> argument is useful for saving storage space and reducing logging latency, it has the following caveats to be aware of:</p>
<ul class="simple">
<li><p><strong>Change in Model Unavailability</strong>: If you are using a model from other users‚Äô repository, the model may be deleted or become private in the HuggingFace Hub. In such cases, MLflow cannot load the model back. For production use cases, it is recommended to save the copy model weight to the artifact store prior to moving from development or staging to production for your model.</p></li>
<li><p><strong>HuggingFace Hub Access</strong>: Downloading a model from the HuggingFace Hub might be slow or unstable due to the network condition or the HuggingFace Hub service status. MLflow doesn‚Äôt provide any retry mechanism or robust error handling for the model downloading. As such, you should not rely on this functionality for your final production-candidate run.</p></li>
<li><p><strong>Limited Databricks Integration</strong>: If you are using Databricks, be aware that the model saved with <cite>save_pretrained=False</cite> cannot be registered to the legacy <a class="reference external" href="https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/workspace-model-registry.html">Workspace Model Registry</a>. If you want to register the reference-only Transformer model, please use <a class="reference external" href="https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/index.html">Unity Catalog</a> instead, or download the model weight in advance using <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.persist_pretrained_model" title="mlflow.transformers.persist_pretrained_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.persist_pretrained_model()</span></code></a> API as described in the next section.</p></li>
</ul>
</div>
<div class="section" id="persist-the-model-weight-to-the-existing-reference-only-model">
<span id="persist-pretrained-guide"></span><h3>Persist the Model Weight to the Existing Reference-Only Model<a class="headerlink" href="#persist-the-model-weight-to-the-existing-reference-only-model" title="Permalink to this headline"> </a></h3>
<p>If you want to update the reference-only model to an instance that contains the model weight, you can use the <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.persist_pretrained_model" title="mlflow.transformers.persist_pretrained_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.persist_pretrained_model()</span></code></a> API. This API will download the model weight from the HuggingFace Hub, save it to the artifact location, and update the metadata of the given reference-only model. After this operation, the model will be equivalent to the one saved with <cite>save_pretrained=True</cite> and be ready for the production use.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.persist_pretrained_model" title="mlflow.transformers.persist_pretrained_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.persist_pretrained_model()</span></code></a> API <strong>does NOT require re-logging a model</strong> but efficiently update the existing model and metadata in-place.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;databricks/dolly-v2-7b&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;torch.float16&quot;</span>
<span class="p">)</span>

<span class="c1"># Save the reference-only Transformer model</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;dolly&quot;</span><span class="p">,</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Model weight is not saved to the artifact store</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">artifact_path</span> <span class="o">+</span> <span class="s2">&quot;/model&quot;</span><span class="p">)</span>

<span class="c1"># This will download the model weight from the HuggingFace Hub and save it</span>
<span class="c1"># to the artifact location</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">persist_pretrained_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">artifact_path</span> <span class="o">+</span> <span class="s2">&quot;/model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="peft-models-in-mlflow-transformers-flavor">
<h2><a class="toc-backref" href="#id12">PEFT Models in MLflow Transformers flavor</a><a class="headerlink" href="#peft-models-in-mlflow-transformers-flavor" title="Permalink to this headline"> </a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The PEFT model is supported in MLflow 2.11.0 and above and is still in the experimental stage. The API and behavior may change in future releases. Moreover, the <a class="reference external" href="https://huggingface.co/docs/peft/en/index">PEFT</a> library is under active development, so not all features
and adapter types might be supported in MLflow.</p>
</div>
<p><a class="reference external" href="https://huggingface.co/docs/peft/en/index">PEFT</a> is a library developed by HuggingFaceü§ó, that provides various optimization methods for pretrained models available on the HuggingFace Hub. With PEFT, you can easily apply various optimization techniques like LoRA and QLoRA to reduce the cost of fine-tuning Transformers models.</p>
<p>For example, <a class="reference external" href="https://huggingface.co/docs/peft/main/en/conceptual_guides/lora">LoRA (Low-Rank Adaptation)</a> is a method that approximate the weight updates of fine-tuning process with two smaller matrices through low-rank decomposition. LoRA typically shrinks the number of parameters to train to only 0.01% ~ a few % of the full model fine-tuning (depending on the configuration), which significantly accelerates the fine-tuning process and reduces the memory footprint, such that you can even <a class="reference external" href="../tutorials/fine-tuning/transformers-peft.html">train a Mistral/Llama2 7B model on a single Nvidia A10G GPU in an hour</a>.
By using PEFT, you can apply LoRA to your Transformers model with only a few lines of code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">peft_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>
</pre></div>
</div>
<p>In MLflow 2.11.0, we introduced support for tracking PEFT models in the MLflow Transformers flavor. You can log and load PEFT models using the same APIs as other Transformers models, such as <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> and <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;databricks/dolly-v2-7b&quot;</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">peft_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="c1"># Your training code here</span>
    <span class="o">...</span>

    <span class="c1"># Log the PEFT model</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">peft_model</span><span class="p">,</span>
            <span class="s2">&quot;tokenizer&quot;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;peft_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load the PEFT model</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="peft-models-in-mlflow-tutorial">
<h3>PEFT Models in MLflow Tutorial<a class="headerlink" href="#peft-models-in-mlflow-tutorial" title="Permalink to this headline"> </a></h3>
<p>Check out the tutorial <a class="reference external" href="../tutorials/fine-tuning/transformers-peft.html">Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT</a> for a more in-depth guide on how to use PEFT with MLflow,</p>
</div>
<div class="section" id="format-of-saved-peft-model">
<h3>Format of Saved PEFT Model<a class="headerlink" href="#format-of-saved-peft-model" title="Permalink to this headline"> </a></h3>
<p>When saving PEFT models, MLflow only saves the PEFT adapter and the configuration, but not the base model‚Äôs weights. This is the same behavior as the Transformer‚Äôs <a class="reference external" href="https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method and is highly efficient in terms of storage space and logging latency. One difference is that MLflow will also save the HuggingFace Hub repository name and version for the base model in the model metadata, so that it can load the same base model when loading the PEFT model. Concretely, the following artifacts are saved in MLflow for PEFT models:</p>
<ul class="simple">
<li><p>The PEFT adapter weight under the <code class="docutils literal notranslate"><span class="pre">/peft</span></code> directory.</p></li>
<li><p>The PEFT configuration as a JSON file under the <code class="docutils literal notranslate"><span class="pre">/peft</span></code> directory.</p></li>
<li><p>The HuggingFace Hub repository name and commit hash for the base model in the <code class="docutils literal notranslate"><span class="pre">MLModel</span></code> metadata file.</p></li>
</ul>
</div>
<div class="section" id="limitations-of-peft-models-in-mlflow">
<h3>Limitations of PEFT Models in MLflow<a class="headerlink" href="#limitations-of-peft-models-in-mlflow" title="Permalink to this headline"> </a></h3>
<p>Since the model saving/loading behavior for PEFT models is similar to that of <code class="docutils literal notranslate"><span class="pre">save_pretrained=False</span></code>, <a class="reference internal" href="#caveats-of-save-pretrained"><span class="std std-ref">the same caveats</span></a> apply to PEFT models. For example, the base model weight may be deleted or become private in the HuggingFace Hub, and PEFT models cannot be registered to the legacy Databricks Workspace Model Registry.</p>
<p>To save the base model weight for PEFT models, you can use the <a class="reference internal" href="../../../python_api/mlflow.transformers.html#mlflow.transformers.persist_pretrained_model" title="mlflow.transformers.persist_pretrained_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.persist_pretrained_model()</span></code></a> API. This will download the base model weight from the HuggingFace Hub and save it to the artifact location, updating the metadata of the given PEFT model. Please refer to <a class="reference internal" href="#persist-pretrained-guide"><span class="std std-ref">this section</span></a> for the detailed usage of this API.</p>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../tutorials/prompt-templating/prompt-templating.html" class="btn btn-neutral" title="Prompt Templating with MLflow and Transformers" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../../openai/index.html" class="btn btn-neutral" title="MLflow OpenAI Flavor" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../',
      VERSION:'2.13.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>