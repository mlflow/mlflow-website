
  

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to MLflow and OpenAI’s Whisper &mdash; MLflow 2.9.3.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/transformers/tutorials/audio-transcription/whisper.html">
  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../../search.html"/>
    <link rel="top" title="MLflow 2.9.3.dev0 documentation" href="../../../../index.html"/>
        <link rel="up" title="MLflow Transformers Flavor" href="../../index.html"/>
        <link rel="next" title="Introduction to Translation with Transformers and MLflow" href="/../translation/component-translation.html"/>
        <link rel="prev" title="Introduction to MLflow and Transformers" href="/../text-generation/text-generation.html"/> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="../../../../None"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.9.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../../index.html" class="main-navigation-home"><img src="../../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id1">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id2">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id3">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../../index.html">MLflow Transformers Flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../langchain/index.html">MLflow LangChain Flavor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id4">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">MLflow Transformers Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Introduction to MLflow and OpenAI’s Whisper</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/audio-transcription/whisper.ipynb" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Introduction-to-MLflow-and-OpenAI’s-Whisper">
<h1>Introduction to MLflow and OpenAI’s Whisper<a class="headerlink" href="#Introduction-to-MLflow-and-OpenAI’s-Whisper" title="Permalink to this headline"> </a></h1>
<p>Discover the integration of <a class="reference external" href="https://huggingface.co/openai">OpenAI’s Whisper</a>, an <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_recognition">ASR system</a>, with MLflow in this tutorial.</p>
<div class="section" id="What-You-Will-Learn-in-This-Tutorial">
<h2>What You Will Learn in This Tutorial<a class="headerlink" href="#What-You-Will-Learn-in-This-Tutorial" title="Permalink to this headline"> </a></h2>
<ul class="simple">
<li><p>Establish an audio transcription <strong>pipeline</strong> using the Whisper model.</p></li>
<li><p><strong>Log</strong> and manage Whisper models with MLflow.</p></li>
<li><p>Infer and understand Whisper model <strong>signatures</strong>.</p></li>
<li><p><strong>Load</strong> and interact with Whisper models stored in MLflow.</p></li>
<li><p>Utilize MLflow’s <strong>pyfunc</strong> for Whisper model serving and transcription tasks.</p></li>
</ul>
<div class="section" id="What-is-Whisper?">
<h3>What is Whisper?<a class="headerlink" href="#What-is-Whisper?" title="Permalink to this headline"> </a></h3>
<p>Whisper, developed by OpenAI, is a versatile ASR model trained for high-accuracy speech-to-text conversion. It stands out due to its training on diverse accents and environments, available via the Transformers library for easy use.</p>
</div>
<div class="section" id="Why-MLflow-with-Whisper?">
<h3>Why MLflow with Whisper?<a class="headerlink" href="#Why-MLflow-with-Whisper?" title="Permalink to this headline"> </a></h3>
<p>Integrating MLflow with Whisper enhances ASR model management:</p>
<ul class="simple">
<li><p><strong>Experiment Tracking</strong>: Facilitates tracking of model configurations and performance for optimal results.</p></li>
<li><p><strong>Model Management</strong>: Centralizes different versions of Whisper models, enhancing organization and accessibility.</p></li>
<li><p><strong>Reproducibility</strong>: Ensures consistency in transcriptions by tracking all components required for reproducing model behavior.</p></li>
<li><p><strong>Deployment</strong>: Streamlines the deployment of Whisper models in various production settings, ensuring efficient application.</p></li>
</ul>
<p>Interested in learning more about Whisper? To read more about the significant breakthroughs in transcription capabilities that Whisper brought to the field of ASR, you can <a class="reference external" href="https://arxiv.org/abs/2212.04356">read the white paper</a> and see more about the active development and <a class="reference external" href="https://openai.com/research/whisper">read more about the progress</a> at OpenAI’s research website.</p>
<p>Ready to enhance your speech-to-text capabilities? Let’s explore automatic speech recognition using MLflow and Whisper!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Disable tokenizers warnings when constructing pipelines</span>
<span class="o">%</span><span class="k">env</span> TOKENIZERS_PARALLELISM=false

<span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Disable a few less-than-useful UserWarnings from setuptools and pydantic</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
env: TOKENIZERS_PARALLELISM=false
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Setting-Up-the-Environment-and-Acquiring-Audio-Data">
<h2>Setting Up the Environment and Acquiring Audio Data<a class="headerlink" href="#Setting-Up-the-Environment-and-Acquiring-Audio-Data" title="Permalink to this headline"> </a></h2>
<p>Initial steps for transcription using <a class="reference external" href="https://github.com/openai/whisper">Whisper</a>: acquiring <a class="reference external" href="https://www.nasa.gov/audio-and-ringtones/">audio</a> and setting up MLflow.</p>
<p>Before diving into the audio transcription process with OpenAI’s Whisper, there are a few preparatory steps to ensure everything is in place for a smooth and effective transcription experience.</p>
<div class="section" id="Audio-Acquisition">
<h3>Audio Acquisition<a class="headerlink" href="#Audio-Acquisition" title="Permalink to this headline"> </a></h3>
<p>The first step is to acquire an audio file to work with. For this tutorial, we use a publicly available audio file from NASA. This sample audio provides a practical example to demonstrate Whisper’s transcription capabilities.</p>
</div>
<div class="section" id="Model-and-Pipeline-Initialization">
<h3>Model and Pipeline Initialization<a class="headerlink" href="#Model-and-Pipeline-Initialization" title="Permalink to this headline"> </a></h3>
<p>We load the Whisper model, along with its tokenizer and feature extractor, from the Transformers library. These components are essential for processing the audio data and converting it into a format that the Whisper model can understand and transcribe. Next, we create a transcription pipeline using the Whisper model. This pipeline simplifies the process of feeding audio data into the model and obtaining the transcription.</p>
</div>
<div class="section" id="MLflow-Environment-Setup">
<h3>MLflow Environment Setup<a class="headerlink" href="#MLflow-Environment-Setup" title="Permalink to this headline"> </a></h3>
<p>In addition to the model and audio data setup, we initialize our MLflow environment. MLflow is used to track and manage our experiments, offering an organized way to document the transcription process and results.</p>
<p>The following code block covers these initial setup steps, providing the foundation for our audio transcription task with the Whisper model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="kn">import</span> <span class="nn">mlflow</span>


<span class="c1"># Acquire an audio file that is in the public domain</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
    <span class="s2">&quot;https://www.nasa.gov/wp-content/uploads/2015/01/590325main_ringtone_kennedy_WeChoose.mp3&quot;</span>
<span class="p">)</span>
<span class="n">resp</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
<span class="n">audio</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">content</span>

<span class="c1"># Set the task that our pipeline implementation will be using</span>
<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;automatic-speech-recognition&quot;</span>

<span class="c1"># Define the model instance</span>
<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;openai/whisper-large-v3&quot;</span>

<span class="c1"># Load the components and necessary configuration for Whisper ASR from the Hugging Face Hub</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">WhisperForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">WhisperTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">WhisperFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">alignment_heads</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>

<span class="c1"># Instantiate our pipeline for ASR using the Whisper model</span>
<span class="n">audio_transcription_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">feature_extractor</span><span class="o">=</span><span class="n">feature_extractor</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Formatting-the-Transcription-Output">
<h2>Formatting the Transcription Output<a class="headerlink" href="#Formatting-the-Transcription-Output" title="Permalink to this headline"> </a></h2>
<p>In this section, we introduce a utility function that is used solely for the purpose of enhancing the readability of the transcription output within this Jupyter notebook demo. It is important to note that this function is designed for demonstration purposes and should not be included in production code or used for any other purpose beyond this tutorial.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">format_transcription</span></code> function takes a long string of transcribed text and formats it by splitting it into sentences and inserting newline characters. This makes the output easier to read when printed in the notebook environment.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">format_transcription</span><span class="p">(</span><span class="n">transcription</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function for formatting a long string by splitting into sentences and adding newlines.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Split the transcription into sentences, ensuring we don&#39;t split on abbreviations or initials</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">sentence</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;.&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">sentence</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">transcription</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;. &quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sentence</span>
    <span class="p">]</span>

    <span class="c1"># Join the sentences with a newline character</span>
    <span class="n">formatted_text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">formatted_text</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Executing-the-Transcription-Pipeline">
<h2>Executing the Transcription Pipeline<a class="headerlink" href="#Executing-the-Transcription-Pipeline" title="Permalink to this headline"> </a></h2>
<p>Perform audio transcription using the Whisper pipeline and review the output.</p>
<p>After setting up the Whisper model and audio transcription pipeline, our next step is to process an audio file to extract its transcription. This part of the tutorial is crucial as it demonstrates the practical application of the Whisper model in converting spoken language into written text.</p>
<div class="section" id="Transcription-Process">
<h3>Transcription Process<a class="headerlink" href="#Transcription-Process" title="Permalink to this headline"> </a></h3>
<p>The code block below feeds an audio file into the pipeline, which then produces the transcription. The <code class="docutils literal notranslate"><span class="pre">format_transcription</span></code> function, defined earlier, enhances readability by formatting the output with sentence splits and newline characters.</p>
</div>
<div class="section" id="Importance-of-Pre-Save-Testing">
<h3>Importance of Pre-Save Testing<a class="headerlink" href="#Importance-of-Pre-Save-Testing" title="Permalink to this headline"> </a></h3>
<p>Testing the transcription pipeline before saving the model in MLflow is vital. This step verifies that the model works as expected, ensuring accuracy and reliability. Such validation avoids issues post-deployment and confirms that the model performs consistently with the training data it was exposed to. It also provides a benchmark to compare against the output after the model is loaded back from MLflow, ensuring consistency in performance.</p>
<p>Execute the following code to transcribe the audio and assess the quality and accuracy of the transcription provided by the Whisper model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify that our pipeline is capable of processing an audio file and transcribing it</span>
<span class="n">transcription</span> <span class="o">=</span> <span class="n">audio_transcription_pipeline</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">format_transcription</span><span class="p">(</span><span class="n">transcription</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
We choose to go to the moon in this decade and do the other things.
Not because they are easy, but because they are hard.
3, 2, 1, 0.
All engines running.
Liftoff.
We have a liftoff.
32 minutes past the hour.
Liftoff on Apollo 11.
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Model-Signature-and-Configuration">
<h2>Model Signature and Configuration<a class="headerlink" href="#Model-Signature-and-Configuration" title="Permalink to this headline"> </a></h2>
<p>Generate a model signature for Whisper to understand its input and output data requirements.</p>
<p>The model signature is critical for defining the schema for the Whisper model’s inputs and outputs, clarifying the data types and structures expected. This step ensures the model processes inputs correctly and outputs structured data.</p>
<div class="section" id="Handling-Different-Audio-Formats">
<h3>Handling Different Audio Formats<a class="headerlink" href="#Handling-Different-Audio-Formats" title="Permalink to this headline"> </a></h3>
<p>While the default signature covers binary audio data, the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor accommodates multiple formats, including numpy arrays and URL-based inputs. This flexibility allows Whisper to transcribe from various sources, although URL-based transcription isn’t demonstrated here.</p>
</div>
<div class="section" id="Model-Configuration">
<h3>Model Configuration<a class="headerlink" href="#Model-Configuration" title="Permalink to this headline"> </a></h3>
<p>Setting the model configuration involves parameters like <em>chunk</em> and <em>stride</em> lengths for audio processing. These settings are adjustable to suit different transcription needs, enhancing Whisper’s performance for specific scenarios.</p>
<p>Run the next code block to infer the model’s signature and configure key parameters, aligning Whisper’s functionality with your project’s requirements.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify parameters and their defaults that we would like to be exposed for manipulation during inference time</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;chunk_length_s&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;stride_length_s&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># Define the model signature by using the input and output of our pipeline, as well as specifying our inference parameters that will allow for those parameters to</span>
<span class="c1"># be overridden at inference time.</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">audio</span><span class="p">,</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generate_signature_output</span><span class="p">(</span><span class="n">audio_transcription_pipeline</span><span class="p">,</span> <span class="n">audio</span><span class="p">),</span>
    <span class="n">params</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Visualize the signature</span>
<span class="n">signature</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
inputs:
  [binary]
outputs:
  [string]
params:
  [&#39;chunk_length_s&#39;: long (default: 20), &#39;stride_length_s&#39;: long (default: [5, 3]) (shape: (-1,))]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Creating-an-experiment">
<h2>Creating an experiment<a class="headerlink" href="#Creating-an-experiment" title="Permalink to this headline"> </a></h2>
<p>We create a new MLflow Experiment so that the run we’re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># If you are running this tutorial in local mode, leave the next line commented out.</span>
<span class="c1"># Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.</span>

<span class="c1"># mlflow.set_tracking_uri(&quot;http://127.0.0.1:8080&quot;)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;Whisper Transcription ASR&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Experiment: artifact_location=&#39;file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/audio-transcription/mlruns/864092483920291025&#39;, creation_time=1701294423466, experiment_id=&#39;864092483920291025&#39;, last_update_time=1701294423466, lifecycle_stage=&#39;active&#39;, name=&#39;Whisper Transcription ASR&#39;, tags={}&gt;
</pre></div></div>
</div>
</div>
<div class="section" id="Logging-the-Model-with-MLflow">
<h2>Logging the Model with MLflow<a class="headerlink" href="#Logging-the-Model-with-MLflow" title="Permalink to this headline"> </a></h2>
<p>Learn how to log the Whisper model and its configurations with MLflow.</p>
<p>Logging the Whisper model in MLflow is a critical step for capturing essential information for model reproduction, sharing, and deployment. This process involves:</p>
<div class="section" id="Key-Components-of-Model-Logging">
<h3>Key Components of Model Logging<a class="headerlink" href="#Key-Components-of-Model-Logging" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><strong>Model Information</strong>: Includes the model, its signature, and an input example.</p></li>
<li><p><strong>Model Configuration</strong>: Any specific parameters set for the model, like <em>chunk length</em> or <em>stride length</em>.</p></li>
</ul>
</div>
<div class="section" id="Using-MLflow’s-log_model-Function">
<h3>Using MLflow’s <code class="docutils literal notranslate"><span class="pre">log_model</span></code> Function<a class="headerlink" href="#Using-MLflow’s-log_model-Function" title="Permalink to this headline"> </a></h3>
<p>This function is utilized within an MLflow run to log the model and its configurations. It ensures that all necessary components for model usage are recorded.</p>
<p>Executing the code in the next cell will log the Whisper model in the current MLflow experiment. This includes storing the model in a specified artifact path and documenting the default configurations that will be applied during inference.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Log the pipeline</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">audio_transcription_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;whisper_transcriber&quot;</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Loading-and-Using-the-Model-Pipeline">
<h2>Loading and Using the Model Pipeline<a class="headerlink" href="#Loading-and-Using-the-Model-Pipeline" title="Permalink to this headline"> </a></h2>
<p>Explore how to load and use the Whisper model pipeline from MLflow.</p>
<p>After logging the Whisper model in MLflow, the next crucial step is to load and use it for inference. This process ensures that our logged model operates as intended and can be effectively used for tasks like audio transcription.</p>
<div class="section" id="Loading-the-Model">
<h3>Loading the Model<a class="headerlink" href="#Loading-the-Model" title="Permalink to this headline"> </a></h3>
<p>The model is loaded in its native format using MLflow’s <code class="docutils literal notranslate"><span class="pre">load_model</span></code> function. This step verifies that the model can be retrieved and used seamlessly after being logged in MLflow.</p>
</div>
<div class="section" id="Using-the-Loaded-Model">
<h3>Using the Loaded Model<a class="headerlink" href="#Using-the-Loaded-Model" title="Permalink to this headline"> </a></h3>
<p>Once loaded, the model is ready for inference. We demonstrate this by passing an MP3 audio file to the model and obtaining its transcription. This test is a practical demonstration of the model’s capabilities post-logging.</p>
<p>This step is a form of validation before moving to more complex deployment scenarios. Ensuring that the model functions correctly in its native format helps in troubleshooting and streamlines the deployment process, especially for large and complex models like Whisper.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the pipeline in its native format</span>
<span class="n">loaded_transcriber</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Perform transcription with the native pipeline implementation</span>
<span class="n">transcription</span> <span class="o">=</span> <span class="n">loaded_transcriber</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Whisper native output transcription:</span><span class="se">\n</span><span class="si">{</span><span class="n">format_transcription</span><span class="p">(</span><span class="n">transcription</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023/11/30 12:51:43 INFO mlflow.transformers: &#39;runs:/f7503a09d20f4fb481544968b5ed28dd/whisper_transcriber&#39; resolved as &#39;file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/audio-transcription/mlruns/864092483920291025/f7503a09d20f4fb481544968b5ed28dd/artifacts/whisper_transcriber&#39;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7f77084954924ccd8701a795685edd79", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Whisper native output transcription:
We choose to go to the moon in this decade and do the other things.
Not because they are easy, but because they are hard.
3, 2, 1, 0.
All engines running.
Liftoff.
We have a liftoff.
32 minutes past the hour.
Liftoff on Apollo 11.
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Using-the-Pyfunc-Flavor-for-Inference">
<h2>Using the Pyfunc Flavor for Inference<a class="headerlink" href="#Using-the-Pyfunc-Flavor-for-Inference" title="Permalink to this headline"> </a></h2>
<p>Learn how MLflow’s <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> flavor facilitates flexible model deployment.</p>
<p>MLflow’s <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> flavor provides a generic interface for model inference, offering flexibility across various machine learning frameworks and deployment environments. This feature is beneficial for deploying models where the original framework may not be available, or a more adaptable interface is required.</p>
<div class="section" id="Loading-and-Predicting-with-Pyfunc">
<h3>Loading and Predicting with Pyfunc<a class="headerlink" href="#Loading-and-Predicting-with-Pyfunc" title="Permalink to this headline"> </a></h3>
<p>The code below illustrates how to load the Whisper model as a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> and use it for prediction. This method highlights MLflow’s capability to adapt and deploy models in diverse scenarios.</p>
</div>
<div class="section" id="Output-Format-Considerations">
<h3>Output Format Considerations<a class="headerlink" href="#Output-Format-Considerations" title="Permalink to this headline"> </a></h3>
<p>Note the difference in the output format when using <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> compared to the native format. The <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> output conforms to standard pyfunc output signatures, typically represented as a <code class="docutils literal notranslate"><span class="pre">List[str]</span></code> type, aligning with broader MLflow standards for model outputs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the saved transcription pipeline as a generic python function</span>
<span class="n">pyfunc_transcriber</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Ensure that the pyfunc wrapper is capable of transcribing passed-in audio</span>
<span class="n">pyfunc_transcription</span> <span class="o">=</span> <span class="n">pyfunc_transcriber</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">audio</span><span class="p">])</span>

<span class="c1"># Note: the pyfunc return type if `return_timestamps` is set is a JSON encoded string.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Pyfunc output transcription:</span><span class="se">\n</span><span class="si">{</span><span class="n">format_transcription</span><span class="p">(</span><span class="n">pyfunc_transcription</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "4ee8e92c15ca4084b87d643368229364", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2023/11/30 12:52:02 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Pyfunc output transcription:
We choose to go to the moon in this decade and do the other things.
Not because they are easy, but because they are hard.
3, 2, 1, 0.
All engines running.
Liftoff.
We have a liftoff.
32 minutes past the hour.
Liftoff on Apollo 11.
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Tutorial-Roundup">
<h2>Tutorial Roundup<a class="headerlink" href="#Tutorial-Roundup" title="Permalink to this headline"> </a></h2>
<p>Throughout this tutorial, we’ve explored how to:</p>
<ul class="simple">
<li><p>Set up an audio transcription pipeline using the OpenAI Whisper model.</p></li>
<li><p>Format and prepare audio data for transcription.</p></li>
<li><p>Log, load, and use the model with MLflow, leveraging both the native and pyfunc flavors for inference.</p></li>
<li><p>Format the output for readability and practical use in a Jupyter Notebook environment.</p></li>
</ul>
<p>We’ve seen the benefits of using MLflow for managing the machine learning lifecycle, including experiment tracking, model versioning, reproducibility, and deployment. By integrating MLflow with the Transformers library, we’ve streamlined the process of working with state-of-the-art NLP models, making it easier to track, manage, and deploy cutting-edge NLP applications.</p>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../text-generation/text-generation.html" class="btn btn-neutral" title="Introduction to MLflow and Transformers" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../translation/component-translation.html" class="btn btn-neutral" title="Introduction to Translation with Transformers and MLflow" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../../',
      VERSION:'2.9.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>