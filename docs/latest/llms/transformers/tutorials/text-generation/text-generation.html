
  

<!DOCTYPE html>
<!-- source: docs/source/llms/transformers/tutorials/text-generation/text-generation.ipynb -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to MLflow and Transformers &mdash; MLflow 2.14.4.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/transformers/tutorials/text-generation/text-generation.html">
  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../../search.html"/>
    <link rel="top" title="MLflow 2.14.4.dev0 documentation" href="../../../../index.html"/>
        <link rel="up" title="MLflow Transformers Flavor" href="../../index.html"/>
        <link rel="next" title="Introduction to MLflow and OpenAIâ€™s Whisper" href="/../audio-transcription/whisper.html"/>
        <link rel="prev" title="MLflow Transformers Flavor" href="/../../index.html"/> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="../../../../None"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.14.4.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../../index.html" class="main-navigation-home"><img src="../../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id2">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../index.html">MLflow Transformers Flavor</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../index.html#introduction">Introduction</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../index.html#getting-started-with-the-mlflow-transformers-flavor-tutorials-and-guides">Getting Started with the MLflow Transformers Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#important-details-to-be-aware-of-with-the-transformers-flavor">Important Details to be aware of with the transformers flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#working-with-tasks-for-transformer-pipelines">Working with <code class="docutils literal notranslate"><span class="pre">tasks</span></code> for Transformer Pipelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#id1">Detailed Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#learn-more-about-transformers">Learn more about Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../langchain/index.html">MLflow LangChain Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../llama-index/index.html">MLflow LlamaIndex Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id5">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">MLflow Transformers Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Introduction to MLflow and Transformers</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/text-generation/text-generation.ipynb" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Introduction-to-MLflow-and-Transformers">
<h1>Introduction to MLflow and Transformers<a class="headerlink" href="#Introduction-to-MLflow-and-Transformers" title="Permalink to this headline"> </a></h1>
<p>Welcome to our tutorial on leveraging the power of <strong>Transformers</strong> with <strong>MLflow</strong>. This guide is designed for beginners, focusing on machine learning workflows and model management.</p>
<a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/transformers/tutorials/text-generation/text-generation.ipynb" class="notebook-download-btn">Download this Notebook</a><div class="section" id="What-Will-You-Learn?">
<h2>What Will You Learn?<a class="headerlink" href="#What-Will-You-Learn?" title="Permalink to this headline"> </a></h2>
<p>In this tutorial, you will learn how to:</p>
<ul class="simple">
<li><p>Set up a simple text generation pipeline using the Transformers library.</p></li>
<li><p>Log the model and its parameters using MLflow.</p></li>
<li><p>Infer the input and output signature of the model automatically.</p></li>
<li><p>Simulate serving the model using MLflow and make predictions with it.</p></li>
</ul>
<div class="section" id="Introduction-to-Transformers">
<h3>Introduction to Transformers<a class="headerlink" href="#Introduction-to-Transformers" title="Permalink to this headline"> </a></h3>
<p>Transformers are a type of deep learning model that have revolutionized natural language processing (NLP). Developed by <a class="reference external" href="https://huggingface.co/docs/transformers/index">ðŸ¤— Hugging Face</a>, the Transformers library offers a variety of state-of-the-art pre-trained models for NLP tasks.</p>
</div>
<div class="section" id="Why-Combine-MLflow-with-Transformers?">
<h3>Why Combine MLflow with Transformers?<a class="headerlink" href="#Why-Combine-MLflow-with-Transformers?" title="Permalink to this headline"> </a></h3>
<p>Integrating MLflow with Transformers offers numerous benefits:</p>
<ul class="simple">
<li><p><strong>Experiment Tracking</strong>: Log and compare model parameters and metrics.</p></li>
<li><p><strong>Model Management</strong>: Track different model versions and their performance.</p></li>
<li><p><strong>Reproducibility</strong>: Document all aspects needed to reproduce predictions.</p></li>
<li><p><strong>Deployment</strong>: Simplify deploying Transformers models to production.</p></li>
</ul>
<p>Now, letâ€™s dive into the world of MLflow and Transformers!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Disable tokenizers warnings when constructing pipelines</span>
<span class="o">%</span><span class="k">env</span> TOKENIZERS_PARALLELISM=false

<span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Disable a few less-than-useful UserWarnings from setuptools and pydantic</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
env: TOKENIZERS_PARALLELISM=false
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Imports-and-Pipeline-configuration">
<h2>Imports and Pipeline configuration<a class="headerlink" href="#Imports-and-Pipeline-configuration" title="Permalink to this headline"> </a></h2>
<p>In this first section, we are setting up our environment and configuring aspects of the transformers pipeline that weâ€™ll be using to generate a text response from the LLM.</p>
<div class="section" id="Setting-up-our-Pipeline">
<h3>Setting up our Pipeline<a class="headerlink" href="#Setting-up-our-Pipeline" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><strong>Import</strong>: We import the necessary libraries: transformers for building our NLP model and mlflow for model tracking and management.</p></li>
<li><p><strong>Task Definition</strong>: We then define the task for our pipeline, which in this case is `text2text-generation`` This task involves generating new text based on the input text.</p></li>
<li><p><strong>Pipeline Declaration</strong>: Next, we create a generation_pipeline using the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> function from the Transformers library. This pipeline is configured to use the <code class="docutils literal notranslate"><span class="pre">declare-lab/flan-alpaca-large</span> <span class="pre">model</span></code>, which is a pre-trained model suitable for text generation.</p></li>
<li><p><strong>Input Example</strong>: For the purposes of generating a signature later on, as well as having a visual indicator of the expected input data to our model when loading as a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>, we next set up an input_example that contains sample prompts.</p></li>
<li><p><strong>Inference Parameters</strong>: Finally, we define parameters that will be used to control the behavior of the model during inference, such as the maximum length of the generated text and whether to sample multiple times.</p></li>
</ul>
</div>
<div class="section" id="Understanding-Pipelines">
<h3>Understanding Pipelines<a class="headerlink" href="#Understanding-Pipelines" title="Permalink to this headline"> </a></h3>
<p>Pipelines are a high-level abstraction provided by the Transformers library that simplifies the process of using models for inference. They encapsulate the complexity of the underlying code, offering a straightforward API for a variety of tasks, such as text classification, question answering, and in our case, text generation.</p>
<div class="section" id="The-pipeline()-function">
<h4>The <code class="docutils literal notranslate"><span class="pre">pipeline()</span></code> function<a class="headerlink" href="#The-pipeline()-function" title="Permalink to this headline"> </a></h4>
<p>The pipeline() function is a versatile tool that can be used to create a pipeline for any supported task. When you specify a task, the function returns a pipeline object tailored for that task, constructing the required calls to sub-components (a tokenizer, encoder, generative model, etc.) in the order needed to fulfill the needs of the specified task. This abstraction dramatically simplifies the code required to use these models and their respective components.</p>
</div>
<div class="section" id="Task-Specific-Pipelines">
<h4>Task-Specific Pipelines<a class="headerlink" href="#Task-Specific-Pipelines" title="Permalink to this headline"> </a></h4>
<p>In addition to the general pipeline() function, there are task-specific pipelines for different domains like audio, computer vision, and natural language processing. These specialized pipelines are optimized for their respective tasks and can provide additional convenience and functionality.</p>
</div>
<div class="section" id="Benefits-of-Using-Pipelines">
<h4>Benefits of Using Pipelines<a class="headerlink" href="#Benefits-of-Using-Pipelines" title="Permalink to this headline"> </a></h4>
<p>Using pipelines has several advantages:</p>
<ul class="simple">
<li><p><strong>Simplicity</strong>: You can perform complex tasks with a minimal amount of code.</p></li>
<li><p><strong>Flexibility</strong>: You can specify different models and configurations to customize the pipeline for your needs.</p></li>
<li><p><strong>Efficiency</strong>: Pipelines handle batching and dataset iteration internally, which can lead to performance improvements.</p></li>
</ul>
<p>Due to the utility and simple, high-level API, MLflowâ€™s <code class="docutils literal notranslate"><span class="pre">transformers</span></code> implementation uses the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> abstraction by default (although it can support component-only mode as well).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Define the task that we want to use (required for proper pipeline construction)</span>
<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;text2text-generation&quot;</span>

<span class="c1"># Define the pipeline, using the task and a model instance that is applicable for our task.</span>
<span class="n">generation_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;declare-lab/flan-alpaca-large&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define a simple input example that will be recorded with the model in MLflow, giving</span>
<span class="c1"># users of the model an indication of the expected input format.</span>
<span class="n">input_example</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;prompt 1&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt 2&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt 3&quot;</span><span class="p">]</span>

<span class="c1"># Define the parameters (and their defaults) for optional overrides at inference time.</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="Introduction-to-Model-Signatures-in-MLflow">
<h2>Introduction to Model Signatures in MLflow<a class="headerlink" href="#Introduction-to-Model-Signatures-in-MLflow" title="Permalink to this headline"> </a></h2>
<p>In the realm of machine learning, model signatures play a crucial role in ensuring that models receive and produce the expected data types and structures. MLflow includes a feature for defining model signatures, helping to standardize and enforce correct model usage.</p>
<div class="section" id="Quick-Learning-Points">
<h3>Quick Learning Points<a class="headerlink" href="#Quick-Learning-Points" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><strong>Model Signature Purpose</strong>: Ensures consistent data types and structures for model inputs and outputs.</p></li>
<li><p><strong>Visibility and Validation</strong>: Visible in MLflowâ€™s UI and validated by MLflowâ€™s deployment tools.</p></li>
<li><p><strong>Signature Types</strong>: Column-based for tabular data, tensor-based for tensor inputs/outputs, and with params for models requiring additional inference parameters.</p></li>
</ul>
</div>
<div class="section" id="Understanding-Model-Signatures">
<h3>Understanding Model Signatures<a class="headerlink" href="#Understanding-Model-Signatures" title="Permalink to this headline"> </a></h3>
<p>A model signature in MLflow describes the schema for inputs, outputs, and parameters of an ML model. It is a blueprint that details the expected data types and shapes, facilitating a clear interface for model usage. Signatures are particularly useful as they are:</p>
<ul class="simple">
<li><p>Displayed in MLflowâ€™s UI for easy reference.</p></li>
<li><p>Employed by MLflowâ€™s deployment tools to validate inputs during inference.</p></li>
<li><p>Stored in a standardized JSON format alongside the modelâ€™s metadata.</p></li>
</ul>
</div>
<div class="section" id="The-Role-of-Signatures-in-Code">
<h3>The Role of Signatures in Code<a class="headerlink" href="#The-Role-of-Signatures-in-Code" title="Permalink to this headline"> </a></h3>
<p>In the following section, we are using MLflow to infer the signature of a machine learning model. This involves specifying an input example, generating a model output example, and defining any additional inference parameters. The resulting signature is used to validate future inputs and document the expected data formats.</p>
</div>
<div class="section" id="Types-of-Model-Signatures">
<h3>Types of Model Signatures<a class="headerlink" href="#Types-of-Model-Signatures" title="Permalink to this headline"> </a></h3>
<p>Model signatures can be:</p>
<ul class="simple">
<li><p><strong>Column-based</strong>: Suitable for models that operate on tabular data, with each column having a specified data type and optional name.</p></li>
<li><p><strong>Tensor-based</strong>: Designed for models that take tensors as inputs and outputs, with each tensor having a specified data type, shape, and optional name.</p></li>
<li><p><strong>With Params</strong>: Some models require additional parameters for inference, which can also be included in the signature.</p></li>
</ul>
<p>For the transformers flavor, all input types are of the Column-based type (referred to within MLflow as <code class="docutils literal notranslate"><span class="pre">ColSpec</span></code> types).</p>
</div>
<div class="section" id="Signature-Enforcement">
<h3>Signature Enforcement<a class="headerlink" href="#Signature-Enforcement" title="Permalink to this headline"> </a></h3>
<p>MLflow enforces the signature at the time of model inference, ensuring that the provided input and parameters match the expected schema. If thereâ€™s a mismatch, MLflow will raise an exception or issue a warning, depending on the nature of the mismatch.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate the signature for the model that will be used for inference validation and type checking (as well as validation of parameters being submitted during inference)</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">input_example</span><span class="p">,</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generate_signature_output</span><span class="p">(</span><span class="n">generation_pipeline</span><span class="p">,</span> <span class="n">input_example</span><span class="p">),</span>
    <span class="n">parameters</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Visualize the signature</span>
<span class="n">signature</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
inputs:
  [string]
outputs:
  [string]
params:
  [&#39;max_length&#39;: long (default: 512), &#39;do_sample&#39;: boolean (default: True), &#39;temperature&#39;: double (default: 0.4)]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Creating-an-experiment">
<h2>Creating an experiment<a class="headerlink" href="#Creating-an-experiment" title="Permalink to this headline"> </a></h2>
<p>We create a new MLflow Experiment so that the run weâ€™re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># If you are running this tutorial in local mode, leave the next line commented out.</span>
<span class="c1"># Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.</span>

<span class="c1"># mlflow.set_tracking_uri(&quot;http://127.0.0.1:8080&quot;)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;Transformers Introduction&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Experiment: artifact_location=&#39;file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/text-generation/mlruns/528654983476503755&#39;, creation_time=1701288466448, experiment_id=&#39;528654983476503755&#39;, last_update_time=1701288466448, lifecycle_stage=&#39;active&#39;, name=&#39;Transformers Introduction&#39;, tags={}&gt;
</pre></div></div>
</div>
</div>
<div class="section" id="Logging-the-Transformers-Model-with-MLflow">
<h2>Logging the Transformers Model with MLflow<a class="headerlink" href="#Logging-the-Transformers-Model-with-MLflow" title="Permalink to this headline"> </a></h2>
<p>We log our model with MLflow to manage its lifecycle efficiently and keep track of its versions and configurations.</p>
<p>Logging a model in MLflow is a crucial step in the model lifecycle management, enabling efficient tracking, versioning, and management. The process involves registering the model along with its essential metadata within the MLflow tracking system.</p>
<p>Utilizing the <a class="reference external" href="https://www.mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model">mlflow.transformers.log_model</a> function, specifically tailored for models and components from the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library, simplifies this task. This function is adept at handling various aspects of the models from this library, including their complex pipelines and configurations.</p>
<p>When logging the model, crucial metadata such as the modelâ€™s signature, which was previously established, is included. This metadata plays a significant role in the subsequent stages of the modelâ€™s lifecycle, from tracking its evolution to facilitating its deployment in different environments. The signature, in particular, ensures the modelâ€™s compatibility and consistent performance across various platforms, thereby enhancing its utility and reliability in practical applications.</p>
<p>By logging our model in this way, we ensure that it is not only well-documented but also primed for future use, whether it be for further development, comparative analysis, or deployment.</p>
<div class="section" id="A-Tip-for-Saving-Storage-Cost">
<h3>A Tip for Saving Storage Cost<a class="headerlink" href="#A-Tip-for-Saving-Storage-Cost" title="Permalink to this headline"> </a></h3>
<p>When you call <a class="reference external" href="https://www.mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model">mlflow.transformers.log_model</a>, MLflow will saves a full copy of the Transformers model weight. However, this could take large storage space (3GB for <code class="docutils literal notranslate"><span class="pre">flan-alpaca-large</span></code> model), but might be redundant when you donâ€™t modify the model weight, because it is exactly same as the one you can download from the HuggingFace model hub.</p>
<p>To avoid the unnecessary copy, you can use â€˜reference-onlyâ€™ save mode which is introduced in MLflow 2.11.0, by setting <code class="docutils literal notranslate"><span class="pre">save_pretrained=False</span></code> when logging or saving the Transformer model. This tells MLflow not to save the copy of the base model weight, but just a reference to the HuggingFace Hub repository and version, hence more storage-efficient and faster in time. For more details about this feature, please refer to <a class="reference external" href="https://www.mlflow.org/docs/latest/llms/transformers/guide/index.html#storage-efficient-model-logging-with-save-pretrained-option">Storage-Efficient Model
Logging</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">generation_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;text_generator&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="c1"># Transformer model does not use Pandas Dataframe as input, internal input type conversion should be skipped.</span>
        <span class="n">example_no_conversion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># Uncomment the following line to save the model in &#39;reference-only&#39; mode:</span>
        <span class="c1"># save_pretrained=False,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Loading-the-Text-Generation-Model">
<h2>Loading the Text Generation Model<a class="headerlink" href="#Loading-the-Text-Generation-Model" title="Permalink to this headline"> </a></h2>
<p>We initialize our text generation model using MLflowâ€™s pyfunc module for seamless model loading and interaction.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> module in MLflow serves as a generic wrapper for Python functions. Its application in MLflow facilitates the loading of machine learning models as standard Python functions. This approach is especially advantageous for models logged or registered via MLflow, streamlining the interaction with the model regardless of its training or serialization specifics.</p>
<p>Utilizing <a class="reference external" href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model">mlflow.pyfunc.load_model</a>, our previously logged text generation model is loaded using its unique model URI. This URI is a reference to the stored model artifacts. MLflow efficiently handles the modelâ€™s deserialization, along with any associated dependencies, preparing it for immediate use.</p>
<p>Once the model, referred to as <code class="docutils literal notranslate"><span class="pre">sentence_generator</span></code>, is loaded, it operates as a conventional Python function. This functionality allows for the generation of text based on given prompts. The model encompasses the complete process of inference, eliminating the need for manual input preprocessing or output postprocessing. This encapsulation not only simplifies model interaction but also ensures the modelâ€™s adaptability for deployment across various platforms.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load our pipeline as a generic python function</span>
<span class="n">sentence_generator</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "60ad7546d18a44789207819b8f0889ad", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="section" id="Formatting-Predictions-for-Tutorial-Readability">
<h3>Formatting Predictions for Tutorial Readability<a class="headerlink" href="#Formatting-Predictions-for-Tutorial-Readability" title="Permalink to this headline"> </a></h3>
<p>Please note that the following function, <code class="docutils literal notranslate"><span class="pre">format_predictions</span></code>, is used only for enhancing the readability of model predictions within this Jupyter Notebook environment. It <strong>is not a standard component</strong> of the modelâ€™s inference pipeline.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">format_predictions</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function for formatting the output for readability in a Jupyter Notebook</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">formatted_predictions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
        <span class="c1"># Split the output into sentences, ensuring we don&#39;t split on abbreviations or initials</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">sentence</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;.&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">sentence</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">prediction</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;. &quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">sentence</span>
        <span class="p">]</span>

        <span class="c1"># Join the sentences with a newline character</span>
        <span class="n">formatted_text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

        <span class="c1"># Add the formatted text to the list</span>
        <span class="n">formatted_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">formatted_text</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">formatted_predictions</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Generating-Predictions-with-Custom-Parameters">
<h2>Generating Predictions with Custom Parameters<a class="headerlink" href="#Generating-Predictions-with-Custom-Parameters" title="Permalink to this headline"> </a></h2>
<p>In this section, we demonstrate generating predictions using a sentence generator model with custom parameters. This includes prompts for selecting weekend activities and requesting a joke.</p>
<div class="section" id="Quick-Overview">
<h3>Quick Overview<a class="headerlink" href="#Quick-Overview" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><strong>Scenario</strong>: Generating text for different prompts.</p></li>
<li><p><strong>Custom Parameter</strong>: Overriding the <code class="docutils literal notranslate"><span class="pre">temperature</span></code> parameter to control text randomness.</p></li>
<li><p><strong>Default Values</strong>: Other parameters use their defaults unless explicitly overridden.</p></li>
</ul>
</div>
<div class="section" id="Prediction-Process-Explained">
<h3>Prediction Process Explained<a class="headerlink" href="#Prediction-Process-Explained" title="Permalink to this headline"> </a></h3>
<p>We use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method on the <code class="docutils literal notranslate"><span class="pre">sentence_generator</span></code> pyfunc model with a list of string prompts, including:</p>
<ul class="simple">
<li><p>A request for help in choosing between hiking and kayaking for a weekend activity.</p></li>
<li><p>A prompt asking for a joke related to hiking.</p></li>
</ul>
<p>To influence the generation process, we override the <code class="docutils literal notranslate"><span class="pre">temperature</span></code> parameter. This parameter impacts the randomness of the generated text:</p>
<ul class="simple">
<li><p><strong>Lower Temperature</strong>: Leads to more predictable and conservative outputs.</p></li>
<li><p><strong>Higher Temperature</strong>: Fosters varied and creative responses.</p></li>
</ul>
</div>
<div class="section" id="Utilizing-Custom-Parameters">
<h3>Utilizing Custom Parameters<a class="headerlink" href="#Utilizing-Custom-Parameters" title="Permalink to this headline"> </a></h3>
<p>In this example, the <code class="docutils literal notranslate"><span class="pre">temperature</span></code> parameter is explicitly set for the prediction call. Other parameters set during model logging will use their default values, unless also overridden in the <code class="docutils literal notranslate"><span class="pre">params</span></code> argument of the prediction call.</p>
</div>
<div class="section" id="Output-Formatting">
<h3>Output Formatting<a class="headerlink" href="#Output-Formatting" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">predictions</span></code> variable captures the modelâ€™s output for each input prompt. We can format these outputs for enhanced readability in the following steps, presenting the generated text in a clear and accessible manner.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Validate that our loaded pipeline, as a generic pyfunc, can produce an output that makes sense</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">sentence_generator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;I can&#39;t decide whether to go hiking or kayaking this weekend. Can you help me decide?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Please tell me a joke about hiking.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Format each prediction for notebook readability</span>
<span class="n">formatted_predictions</span> <span class="o">=</span> <span class="n">format_predictions</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">formatted_text</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">formatted_predictions</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response to prompt </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="si">{</span><span class="n">formatted_text</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023/11/30 14:24:08 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Response to prompt 1:
Going hiking can be a great way to explore the outdoors and have fun, while kayaking can be an exciting way to take in the scenery and have a great experience.

Response to prompt 2:
Q: What did the bird say when he was walking in the woods? a: &#34;Hey, I&#39;m going to get some food!&#34;.

</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Closing-Remarks">
<h2>Closing Remarks<a class="headerlink" href="#Closing-Remarks" title="Permalink to this headline"> </a></h2>
<p>This demonstration showcases the flexibility and power of the model in generating contextually relevant and creative text responses. By formatting the outputs, we ensure that the results are not only accurate but also presented in a manner that is easy to read and understand, enhancing the overall user experience within this Jupyter Notebook environment.</p>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../index.html" class="btn btn-neutral" title="MLflow Transformers Flavor" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../audio-transcription/whisper.html" class="btn btn-neutral" title="Introduction to MLflow and OpenAIâ€™s Whisper" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../../',
      VERSION:'2.14.4.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>