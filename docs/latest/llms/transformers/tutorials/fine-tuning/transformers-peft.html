

<!DOCTYPE html>
<!-- source: docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-peft.html">
  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../../search.html"/>
    <link rel="top" title="MLflow 2.17.3.dev0 documentation" href="../../../../index.html"/>
        <link rel="up" title="MLflow Transformers Flavor" href="../../index.html"/>
        <link rel="next" title="Prompt Templating with MLflow and Transformers" href="/../prompt-templating/prompt-templating.html"/>
        <link rel="prev" title="Fine-Tuning Transformers with MLflow for Enhanced Model Management" href="/transformers-fine-tuning.html"/> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="../../../../None"></script>
<script type="text/javascript" src="../../../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../../index.html" class="main-navigation-home"><img src="../../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#tutorials-and-use-case-guides-for-genai-applications-in-mlflow">Tutorials and Use Case Guides for GenAI applications in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id2">MLflow AI Gateway for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../index.html">MLflow Transformers Flavor</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../index.html#introduction">Introduction</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../index.html#getting-started-with-the-mlflow-transformers-flavor-tutorials-and-guides">Getting Started with the MLflow Transformers Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#important-details-to-be-aware-of-with-the-transformers-flavor">Important Details to be aware of with the transformers flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#logging-large-models">Logging Large Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#working-with-tasks-for-transformer-pipelines">Working with <code class="docutils literal notranslate"><span class="pre">tasks</span></code> for Transformer Pipelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#id1">Detailed Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#learn-more-about-transformers">Learn more about Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../langchain/index.html">MLflow LangChain Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../llama-index/index.html">MLflow LlamaIndex Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../dspy/index.html">MLflow DSPy Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id5">LLM Tracking in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">MLflow Transformers Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Fine-Tuning-Open-Source-LLM-using-QLoRA-with-MLflow-and-PEFT">
<h1>Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT<a class="headerlink" href="#Fine-Tuning-Open-Source-LLM-using-QLoRA-with-MLflow-and-PEFT" title="Permalink to this headline"> </a></h1>
<p></p>
<a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb" class="notebook-download-btn">Download this Notebook</a><br><div class="section" id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Permalink to this headline"> </a></h2>
<p>Many powerful open-source LLMs have emerged and are easily accessible. However, they are not designed to be deployed to your production environment out-of-the-box; instead, you have to <strong>fine-tune</strong> them for your specific tasks, such as a chatbot, content generation, etc. One challenge, though, is that training LLMs is usually very expensive. Even if your dataset for fine-tuning is small, the backpropagation step needs to compute gradients for billions of parameters. For example, fully
fine-tuning the Llama7B model requires 112GB of VRAM, i.e. at least two 80GB A100 GPUs. Fortunately, there are many research efforts on how to reduce the cost of LLM fine-tuning.</p>
<p>In this tutorial, we will demonstrate how to build a powerful <strong>text-to-SQL</strong> generator by fine-tuning the Mistral 7B model with <strong>a single 24GB VRAM GPU</strong>.</p>
<div class="section" id="What-You-Will-Learn">
<h3>What You Will Learn<a class="headerlink" href="#What-You-Will-Learn" title="Permalink to this headline"> </a></h3>
<ol class="arabic simple">
<li><p>Hands-on learning of the typical LLM fine-tuning process.</p></li>
<li><p>Understand how to use <strong>QLoRA</strong> and <strong>PEFT</strong> to overcome the GPU memory limitation for fine-tuning.</p></li>
<li><p>Manage the model training cycle using <strong>MLflow</strong> to log the model artifacts, hyperparameters, metrics, and prompts.</p></li>
<li><p>How to save prompt template and inference parameters (e.g. max_token_length) in MLflow to simplify prediction interface.</p></li>
</ol>
</div>
<div class="section" id="Key-Actors">
<h3>Key Actors<a class="headerlink" href="#Key-Actors" title="Permalink to this headline"> </a></h3>
<p>In this tutorial, you will learn about the techniques and methods behind efficient LLM fine-tuning by actually running the code. There are more detailed explanations for each cell below, but let’s start with a brief preview of a few main important libraries/methods used in this tutorial.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-v0.1">Mistral-7B-v0.1</a> model is a pretrained text-generation model with 7 billion parameters, developed by <a class="reference external" href="https://mistral.ai/">mistral.ai</a>. The model employs various optimization techniques such as Group-Query Attention, Sliding-Window Attention, Byte-fallback BPE tokenizer, and outperforms the Llama 2 13B on benchmarks with fewer parameters.</p></li>
<li><p><a class="reference external" href="https://github.com/artidoro/qlora">QLoRA</a> is a novel method that allows us to fine-tune large foundational models with limited GPU resources. It reduces the number of trainable parameters by learning pairs of rank-decomposition matrices and also applies 4-bit quantization to the frozen pretrained model to further reduce the memory footprint.</p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/peft/en/index">PEFT</a> is a library developed by HuggingFace🤗, that enables developers to easily integrate various optimization methods with pretrained models available on the HuggingFace Hub. With PEFT, you can apply QLoRA to the pretrained model with a few lines of configurations and run fine-tuning just like the normal Transformers model training.</p></li>
<li><p><a class="reference external" href="https://mlflow.org/">MLflow</a> manages an exploding number of configurations, assets, and metrics during the LLM training on your behalf. MLflow is natively integrated with Transformers and PEFT, and plays a crucial role in organizing the fine-tuning cycle.</p></li>
</ul>
</div>
</div>
<div class="section" id="1.-Environment-Set-up">
<h2>1. Environment Set up<a class="headerlink" href="#1.-Environment-Set-up" title="Permalink to this headline"> </a></h2>
<div class="section" id="Hardware-Requirement">
<h3>Hardware Requirement<a class="headerlink" href="#Hardware-Requirement" title="Permalink to this headline"> </a></h3>
<p>Please ensure your GPU has at least 20GB of VRAM available. This notebook has been tested on a single NVIDIA A10G GPU with 24GB of VRAM.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">sh</span> <span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Wed Feb 21 07:16:13 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |
|  0%   15C    P8              16W / 300W |      4MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
</pre></div></div>
</div>
</div>
<div class="section" id="Install-Python-Libraries">
<h3>Install Python Libraries<a class="headerlink" href="#Install-Python-Libraries" title="Permalink to this headline"> </a></h3>
<p>This tutorial utilizes the following Python libraries:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pypi.org/project/mlflow/">mlflow</a> - for tracking parameters, metrics, and saving trained models. Version <strong>2.11.0 or later</strong> is required to log PEFT models with MLflow.</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/transformers/">transformers</a> - for defining the model, tokenizer, and trainer.</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/peft/">peft</a> - for creating a LoRA adapter on top of the Transformer model.</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/bitsandbytes/">bitsandbytes</a> - for loading the base model with 4-bit quantization for QLoRA.</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/accelerate/">accelerate</a> - a dependency required by bitsandbytes.</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/datasets/">datasets</a> - for loading the training dataset from the HuggingFace hub.</p></li>
</ul>
<p><strong>Note</strong>: Restarting the Python kernel may be necessary after installing these dependencies.</p>
<p>The notebook has been tested with <code class="docutils literal notranslate"><span class="pre">mlflow==2.11.0</span></code>, <code class="docutils literal notranslate"><span class="pre">transformers==4.35.2</span></code>, <code class="docutils literal notranslate"><span class="pre">peft==0.8.2</span></code>, <code class="docutils literal notranslate"><span class="pre">bitsandbytes==0.42.0</span></code>, <code class="docutils literal notranslate"><span class="pre">accelerate==0.27.2</span></code>, and <code class="docutils literal notranslate"><span class="pre">datasets==2.17.1</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">mlflow</span><span class="o">&gt;=</span><span class="mf">2.11.0</span>
<span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span> <span class="n">peft</span> <span class="n">accelerate</span> <span class="n">bitsandbytes</span> <span class="n">datasets</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="2.-Dataset-Preparation">
<h2>2. Dataset Preparation<a class="headerlink" href="#2.-Dataset-Preparation" title="Permalink to this headline"> </a></h2>
<div class="section" id="Load-Dataset-from-HuggingFace-Hub">
<h3>Load Dataset from HuggingFace Hub<a class="headerlink" href="#Load-Dataset-from-HuggingFace-Hub" title="Permalink to this headline"> </a></h3>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">b-mc2/sql-create-context</span></code> dataset from the <a class="reference external" href="https://huggingface.co/datasets/b-mc2/sql-create-context">Hugging Face Hub</a> for this tutorial. This dataset comprises 78.6k pairs of natural language queries and their corresponding SQL statements, making it ideal for training a text-to-SQL model. The dataset includes three columns:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">question</span></code>: A natural language question posed regarding the data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: Additional information about the data, such as the schema for the table being queried.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">answer</span></code>: The SQL query that represents the expected output.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>

<span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;b-mc2/sql-create-context&quot;</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">display_table</span><span class="p">(</span><span class="n">dataset_or_sample</span><span class="p">):</span>
    <span class="c1"># A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.width&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_rows&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset_or_sample</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset_or_sample</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset_or_sample</span><span class="p">)</span>

    <span class="n">html</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">to_html</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">n&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;br&gt;&quot;</span><span class="p">)</span>
    <span class="n">styled_html</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;&lt;style&gt; .dataframe th, .dataframe tbody td </span><span class="se">{{</span><span class="s2"> text-align: left; padding-right: 30px; </span><span class="se">}}</span><span class="s2"> &lt;/style&gt; </span><span class="si">{</span><span class="n">html</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
    <span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="n">styled_html</span><span class="p">))</span>


<span class="n">display_table</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>question</th>
      <th>context</th>
      <th>answer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>How many heads of the departments are older than 56 ?</td>
      <td>CREATE TABLE head (age INTEGER)</td>
      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td>
    </tr>
    <tr>
      <th>1</th>
      <td>List the name, born state and age of the heads of departments ordered by age.</td>
      <td>CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)</td>
      <td>SELECT name, born_state, age FROM head ORDER BY age</td>
    </tr>
    <tr>
      <th>2</th>
      <td>List the creation year, name and budget of each department.</td>
      <td>CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)</td>
      <td>SELECT creation, name, budget_in_billions FROM department</td>
    </tr>
  </tbody>
</table></div>
</div>
</div>
<div class="section" id="Split-Train-and-Test-Dataset">
<h3>Split Train and Test Dataset<a class="headerlink" href="#Split-Train-and-Test-Dataset" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">b-mc2/sql-create-context</span></code> dataset consists of a single split, “train”. We will separate 20% of this as test samples.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">split_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training dataset contains </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2"> text-to-SQL pairs&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test dataset contains </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2"> text-to-SQL pairs&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training dataset contains 62861 text-to-SQL pairs
Test dataset contains 15716 text-to-SQL pairs
</pre></div></div>
</div>
</div>
<div class="section" id="Define-Prompt-Template">
<h3>Define Prompt Template<a class="headerlink" href="#Define-Prompt-Template" title="Permalink to this headline"> </a></h3>
<p>The Mistral 7B model is a text comprehension model, so we have to construct a text prompt that incorporates the user’s question, context, and our system instructions. The new <code class="docutils literal notranslate"><span class="pre">prompt</span></code> column in the dataset will contain the text prompt to be fed into the model during training. It is important to note that we also include the expected response within the prompt, allowing the model to be trained in a self-supervised manner.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PROMPT_TEMPLATE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.</span>

<span class="s2">### Table:</span>
<span class="si">{context}</span>

<span class="s2">### Question:</span>
<span class="si">{question}</span>

<span class="s2">### Response:</span>
<span class="si">{output}</span><span class="s2">&quot;&quot;&quot;</span>


<span class="k">def</span> <span class="nf">apply_prompt_template</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">PROMPT_TEMPLATE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">question</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">],</span>
        <span class="n">context</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="n">output</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">apply_prompt_template</span><span class="p">)</span>
<span class="n">display_table</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>question</th>
      <th>context</th>
      <th>answer</th>
      <th>prompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td>
      <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td>
      <td>SELECT perth FROM table_name_56 WHERE gold_coast = "yes" AND sydney = "yes" AND melbourne = "yes" AND adelaide = "yes"</td>
      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)<br><br>### Question:<br>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?<br><br>### Response:<br>SELECT perth FROM table_name_56 WHERE gold_coast = "yes" AND sydney = "yes" AND melbourne = "yes" AND adelaide = "yes"</td>
    </tr>
  </tbody>
</table></div>
</div>
</div>
<div class="section" id="Padding-the-Training-Dataset">
<h3>Padding the Training Dataset<a class="headerlink" href="#Padding-the-Training-Dataset" title="Permalink to this headline"> </a></h3>
<p>As a final step of dataset preparation, we need to apply <strong>padding</strong> to the training dataset. Padding ensures that all input sequences in a batch are of the same length.</p>
<p>A crucial point to note is the need to <em>add padding to the left</em>. This approach is adopted because the model generates tokens autoregressively, meaning it continues from the last token. Adding padding to the right would cause the model to generate new tokens from these padding tokens, resulting in the output sequence including padding tokens in the middle.</p>
<ul class="simple">
<li><p>Padding to right</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Today |  is  |   a    |  cold  |  &lt;pad&gt;  ==generate=&gt;  &quot;Today is a cold &lt;pad&gt; day&quot;
 How  |  to  | become |  &lt;pad&gt; |  &lt;pad&gt;  ==generate=&gt;  &quot;How to become a &lt;pad&gt; &lt;pad&gt; great engineer&quot;.
</pre></div>
</div>
<ul class="simple">
<li><p>Padding to left:</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;pad&gt; |  Today  |  is  |  a   |  cold     ==generate=&gt;  &quot;&lt;pad&gt; Today is a cold day&quot;
&lt;pad&gt; |  &lt;pad&gt;  |  How |  to  |  become   ==generate=&gt;  &quot;&lt;pad&gt; &lt;pad&gt; How to become a great engineer&quot;.
</pre></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">base_model_id</span> <span class="o">=</span> <span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span>

<span class="c1"># You can use a different max length if your custom dataset has shorter/longer input sequences.</span>
<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">base_model_id</span><span class="p">,</span>
    <span class="n">model_max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">,</span>
    <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span>
    <span class="n">add_eos_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>


<span class="k">def</span> <span class="nf">tokenize_and_pad_to_fixed_length</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">result</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="n">tokenized_train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_and_pad_to_fixed_length</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="n">MAX_LENGTH</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tokenized_train_dataset</span><span class="p">)</span>

<span class="n">display_table</span><span class="p">(</span><span class="n">tokenized_train_dataset</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="3.-Load-the-Base-Model-(with-4-bit-quantization)">
<h2>3. Load the Base Model (with 4-bit quantization)<a class="headerlink" href="#3.-Load-the-Base-Model-(with-4-bit-quantization)" title="Permalink to this headline"> </a></h2>
<p>Next, we’ll load the Mistral 7B model, which will serve as our base model for fine-tuning. This model can be loaded from the HuggingFace Hub repository <a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-v0.1">mistralai/Mistral-7B-v0.1</a> using the Transformers’ <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> API. However, here we are also providing a <code class="docutils literal notranslate"><span class="pre">quantization_config</span></code> parameter.</p>
<p>This parameter embodies the key technique of <a class="reference external" href="https://github.com/artidoro/qlora">QLoRA</a> that significantly reduces memory usage during fine-tuning. The following paragraph details the method and the implications of this configuration. However, feel free to skip if it appears complex. After all, we rarely need to modify the <code class="docutils literal notranslate"><span class="pre">quantization_config</span></code> values ourselves :)</p>
<p><strong>How It Works</strong></p>
<p>In short, QLoRA is a combination of <strong>Q</strong>uantization and <strong>LoRA</strong>. To grasp its functionality, it’s simpler to begin with LoRA. <a class="reference external" href="https://github.com/microsoft/LoRA">LoRA (Low Rank Adaptation)</a> is a preceding method for resource-efficient fine-tuning, by reducing the number of trainable parameters through matrix decomposition. Let <code class="docutils literal notranslate"><span class="pre">W'</span></code> represent the final weight matrix from fine-tuning. In LoRA, <code class="docutils literal notranslate"><span class="pre">W'</span></code> is approximated by the sum of the original weight and its update, i.e., <code class="docutils literal notranslate"><span class="pre">W</span> <span class="pre">+</span> <span class="pre">ΔW</span></code>, then
decomposing the delta part into two low-dimensional matrices, i.e., <code class="docutils literal notranslate"><span class="pre">ΔW</span> <span class="pre">≈</span> <span class="pre">AB</span></code>. Suppose <code class="docutils literal notranslate"><span class="pre">W</span></code> is <code class="docutils literal notranslate"><span class="pre">m</span></code>x<code class="docutils literal notranslate"><span class="pre">m</span></code>, and we select a smaller <code class="docutils literal notranslate"><span class="pre">r</span></code> for the rank of <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>, where <code class="docutils literal notranslate"><span class="pre">A</span></code> is <code class="docutils literal notranslate"><span class="pre">m</span></code>x<code class="docutils literal notranslate"><span class="pre">r</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> is <code class="docutils literal notranslate"><span class="pre">r</span></code>x<code class="docutils literal notranslate"><span class="pre">m</span></code>. Now, the original trainable parameters, which are quadratic in size of <code class="docutils literal notranslate"><span class="pre">W</span></code> (i.e., <code class="docutils literal notranslate"><span class="pre">m^2</span></code>), after decomposition, become <code class="docutils literal notranslate"><span class="pre">2mr</span></code>. Empirically, we can choose a much smaller number for <code class="docutils literal notranslate"><span class="pre">r</span></code>, e.g., 32, 64, compared to the full weight matrix size, therefore this
significantly reduces the number of parameters to train.</p>
<p><a class="reference external" href="https://github.com/artidoro/qlora">QLoRA</a> extends LoRA, employing the same strategy for matrix decomposition. However, it further reduces memory usage by applying 4-bit quantization to the frozen pretrained model <code class="docutils literal notranslate"><span class="pre">W</span></code>. According to their research, the largest memory usage during LoRA fine-tuning is the backpropagation through the frozen parameters <code class="docutils literal notranslate"><span class="pre">W</span></code> to compute gradients for the adaptors <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>. Thus, quantizing <code class="docutils literal notranslate"><span class="pre">W</span></code> to 4-bit significantly reduces the overall memory
consumption. This is achieved with the <code class="docutils literal notranslate"><span class="pre">load_in_4bit=True</span></code> setting shown below.</p>
<p>Moreover, QLoRA introduces additional techniques to optimize resource usage without significantly impacting model performance. For more technical details, please refer to <a class="reference external" href="https://arxiv.org/pdf/2305.14314.pdf">the paper</a>, but we implement them by setting the following quantization configurations in bitsandbytes: * The 4-bit NormalFloat type is specified by <code class="docutils literal notranslate"><span class="pre">bnb_4bit_quant_type=&quot;nf4&quot;</span></code>. * Double quantization is activated by <code class="docutils literal notranslate"><span class="pre">bnb_4bit_use_double_quant=True</span></code>. * QLoRA re-quantizes the 4-bit
weights back to a higher precision when computing the gradients for <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>, to prevent performance degradation. This datatype is specified by <code class="docutils literal notranslate"><span class="pre">bnb_4bit_compute_dtype=torch.bfloat16</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="c1"># Load the model with 4-bit quantization</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># Use double quantization</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># Use 4-bit Normal Float for storing the base model weights in GPU memory</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
    <span class="c1"># De-quantize the weights to 16-bit (Brain) float before the forward/backward pass</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="How-Does-the-Base-Model-Perform?">
<h3>How Does the Base Model Perform?<a class="headerlink" href="#How-Does-the-Base-Model-Perform?" title="Permalink to this headline"> </a></h3>
<p>First, let’s assess the performance of the vanilla Mistral model on the SQL generation task before any fine-tuning. As expected, the model does not produce correct SQL queries; instead, it generates random answers in natural language. This outcome indicates the necessity of fine-tuning the model for our specific task.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_id</span><span class="p">)</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">)</span>

<span class="n">sample</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PROMPT_TEMPLATE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="n">question</span><span class="o">=</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="p">)</span>  <span class="c1"># Leave the answer part blank</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.15</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">display_table</span><span class="p">({</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span> <span class="s2">&quot;generated_query&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">]})</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>prompt</th>
      <th>generated_query</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)<br><br>### Question:<br>What is the lowest numbered game against Phoenix with a record of 29-17?<br><br>### Response:<br></td>
      <td><br>A: The lowest numbered game against Phoenix was played on 03/04/2018. The score was PHO 115 - DAL 106.<br>What is the highest numbered game against Phoenix?<br>A: The highest numbered game against Phoenix was played on 03/04/2018. The score was PHO 115 - DAL 106.<br>Which players have started at Point Guard for Dallas in a regular season game against Phoenix?</td>
    </tr>
  </tbody>
</table></div>
</div>
</div>
</div>
<div class="section" id="4.-Define-a-PEFT-Model">
<h2>4. Define a PEFT Model<a class="headerlink" href="#4.-Define-a-PEFT-Model" title="Permalink to this headline"> </a></h2>
<p>As discussed earlier, QLoRA stands for <strong>Quantization</strong> + <strong>LoRA</strong>. Having applied the quantization part, we now proceed with the LoRA aspect. Although the mathematics behind LoRA is intricate, <a class="reference external" href="https://huggingface.co/docs/peft/en/index">PEFT</a> helps us by simplifying the process of adapting LoRA to the pretrained Transformer model.</p>
<p>In the next cell, we create a <a class="reference external" href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py">LoraConfig</a> with various settings for LoRA. Contrary to the earlier <code class="docutils literal notranslate"><span class="pre">quantization_config</span></code>, these hyperparameters might need optimization to achieve the best model performance for your specific task. <strong>MLflow</strong> facilitates this process by tracking these hyperparameters, the associated model, and its outcomes.</p>
<p>At the end of the cell, we display the number of trainable parameters during fine-tuning, and their percentage relative to the total model parameters. Here, we are training only 1.16% of the total 7 billion parameters.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span>

<span class="c1"># Enabling gradient checkpointing, to make the training further efficient</span>
<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="c1"># Set up the model for quantization-aware training e.g. casting layers, parameter freezing, etc.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;CAUSAL_LM&quot;</span><span class="p">,</span>
    <span class="c1"># This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance.</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="c1"># This is the coefficient for the learned ΔW factor, so the larger number will typically result in a larger behavior change after fine-tuning.</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="c1"># Drop out ratio for the layers in LoRA adaptors A and B.</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model.</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;q_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;down_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;lm_head&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="c1"># Bias parameters to train. &#39;none&#39; is recommended to keep the original model performing equally when turning off the adapter.</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">peft_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">peft_model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
trainable params: 85,041,152 || all params: 7,326,773,248 || trainable%: 1.1606903765339511
</pre></div></div>
</div>
<p><strong>That’s it!!!</strong> PEFT has made the LoRA setup super easy.</p>
<p>An additional bonus is that the PEFT model exposes the same interfaces as a Transformers model. This means that everything from here on is quite similar to the standard model training process using Transformers.</p>
</div>
<div class="section" id="5.-Kick-off-a-Training-Job">
<h2>5. Kick-off a Training Job<a class="headerlink" href="#5.-Kick-off-a-Training-Job" title="Permalink to this headline"> </a></h2>
<p>Similar to conventional Transformers training, we’ll first set up a Trainer object to organize the training iterations. There are numerous hyperparameters to configure, but MLflow will manage them on your behalf.</p>
<p>To enable MLflow logging, you can specify <code class="docutils literal notranslate"><span class="pre">report_to=&quot;mlflow&quot;</span></code> and name your training trial with the <code class="docutils literal notranslate"><span class="pre">run_name</span></code> parameter. This action initiates an <a class="reference external" href="https://mlflow.org/docs/latest/tracking.html#runs">MLflow run</a> that automatically logs training metrics, hyperparameters, configurations, and the trained model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Comment-out this line if you are running the tutorial on Databricks</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;MLflow PEFT Tutorial&quot;</span><span class="p">)</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="c1"># Set this to mlflow for logging your training</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;mlflow&quot;</span><span class="p">,</span>
    <span class="c1"># Name the MLflow run</span>
    <span class="n">run_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Mistral-7B-SQL-QLoRA-</span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">-%H-%M-</span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="c1"># Replace with your output destination</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;YOUR_OUTPUT_DIR&quot;</span><span class="p">,</span>
    <span class="c1"># For the following arguments, refer to https://huggingface.co/docs/transformers/main_classes/trainer</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="s2">&quot;paged_adamw_8bit&quot;</span><span class="p">,</span>
    <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="c1"># https://discuss.huggingface.co/t/training-llama-with-lora-on-multiple-gpus-may-exist-bug/47005/3</span>
    <span class="n">ddp_find_unused_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">peft_model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_dataset</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># use_cache=True is incompatible with gradient checkpointing.</span>
<span class="n">peft_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
<p>The training duration may span several hours, contingent upon your hardware specifications. Nonetheless, the primary objective of this tutorial is to acquaint you with the process of fine-tuning using PEFT and MLflow, rather than to cultivate a highly performant SQL generator. If you don’t care much about the model performance, you may specify a smaller number of steps or interrupt the following cell to proceed with the rest of the notebook.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
    <div>

      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [500/500 45:41, Epoch 0/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>100</td>
      <td>0.681700</td>
    </tr>
    <tr>
      <td>200</td>
      <td>0.522400</td>
    </tr>
    <tr>
      <td>300</td>
      <td>0.507300</td>
    </tr>
    <tr>
      <td>400</td>
      <td>0.494800</td>
    </tr>
    <tr>
      <td>500</td>
      <td>0.474600</td>
    </tr>
  </tbody>
</table><p></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
TrainOutput(global_step=500, training_loss=0.5361956100463867, metrics={&#39;train_runtime&#39;: 2747.9223, &#39;train_samples_per_second&#39;: 1.456, &#39;train_steps_per_second&#39;: 0.182, &#39;total_flos&#39;: 4.421038813216768e+16, &#39;train_loss&#39;: 0.5361956100463867, &#39;epoch&#39;: 0.06})
</pre></div></div>
</div>
</div>
<div class="section" id="6.-Save-the-PEFT-Model-to-MLflow">
<h2>6. Save the PEFT Model to MLflow<a class="headerlink" href="#6.-Save-the-PEFT-Model-to-MLflow" title="Permalink to this headline"> </a></h2>
<p>Hooray! We have successfully fine-tuned the Mistral 7B model into an SQL generator. Before concluding the training, one final step is to save the trained PEFT model to MLflow.</p>
<div class="section" id="Set-Prompt-Template-and-Default-Inference-Parameters-(optional)">
<h3>Set Prompt Template and Default Inference Parameters (optional)<a class="headerlink" href="#Set-Prompt-Template-and-Default-Inference-Parameters-(optional)" title="Permalink to this headline"> </a></h3>
<p>LLMs prediction behavior is not only defined by the model weights, but also largely controlled by the prompt and inference paramters such as <code class="docutils literal notranslate"><span class="pre">max_token_length</span></code>, <code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code>. Therefore, it is highly advisable to save those metadata along with the model, so that you can expect the consistent behavior when loading the model later.</p>
<div class="section" id="Prompt-Template">
<h4>Prompt Template<a class="headerlink" href="#Prompt-Template" title="Permalink to this headline"> </a></h4>
<p>The user prompt itself is free text, but you can harness the input by applying a ‘template’. MLflow Transformer flavor supports saving a prompt template with the model, and apply it automatically before the prediction. This also allows you to hide the system prompt from model clients. To save the prompt template, we have to define a single string that contains <code class="docutils literal notranslate"><span class="pre">{prompt}</span></code> variable, and pass it to the <code class="docutils literal notranslate"><span class="pre">prompt_template</span></code> argument of
<a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model">mlflow.transformers.log_model</a> API. Refer to <a class="reference external" href="https://mlflow.org/docs/latest/llms/transformers/guide/index.html#saving-prompt-templates-with-transformer-pipelines">Saving Prompt Templates with Transformer Pipelines</a> for more detailed usage of this feature.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Basically the same format as we applied to the dataset. However, the template only accepts {prompt} variable so both table and question need to be fed in there.</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.</span>

<span class="si">{prompt}</span>

<span class="s2">### Response:</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Inference-Parameters">
<h4>Inference Parameters<a class="headerlink" href="#Inference-Parameters" title="Permalink to this headline"> </a></h4>
<p>Inference parameters can be saved with MLflow model as a part of <a class="reference external" href="https://mlflow.org/docs/latest/model/signatures.html">Model Signature</a>. The signature defines model input and output format with additional parameters passed to the model prediction, and you can let MLflow to infer it from some sample input using <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.infer_signature">mlflow.models.infer_signature</a> API. If you pass the concrete value for parameters,
MLflow treats them as default values and apply them at the inference if they are not provided by users. For more details about the Model Signature, please refer to the <a class="reference external" href="https://mlflow.org/docs/latest/model/signatures.html">MLflow documentation</a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>

<span class="n">sample</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># MLflow infers schema from the provided sample input/output/params</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">model_input</span><span class="o">=</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span>
    <span class="n">model_output</span><span class="o">=</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">],</span>
    <span class="c1"># Parameters are saved with default values if specified</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.15</span><span class="p">,</span> <span class="s2">&quot;return_full_text&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">signature</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
inputs:
  [string (required)]
outputs:
  [string (required)]
params:
  [&#39;max_new_tokens&#39;: long (default: 256), &#39;repetition_penalty&#39;: double (default: 1.15), &#39;return_full_text&#39;: boolean (default: False)]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Save-the-PEFT-Model-to-MLflow">
<h3>Save the PEFT Model to MLflow<a class="headerlink" href="#Save-the-PEFT-Model-to-MLflow" title="Permalink to this headline"> </a></h3>
<p>Finally, we will call <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model">mlflow.transformers.log_model</a> API to log the model to MLflow. A few critical points to remember when logging a PEFT model to MLflow are:</p>
<ol class="arabic simple">
<li><p><strong>MLflow logs the Transformer model as a</strong> <a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/pipelines">Pipeline</a><strong>.</strong> A pipeline bundles a model with its tokenizer (or other components, depending on the task type) and simplifies the prediction steps into an easy-to-use interface, making it an excellent tool for ensuring reproducibility. In the code below, we pass the model and tokenizer as a dictionary, then MLflow automatically deduces the correct pipeline type and saves it.</p></li>
<li><p><strong>MLflow does not save the base model weight for the PEFT model</strong>. When executing <code class="docutils literal notranslate"><span class="pre">mlflow.transformers.log_model</span></code>, MLflow only saves the small number of trained parameters, i.e., the PEFT adapter. For the base model, MLflow instead records a reference to the HuggingFace hub (repository name and commit hash), and downloads the base model weights on the fly when loading the PEFT model. This approach significantly reduces storage usage and logging latency; for instance, the logged artifacts
size in this tutorial is less than 1GB, while the full Mistral 7B model is about 20GB.</p></li>
<li><p><strong>Save a tokenizer without padding</strong>. During fine-tuning, we applied padding to the dataset to standardize the sequence length in a batch. However, padding is no longer necessary at inference, so we save a different tokenizer without padding. This ensures the loaded model can be used for inference immediately.</p></li>
</ol>
<p><strong>Note</strong>: Currently, manual logging is required for the PEFT adapter and config, while other information, such as dataset, metrics, Trainer parameters, etc., are automatically logged. However, this process may be automated in future versions of MLflow and Transformers.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Get the ID of the MLflow Run that was automatically created above</span>
<span class="n">last_run_id</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">last_active_run</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>

<span class="c1"># Save a tokenizer without padding because it is only needed for training</span>
<span class="n">tokenizer_no_pad</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_id</span><span class="p">,</span> <span class="n">add_bos_token</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># If you interrupt the training, uncomment the following line to stop the MLflow run</span>
<span class="c1"># mlflow.end_run()</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">run_id</span><span class="o">=</span><span class="n">last_run_id</span><span class="p">):</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">peft_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">:</span> <span class="n">tokenizer_no_pad</span><span class="p">},</span>
        <span class="n">prompt_template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>  <span class="c1"># This is a relative path to save model files within MLflow run</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="What’s-Logged-to-MLflow?">
<h3>What’s Logged to MLflow?<a class="headerlink" href="#What’s-Logged-to-MLflow?" title="Permalink to this headline"> </a></h3>
<p>Let’s briefly review what is logged/saved to MLflow as a result of your training. To access the MLflow UI, run <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">ui</span></code> commands and open <a class="reference external" href="https://localhost:PORT">https://localhost:PORT</a> (PORT is 5000 by default). Select the experiment “MLflow PEFT Tutorial” (or the notebook name when running on Databricks) on the left side. Then click on the latest MLflow Run named <code class="docutils literal notranslate"><span class="pre">Mistral-7B-SQL-QLoRA-2024-...</span></code> to view the Run details.</p>
<div class="section" id="Parameters">
<h4>Parameters<a class="headerlink" href="#Parameters" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">Parameters</span></code> section displays hundreds of parameters specified for the Trainer, LoraConfig, and BitsAndBytesConfig, such as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, <code class="docutils literal notranslate"><span class="pre">r</span></code>, <code class="docutils literal notranslate"><span class="pre">bnb_4bit_quant_type</span></code>. It also includes default parameters that were not explicitly specified, which is crucial for ensuring reproducibility, especially if the library’s default values change.</p>
</div>
<div class="section" id="Metrics">
<h4>Metrics<a class="headerlink" href="#Metrics" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">Metrics</span></code> section presents the model metrics collected during the run, such as <code class="docutils literal notranslate"><span class="pre">train_loss</span></code>. You can visualize these metrics with various types of graphs in the “Chart” tab.</p>
</div>
<div class="section" id="Artifacts">
<h4>Artifacts<a class="headerlink" href="#Artifacts" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">Artifacts</span></code> section displays the files/directories saved in MLflow as a result of training. For Transformers PEFT training, you should see the following files/directories:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>model/
  ├─ peft/
  │  ├─ adapter_config.json       # JSON file of the LoraConfig
  │  ├─ adapter_module.safetensor # The weight file of the LoRA adapter
  │  └─ README.md                 # Empty README file generated by Transformers
  │
  ├─ LICENSE.txt                  # License information about the base model (Mistral-7B-0.1)
  ├─ MLModel                      # Contains various metadata about your model
  ├─ conda.yaml                   # Dependencies to create conda environment
  ├─ model_card.md                # Model card text for the base model
  ├─ model_card_data.yaml         # Model card data for the base model
  ├─ python_env.yaml              # Dependencies to create Python virtual environment
  └─ requirements.txt             # Pip requirements for model inference
</pre></div>
</div>
</div>
<div class="section" id="Model-Metadata">
<h4>Model Metadata<a class="headerlink" href="#Model-Metadata" title="Permalink to this headline"> </a></h4>
<p>In the MLModel file, you can see the many detailed metadata are saved about the PEFT and base model. Here is an excerpt of the MLModel file (some fields are omitted for simplicity)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>flavors:
  transformers:
    peft_adaptor: peft                                 # Points the location of the saved PEFT model
    pipeline_model_type: MistralForCausalLM            # The base model implementation
    source_model_name: mistralai/Mistral-7B-v0.1.      # Repository name of the base model
    source_model_revision: xxxxxxx                     # Commit hash in the repository for the base model
    task: text-generation                              # Pipeline type
    torch_dtype: torch.bfloat16                        # Dtype for loading the model
    tokenizer_type: LlamaTokenizerFast                 # Tokenizer implementation

# Prompt template saved with the model above
metadata:
  prompt_template: &#39;You are a powerful text-to-SQL model. Given the SQL tables and
    natural language question, your job is to write SQL query that answers the question.


    {prompt}


    ### Response:

    &#39;
# Defines the input and output format of the model, with additional inference parameters with default values
signature:
  inputs: &#39;[{&quot;type&quot;: &quot;string&quot;, &quot;required&quot;: true}]&#39;
  outputs: &#39;[{&quot;type&quot;: &quot;string&quot;, &quot;required&quot;: true}]&#39;
  params: &#39;[{&quot;name&quot;: &quot;max_new_tokens&quot;, &quot;type&quot;: &quot;long&quot;, &quot;default&quot;: 256, &quot;shape&quot;: null},
    {&quot;name&quot;: &quot;repetition_penalty&quot;, &quot;type&quot;: &quot;double&quot;, &quot;default&quot;: 1.15, &quot;shape&quot;: null},
    {&quot;name&quot;: &quot;return_full_text&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: false, &quot;shape&quot;: null}]&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="7.-Load-the-Saved-PEFT-Model-from-MLflow">
<h2>7. Load the Saved PEFT Model from MLflow<a class="headerlink" href="#7.-Load-the-Saved-PEFT-Model-from-MLflow" title="Permalink to this headline"> </a></h2>
<p>Finally, let’s load the model logged in MLflow and evaluate its performance as a text-to-SQL generator. There are two ways to load a Transformer model in MLflow:</p>
<ol class="arabic simple">
<li><p>Use <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.load_model">mlflow.transformers.load_model()</a>. This method returns a native Transformers pipeline instance.</p></li>
<li><p>Use <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model">mlflow.pyfunc.load_model()</a>. This method returns an MLflow’s PythonModel instance that wraps the Transformers pipeline, offering additional features over the native pipeline, such as (1) a unified <code class="docutils literal notranslate"><span class="pre">predict()</span></code> API for inference, (2) model signature enforcement, and (3) automatically applying a prompt template and default parameters if saved. Please note that not all the Transformer pipelines are
supported for pyfunc loading, refer to the <a class="reference external" href="https://mlflow.org/docs/latest/llms/transformers/guide/index.html#supported-transformers-pipeline-types-for-pyfunc">MLflow documentation</a> for the full list of supported pipeline types.</p></li>
</ol>
<p>The first option is preferable if you wish to use the model via the native Transformers interface. The second option offers a simplified and unified interface across different model types and is particularly useful for model testing before production deployment. In the following code, we will use the <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model">mlflow.pyfunc.load_model()</a> to show how it applies the prompt template and the default inference parameters
defined above.</p>
<p><strong>NOTE</strong>: Invoking <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> loads a new model instance onto your GPU, which may exceed GPU memory limits and trigger an Out Of Memory (OOM) error, or cause the Transformers library to attempt to offload parts of the model to other devices or disk. This offloading can lead to issues, such as a “ValueError: We need an <code class="docutils literal notranslate"><span class="pre">offload_dir</span></code> to dispatch this model according to this <code class="docutils literal notranslate"><span class="pre">decide_map</span></code>.” If you encounter this error, consider restarting the Python Kernel and loading the model again.</p>
<p><strong>CAUTION</strong>: Restarting the Python Kernel will erase all intermediate states and variables from the above cells. Ensure that the trained PEFT model is properly logged in MLflow before restarting.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># You can find the ID of run in the Run detail page on MLflow UI</span>
<span class="n">mlflow_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;runs:/YOUR_RUN_ID/model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We only input table and question, since system prompt is adeed in the prompt template.</span>
<span class="n">test_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">### Table:</span>
<span class="s2">CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)</span>

<span class="s2">### Question:</span>
<span class="s2">When Essendon played away; where did they play?</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Inference parameters like max_tokens_length are set to default values specified in the Model Signature</span>
<span class="n">generated_query</span> <span class="o">=</span> <span class="n">mlflow_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_prompt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">display_table</span><span class="p">({</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">test_prompt</span><span class="p">,</span> <span class="s2">&quot;generated_query&quot;</span><span class="p">:</span> <span class="n">generated_query</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>prompt</th>
      <th>generated_query</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td><br>### Table:<br>CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)<br><br>### Question:<br>When Essendon played away; where did they play?<br></td>
      <td>SELECT venue FROM table_name_50 WHERE away_team = "essendon"</td>
    </tr>
  </tbody>
</table></div>
</div>
<p>Perfect!! The fine-tuned model now generates the SQL query properly. As you can see in the code and result above, the system prompt and default inference parameters are applied automatically, so we don’t have to pass it to the loaded model. This is super powerful when you want to deploy multiple models (or update an existing model) with different the system prompt or parameters, because you don’t have to edit client’s implementation as they are abstracted behind the MLflow model :)</p>
</div>
<div class="section" id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline"> </a></h2>
<p>In this tutorial, you learned how to fine-tune a large language model with QLoRA for text-to-SQL task using PEFT. You also learned the key role of MLflow in the LLM fine-tuning process, which tracks parameters and metrics during the fine-tuning, and manage models and other assets.</p>
<div class="section" id="What’s-Next?">
<h3>What’s Next?<a class="headerlink" href="#What’s-Next?" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/huggingface-evaluation.html">Evaluate a Hugging Face LLM with MLflow</a> - Model evaluation is a critical steps in the model development. Checkout this guidance to learn how to evaluate LLMs efficiently with MLflow including LLM-as-a-judge.</p></li>
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/deployment/index.html">Deploy MLflow Model to Production</a> - MLflow model stores rich metadata and provides unified interface for prediction, which streamline the easy deployment process. Learn how to deploy your fine-tuned models to various target such as AWS SageMaker, Azure ML, Kubernetes, Databricks Model Serving, with detailed guidance and hands-on notebooks.</p></li>
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/transformers/index.html">MLflow Transformers Flavor Documentation</a> - Learn more about MLflow and Transformers integration and continue on more tutorials.</p></li>
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/index.html">Large Language Models in MLflow</a> - MLflow provides more LLM-related features and integrates to many other libraries such as OpenAI and Langchain.</p></li>
</ul>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="transformers-fine-tuning.html" class="btn btn-neutral" title="Fine-Tuning Transformers with MLflow for Enhanced Model Management" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../prompt-templating/prompt-templating.html" class="btn btn-neutral" title="Prompt Templating with MLflow and Transformers" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../../',
      VERSION:'2.17.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>