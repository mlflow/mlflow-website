
  

<!DOCTYPE html>
<!-- source: docs/source/llms/transformers/tutorials/conversational/pyfunc-chat-model.ipynb -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Building and Serving an OpenAI-compatible Chatbot &mdash; MLflow 2.14.2.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/transformers/tutorials/conversational/pyfunc-chat-model.html">
  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../../search.html"/>
    <link rel="top" title="MLflow 2.14.2.dev0 documentation" href="../../../../index.html"/>
        <link rel="up" title="MLflow Transformers Flavor" href="../../index.html"/>
        <link rel="next" title="Fine-Tuning Transformers with MLflow for Enhanced Model Management" href="/../fine-tuning/transformers-fine-tuning.html"/>
        <link rel="prev" title="Introduction to Conversational AI with MLflow and DialoGPT" href="/conversational-model.html"/> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="../../../../None"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.14.2.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../../index.html" class="main-navigation-home"><img src="../../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id2">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../index.html">MLflow Transformers Flavor</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../index.html#introduction">Introduction</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../index.html#getting-started-with-the-mlflow-transformers-flavor-tutorials-and-guides">Getting Started with the MLflow Transformers Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#important-details-to-be-aware-of-with-the-transformers-flavor">Important Details to be aware of with the transformers flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#working-with-tasks-for-transformer-pipelines">Working with <code class="docutils literal notranslate"><span class="pre">tasks</span></code> for Transformer Pipelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#id1">Detailed Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../index.html#learn-more-about-transformers">Learn more about Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../langchain/index.html">MLflow LangChain Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#id5">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">MLflow Transformers Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Building and Serving an OpenAI-compatible Chatbot</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/conversational/pyfunc-chat-model.ipynb" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Building-and-Serving-an-OpenAI-compatible-Chatbot">
<h1>Building and Serving an OpenAI-compatible Chatbot<a class="headerlink" href="#Building-and-Serving-an-OpenAI-compatible-Chatbot" title="Permalink to this headline"> </a></h1>
<p>Welcome to our tutorial on using Transformers and MLflow to create an OpenAI-compatible chat model. In MLflow 2.11 and up, the <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.types.html#">ChatModel</a> class has been added, allowing for convenient creation of served models that conform to the OpenAI API spec. This enables you to seamlessly swap out your chat app’s backing LLM, or to easily evaluate different models without having to edit your client-side code.</p>
<p>If you haven’t already seen it, you may find it helpful to go through our <a class="reference external" href="https://mlflow.org/docs/latest/llms/transformers/tutorials/conversational/conversational-model.html">introductory notebook on chat and Transformers</a> before proceeding with this one, as this notebook is slightly higher-level and does not delve too deeply into the inner workings of Transformers or MLflow Tracking.</p>
<div class="section" id="Learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#Learning-objectives" title="Permalink to this headline"> </a></h2>
<p>In this tutorial, you will:</p>
<ul class="simple">
<li><p>Create an OpenAI-compatible chat model using TinyLLama-1.1B-Chat</p></li>
<li><p>Serve the model with MLflow Model Serving</p></li>
<li><p>Learn how to use MLflow’s <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> API to add arbitrary customization to your model</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install mlflow&gt;=2.11.0 -q -U
<span class="c1"># OpenAI-compatible chat model support is available for Transformers 4.34.0 and above</span>
<span class="o">%</span><span class="k">pip</span> install transformers&gt;=4.34.0 -q -U
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Disable tokenizers warnings when constructing pipelines</span>
<span class="o">%</span><span class="k">env</span> TOKENIZERS_PARALLELISM=false

<span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Disable a few less-than-useful UserWarnings from setuptools and pydantic</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
env: TOKENIZERS_PARALLELISM=false
</pre></div></div>
</div>
</div>
<div class="section" id="Building-a-Chat-Model">
<h2>Building a Chat Model<a class="headerlink" href="#Building-a-Chat-Model" title="Permalink to this headline"> </a></h2>
<p>MLflow’s native Transformers integration allows you to specify the <code class="docutils literal notranslate"><span class="pre">task</span></code> param when saving or logging your pipelines. Originally, this param accepts any of the <a class="reference external" href="https://huggingface.co/tasks">Transformers pipeline task types</a>, but the <code class="docutils literal notranslate"><span class="pre">mlflow.transformers</span></code> flavor adds a few more MLflow-specific keys for <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipeline types.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipelines, instead of specifying <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> as the task type, you can provide one of two string literals conforming to the <a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#general-configuration-parameters">MLflow Deployments Server’s endpoint_type specification</a> (“llm/v1/embeddings” can be specified as a task on models saved with <code class="docutils literal notranslate"><span class="pre">mlflow.sentence_transformers</span></code>):</p>
<ul class="simple">
<li><p>“llm/v1/chat” for chat-style applications</p></li>
<li><p>“llm/v1/completions” for generic completions</p></li>
</ul>
<p>When one of these keys is specified, MLflow will automatically handle everything required to serve a chat or completions model. This includes:</p>
<ul class="simple">
<li><p>Setting a chat/completions compatible signature on the model</p></li>
<li><p>Performing data pre- and post-processing to ensure the inputs and outputs conform to the <a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#chat">Chat/Completions API spec</a>, which is compatible with OpenAI’s API spec.</p></li>
</ul>
<p>Note that these modifications only apply when the model is loaded with <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code> (e.g. when serving the model with the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span> <span class="pre">serve</span></code> CLI tool). If you want to load just the base pipeline, you can always do so via <code class="docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code>.</p>
<p>In the next few cells, we’ll learn how serve a chat model with a local Transformers pipeline and MLflow, using TinyLlama-1.1B-Chat as an example.</p>
<p>To begin, let’s go through the original flow of saving a text generation pipeline:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># save the model using the vanilla `text-generation` task type</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;tinyllama-text-generation&quot;</span><span class="p">,</span> <span class="n">transformers_model</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/4268198845.py:11: FutureWarning: The &#39;transformers&#39; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
  mlflow.transformers.save_model(
</pre></div></div>
</div>
<p>Now, let’s load the model and use it for inference. Our loaded model is a <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipeline, and let’s take a look at its signature to see its expected inputs and outputs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the model for inference</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;tinyllama-text-generation&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">signature</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "5b41063f67ce4e2cac11e68b7d838f55", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/02/26 21:06:51 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d1e4fe7d982748e0b81204261de839ab", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
inputs:
  [string (required)]
outputs:
  [string (required)]
params:
  None
</pre></div></div>
</div>
<p>Unfortunately, it only accepts <code class="docutils literal notranslate"><span class="pre">string</span></code> as input, which isn’t directly compatible with a chat interface. When interacting with OpenAI’s API, for example, we expect to simply be able to input a list of messages. In order to do this with our current model, we’ll have to write some additional boilerplate:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first, apply the tokenizer&#39;s chat template, since the</span>
<span class="c1"># model is tuned to accept prompts in a chat format. this</span>
<span class="c1"># also converts the list of messages to a string.</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write me a hello world program in python&quot;</span><span class="p">}]</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;&lt;|user|&gt;\nWrite me a hello world program in python&lt;/s&gt;\n&lt;|assistant|&gt;\nHere\&#39;s a simple hello world program in Python:\n\n```python\nprint(&#34;Hello, world!&#34;)\n```\n\nThis program prints the string &#34;Hello, world!&#34; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#39;]
</pre></div></div>
</div>
<p>Now we’re getting somewhere, but formatting our messages prior to inference is cumbersome.</p>
<p>Additionally, the output format isn’t compatible with the OpenAI API spec either–it’s just a list of strings. If we were looking to evaluate different model backends for our chat app, we’d have to rewrite some of our client-side code to both format the input, and to parse this new response.</p>
<p>To simplify all this, let’s just pass in <code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/chat&quot;</span></code> as the task param when saving the model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># save the model using the `&quot;llm/v1/chat&quot;`</span>
<span class="c1"># task type instead of `text-generation`</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;tinyllama-chat&quot;</span><span class="p">,</span> <span class="n">transformers_model</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;llm/v1/chat&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/609241782.py:3: FutureWarning: The &#39;transformers&#39; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
  mlflow.transformers.save_model(
</pre></div></div>
</div>
<p>Once again, let’s load the model and inspect the signature:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;tinyllama-chat&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">signature</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e74df03331624bbdb7527cb813ae70bb", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/02/26 21:10:04 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d147b02249f5430dbb8780d207ba3bc1", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
inputs:
  [&#39;messages&#39;: Array({content: string (required), name: string (optional), role: string (required)}) (required), &#39;temperature&#39;: double (optional), &#39;max_tokens&#39;: long (optional), &#39;stop&#39;: Array(string) (optional), &#39;n&#39;: long (optional), &#39;stream&#39;: boolean (optional)]
outputs:
  [&#39;id&#39;: string (required), &#39;object&#39;: string (required), &#39;created&#39;: long (required), &#39;model&#39;: string (required), &#39;choices&#39;: Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), &#39;usage&#39;: {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]
params:
  None
</pre></div></div>
</div>
<p>Now when performing inference, we can pass our messages in a dict as we’d expect to do when interacting with the OpenAI API. Furthermore, the response we receive back from the model also conforms to the spec.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write me a hello world program in python&quot;</span><span class="p">}]</span>

<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[{&#39;id&#39;: &#39;8435a57d-9895-485e-98d3-95b1cbe007c0&#39;,
  &#39;object&#39;: &#39;chat.completion&#39;,
  &#39;created&#39;: 1708949437,
  &#39;model&#39;: &#39;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#39;,
  &#39;usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;completion_tokens&#39;: 71, &#39;total_tokens&#39;: 95},
  &#39;choices&#39;: [{&#39;index&#39;: 0,
    &#39;finish_reason&#39;: &#39;stop&#39;,
    &#39;message&#39;: {&#39;role&#39;: &#39;assistant&#39;,
     &#39;content&#39;: &#39;Here\&#39;s a simple hello world program in Python:\n\n```python\nprint(&#34;Hello, world!&#34;)\n```\n\nThis program prints the string &#34;Hello, world!&#34; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#39;}}]}]
</pre></div></div>
</div>
</div>
<div class="section" id="Serving-the-Chat-Model">
<h2>Serving the Chat Model<a class="headerlink" href="#Serving-the-Chat-Model" title="Permalink to this headline"> </a></h2>
<p>To take this example further, let’s use MLflow to serve our chat model, so we can interact with it like a web API. To do this, we can use the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span> <span class="pre">serve</span></code> CLI tool.</p>
<p>In a terminal shell, run:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mlflow models serve -m tinyllama-chat
</pre></div>
</div>
<p>When the server has finished initializing, you should be able to interact with the model via HTTP requests. The input format is almost identical to the format described in the <a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#chat">MLflow Deployments Server docs</a>, with the exception that <code class="docutils literal notranslate"><span class="pre">temperature</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">1.0</span></code> instead of <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p>
<p>Here’s a quick example:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-sh notranslate"><div class="highlight"><pre><span></span>%%sh
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span><span class="s1">&#39;{ &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write me a hello world program in python&quot;}] }&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="p">|</span><span class="w"> </span>jq
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   706  100   617  100    89     25      3  0:00:29  0:00:23  0:00:06   160
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[
  {
    &#34;id&#34;: &#34;fc3d08c3-d37d-420d-a754-50f77eb32a92&#34;,
    &#34;object&#34;: &#34;chat.completion&#34;,
    &#34;created&#34;: 1708949465,
    &#34;model&#34;: &#34;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#34;,
    &#34;usage&#34;: {
      &#34;prompt_tokens&#34;: 24,
      &#34;completion_tokens&#34;: 71,
      &#34;total_tokens&#34;: 95
    },
    &#34;choices&#34;: [
      {
        &#34;index&#34;: 0,
        &#34;finish_reason&#34;: &#34;stop&#34;,
        &#34;message&#34;: {
          &#34;role&#34;: &#34;assistant&#34;,
          &#34;content&#34;: &#34;Here&#39;s a simple hello world program in Python:\n\n```python\nprint(\&#34;Hello, world!\&#34;)\n```\n\nThis program prints the string \&#34;Hello, world!\&#34; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#34;
        }
      }
    ]
  }
]
</pre></div></div>
</div>
<p>It’s that easy!</p>
<p>You can also call the API with a few optional inference params to adjust the model’s responses. These map to Transformers pipeline params, and are passed in directly at inference time.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code> (maps to <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code>): The maximum number of new tokens the model should generate.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code> (maps to <code class="docutils literal notranslate"><span class="pre">temperature</span></code>): Controls the creativity of the model’s response. Note that this is not guaranteed to be supported by all models, and in order for this param to have an effect, the pipeline must have been created with <code class="docutils literal notranslate"><span class="pre">do_sample=True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stop</span></code> (maps to <code class="docutils literal notranslate"><span class="pre">stopping_criteria</span></code>): A list of tokens at which to stop generation.</p></li>
</ul>
<p>Note: <code class="docutils literal notranslate"><span class="pre">n</span></code> does not have an equivalent Transformers pipeline param, and is not supported in queries. However, you can implement a model that consumes the <code class="docutils literal notranslate"><span class="pre">n</span></code> param using Custom Pyfunc (details below).</p>
</div>
<div class="section" id="Customizing-the-model">
<h2>Customizing the model<a class="headerlink" href="#Customizing-the-model" title="Permalink to this headline"> </a></h2>
<p>As always, custom functionality can be achieved with MLflow’s <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> API. In the cell below, we create a custom Chat-flavored <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model by subclassing <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.ChatModel</span></code>.</p>
<p>In this example, we’ll use our previously-saved TinyLlama pipeline as the backing model by loading it with <code class="docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code>, and then build our own customizations on top of it. However, <code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> is agnostic to how you generate the outputs. You could use another transformer, Langchain, native OpenAI integrations, or even no LLM at all! As long the result of <code class="docutils literal notranslate"><span class="pre">predict</span></code> is of type <code class="docutils literal notranslate"><span class="pre">mlflow.types.llm.ChatResponse</span></code>, you’ll be able to take advantage of the automatic
signature generation and input/output parsing.</p>
<p>The possibilities for customization are endless here, but as a quick example, the code below simply edits the ID of the response, rather than having it be a random UUID. Of course, you could also insert any side-effects you wanted here, such as asynchronously logging some metadata for analytics.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.types.llm</span> <span class="kn">import</span> <span class="n">ChatResponse</span>


<span class="k">class</span> <span class="nc">MyChatModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">ChatModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">load_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="c1"># load our previously-saved Transformers pipeline from context.artifacts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">artifacts</span><span class="p">[</span><span class="s2">&quot;chat_model_path&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">messages</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">tokenizer</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># perform inference using the loaded pipeline</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">generation_kwargs</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">]</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;some_meaningful_id_</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># construct token usage information</span>
        <span class="n">prompt_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span>
        <span class="n">completion_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
        <span class="n">usage</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;prompt_tokens&quot;</span><span class="p">:</span> <span class="n">prompt_tokens</span><span class="p">,</span>
            <span class="s2">&quot;completion_tokens&quot;</span><span class="p">:</span> <span class="n">completion_tokens</span><span class="p">,</span>
            <span class="s2">&quot;total_tokens&quot;</span><span class="p">:</span> <span class="n">prompt_tokens</span> <span class="o">+</span> <span class="n">completion_tokens</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># here, we can do any post-processing or side-effects required.</span>
        <span class="c1"># for example, we could log the generated text to a database for</span>
        <span class="c1"># analytics, or check the output for any banned words or phrases</span>
        <span class="c1"># and return a different response if any are found.</span>

        <span class="c1"># in this example, we just return the generated text as the response</span>

        <span class="n">response</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="nb">id</span><span class="p">,</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;MyChatModel&quot;</span><span class="p">,</span>
            <span class="s2">&quot;choices&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s2">&quot;message&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">},</span>
                    <span class="s2">&quot;finish_reason&quot;</span><span class="p">:</span> <span class="s2">&quot;stop&quot;</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">],</span>
            <span class="s2">&quot;usage&quot;</span><span class="p">:</span> <span class="n">usage</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">ChatResponse</span><span class="p">(</span><span class="o">**</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Similar to what happens above, upon saving an instance of <code class="docutils literal notranslate"><span class="pre">MyChatModel</span></code>, MLflow will automatically recognize the <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.ChatModel</span></code> subclass, and set chat signatures and handle input and output parsing automatically. Note that enforcement is performed on the output–MLflow will run inference on an example input, and assert that the output is of type <code class="docutils literal notranslate"><span class="pre">ChatResponse</span></code>.</p>
<p>Full documentation for the <code class="docutils literal notranslate"><span class="pre">ChatResponse</span></code> type can be found in the <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatResponse">API reference</a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;my-model&quot;</span><span class="p">,</span>
    <span class="n">python_model</span><span class="o">=</span><span class="n">MyChatModel</span><span class="p">(),</span>
    <span class="c1"># provide the path to the pipeline we saved earlier</span>
    <span class="n">artifacts</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;chat_model_path&quot;</span><span class="p">:</span> <span class="s2">&quot;tinyllama-chat&quot;</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/02/26 21:12:47 INFO mlflow.pyfunc: Predicting on input example to validate output
/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/2958668123.py:10: FutureWarning: The &#39;transformers&#39; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
  self.pipeline = mlflow.transformers.load_model(context.artifacts[&#34;chat_model_path&#34;])
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "cd12a6afe29847bcb79bdb01fc4892bd", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/02/26 21:12:47 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8986e18350c44293944a8dc63d5e8324", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ab96f52af76949b790de16a3567619f3", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/02/26 21:13:19 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false
</pre></div></div>
</div>
<p>As before, we can now serve the model by running the following in a terminal shell:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mlflow models serve -m my-model
</pre></div>
</div>
<p>And we should now be able to query it via HTTP request:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-sh notranslate"><div class="highlight"><pre><span></span>%%sh
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span><span class="s1">&#39;{ &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write me a hello world program in python&quot;}] }&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="p">|</span><span class="w"> </span>jq
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   666  100   577  100    89     23      3  0:00:29  0:00:24  0:00:05   141
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{
  &#34;id&#34;: &#34;some_meaningful_id_33&#34;,
  &#34;model&#34;: &#34;MyChatModel&#34;,
  &#34;choices&#34;: [
    {
      &#34;index&#34;: 0,
      &#34;message&#34;: {
        &#34;role&#34;: &#34;assistant&#34;,
        &#34;content&#34;: &#34;Here&#39;s a simple hello world program in Python:\n\n```python\nprint(\&#34;Hello, world!\&#34;)\n```\n\nThis program prints the string \&#34;Hello, world!\&#34; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#34;
      },
      &#34;finish_reason&#34;: &#34;stop&#34;
    }
  ],
  &#34;usage&#34;: {
    &#34;prompt_tokens&#34;: 25,
    &#34;completion_tokens&#34;: 71,
    &#34;total_tokens&#34;: 96
  },
  &#34;object&#34;: &#34;chat.completion&#34;,
  &#34;created&#34;: 1708949708
}
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Conclusion">
<h1>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline"> </a></h1>
<p>In this tutorial, you learned how to create an OpenAI-compatible chat model by specifying “llm/v1/chat” as the task when saving Transformers pipelines. You also learned how to leverage Custom Pyfunc to add customizations that fit your specific use-case.</p>
<div class="section" id="What’s-next?">
<h2>What’s next?<a class="headerlink" href="#What’s-next?" title="Permalink to this headline"> </a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html">In-depth Pyfunc Walkthrough</a>. We briefly touched on custom <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> in this tutorial, but if you’re looking for more detail on the anatomy of a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model, the linked page provides an in-depth overview of all the components.</p></li>
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/deployment/index.html">More on MLflow Deployments</a>. In this tutorial, we saw how to deploy a model using a local server, but MLflow provides many other ways to deploy your models to production. Check out this page to learn more about the different options.</p></li>
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/transformers/index.html">More on MLflow’s Transformers Integration</a>. This page provides a comprehensive overview on MLflow’s Transformers integrations, along with lots of hands-on guides and notebooks. Learn how to fine-tune models, use prompt templates, and more!</p></li>
<li><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/index.html">Other LLM Integrations</a>. Aside from Transformers, MLflow has integrations with many other popular LLM libraries, such as Langchain and OpenAI.</p></li>
</ul>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="conversational-model.html" class="btn btn-neutral" title="Introduction to Conversational AI with MLflow and DialoGPT" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../fine-tuning/transformers-fine-tuning.html" class="btn btn-neutral" title="Fine-Tuning Transformers with MLflow for Enhanced Model Management" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../../',
      VERSION:'2.14.2.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>