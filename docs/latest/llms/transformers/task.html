

<!DOCTYPE html>
<!-- source: docs/source/llms/transformers/task.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tasks in MLflow Transformers Flavor</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/transformers/task.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.16.1.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="MLflow Transformers Flavor" href="index.html"/>
        <link rel="next" title="ðŸ¤— Transformers within MLflow" href="/guide/index.html"/>
        <link rel="prev" title="Working with Large Models in MLflow Transformers flavor" href="/large-models.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.16.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="index.html">MLflow Transformers Flavor</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="index.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#getting-started-with-the-mlflow-transformers-flavor-tutorials-and-guides">Getting Started with the MLflow Transformers Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#important-details-to-be-aware-of-with-the-transformers-flavor">Important Details to be aware of with the transformers flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#logging-large-models">Logging Large Models</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="index.html#working-with-tasks-for-transformer-pipelines">Working with <code class="docutils literal notranslate"><span class="pre">tasks</span></code> for Transformer Pipelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#id1">Detailed Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#learn-more-about-transformers">Learn more about Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../langchain/index.html">MLflow LangChain Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llama-index/index.html">MLflow LlamaIndex Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="index.html">MLflow Transformers Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Tasks in MLflow Transformers Flavor</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/task.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="tasks-in-mlflow-transformers-flavor">
<h1>Tasks in MLflow Transformers Flavor<a class="headerlink" href="#tasks-in-mlflow-transformers-flavor" title="Permalink to this headline"> </a></h1>
<p>This page provides an overview of how to use the <code class="docutils literal notranslate"><span class="pre">task</span></code> parameter in the MLflow Transformers flavor to control the inference interface of the model.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#native-transformers-task-types" id="id2">Native Transformers Task Types</a></p></li>
<li><p><a class="reference internal" href="#advanced-tasks-for-openai-compatible-inference" id="id3">Advanced Tasks for OpenAI-Compatible Inference</a></p>
<ul>
<li><p><a class="reference internal" href="#input-and-output-formats" id="id4">Input and Output Formats</a></p></li>
<li><p><a class="reference internal" href="#code-example-of-using-llm-v1-tasks" id="id5">Code Example of Using <code class="docutils literal notranslate"><span class="pre">llm/v1</span></code> Tasks</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#provisioned-throughput-on-databricks-model-serving" id="id6">Provisioned Throughput on Databricks Model Serving</a></p></li>
<li><p><a class="reference internal" href="#faq" id="id7">FAQ</a></p>
<ul>
<li><p><a class="reference internal" href="#how-to-override-the-default-query-parameters-for-the-openai-compatible-inference" id="id8">How to override the default query parameters for the OpenAI-compatible inference?</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id1">Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline"> </a></h2>
<p>In the MLflow Transformers flavor, <code class="docutils literal notranslate"><span class="pre">task</span></code> plays a crucial role in determining the input and output format of the model. The <code class="docutils literal notranslate"><span class="pre">task</span></code> is a fundamental concept in the Transformers library, which describe the structure of each modelâ€™s API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model.</p>
<p>MLflow utilizes this concept to determine the input and output format of the model, persists the correct <a class="reference external" href="https://mlflow.org/docs/latest/models.html#model-signatures-and-input-examples">Model Signature</a>, and provides a consistent <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#inference-api">Pyfunc Inference API</a> for serving different types of models. Additionally, on top of the native Transformers task types, MLflow defines a few additional task types to support more complex use cases, such as chat-style applications.</p>
</div>
<div class="section" id="native-transformers-task-types">
<h2><a class="toc-backref" href="#id2">Native Transformers Task Types</a><a class="headerlink" href="#native-transformers-task-types" title="Permalink to this headline"> </a></h2>
<p>For native Transformers tasks, MLflow will automatically infer the task type from the pipeline when you save a pipeline with <a class="reference internal" href="../../python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a>. You can also specify the task type explicitly by passing the <code class="docutils literal notranslate"><span class="pre">task</span></code> parameter. The full list of supported task types is available in the <a class="reference external" href="https://huggingface.co/tasks">Transformers documentation</a>, but note that <strong>not all task types are supported in MLflow</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inferred task: </span><span class="si">{</span><span class="n">model_info</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="s1">&#39;transformers&#39;</span><span class="p">][</span><span class="s1">&#39;task&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># &gt;&gt; Inferred task: text-generation</span>
</pre></div>
</div>
</div>
<div class="section" id="advanced-tasks-for-openai-compatible-inference">
<h2><a class="toc-backref" href="#id3">Advanced Tasks for OpenAI-Compatible Inference</a><a class="headerlink" href="#advanced-tasks-for-openai-compatible-inference" title="Permalink to this headline"> </a></h2>
<p>In addition to the native Transformers task types, MLflow defines a few additional task types. Those advanced task types allows you to extend the Transformers pipeline with OpenAI-compatible inference interface, to serve models for specific use cases.
In addition to the native Transformers task types, MLflow defines several additional task types. These advanced task types allow you to extend the Transformers pipeline with an OpenAI-compatible inference interface to serve models for specific use cases.</p>
<p>For example, the Transformers <code class="docutils literal notranslate"><span class="pre">text-generation</span></code> pipeline inputs and outputs a single string or a list of strings. However, when serving a model, it is often necessary to have a more structured input and output format. For instance, in a chat-style application, the input may be a list of messages.</p>
<p>To support these use cases, MLflow defines a set of advanced task types prefixed with <code class="docutils literal notranslate"><span class="pre">llm/v1</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/chat&quot;</span></code> for chat-style applications</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/completions&quot;</span></code> for generic completions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;llm/v1/embeddings&quot;</span></code> for text embeddings generation</p></li>
</ul>
<p>The required step to use these advanced task types is just to specify the <code class="docutils literal notranslate"><span class="pre">task</span></code> parameter as an <code class="docutils literal notranslate"><span class="pre">llm/v1</span></code> task when logging the models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="s2">&quot;llm/v1/chat&quot;</span><span class="p">,</span>  <span class="c1"># &lt;= Specify the llm/v1 task type</span>
        <span class="c1"># Optional, recommended for large models to avoid creating a local copy of the model weights</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is only available in MLflow 2.11.0 and above. Also, the <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> task type is only available for models saved with <code class="docutils literal notranslate"><span class="pre">transformers</span> <span class="pre">&gt;=</span> <span class="pre">4.34.0</span></code>.</p>
</div>
<div class="section" id="input-and-output-formats">
<h3><a class="toc-backref" href="#id4">Input and Output Formats</a><a class="headerlink" href="#input-and-output-formats" title="Permalink to this headline"> </a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>Supported pipeline</p></th>
<th class="head"><p>Input</p></th>
<th class="head"><p>Output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">text-generation</span></code></p></td>
<td><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#chat">Chat API spec</a></p></td>
<td><p>Returns a <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/object">Chat Completion</a> object in the json format.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">llm/v1/completions</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">text-generation</span></code></p></td>
<td><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#completions">Completions API spec</a></p></td>
<td><p>Returns a <a class="reference external" href="https://platform.openai.com/docs/guides/text-generation/completions-api">Completion</a> object in the json format.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">llm/v1/embeddings</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">feature-extraction</span></code></p></td>
<td><p><a class="reference external" href="https://mlflow.org/docs/latest/llms/deployments/index.html#embeddings">Embeddings API spec</a></p></td>
<td><p>Returns a list of <a class="reference external" href="https://platform.openai.com/docs/api-reference/embeddings/object">Embedding</a> object. Additionally, the model returns <code class="docutils literal notranslate"><span class="pre">usage</span></code> field, which contains the number of tokens used for the embeddings generation.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Completion API is considered as legacy, but it is still supported in MLflow for backward compatibility. We recommend using the Chat API for compatibility with the latest APIs from OpenAI and other model providers.</p>
</div>
</div>
<div class="section" id="code-example-of-using-llm-v1-tasks">
<h3><a class="toc-backref" href="#id5">Code Example of Using <code class="docutils literal notranslate"><span class="pre">llm/v1</span></code> Tasks</a><a class="headerlink" href="#code-example-of-using-llm-v1-tasks" title="Permalink to this headline"> </a></h3>
<p>The following code snippet demonstrates how to log a Transformers pipeline with the <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> task type, and use the model for chat-style inference. Check out the <a class="reference external" href="tutorials/conversational/pyfunc-chat-model.html">notebook tutorial</a> to see more examples in action!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="s2">&quot;llm/v1/chat&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a bot.&quot;</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">},</span>
            <span class="p">]</span>
        <span class="p">},</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Model metadata logs additional field &quot;inference_task&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="s2">&quot;transformers&quot;</span><span class="p">][</span><span class="s2">&quot;inference_task&quot;</span><span class="p">])</span>
<span class="c1"># &gt;&gt; llm/v1/chat</span>

<span class="c1"># The original native task type is also saved</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="s2">&quot;transformers&quot;</span><span class="p">][</span><span class="s2">&quot;task&quot;</span><span class="p">])</span>
<span class="c1"># &gt;&gt; text-generation</span>

<span class="c1"># Model signature is set to the chat API spec</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">signature</span><span class="p">)</span>
<span class="c1"># &gt;&gt; inputs:</span>
<span class="c1"># &gt;&gt;   [&#39;messages&#39;: Array({content: string (required), name: string (optional), role: string (required)}) (required), &#39;temperature&#39;: double (optional), &#39;max_tokens&#39;: long (optional), &#39;stop&#39;: Array(string) (optional), &#39;n&#39;: long (optional), &#39;stream&#39;: boolean (optional)]</span>
<span class="c1"># &gt;&gt; outputs:</span>
<span class="c1"># &gt;&gt;   [&#39;id&#39;: string (required), &#39;object&#39;: string (required), &#39;created&#39;: long (required), &#39;model&#39;: string (required), &#39;choices&#39;: Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), &#39;usage&#39;: {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]</span>
<span class="c1"># &gt;&gt; params:</span>
<span class="c1"># &gt;&gt;     None</span>

<span class="c1"># The model can be served with the OpenAI-compatible inference API</span>
<span class="n">pyfunc_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">pyfunc_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a bot.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
<span class="c1"># &gt;&gt; [{&#39;choices&#39;: [{&#39;finish_reason&#39;: &#39;stop&#39;,</span>
<span class="c1"># &gt;&gt;               &#39;index&#39;: 0,</span>
<span class="c1"># &gt;&gt;               &#39;message&#39;: {&#39;content&#39;: &#39;I&#39;m doing well, thank you for asking.&#39;, &#39;role&#39;: &#39;assistant&#39;}},</span>
<span class="c1"># &gt;&gt;   &#39;created&#39;: 1719875820,</span>
<span class="c1"># &gt;&gt;   &#39;id&#39;: &#39;355c4e9e-040b-46b0-bf22-00e93486100c&#39;,</span>
<span class="c1"># &gt;&gt;   &#39;model&#39;: &#39;gpt2&#39;,</span>
<span class="c1"># &gt;&gt;   &#39;object&#39;: &#39;chat.completion&#39;,</span>
<span class="c1"># &gt;&gt;   &#39;usage&#39;: {&#39;completion_tokens&#39;: 7, &#39;prompt_tokens&#39;: 13, &#39;total_tokens&#39;: 20}}]</span>
</pre></div>
</div>
<p>Note that the input and output modifications only apply when the model is loaded with <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> (e.g. when
serving the model with the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span> <span class="pre">serve</span></code> CLI tool). If you want to load just the raw pipeline, you can
use <a class="reference internal" href="../../python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a>.</p>
</div>
</div>
<div class="section" id="provisioned-throughput-on-databricks-model-serving">
<h2><a class="toc-backref" href="#id6">Provisioned Throughput on Databricks Model Serving</a><a class="headerlink" href="#provisioned-throughput-on-databricks-model-serving" title="Permalink to this headline"> </a></h2>
<p><a class="reference external" href="https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html">Provisioned Throughput</a> on Databricks Model Serving is a capability that optimizes inference performance for foundation models with performance guarantees. To serve Transformers models with provisioned throughput, specify <code class="docutils literal notranslate"><span class="pre">llm/v1/xxx</span></code> task type when logging the model. MLflow logs the required metadata to enable provisioned throughput on Databricks Model Serving.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When logging large models, you can use <code class="docutils literal notranslate"><span class="pre">save_pretrained=False</span></code> to avoid creating a local copy of the model weights for saving time and disk space. Please refer to the <a class="reference internal" href="large-models.html#transformers-save-pretrained-guide"><span class="std std-ref">documentation</span></a> for more details.</p>
</div>
</div>
<div class="section" id="faq">
<h2><a class="toc-backref" href="#id7">FAQ</a><a class="headerlink" href="#faq" title="Permalink to this headline"> </a></h2>
<div class="section" id="how-to-override-the-default-query-parameters-for-the-openai-compatible-inference">
<h3><a class="toc-backref" href="#id8">How to override the default query parameters for the OpenAI-compatible inference?</a><a class="headerlink" href="#how-to-override-the-default-query-parameters-for-the-openai-compatible-inference" title="Permalink to this headline"> </a></h3>
<p>When serving the model saved with the <code class="docutils literal notranslate"><span class="pre">llm/v1</span></code> task type, MLflow uses the same default value as OpenAI APIs for the parameters like <code class="docutils literal notranslate"><span class="pre">temperature</span></code> and <code class="docutils literal notranslate"><span class="pre">stop</span></code>. You can override them by either passing the values at inference time, or by setting different default values when logging the model.</p>
<ol class="arabic simple">
<li><p>At inference time: You can pass the parameters as part of the input dictionary when calling the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method, just like how you pass the input messages.</p></li>
<li><p>When logging the model: You can override the default values for the parameters by saving a <code class="docutils literal notranslate"><span class="pre">model_config</span></code> parameter when logging the model.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="s2">&quot;llm/v1/chat&quot;</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># &lt;= Set the default temperature</span>
            <span class="s2">&quot;stop&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="s2">&quot;bar&quot;</span><span class="p">],</span>  <span class="c1"># &lt;= Set the default stop sequence</span>
        <span class="p">},</span>
        <span class="n">save_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">stop</span></code> parameter can be used to specify the stop sequence for the <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> and <code class="docutils literal notranslate"><span class="pre">llm/v1/completions</span></code> tasks. We emulate the behavior of the <code class="docutils literal notranslate"><span class="pre">stop</span></code> parameter in the OpenAI APIs by passing the <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate.stopping_criteria">stopping_criteria</a> to the Transformers pipeline, with the token IDs of the given stop sequence. However, the behavior may not be stable because the tokenizer does not always generate the same token IDs for the same sequence in different sentences, especially for <code class="docutils literal notranslate"><span class="pre">sentence-piece</span></code> based tokenizers.</p>
</div>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="large-models.html" class="btn btn-neutral" title="Working with Large Models in MLflow Transformers flavor" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="guide/index.html" class="btn btn-neutral" title="ðŸ¤— Transformers within MLflow" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.16.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>