

<!DOCTYPE html>
<!-- source: docs/source/llms/prompt-engineering/index.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Prompt Engineering UI (Experimental)</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/prompt-engineering/index.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.3.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="LLMs" href="../index.html"/>
        <link rel="next" title="MLflow Transformers Flavor" href="/../transformers/index.html"/>
        <link rel="prev" title="MLflow LLM Evaluation" href="/../llm-evaluate/index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-genai-applications-in-mlflow">Tutorials and Use Case Guides for GenAI applications in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow AI Gateway for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Prompt Engineering UI (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quickstart">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deployment-for-real-time-serving">Deployment for real-time serving</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../index.html#benefits-of-the-mlflow-prompt-engineering-ui">Benefits of the MLflow Prompt Engineering UI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Prompt Engineering UI (Experimental)</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/prompt-engineering/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="prompt-engineering-ui-experimental">
<span id="prompt-engineering"></span><h1>Prompt Engineering UI (Experimental)<a class="headerlink" href="#prompt-engineering-ui-experimental" title="Permalink to this headline"> </a></h1>
<p>Starting in MLflow 2.7, the MLflow Tracking UI provides a best-in-class experience for prompt
engineering. With no code required, you can try out multiple LLMs from the
<a class="reference internal" href="../deployments/index.html#deployments"><span class="std std-ref">MLflow AI Gateway</span></a>, parameter configurations, and prompts to build a variety of models for
question answering, document summarization, and beyond. Using the embedded Evaluation UI, you can
also evaluate multiple models on a set of inputs and compare the responses to select the best one.
Every model created with the prompt engineering UI is stored in the <a class="reference internal" href="../../models.html#models"><span class="std std-ref">MLflow Model</span></a>
format and can be deployed for batch or real time inference. All configurations (prompt templates,
choice of LLM, parameters, etc.) are tracked as <a class="reference internal" href="../../tracking.html#tracking"><span class="std std-ref">MLflow Runs</span></a>.</p>
<div class="section" id="quickstart">
<span id="prompt-engineering-quickstart"></span><h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline"> </a></h2>
<p>The following guide will get you started with MLflow’s UI for prompt engineering.</p>
<div class="section" id="step-1-create-an-mlflow-ai-gateway-completions-or-chat-endpoint">
<h3>Step 1: Create an MLflow AI Gateway Completions or Chat Endpoint<a class="headerlink" href="#step-1-create-an-mlflow-ai-gateway-completions-or-chat-endpoint" title="Permalink to this headline"> </a></h3>
<p>To use the prompt engineering UI, you need to create one or more <a class="reference internal" href="../deployments/index.html#deployments"><span class="std std-ref">MLflow AI Gateway</span></a>
completions or chat <a class="reference internal" href="../deployments/index.html#deployments-endpoints"><span class="std std-ref">Endpoints</span></a>. Follow the
<a class="reference internal" href="../deployments/index.html#deployments-quickstart"><span class="std std-ref">MLflow AI Gateway Quickstart guide</span></a> to easily create an endpoint in less than five
minutes. If you already have access to an MLflow AI Gateway endpoint of type <code class="docutils literal notranslate"><span class="pre">llm/v1/completions</span></code>
or <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code>, you can skip this step.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>gateway<span class="w"> </span>start<span class="w"> </span>--config-path<span class="w"> </span>config.yaml<span class="w"> </span>--port<span class="w"> </span><span class="m">7000</span>
</pre></div>
</div>
</div>
<div class="section" id="step-2-connect-the-mlflow-ai-gateway-to-your-mlflow-tracking-server">
<h3>Step 2: Connect the MLflow AI Gateway to your MLflow Tracking Server<a class="headerlink" href="#step-2-connect-the-mlflow-ai-gateway-to-your-mlflow-tracking-server" title="Permalink to this headline"> </a></h3>
<p>The prompt engineering UI also requires a connection between the MLflow AI Gateway and the MLflow
Tracking Server. To connect the MLflow AI Gateway with the MLflow Tracking Server, simply set the
<code class="docutils literal notranslate"><span class="pre">MLFLOW_DEPLOYMENTS_TARGET</span></code> environment variable in the environment where the server is running and
restart the server. For example, if the MLflow AI Gateway is running at <code class="docutils literal notranslate"><span class="pre">http://localhost:7000</span></code>, you
can start an MLflow Tracking Server in a shell on your local machine and connect it to the
MLflow AI Gateway using the <a class="reference internal" href="../../cli.html#cli"><span class="std std-ref">mlflow server</span></a> command as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MLFLOW_DEPLOYMENTS_TARGET</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:7000&quot;</span>
mlflow<span class="w"> </span>server<span class="w"> </span>--port<span class="w"> </span><span class="m">5000</span>
</pre></div>
</div>
</div>
<div class="section" id="step-3-create-or-find-an-mlflow-experiment">
<h3>Step 3: Create or find an MLflow Experiment<a class="headerlink" href="#step-3-create-or-find-an-mlflow-experiment" title="Permalink to this headline"> </a></h3>
<p>Next, open an existing MLflow Experiment in the MLflow UI, or create a new experiment.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/experiment_page.png"><img alt="../../_images/experiment_page.png" src="../../_images/experiment_page.png" style="width: 648.0px; height: 493.0px;" /></a>
</div>
</div>
<div class="section" id="step-4-create-a-run-with-prompt-engineering">
<h3>Step 4: Create a run with prompt engineering<a class="headerlink" href="#step-4-create-a-run-with-prompt-engineering" title="Permalink to this headline"> </a></h3>
<p>Once you have opened the Experiment, click the <strong>New Run</strong> button and select
<em>using Prompt Engineering</em>. This will open the prompt engineering playground where you can try
out different LLMs, parameters, and prompts.</p>
<p><a class="reference internal" href="../../_images/new_run.png"><img alt="new_run" src="../../_images/new_run.png" style="width: 25%;" /></a> <a class="reference internal" href="../../_images/prompt_modal_1.png"><img alt="prompt_modal_1" src="../../_images/prompt_modal_1.png" style="width: 70%;" /></a></p>
</div>
<div class="section" id="step-5-select-your-endpoint-and-evaluate-the-example-prompt">
<h3>Step 5: Select your endpoint and evaluate the example prompt<a class="headerlink" href="#step-5-select-your-endpoint-and-evaluate-the-example-prompt" title="Permalink to this headline"> </a></h3>
<p>Next, click the <em>Select endpoint</em> dropdown and select the MLflow AI Gateway completions endpoint you created in
Step 1. Then, click the <strong>Evaluate</strong> button to test out an example prompt engineering use case
for generating product advertisements.</p>
<p>MLflow will embed the specified <em>stock_type</em> input
variable value - <code class="docutils literal notranslate"><span class="pre">&quot;books&quot;</span></code> - into the specified <em>prompt  template</em> and send it to the LLM
associated with the MLflow AI Gateway endpoint with the configured <em>temperature</em> (currently <code class="docutils literal notranslate"><span class="pre">0.01</span></code>)
and <em>max_tokens</em> (currently 1000). The LLM response will appear in the <em>Output</em> section.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/prompt_modal_2.png"><img alt="../../_images/prompt_modal_2.png" src="../../_images/prompt_modal_2.png" style="width: 1031.5px; height: 566.5px;" /></a>
</div>
</div>
<div class="section" id="step-6-try-a-prompt-of-your-choosing">
<h3>Step 6: Try a prompt of your choosing<a class="headerlink" href="#step-6-try-a-prompt-of-your-choosing" title="Permalink to this headline"> </a></h3>
<p>Replace the prompt template from the previous step with a prompt template of your choosing.
Prompts can define multiple variables. For example, you can use the following prompt template
to instruct the LLM to answer questions about the MLflow documentation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Read the following article from the MLflow documentation that appears between triple
backticks. Then, answer the question about the documentation that appears between triple quotes.
Include relevant links and code examples in your answer.

```{{article}}```

&quot;&quot;&quot;
{{question}}
&quot;&quot;&quot;
</pre></div>
</div>
<p>Then, fill in the input variables. For example, in the MLflow documentation
use case, the <em>article</em> input variable can be set to the contents of
<a class="reference external" href="https://mlflow.org/docs/latest/tracking.html#logging-data-to-runs">https://mlflow.org/docs/latest/tracking.html#logging-data-to-runs</a> and the <em>question</em> input variable
can be set to <code class="docutils literal notranslate"><span class="pre">&quot;How</span> <span class="pre">do</span> <span class="pre">I</span> <span class="pre">create</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">MLflow</span> <span class="pre">Run</span> <span class="pre">using</span> <span class="pre">the</span> <span class="pre">Python</span> <span class="pre">API?&quot;</span></code>.</p>
<p>Finally, click the <strong>Evaluate</strong> button to see the new output. You can also try choosing a larger
value of <em>temperature</em> to observe how the LLM’s output changes.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/prompt_modal_3.png"><img alt="../../_images/prompt_modal_3.png" src="../../_images/prompt_modal_3.png" style="width: 826.0px; height: 606.1999999999999px;" /></a>
</div>
</div>
<div class="section" id="step-7-capture-your-choice-of-llm-prompt-template-and-parameters-as-an-mlflow-run">
<h3>Step 7: Capture your choice of LLM, prompt template, and parameters as an MLflow Run<a class="headerlink" href="#step-7-capture-your-choice-of-llm-prompt-template-and-parameters-as-an-mlflow-run" title="Permalink to this headline"> </a></h3>
<p>Once you’re satisfied with your chosen prompt template and parameters, click the <strong>Create Run</strong>
button to store this information, along with your choice of LLM, as an MLflow Run. This will
create a new Run with the prompt template, parameters, and choice of LLM stored as Run params.
It will also automatically create an MLflow Model with this information that can be used for batch
or real-time inference.</p>
<ol class="arabic">
<li><p>To view this information, click the Run name to open the <strong>Run</strong> page:</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/prompt_eng_run_page.png"><img alt="../../_images/prompt_eng_run_page.png" src="../../_images/prompt_eng_run_page.png" style="width: 753.5px; height: 497.5px;" /></a>
</div>
</div></blockquote>
</li>
<li><p>You can also see the parameters and compare them with other configurations by opening the <strong>Table</strong>
view tab:</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/prompt_eng_table_view.png"><img alt="../../_images/prompt_eng_table_view.png" src="../../_images/prompt_eng_table_view.png" style="width: 723.0px; height: 213.0px;" /></a>
</div>
</div></blockquote>
</li>
<li><p>After your Run is created, MLflow will open the <strong>Evaluation</strong> tab where you can see your latest
playground input &amp; output and try out additional inputs:</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/eval_view_1.png"><img alt="../../_images/eval_view_1.png" src="../../_images/eval_view_1.png" style="width: 724.5px; height: 367.0px;" /></a>
</div>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="step-8-try-new-inputs">
<h3>Step 8: Try new inputs<a class="headerlink" href="#step-8-try-new-inputs" title="Permalink to this headline"> </a></h3>
<p>To test the behavior of your chosen LLM, prompt template, and parameters on a new inputs:</p>
<ol class="arabic">
<li><p>Click the <em>Add Row</em> button and fill in a value(s) your prompt template’s input variable(s).
For example, in the MLflow documentation use case, you can try asking a question
unrelated to MLflow to see how the LLM responds. This is important to ensure that the application
is robust to irrelevant inputs.</p>
<blockquote>
<div><p><a class="reference internal" href="../../_images/add_row.png"><img alt="add_row" src="../../_images/add_row.png" style="width: 10%;" /></a> <a class="reference internal" href="../../_images/add_row_modal.png"><img alt="add_row_modal" src="../../_images/add_row_modal.png" style="width: 50%;" /></a></p>
</div></blockquote>
</li>
<li><p>Then, click the <strong>Evaluate</strong> button to see the output.</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/evaluate_new_input.png"><img alt="../../_images/evaluate_new_input.png" src="../../_images/evaluate_new_input.png" style="width: 654.4000000000001px; height: 482.40000000000003px;" /></a>
</div>
</div></blockquote>
</li>
<li><p>Finally, click the <strong>Save</strong> button to store the new inputs and output.</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/save_new_input.png"><img alt="../../_images/save_new_input.png" src="../../_images/save_new_input.png" style="width: 654.4000000000001px; height: 483.20000000000005px;" /></a>
</div>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="step-9-adjust-your-prompt-template-and-create-a-new-run">
<h3>Step 9: Adjust your prompt template and create a new Run<a class="headerlink" href="#step-9-adjust-your-prompt-template-and-create-a-new-run" title="Permalink to this headline"> </a></h3>
<p>As you try additional inputs, you might discover scenarios where your choice of LLM, prompt
template, and parameters doesn’t perform as well as you would like. For example, in the
MLflow documentation use case, the LLM still attempts to answer irrelevant
questions about <a class="reference internal" href="../../projects.html#projects"><span class="std std-ref">MLflow Projects</span></a> even if the answer does not appear in the
specified article.</p>
<ol class="arabic">
<li><p>To improve performance, create a new Run by selecting the <em>Duplicate run</em> option from the context
menu. For example, in the MLflow documentation use case, adding the following text to
the prompt template helps improve robustness to irrelevant questions:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>If the question does not relate to the article, respond exactly with the phrase
&quot;I do not know how to answer that question.&quot; Do not include any additional text in your
response.
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/duplicate_run.png"><img alt="../../_images/duplicate_run.png" src="../../_images/duplicate_run.png" style="width: 438.40000000000003px; height: 389.6px;" /></a>
</div>
</li>
<li><p>Then, from the prompt engineering playground, adjust the prompt template (and / or choice of
LLM and parameters), evaluate an input, and click the <strong>Create Run</strong> button to create a new Run.</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/prompt_modal_4.png"><img alt="../../_images/prompt_modal_4.png" src="../../_images/prompt_modal_4.png" style="width: 950.4000000000001px; height: 701.6px;" /></a>
</div>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="step-10-evaluate-the-new-prompt-template-on-previous-inputs">
<h3>Step 10: Evaluate the new prompt template on previous inputs<a class="headerlink" href="#step-10-evaluate-the-new-prompt-template-on-previous-inputs" title="Permalink to this headline"> </a></h3>
<p>Now that you’ve made an adjustment to your prompt template, it’s important to make sure that
the new template performs well on the previous inputs and compare the outputs with older
configurations.</p>
<ol class="arabic">
<li><p>From the <strong>Evaluation</strong> tab, click the <strong>Evaluate all</strong> button next to the new Run to evaluate
all of the previous inputs.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/evaluate_all.png"><img alt="../../_images/evaluate_all.png" src="../../_images/evaluate_all.png" style="width: 290.40000000000003px; height: 398.40000000000003px;" /></a>
</div>
</li>
<li><p>Click the <strong>Save</strong> button to store the results.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/evaluate_all_results.png"><img alt="../../_images/evaluate_all_results.png" src="../../_images/evaluate_all_results.png" style="width: 934.4000000000001px; height: 481.6px;" /></a>
</div>
</li>
</ol>
</div>
<div class="section" id="step-11-load-evaluation-data-programmatically">
<h3>Step 11: Load evaluation data programmatically<a class="headerlink" href="#step-11-load-evaluation-data-programmatically" title="Permalink to this headline"> </a></h3>
<p>All of the inputs and outputs produced by the MLflow prompt engineering UI and Evaluation UI are stored
as artifacts in MLflow Runs. They can be accessed programmatically using the <a class="reference internal" href="../../python_api/mlflow.html#mlflow.load_table" title="mlflow.load_table"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.load_table()</span></code></a> API
as follows:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;/Path/to/your/prompt/engineering/experiment&quot;</span><span class="p">)</span>

<span class="c1"># Load input and output data across all Runs (configurations) as a Pandas DataFrame</span>
<span class="n">inputs_outputs_pdf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">load_table</span><span class="p">(</span>
    <span class="c1"># All inputs and outputs created from the MLflow UI are stored in an artifact called</span>
    <span class="c1"># &quot;eval_results_table.json&quot;</span>
    <span class="n">artifact_file</span><span class="o">=</span><span class="s2">&quot;eval_results_table.json&quot;</span><span class="p">,</span>
    <span class="c1"># Include the run ID as a column in the table to distinguish inputs and outputs</span>
    <span class="c1"># produced by different runs</span>
    <span class="n">extra_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;run_id&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="c1"># Optionally convert the Pandas DataFrame to Spark where it can be stored as a Delta</span>
<span class="c1"># table or joined with existing Delta tables</span>
<span class="n">inputs_outputs_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">inputs_outputs_pdf</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="step-12-generate-predictions-programmatically">
<span id="quickstart-score"></span><h3>Step 12: Generate predictions programmatically<a class="headerlink" href="#step-12-generate-predictions-programmatically" title="Permalink to this headline"> </a></h3>
<p>Once you have found a configuration of LLM, prompt template, and parameters that performs well, you
can generate predictions using the corresponding MLflow Model in a Python environment of your choosing,
or you can <a class="reference internal" href="#deploy-prompt-serving"><span class="std std-ref">deploy it for real-time serving</span></a>.</p>
<ol class="arabic">
<li><p>To load the MLflow Model in a notebook for batch inference, click on the Run’s name to open the
<strong>Run Page</strong> and select the <em>model</em> directory in the <strong>Artifact Viewer</strong>. Then, copy the first
few lines of code from the <em>Predict on a Pandas DataFrame</em> section and run them in a Python
environment of your choosing, for example:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/load_model.png"><img alt="../../_images/load_model.png" src="../../_images/load_model.png" style="width: 901.1999999999999px; height: 416.4px;" /></a>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">logged_model</span> <span class="o">=</span> <span class="s2">&quot;runs:/8451075c46964f82b85fe16c3d2b7ea0/model&quot;</span>

<span class="c1"># Load model as a PyFuncModel.</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Then, to generate predictions, call the <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel.predict" title="mlflow.pyfunc.PyFuncModel.predict"><code class="xref py py-func docutils literal notranslate"><span class="pre">predict()</span></code></a> method
and pass in a dictionary of input variables. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">article_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">An MLflow Project is a format for packaging data science code in a reusable and reproducible way.</span>
<span class="s2">The MLflow Projects component includes an API and command-line tools for running projects, which</span>
<span class="s2">also integrate with the Tracking component to automatically record the parameters and git commit</span>
<span class="s2">of your source code for reproducibility.</span>

<span class="s2">This article describes the format of an MLflow Project and how to run an MLflow project remotely</span>
<span class="s2">using the MLflow CLI, which makes it easy to vertically scale your data science code.</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;What is an MLflow project?&quot;</span>

<span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">({</span><span class="s2">&quot;article&quot;</span><span class="p">:</span> <span class="n">article_text</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
</pre></div>
</div>
<p>For more information about deployment for real-time serving with MLflow,
see the <a class="reference internal" href="#deploy-prompt-serving"><span class="std std-ref">instructions below</span></a>.</p>
</li>
</ol>
</div>
<div class="section" id="step-13-perform-metric-based-evaluation-of-your-model-s-outputs">
<h3>Step 13: Perform metric-based evaluation of your model’s outputs<a class="headerlink" href="#step-13-perform-metric-based-evaluation-of-your-model-s-outputs" title="Permalink to this headline"> </a></h3>
<p>If you’d like to assess your model’s performance on specific metrics, MLflow provides the <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>
API. Let’s evaluate our model on some <a class="reference internal" href="../llm-evaluate/index.html#llm-eval-default-metrics"><span class="std std-ref">pre-defined metrics</span></a>
for text summarization:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">logged_model</span> <span class="o">=</span> <span class="s2">&quot;runs:/840a5c43f3fb46f2a2059b761557c1d0/model&quot;</span>

<span class="n">article_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">An MLflow Project is a format for packaging data science code in a reusable and reproducible way.</span>
<span class="s2">The MLflow Projects component includes an API and command-line tools for running projects, which</span>
<span class="s2">also integrate with the Tracking component to automatically record the parameters and git commit</span>
<span class="s2">of your source code for reproducibility.</span>

<span class="s2">This article describes the format of an MLflow Project and how to run an MLflow project remotely</span>
<span class="s2">using the MLflow CLI, which makes it easy to vertically scale your data science code.</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;What is an MLflow project?&quot;</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;article&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">article_text</span><span class="p">],</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">question</span><span class="p">],</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">article_text</span>
        <span class="p">],</span>  <span class="c1"># used for certain evaluation metrics, such as ROUGE score</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">logged_model</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;text-summarization&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">eval_table</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;See evaluation table below: </span><span class="se">\n</span><span class="si">{</span><span class="n">eval_table</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>The evaluation results can also be viewed in the MLflow Evaluation UI:</p>
<blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/evaluate_metrics.png"><img alt="../../_images/evaluate_metrics.png" src="../../_images/evaluate_metrics.png" style="width: 960.8000000000001px; height: 420.8px;" /></a>
</div>
</div></blockquote>
<p>The <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> API also supports <a class="reference internal" href="../llm-evaluate/index.html#llm-eval-custom-metrics"><span class="std std-ref">custom metrics</span></a>,
<a class="reference internal" href="../llm-evaluate/index.html#llm-eval-static-dataset"><span class="std std-ref">static dataset evaluation</span></a>, and much more. For a
more in-depth guide, see <a class="reference internal" href="../llm-evaluate/index.html#llm-eval"><span class="std std-ref">MLflow LLM Evaluation</span></a>.</p>
</div>
</div>
<div class="section" id="deployment-for-real-time-serving">
<span id="deploy-prompt-serving"></span><h2>Deployment for real-time serving<a class="headerlink" href="#deployment-for-real-time-serving" title="Permalink to this headline"> </a></h2>
<p>Once you have found a configuration of LLM, prompt template, and parameters that performs well, you
can deploy the corresponding MLflow Model for real-time serving as follows:</p>
<ol class="arabic">
<li><p>Register your model with the MLflow Model Registry. The following example registers
an MLflow Model created from the <a class="reference internal" href="#quickstart-score"><span class="std std-ref">Quickstart</span></a> as Version 1 of the
Registered Model named <code class="docutils literal notranslate"><span class="pre">&quot;mlflow_docs_qa_model&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mlflow</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span>
    <span class="n">model_uri</span><span class="o">=</span><span class="s2">&quot;runs:/8451075c46964f82b85fe16c3d2b7ea0/model&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mlflow_docs_qa_model&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Define the following environment variables in the environment where you will run your
MLflow Model Server, such as a shell on your local machine:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MLFLOW_DEPLOYMENTS_TARGET</span></code>: The URL of the MLflow AI Gateway</p></li>
</ul>
</li>
<li><p>Use the <a class="reference internal" href="../../cli.html#cli"><span class="std std-ref">mlflow models serve</span></a> command to start the MLflow Model Server. For example,
running the following command from a shell on your local machine will serve the model
on port 8000:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>--model-uri<span class="w"> </span>models:/mlflow_docs_qa_model/1<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span>
</pre></div>
</div>
</li>
<li><p>Once the server has been started, it can be queried via REST API call. For example:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">input</span><span class="o">=</span><span class="s1">&#39;</span>
<span class="s1">{</span>
<span class="s1">    &quot;dataframe_records&quot;: [</span>
<span class="s1">        {</span>
<span class="s1">            &quot;article&quot;: &quot;An MLflow Project is a format for packaging data science code...&quot;,</span>
<span class="s1">            &quot;question&quot;: &quot;What is an MLflow Project?&quot;</span>
<span class="s1">        }</span>
<span class="s1">    ]</span>
<span class="s1">}&#39;</span>

<span class="nb">echo</span><span class="w"> </span><span class="nv">$input</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>curl<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-s<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>https://localhost:8000/invocations
<span class="w">  </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span>@-
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">article</span></code> and <code class="docutils literal notranslate"><span class="pre">question</span></code> are replaced with the input variable(s) from your
prompt template.</p>
</div></blockquote>
</li>
</ol>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../llm-evaluate/index.html" class="btn btn-neutral" title="MLflow LLM Evaluation" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../transformers/index.html" class="btn btn-neutral" title="MLflow Transformers Flavor" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>