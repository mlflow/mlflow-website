

<!DOCTYPE html>
<!-- source: docs/source/llms/llama-index/index.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MLflow LlamaIndex Flavor</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/llama-index/index.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.3.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="LLMs" href="../index.html"/>
        <link rel="next" title="Building a Tool-calling Agent with LlamaIndex Workflow and MLflow" href="/notebooks/llama_index_workflow_tutorial.html"/>
        <link rel="prev" title="LangChain within MLflow (Experimental)" href="/../langchain/guide/index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-genai-applications-in-mlflow">Tutorials and Use Case Guides for GenAI applications in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow AI Gateway for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../transformers/index.html">MLflow Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../langchain/index.html">MLflow LangChain Flavor</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">MLflow LlamaIndex Flavor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#why-use-llamaindex-with-mlflow">Why use LlamaIndex with MLflow?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started">Getting Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="#concepts">Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#faq">FAQ</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../dspy/index.html">MLflow DSPy Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>MLflow LlamaIndex Flavor</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/llama-index/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="mlflow-llamaindex-flavor">
<h1>MLflow LlamaIndex Flavor<a class="headerlink" href="#mlflow-llamaindex-flavor" title="Permalink to this headline"> </a></h1>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">llama_index</span></code> flavor is under active development and is marked as Experimental. Public APIs are
subject to change and new features may be added as the flavor evolves.</p>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"> </a></h2>
<p><strong>LlamaIndex</strong> 🦙 is a powerful data-centric framework designed to seamlessly connect custom data sources to large language models (LLMs).
It offers a comprehensive suite of data structures and tools that simplify the process of ingesting, structuring, and
accessing private or domain-specific data for use with LLMs. LlamaIndex excels in enabling context-aware AI applications
by providing efficient indexing and retrieval mechanisms, making it easier to build advanced QA systems, chatbots,
and other AI-driven applications that require integration of external knowledge.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/llama-index-gateway.png"><img alt="Overview of LlamaIndex and MLflow integration" src="../../_images/llama-index-gateway.png" style="width: 70%;" /></a>
</div>
</div>
<div class="section" id="why-use-llamaindex-with-mlflow">
<h2>Why use LlamaIndex with MLflow?<a class="headerlink" href="#why-use-llamaindex-with-mlflow" title="Permalink to this headline"> </a></h2>
<p>The integration of the LlamaIndex library with MLflow provides a seamless experience for managing and deploying LlamaIndex engines. The following are some of the key benefits of using LlamaIndex with MLflow:</p>
<ul class="simple">
<li><p><a class="reference external" href="../../tracking.html">MLflow Tracking</a> allows you to track your indices within MLflow and manage the many moving parts that comprise your LlamaIndex project, such as prompts, LLMs, workflows, tools, global configurations, and more.</p></li>
<li><p><a class="reference external" href="../../models.html">MLflow Model</a> packages your LlamaIndex index/engine/workflows with all its dependency versions, input and output interfaces, and other essential metadata. This allows you to deploy your LlamaIndex models for inference with ease, knowing that the environment is consistent across different stages of the ML lifecycle.</p></li>
<li><p><a class="reference external" href="../llm-evaluate/index.html">MLflow Evaluate</a> provides native capabilities within MLflow to evaluate GenAI applications. This capability facilitates the efficient assessment of inference results from your LlamaIndex models, ensuring robust performance analytics and facilitating quick iterations.</p></li>
<li><p><a class="reference external" href="../tracing/index.html">MLflow Tracing</a> is a powerful observability tool for monitoring and debugging what happens inside the LlamaIndex models, helping you identify potential bottlenecks or issues quickly. With its powerful automatic logging capability, you can instrument your LlamaIndex application without needing to add any code apart from running a single command.</p></li>
</ul>
</div>
<div class="section" id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline"> </a></h2>
<p>In these introductory tutorials, you will learn the most fundamental components of LlamaIndex and how to leverage the integration with MLflow to bring better maintainability and observability to your LlamaIndex applications.</p>
<section>
    <article class="simple-grid">
        <div class="simple-card">
            <a href="notebooks/llama_index_workflow_tutorial.html">
                <div class="header">
                    LlamaIndex Workflows with MLflow
                </div>
                <p>
                    Get started with MLflow and LLamaIndex by building a simple agentic Workflow. Learn how to log and load the Workflow for inference, as well as enable tracing for observability.
                </p>
            </a>
        </div>
        <div class="simple-card">
            <a href="notebooks/llama_index_quickstart.html">
                <div class="header">
                    Building Index with MLflow
                </div>
                <p>
                    Get started with MLflow and LlamaIndex by exploring the simplest possible index configuration of a VectorStoreIndex.
                </p>
            </a>
        </div>
    </article>
</section><div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="concepts">
<h2>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline"> </a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Workflow integration is only available in LlamaIndex &gt;= 0.11.0 and MLflow &gt;= 2.17.0.</p>
</div>
<div class="section" id="workflow">
<h3><code class="docutils literal notranslate"><span class="pre">Workflow</span></code> 🆕<a class="headerlink" href="#workflow" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Workflow</span></code> is LlamaIndex’s event-driven orchestration framework. It is designed
as a flexible and interpretable framework for building arbitrary LLM applications such as an agent, a RAG flow, a data extraction pipeline, etc.
MLflow supports tracking, evaluating, and tracing the <code class="docutils literal notranslate"><span class="pre">Workflow</span></code> objects, which makes them more observable and maintainable.</p>
</div>
<div class="section" id="index">
<h3><code class="docutils literal notranslate"><span class="pre">Index</span></code><a class="headerlink" href="#index" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Index</span></code> object is a collection of documents that are indexed for fast information retrieval, providing capabilities for applications such as Retrieval-Augmented Generation (RAG) and Agents. The <code class="docutils literal notranslate"><span class="pre">Index</span></code> object can be logged directly to an MLflow run and loaded back for use as an inference engine.</p>
</div>
<div class="section" id="engine">
<h3><code class="docutils literal notranslate"><span class="pre">Engine</span></code><a class="headerlink" href="#engine" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Engine</span></code> is a generic interface built on top of the <code class="docutils literal notranslate"><span class="pre">Index</span></code> object, which provides a set of APIs to interact with the index. LlamaIndex provides two types of engines: <code class="docutils literal notranslate"><span class="pre">QueryEngine</span></code> and <code class="docutils literal notranslate"><span class="pre">ChatEngine</span></code>. The <code class="docutils literal notranslate"><span class="pre">QueryEngine</span></code> simply takes a single
query and returns a response based on the index. The <code class="docutils literal notranslate"><span class="pre">ChatEngine</span></code> is designed for conversational agents, which keeps track of the conversation history as well.</p>
</div>
<div class="section" id="settings">
<h3><code class="docutils literal notranslate"><span class="pre">Settings</span></code><a class="headerlink" href="#settings" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Settings</span></code> object is a global service context that bundles commonly used resources throughout the
LlamaIndex application. It includes settings such as the LLM model, embedding model, callbacks, and more. When logging a LlamaIndex index/engine/workflow, MLflow tracks
the state of the <code class="docutils literal notranslate"><span class="pre">Settings</span></code> object so that you can easily reproduce the same result when loading the model back for inference (note that some objects like API keys, non-serializable objects, etc., are not tracked).</p>
</div>
</div>
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline"> </a></h2>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="saving-and-loading-index-in-mlflow-experiment">
<h3>Saving and Loading Index in MLflow Experiment<a class="headerlink" href="#saving-and-loading-index-in-mlflow-experiment" title="Permalink to this headline"> </a></h3>
<div class="section" id="creating-an-index">
<h4>Creating an Index<a class="headerlink" href="#creating-an-index" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">index</span></code> object is the centerpiece of the LlamaIndex and MLflow integration. With LlamaIndex, you can create an index from a collection of documents or external vector stores. The following code creates a sample index from Paul Graham’s essay data available within the LlamaIndex repository.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>data
curl<span class="w"> </span>-L<span class="w"> </span>https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt<span class="w"> </span>-o<span class="w"> </span>./data/paul_graham_essay.txt
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">SimpleDirectoryReader</span>

<span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="logging-the-index-to-mlflow">
<h4>Logging the Index to MLflow<a class="headerlink" href="#logging-the-index-to-mlflow" title="Permalink to this headline"> </a></h4>
<p>You can log the <code class="docutils literal notranslate"><span class="pre">index</span></code> object to the MLflow experiment using the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.log_model" title="mlflow.llama_index.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.log_model()</span></code></a> function.</p>
<p>One key step here is to specify the <code class="docutils literal notranslate"><span class="pre">engine_type</span></code> parameter. The choice of engine type does not affect the index itself,
but dictates the interface of how you query the index when you load it back for inference.</p>
<ol class="arabic simple">
<li><p>QueryEngine (<code class="docutils literal notranslate"><span class="pre">engine_type=&quot;query&quot;</span></code>) is designed for a simple query-response system that takes a single query string and returns a response.</p></li>
<li><p>ChatEngine (<code class="docutils literal notranslate"><span class="pre">engine_type=&quot;chat&quot;</span></code>) is designed for a conversational agent that keeps track of the conversation history and responds to a user message.</p></li>
<li><p>Retriever (<code class="docutils literal notranslate"><span class="pre">engine_type=&quot;retriever&quot;</span></code>) is a lower-level component that returns the top-k relevant documents matching the query.</p></li>
</ol>
<p>The following code is an example of logging an index to MLflow with the <code class="docutils literal notranslate"><span class="pre">chat</span></code> engine type.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;llama-index-demo&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">index</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">,</span>
        <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;chat&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="s2">&quot;What did the author do growing up?&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above code snippet passes the index object directly to the <code class="docutils literal notranslate"><span class="pre">log_model</span></code> function.
This method only works with the default <code class="docutils literal notranslate"><span class="pre">SimpleVectorStore</span></code> vector store, which
simply keeps the embedded documents in memory. If your index uses <strong>external vector stores</strong> such as <code class="docutils literal notranslate"><span class="pre">QdrantVectorStore</span></code> or <code class="docutils literal notranslate"><span class="pre">DatabricksVectorSearch</span></code>, you can use the Model-from-Code
logging method. See the <a class="reference external" href="#how-to-log-and-load-an-index-with-external-vector-stores">How to log an index with external vector stores</a> for more details.</p>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/llama-index-artifacts.png"><img alt="MLflow artifacts for the LlamaIndex index" src="../../_images/llama-index-artifacts.png" style="width: 80%;" /></a>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Under the hood, MLflow calls <code class="docutils literal notranslate"><span class="pre">as_query_engine()</span></code> / <code class="docutils literal notranslate"><span class="pre">as_chat_engine()</span></code> / <code class="docutils literal notranslate"><span class="pre">as_retriever()</span></code> method on the index object to convert it to the respective engine instance.</p>
</div>
</div>
<div class="section" id="loading-the-index-back-for-inference">
<h4>Loading the Index Back for inference<a class="headerlink" href="#loading-the-index-back-for-inference" title="Permalink to this headline"> </a></h4>
<p>The saved index can be loaded back for inference using the <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> function. This function
gives an MLflow Python Model backed by the LlamaIndex engine, with the engine type specified during logging.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">&quot;What was the first program the author wrote?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="c1"># &gt;&gt; The first program the author wrote was on the IBM 1401 ...</span>

<span class="c1"># The chat engine keeps track of the conversation history</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">&quot;How did the author feel about it?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="c1"># &gt;&gt; The author felt puzzled by the first program ...</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To load the index itself back instead of the engine, use the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.load_model" title="mlflow.llama_index.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.load_model()</span></code></a> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;runs:/&lt;run_id&gt;/index&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="enable-tracing">
<h3>Enable Tracing<a class="headerlink" href="#enable-tracing" title="Permalink to this headline"> </a></h3>
<p>You can enable tracing for your LlamaIndex code by calling the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.autolog" title="mlflow.llama_index.autolog"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.autolog()</span></code></a> function. MLflow automatically logs the input and output of the LlamaIndex execution to the active MLflow experiment, providing you with a detailed view of the model’s behavior.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">autolog</span><span class="p">()</span>

<span class="n">chat_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_chat_engine</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat_engine</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="s2">&quot;What was the first program the author wrote?&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Then you can navigate to the MLflow UI, select the experiment, and open the “Traces” tab to find the logged trace for the prediction made by the engine. It is impressive to see how the chat engine coordinates and executes a number of tasks to answer your question!</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/llama-index-trace.png"><img alt="Trace view in MLflow UI" src="../../_images/llama-index-trace.png" style="width: 80%;" /></a>
</div>
<p>You can disable tracing by running the same function with the <code class="docutils literal notranslate"><span class="pre">disable</span></code> parameter set to <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span><span class="n">disable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The tracing supports async prediction and streaming response, however, it does not
support the combination of async and streaming, such as the <code class="docutils literal notranslate"><span class="pre">astream_chat</span></code> method.</p>
</div>
</div>
</div>
<div class="section" id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline"> </a></h2>
<div class="section" id="how-to-log-and-load-an-index-with-external-vector-stores">
<h3>How to log and load an index with external vector stores?<a class="headerlink" href="#how-to-log-and-load-an-index-with-external-vector-stores" title="Permalink to this headline"> </a></h3>
<p>If your index uses the default <code class="docutils literal notranslate"><span class="pre">SimpleVectorStore</span></code>, you can log the index directly to MLflow using the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.log_model" title="mlflow.llama_index.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.log_model()</span></code></a> function. MLflow persists the in-memory index data (embedded documents) to MLflow artifact store, which allows loading the index back with the same data without re-indexing the documents.</p>
<p>However, when the index uses external vector stores like <code class="docutils literal notranslate"><span class="pre">DatabricksVectorSearch</span></code> and <code class="docutils literal notranslate"><span class="pre">QdrantVectorStore</span></code>, the index data is stored remotely and they do not support local serialization. Thereby, you cannot log the index with these stores directly. For such cases, you can use the <a class="reference external" href="../../../models.html#models-from-code">Model-from-Code</a> logging that provides more control over the index saving process and allow you to use the external vector store.</p>
<p>To use model-from-code logging, you first need to create a separate Python file that defines the index. If you are on Jupyter notebook, you can use the <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> magic command to save the cell code to a Python file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">index</span><span class="o">.</span><span class="n">py</span>

<span class="c1"># Create Qdrant client with your own settings.</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">qdrant_client</span><span class="o">.</span><span class="n">QdrantClient</span><span class="p">(</span>
    <span class="n">host</span><span class="o">=</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span>
    <span class="n">port</span><span class="o">=</span><span class="mi">6333</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Here we simply load vector store from the existing collection to avoid</span>
<span class="c1"># re-indexing documents, because this Python file is executed every time</span>
<span class="c1"># when the model is loaded. If you don&#39;t have an existing collection, create</span>
<span class="c1"># a new one by following the official tutorial:</span>
<span class="c1"># https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">QdrantVectorStore</span><span class="p">(</span><span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span> <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;my_collection&quot;</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_vector_store</span><span class="p">(</span><span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span><span class="p">)</span>

<span class="c1"># IMPORTANT: call set_model() method to tell MLflow to log this index</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
<p>Then you can log the index by passing the Python file path to the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.log_model" title="mlflow.llama_index.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.log_model()</span></code></a> function. The global <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/">Settings</a> object is saved normally as part of the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="s2">&quot;index.py&quot;</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">,</span>
        <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;query&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The logged index can be loaded back using the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.load_model" title="mlflow.llama_index.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.load_model()</span></code></a> or <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> function, in the same way as with the local index.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The object that is passed to the <code class="docutils literal notranslate"><span class="pre">set_model()</span></code> method must be a LlamaIndex index that is compatible with the engine type specified during logging. More
objects support will be added in the future releases.</p>
</div>
</div>
<div class="section" id="how-to-log-and-load-a-llamaindex-workflow">
<h3>How to log and load a LlamaIndex Workflow?<a class="headerlink" href="#how-to-log-and-load-a-llamaindex-workflow" title="Permalink to this headline"> </a></h3>
<p>Mlflow supports logging and loading a LlamaIndex Workflow via the <a class="reference external" href="../../../models.html#models-from-code">Model-from-Code</a> feature. For a detailed example of logging and loading a LlamaIndex Workflow, see the <a class="reference external" href="notebooks/llama_index_workflow_tutorial.html">LlamaIndex Workflows with MLflow</a> notebook.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="s2">&quot;/path/to/workflow.py&quot;</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">},</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The logged workflow can be loaded back using the <a class="reference internal" href="../../python_api/mlflow.llama_index.html#mlflow.llama_index.load_model" title="mlflow.llama_index.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.llama_index.load_model()</span></code></a> or <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use mlflow.llama_index.load_model to load the workflow object as is</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="k">await</span> <span class="n">workflow</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">)</span>

<span class="c1"># Use mlflow.pyfunc.load_model to load the workflow as a MLflow Pyfunc Model</span>
<span class="c1"># with standard inference APIs for deployment and evaluation.</span>
<span class="n">pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The MLflow PyFunc Model does not support async inference. When you load the workflow with <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>, the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method becomes <strong>synchronous</strong> and will block until the workflow execution is completed. This also applies when deploying the logged LlamaIndex workflow to a production endpoint using <a class="reference external" href="https://mlflow.org/docs/latest/deployment/index.html">MLflow Deployment</a> or Databricks <a class="reference external" href="https://docs.databricks.com/en/machine-learning/model-serving/index.html">Model Serving</a>.</p>
</div>
</div>
<div class="section" id="i-have-an-index-logged-with-query-engine-type-can-i-load-it-back-a-chat-engine">
<h3>I have an index logged with <code class="docutils literal notranslate"><span class="pre">query</span></code> engine type. Can I load it back a <code class="docutils literal notranslate"><span class="pre">chat</span></code> engine?<a class="headerlink" href="#i-have-an-index-logged-with-query-engine-type-can-i-load-it-back-a-chat-engine" title="Permalink to this headline"> </a></h3>
<p>While it is not possible to update the engine type of the logged model in-place,
you can always load the index back and re-log it with the desired engine type. This process
does <strong>not require re-creating the index</strong>, so it is an efficient way to switch between
different engine types.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Log the index with the query engine type first</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">index</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;index-query&quot;</span><span class="p">,</span>
        <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;query&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load the index back and re-log it with the chat engine type</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">index</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;index-chat&quot;</span><span class="p">,</span>
        <span class="c1"># Specify the chat engine type this time</span>
        <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;chat&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can leverage their standard inference APIs on the loaded LlamaIndex native index object, specifically:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">index.as_chat_engine().chat(&quot;hi&quot;)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">index.as_query_engine().query(&quot;hi&quot;)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">index.as_retriever().retrieve(&quot;hi&quot;)</span></code></p></li>
</ul>
</div>
<div class="section" id="how-to-use-different-llms-for-inference-with-the-loaded-engine">
<h3>How to use different LLMs for inference with the loaded engine?<a class="headerlink" href="#how-to-use-different-llms-for-inference-with-the-loaded-engine" title="Permalink to this headline"> </a></h3>
<p>When saving the index to MLflow, it persists the global <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/">Settings</a> object as a part of the model. This object contains settings such as LLM and embedding
models to be used by engines.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">Settings</span>
<span class="kn">from</span> <span class="nn">llama_index.llms.openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">Settings</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>

<span class="c1"># MLflow saves GPT-4o-Mini as the LLM to use for inference</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">index</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;chat&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Then later when you load the index back, the persisted settings are also applied globally. This means that the loaded engine will use the same LLM as when it was logged.</p>
<p>However, sometimes you may want to use a different LLM for inference. In such cases, you can update the global <code class="docutils literal notranslate"><span class="pre">Settings</span></code> object directly after loading the index.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Load the index back</span>
<span class="n">loaded_index</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">llama_index</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">Settings</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;gpt-4o-mini&quot;</span>


<span class="c1"># Update the settings to use GPT-4 instead</span>
<span class="n">Settings</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">)</span>
<span class="n">query_engine</span> <span class="o">=</span> <span class="n">loaded_index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../langchain/guide/index.html" class="btn btn-neutral" title="LangChain within MLflow (Experimental)" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="notebooks/llama_index_workflow_tutorial.html" class="btn btn-neutral" title="Building a Tool-calling Agent with LlamaIndex Workflow and MLflow" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>