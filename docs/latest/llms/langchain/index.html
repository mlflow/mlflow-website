

<!DOCTYPE html>
<!-- source: docs/source/llms/langchain/index.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MLflow LangChain Flavor</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/langchain/index.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.1.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="LLMs" href="../index.html"/>
        <link rel="next" title="MLflow Langchain Autologging" href="/autologging.html"/>
        <link rel="prev" title="Sentence Transformers within MLflow" href="/../sentence-transformers/guide/index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/tabs.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-genai-applications-in-mlflow">Tutorials and Use Case Guides for GenAI applications in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow AI Gateway for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../transformers/index.html">MLflow Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">MLflow LangChain Flavor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#why-use-mlflow-with-langchain">Why use MLflow with LangChain?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#automatic-logging">Automatic Logging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#supported-elements-in-mlflow-langchain-integration">Supported Elements in MLflow LangChain Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#overview-of-chains-agents-and-retrievers">Overview of Chains, Agents, and Retrievers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-started-with-the-mlflow-langchain-flavor-tutorials-and-guides">Getting Started with the MLflow LangChain Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Detailed Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#faq">FAQ</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../llama-index/index.html">MLflow LlamaIndex Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dspy/index.html">MLflow DSPy Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>MLflow LangChain Flavor</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/langchain/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="mlflow-langchain-flavor">
<h1>MLflow LangChain Flavor<a class="headerlink" href="#mlflow-langchain-flavor" title="Permalink to this headline"> </a></h1>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor is under active development and is marked as Experimental. Public APIs are
subject to change, and new features may be added as the flavor evolves.</p>
</div>
<p>Welcome to the developer guide for the integration of <a class="reference external" href="https://www.langchain.com/">LangChain</a> with MLflow. This guide serves as a comprehensive
resource for understanding and leveraging the combined capabilities of LangChain and MLflow in developing advanced language model applications.</p>
<p><a class="reference external" href="https://www.langchain.com/">LangChain</a> is a versatile framework designed for building applications powered by language models. It excels in creating context-aware applications
that utilize language models for reasoning and generating responses, enabling the development of sophisticated NLP applications.</p>
<p><a class="reference external" href="https://langchain-ai.github.io/langgraph/">LangGraph</a> is a complementary agent-based framework from the creators of Langchain, supporting the creation of
stateful agent and multi-agent GenAI applications. LangGraph utilizes LangChain in order to interface with GenAI agent components.</p>
<div class="section" id="why-use-mlflow-with-langchain">
<h2>Why use MLflow with LangChain?<a class="headerlink" href="#why-use-mlflow-with-langchain" title="Permalink to this headline"> </a></h2>
<p>Aside from the benefits of using MLflow for managing and deploying machine learning models, the integration of LangChain with MLflow provides a number of
benefits that are associated with using LangChain within the broader MLflow ecosystem.</p>
<div class="section" id="experiment-tracking">
<h3>Experiment Tracking<a class="headerlink" href="#experiment-tracking" title="Permalink to this headline"> </a></h3>
<p>LangChain’s flexibility in experimenting with various agents, tools, and retrievers becomes even more powerful when paired with <a class="reference external" href="../../tracking.html">MLflow Tracking</a>. This combination allows for rapid experimentation and iteration. You can effortlessly compare runs, making it easier to refine models and accelerate the journey from development to production deployment.</p>
</div>
<div class="section" id="dependency-management">
<h3>Dependency Management<a class="headerlink" href="#dependency-management" title="Permalink to this headline"> </a></h3>
<p>Deploy your LangChain application with confidence, leveraging MLflow’s ability to <a class="reference external" href="../../model/dependencies.html">manage and record all external dependencies</a> automatically. This ensures consistency between development and production environments, reducing deployment risks with less manual intervention.</p>
</div>
<div class="section" id="mlflow-evaluate">
<h3>MLflow Evaluate<a class="headerlink" href="#mlflow-evaluate" title="Permalink to this headline"> </a></h3>
<p><a class="reference external" href="../llm-evaluate/index.html">MLflow Evaluate</a> provides native capabilities within MLflow to evaluate language models. With this feature you can easily utilize automated evaluation algorithms on the results of your LangChain application’s inference results. This capability facilitates the efficient assessment of inference results from your LangChain application, ensuring robust performance analytics.</p>
</div>
<div class="section" id="observability">
<h3>Observability<a class="headerlink" href="#observability" title="Permalink to this headline"> </a></h3>
<p><a class="reference external" href="../tracing/index.html">MLflow Tracing</a> is a new feature of MLflow that allows you to trace how data flows through your LangChain chain/agents/etc. This feature provides a visual representation of the data flow, making it easier to understand the behavior of your LangChain application and identify potential bottlenecks or issues. With its powerful <a class="reference external" href="../tracing/index.html#automatic-tracing">Automatic Tracing</a> capability, you can instrument your LangChain application without any code change but just running <code class="docutils literal notranslate"><span class="pre">mlflow.langchain.autolog()</span></code> command once.</p>
</div>
</div>
<div class="section" id="automatic-logging">
<h2>Automatic Logging<a class="headerlink" href="#automatic-logging" title="Permalink to this headline"> </a></h2>
<p>Autologging is a powerful one stop solution to achieve all the above benefits with just one line of code <code class="docutils literal notranslate"><span class="pre">mlflow.langchain.autolog()</span></code>. By enabling autologging, you can automatically log all the components of your LangChain application, including chains, agents, and retrievers, with minimal effort. This feature simplifies the process of tracking and managing your LangChain application, allowing you to focus on developing and improving your models. For more information on how to use this feature, refer to the <a class="reference external" href="autologging.html">MLflow LangChain Autologging Documentation</a>.</p>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="supported-elements-in-mlflow-langchain-integration">
<h2>Supported Elements in MLflow LangChain Integration<a class="headerlink" href="#supported-elements-in-mlflow-langchain-integration" title="Permalink to this headline"> </a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://python.langchain.com/docs/modules/agents/">Agents</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/modules/data_connection/retrievers/">Retrievers</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/v0.1/docs/expression_language/interface/">Runnables</a></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/reference/graphs/">LangGraph Complied Graph</a> (only supported via <a class="reference external" href="#logging-models-from-code">Model-from-Code</a>)</p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/modules/chains/foundational/llm_chain">LLMChain</a> (deprecated, only support for <code class="docutils literal notranslate"><span class="pre">langchain&lt;0.3.0</span></code>)</p></li>
<li><p><a class="reference external" href="https://js.langchain.com/docs/modules/chains/popular/vector_db_qa">RetrievalQA</a> (deprecated, only support for <code class="docutils literal notranslate"><span class="pre">langchain&lt;0.3.0</span></code>)</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>There is a known deserialization issue when logging chains or agents dependent upon LangChain components from <a class="reference external" href="https://python.langchain.com/v0.1/docs/integrations/platforms/#partner-packages">the partner packages</a> such as <code class="docutils literal notranslate"><span class="pre">langchain-openai</span></code>. If you log such models using the legacy serialization based logging, some components may be loaded from the respective <code class="docutils literal notranslate"><span class="pre">langchain-community</span></code> package instead of the partner package library, which can lead to unexpected behavior or import errors when executing your code.
To avoid this issue, we strongly recommend using the <a class="reference external" href="#logging-models-from-code">Model-from-Code</a> method for logging such models. This method allows you to bypass the model serialization and robustly save the model definition.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Logging chains/agents that include <a class="reference external" href="https://python.langchain.com/docs/integrations/chat/openai">ChatOpenAI</a> and <a class="reference external" href="https://python.langchain.com/docs/integrations/chat/azure_chat_openai">AzureChatOpenAI</a> requires <code class="docutils literal notranslate"><span class="pre">MLflow&gt;=2.12.0</span></code> and <code class="docutils literal notranslate"><span class="pre">LangChain&gt;=0.0.307</span></code>.</p>
</div>
</div>
<div class="section" id="overview-of-chains-agents-and-retrievers">
<h2>Overview of Chains, Agents, and Retrievers<a class="headerlink" href="#overview-of-chains-agents-and-retrievers" title="Permalink to this headline"> </a></h2>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Chain</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Agents</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Retrievers</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>Sequences of actions or steps hardcoded in code. Chains in LangChain combine various components like prompts, models, and output parsers to create a flow of processing steps.</p>
<p>The figure below shows an example of interfacing directly with a SaaS LLM via API calls with no context to the history of the conversation in the top portion. The
bottom portion shows the same queries being submitted to a LangChain chain that incorporates a conversation history state such that the entire conversation’s history
is included with each subsequent input. Preserving conversational context in this manner is key to creating a “chat bot”.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/stateful-chains.png"><img alt="The importance of stateful storage of conversation history for chat applications" src="../../_images/stateful-chains.png" style="width: 70%;" /></a>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>Dynamic constructs that use language models to choose a sequence of actions. Unlike chains, agents decide the order of actions based on inputs, tools available, and intermediate outcomes.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/langchain-agents.png"><img alt="Complex LLM queries with LangChain agents" src="../../_images/langchain-agents.png" style="width: 80%;" /></a>
</div>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>Components in RetrievalQA chains responsible for sourcing relevant documents or data. Retrievers are key in applications where LLMs need to reference specific external information for accurate responses.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/langchain-retrievalqa.png"><img alt="MLflow LangChain RetrievalQA architecture" src="../../_images/langchain-retrievalqa.png" style="width: 70%;" /></a>
</div>
</div></div>
</div>
<div class="section" id="getting-started-with-the-mlflow-langchain-flavor-tutorials-and-guides">
<h2>Getting Started with the MLflow LangChain Flavor - Tutorials and Guides<a class="headerlink" href="#getting-started-with-the-mlflow-langchain-flavor-tutorials-and-guides" title="Permalink to this headline"> </a></h2>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="introductory-tutorial">
<h3>Introductory Tutorial<a class="headerlink" href="#introductory-tutorial" title="Permalink to this headline"> </a></h3>
<p>In this introductory tutorial, you will learn the most fundamental components of LangChain and how to leverage the integration with MLflow to store, retrieve, and
use a chain.</p>
<section>
    <article class="simple-grid">
        <div class="simple-card">
            <a href="notebooks/langchain-quickstart.html">
                <div class="header">
                    LangChain Quickstart
                </div>
                <p>
                    Get started with MLflow and LangChain by exploring the simplest possible chain configuration of a prompt and model chained to create
                    a single-purpose utility application.
                </p>
            </a>
        </div>
    </article>
</section></div>
<div class="section" id="advanced-tutorials">
<h3>Advanced Tutorials<a class="headerlink" href="#advanced-tutorials" title="Permalink to this headline"> </a></h3>
<p>In these tutorials, you can learn about more complex usages of LangChain with MLflow. It is highly advised to read through the introductory tutorial prior to
exploring these more advanced use cases.</p>
<section>
    <article class="simple-grid">
        <div class="simple-card">
            <a href="notebooks/langchain-retriever.html">
                <div class="header">
                    RAG tutorial with LangChain
                </div>
                <p>
                    Learn how to build a LangChain RAG with MLflow integration to answer highly specific questions about the legality of business ventures.
                </p>
            </a>
        </div>
    </article>
</section></div>
<div class="section" id="logging-models-from-code">
<h3>Logging models from Code<a class="headerlink" href="#logging-models-from-code" title="Permalink to this headline"> </a></h3>
<p>Since MLflow 2.12.2, MLflow introduced the ability to log LangChain models directly from a code definition.</p>
<p>The feature provides several benefits to manage LangChain models:</p>
<ol class="arabic simple">
<li><p><strong>Avoid Serialization Complication</strong>: File handles, sockets, external connections, dynamic references, lambda functions and system resources are unpicklable. Some LangChain components do not support native serialization, e.g. <code class="docutils literal notranslate"><span class="pre">RunnableLambda</span></code>.</p></li>
<li><p><strong>No Pickling</strong>: Loading a pickle or cloudpickle file in a Python version that was different than the one used to serialize the object does not guarantee compatibility.</p></li>
<li><p><strong>Readability</strong>: The serialized objects are often hardly readable by humans. Model-from-code allows you to review your model definition via code.</p></li>
</ol>
<p>Refer to the <a class="reference external" href="../../models.html#models-from-code">Models From Code feature documentation</a> for more information about this feature.</p>
<p>In order to use this feature, you will utilize the <a class="reference internal" href="../../python_api/mlflow.models.html#mlflow.models.set_model" title="mlflow.models.set_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.set_model()</span></code></a> API to define the chain that you would like to log as an MLflow model.
After having this set within your code that defines your chain, when logging your model, you will specify the <strong>path</strong> to the file that defines your chain.</p>
<p>The following example demonstrates how to log a simple chain with this method:</p>
<ol class="arabic">
<li><p>Define the chain in a separate Python file.**</p>
<blockquote>
<div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you are using Jupyter Notebook, you can use the <cite>%%writefile</cite> magic command to write the code cell directly to a file, without leaving the notebook to create it manually.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">chain</span><span class="o">.</span><span class="n">py</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;Homework Helper&quot;</span><span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">autolog</span><span class="p">()</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;You are a helpful tutor that evaluates my homework assignments and provides suggestions on areas for me to study further.&quot;</span>
    <span class="s2">&quot; Here is the question: </span><span class="si">{question}</span><span class="s2"> and my answer which I got wrong: </span><span class="si">{answer}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="s2">&quot;answer&quot;</span><span class="p">],</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">get_question</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">default</span> <span class="o">=</span> <span class="s2">&quot;What is your name?&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default</span>


<span class="k">def</span> <span class="nf">get_answer</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">default</span> <span class="o">=</span> <span class="s2">&quot;My name is Bobo&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">get_question</span><span class="p">),</span>
        <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">get_answer</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">model</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Then from the main notebook, log the model via supplying the path to the file that defines the chain:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">chain_path</span> <span class="o">=</span> <span class="s2">&quot;chain.py&quot;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">lc_model</span><span class="o">=</span><span class="n">chain_path</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;chain&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>The model defined in <code class="docutils literal notranslate"><span class="pre">chain.py</span></code> is now logged to MLflow. You can load the model back and run inference:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the model and run inference</span>
<span class="n">homework_chain</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">exam_question</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the primary function of control rods in a nuclear reactor?&quot;</span><span class="p">,</span>
                <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;To stir the primary coolant so that the neutrons are mixed well.&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">homework_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">exam_question</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>You can see the model is logged as a code on MLflow UI:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/langchain-code-model.png"><img alt="Logging a LangChain model from a code script file" src="../../_images/langchain-code-model.png" style="width: 100%;" /></a>
</div>
</div></blockquote>
</li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When logging models from code, make sure that your code does not contain any sensitive information, such as API keys, passwords, or other confidential data. The code will be stored in plain text in the MLflow model artifact, and anyone with access to the artifact will be able to view the code.</p>
</div>
</div>
</div>
<div class="section" id="id4">
<h2><a class="reference external" href="guide/index.html">Detailed Documentation</a><a class="headerlink" href="#id4" title="Permalink to this headline"> </a></h2>
<p>To learn more about the details of the MLflow LangChain flavor, read the detailed guide below.</p>
<a href="guide/index.html" class="download-btn">View the Comprehensive Guide</a><div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline"> </a></h2>
<div class="section" id="i-can-t-load-my-chain">
<h3>I can’t load my chain!<a class="headerlink" href="#i-can-t-load-my-chain" title="Permalink to this headline"> </a></h3>
<ul>
<li><p><strong>Allowing for Dangerous Deserialization</strong>: Pickle opt-in logic in LangChain will prevent components from being loaded via MLflow. You might see an error like this:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ValueError: This code relies on the pickle module. You will need to set allow_dangerous_deserialization=True if you want to opt-in to
allow deserialization of data using pickle. Data can be compromised by a malicious actor if not handled properly to include a malicious
payload that when deserialized with pickle can execute arbitrary code on your machine.
</pre></div>
</div>
<p>A change within LangChain that <a class="reference external" href="https://github.com/langchain-ai/langchain/pull/18696">forces users to opt-in to pickle deserialization</a> can create
some issues with loading chains, vector stores, retrievers, and agents that have been logged using MLflow. Because the option is not exposed per component
to set this argument on the loader function, you will need to ensure that you are setting this option directly within the defined loader function when
logging the model. LangChain components that do not set this value will be saved without issue, but a <code class="docutils literal notranslate"><span class="pre">ValueError</span></code> will be raised when loading if unset.</p>
<p>To fix this, simply re-log your model, specifying the option <code class="docutils literal notranslate"><span class="pre">allow_dangerous_deserialization=True</span></code> in your defined loader function. See the tutorial
<a class="reference external" href="notebooks/langchain-retriever.html#Establishing-RetrievalQA-Chain-and-Logging-with-MLflow">for LangChain retrievers</a> for an example of specifying this
option when logging a <code class="docutils literal notranslate"><span class="pre">FAISS</span></code> vector store instance within a <code class="docutils literal notranslate"><span class="pre">loader_fn</span></code> declaration.</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="i-can-t-save-my-chain-agent-or-retriever-with-mlflow">
<h3>I can’t save my chain, agent, or retriever with MLflow.<a class="headerlink" href="#i-can-t-save-my-chain-agent-or-retriever-with-mlflow" title="Permalink to this headline"> </a></h3>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you’re encountering issues with logging or saving LangChain components with MLflow, see the <a class="reference external" href="../../models.html#models-from-code">models from code</a>
feature documentation to determine if logging your model from a script file provides a simpler and more robust logging solution!</p>
</div>
<ul>
<li><p><strong>Serialization Challenges with Cloudpickle</strong>: Serialization with cloudpickle can encounter limitations depending on the complexity of the objects.</p>
<blockquote>
<div><p>Some objects, especially those with intricate internal states or dependencies on external system resources, are not inherently pickleable. This limitation
arises because serialization essentially requires converting an object to a byte stream, which can be complex for objects tightly coupled with system states
or those having external I/O operations. Try upgrading PyDantic to 2.x version to resolve this issue.</p>
</div></blockquote>
</li>
<li><p><strong>Verifying Native Serialization Support</strong>: Ensure that the langchain object (chain, agent, or retriever) is serializable natively using langchain APIs if saving or logging with MLflow doesn’t work.</p>
<blockquote>
<div><p>Due to their complex structures, not all langchain components are readily serializable. If native serialization
is not supported and MLflow doesn’t support saving the model, you can file an issue <a class="reference external" href="https://github.com/langchain-ai/langchain/issues">in the LangChain repository</a> or
ask for guidance in the <a class="reference external" href="https://github.com/langchain-ai/langchain/discussions">LangChain Discussions board</a>.</p>
</div></blockquote>
</li>
<li><p><strong>Keeping Up with New Features in MLflow</strong>: MLflow might not immediately support the latest LangChain features immediately.</p>
<blockquote>
<div><p>If a new feature is not supported in MLflow, consider <a class="reference external" href="https://github.com/mlflow/mlflow/issues">filing a feature request on the MLflow GitHub issues page</a>.
With the rapid pace of changes in libraries that are in heavy active development (such as <a class="reference external" href="https://pypi.org/project/langchain/#history">LangChain’s release velocity</a>),
breaking changes, API refactoring, and fundamental functionality support for even existing features can cause integration issues. If there is a chain, agent,
retriever, or any future structure within LangChain that you’d like to see supported, please let us know!</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="i-m-getting-an-attributeerror-when-saving-my-model">
<h3>I’m getting an AttributeError when saving my model<a class="headerlink" href="#i-m-getting-an-attributeerror-when-saving-my-model" title="Permalink to this headline"> </a></h3>
<ul>
<li><p><strong>Handling Dependency Installation in LangChain and MLflow</strong>: LangChain and MLflow do not automatically install all dependencies.</p>
<blockquote>
<div><p>Other packages that might be required for specific agents, retrievers, or tools may need to be explicitly defined when saving or logging your model.
If your model relies on these external component libraries (particularly for tools) that not included in the standard LangChain package, these dependencies
will not be automatically logged as part of the model at all times (see below for guidance on how to include them).</p>
</div></blockquote>
</li>
<li><p><strong>Declaring Extra Dependencies</strong>: Use the <code class="docutils literal notranslate"><span class="pre">extra_pip_requirements</span></code> parameter when saving and logging.</p>
<blockquote>
<div><p>When saving or logging your model that contains external dependencies that are not part of the core langchain installation, you will need these additional
dependencies. The model flavor contains two options for declaring these dependencies: <code class="docutils literal notranslate"><span class="pre">extra_pip_requirements</span></code> and <code class="docutils literal notranslate"><span class="pre">pip_requirements</span></code>. While specifying
<code class="docutils literal notranslate"><span class="pre">pip_requirements</span></code> is entirely valid, we recommend using <code class="docutils literal notranslate"><span class="pre">extra_pip_requirements</span></code> as it does not rely on defining all of the core dependent packages that
are required to use the langchain model for inference (the other core dependencies will be inferred automatically).</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="how-can-i-use-a-streaming-api-with-langchain">
<h3>How can I use a streaming API with LangChain?<a class="headerlink" href="#how-can-i-use-a-streaming-api-with-langchain" title="Permalink to this headline"> </a></h3>
<ul>
<li><p><strong>Streaming with LangChain Models</strong>: Ensure that the LangChain model supports a streaming response and use an MLflow version &gt;= 2.12.2.</p>
<blockquote>
<div><p>As of the MLflow 2.12.2 release, LangChain models that support streaming responses that have been saved using MLflow 2.12.2 (or higher) can be loaded and used for
streamable inference using the <code class="docutils literal notranslate"><span class="pre">predict_stream</span></code> API. Ensure that you are consuming the return type correctly, as the return from these models is a <code class="docutils literal notranslate"><span class="pre">Generator</span></code> object.
To learn more, refer to the <a class="reference external" href="https://mlflow.org/docs/latest/models.html#how-to-load-and-score-python-function-models">predict_stream guide</a>.</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="how-can-i-log-an-agent-built-with-langgraph-to-mlflow">
<h3>How can I log an agent built with LangGraph to MLflow?<a class="headerlink" href="#how-can-i-log-an-agent-built-with-langgraph-to-mlflow" title="Permalink to this headline"> </a></h3>
<p>The LangGraph integration with MLflow is designed to utilize the <a class="reference external" href="../../model/models-from-code.html">Models From Code feature</a>
in MLflow to broaden and simplify the support of agent serialization.</p>
<p>To log a LangGraph agent, you can define your agent code within a script, as shown below, saved to a file <code class="docutils literal notranslate"><span class="pre">langgraph.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">create_react_agent</span>

<span class="kn">import</span> <span class="nn">mlflow</span>


<span class="nd">@tool</span>
<span class="k">def</span> <span class="nf">get_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;seattle&quot;</span><span class="p">,</span> <span class="s2">&quot;sf&quot;</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this to get weather information.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">city</span> <span class="o">==</span> <span class="s2">&quot;seattle&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;It&#39;s probably raining. Again.&quot;</span>
    <span class="k">elif</span> <span class="n">city</span> <span class="o">==</span> <span class="s2">&quot;sf&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;It&#39;s always sunny in sf&quot;</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">()</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_weather</span><span class="p">]</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">create_react_agent</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">tools</span><span class="p">)</span>

<span class="c1"># specify the Agent as the model interface to be loaded when executing the script</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<p>When you’re ready to log this agent script definition to MLflow, you can refer to
this saved script directly when defining the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">input_example</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;what is the weather in seattle today?&quot;</span><span class="p">}]</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">lc_model</span><span class="o">=</span><span class="s2">&quot;./langgraph.py&quot;</span><span class="p">,</span>  <span class="c1"># specify the path to the LangGraph agent script definition</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;langgraph&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>When the agent is loaded from MLflow, the script will be executed and the defined agent will be
made available for use for invocation.</p>
<p>The agent can be loaded and used for inference as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">query</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Should I bring an umbrella today when I go to work in San Francisco?&quot;</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
<span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="how-can-i-evaluate-a-langgraph-agent">
<h3>How can I evaluate a LangGraph Agent?<a class="headerlink" href="#how-can-i-evaluate-a-langgraph-agent" title="Permalink to this headline"> </a></h3>
<p>The <a class="reference external" href="https://mlflow.org/docs/latest/model-evaluation/index.html">mlflow.evaluate</a> function provides
a robust way to evaluate model performance.</p>
<p>LangGraph agents, especially those with chat functionality, can return multiple messages in one
inference call. Given <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate</span></code> performs naive comparisons between raw predictions and a specified
ground truth value, it is the user’s responsibility to reconcile potential differences prediction output
and ground truth.</p>
<p>Often, the best approach is to use a <a class="reference external" href="https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#evaluating-with-a-custom-function">custom function</a>
to process the response. Below we provide an example of a custom function that extracts the last chat
message from a LangGraph model. This function is then used in mlflow.evaluate to return a single
string response, which can be compared to the <cite>“ground_truth”</cite> column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="c1"># Note that we assume the `model_uri` variable is present</span>
<span class="c1"># Also note that registering and loading the model is optional and you</span>
<span class="c1"># can simply leverage your langgraph object in the custom function.</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">custom_langgraph_wrapper</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract the predictions from a chat message sequence.&quot;&quot;&quot;</span>
    <span class="n">answers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]:</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">content</span><span class="p">}]}</span>
        <span class="p">)</span>
        <span class="n">last_message_content</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_message_content</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">answers</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">custom_langgraph_wrapper</span><span class="p">,</span>  <span class="c1"># Pass our function defined above</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">latency</span><span class="p">(),</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_correctness</span><span class="p">(</span><span class="s2">&quot;openai:/gpt-4o&quot;</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id6" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;latency/mean&quot;</span><span class="p">:</span> <span class="mf">1.8976624011993408</span><span class="p">,</span>
    <span class="s2">&quot;latency/variance&quot;</span><span class="p">:</span> <span class="mf">0.10328687906900313</span><span class="p">,</span>
    <span class="s2">&quot;latency/p90&quot;</span><span class="p">:</span> <span class="mf">2.1547686100006103</span><span class="p">,</span>
    <span class="s2">&quot;flesch_kincaid_grade_level/v1/mean&quot;</span><span class="p">:</span> <span class="mf">12.1</span><span class="p">,</span>
    <span class="s2">&quot;flesch_kincaid_grade_level/v1/variance&quot;</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="s2">&quot;flesch_kincaid_grade_level/v1/p90&quot;</span><span class="p">:</span> <span class="mf">12.5</span><span class="p">,</span>
    <span class="s2">&quot;ari_grade_level/v1/mean&quot;</span><span class="p">:</span> <span class="mf">15.850000000000001</span><span class="p">,</span>
    <span class="s2">&quot;ari_grade_level/v1/variance&quot;</span><span class="p">:</span> <span class="mf">0.06250000000000044</span><span class="p">,</span>
    <span class="s2">&quot;ari_grade_level/v1/p90&quot;</span><span class="p">:</span> <span class="mf">16.05</span><span class="p">,</span>
    <span class="s2">&quot;exact_match/v1&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;answer_correctness/v1/mean&quot;</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="s2">&quot;answer_correctness/v1/variance&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;answer_correctness/v1/p90&quot;</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>For a complete example of a LangGraph model that works with this evaluation example, see the
<a class="reference external" href="https://mlflow.org/blog/langgraph-model-from-code">MLflow LangGraph blog</a>.</p>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../sentence-transformers/guide/index.html" class="btn btn-neutral" title="Sentence Transformers within MLflow" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="autologging.html" class="btn btn-neutral" title="MLflow Langchain Autologging" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>