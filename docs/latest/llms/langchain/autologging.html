
  

<!DOCTYPE html>
<!-- source: docs/source/llms/langchain/autologging.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MLflow Langchain Autologging &mdash; MLflow 2.14.2.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/langchain/autologging.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.14.2.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="MLflow LangChain Flavor" href="index.html"/>
        <link rel="next" title="Introduction to Using LangChain with MLflow" href="/notebooks/langchain-quickstart.html"/>
        <link rel="prev" title="MLflow LangChain Flavor" href="/index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.14.2.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../transformers/index.html">MLflow Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html">MLflow OpenAI Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">MLflow LangChain Flavor</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="index.html#why-use-mlflow-with-langchain">Why use MLflow with LangChain?</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="index.html#automatic-logging">Automatic Logging</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#supported-elements-in-mlflow-langchain-integration">Supported Elements in MLflow LangChain Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#overview-of-chains-agents-and-retrievers">Overview of Chains, Agents, and Retrievers</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#getting-started-with-the-mlflow-langchain-flavor-tutorials-and-guides">Getting Started with the MLflow LangChain Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#id3">Detailed Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#faq">FAQ</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="index.html">MLflow LangChain Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>MLflow Langchain Autologging</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/langchain/autologging.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="mlflow-langchain-autologging">
<h1>MLflow Langchain Autologging<a class="headerlink" href="#mlflow-langchain-autologging" title="Permalink to this headline"> </a></h1>
<p>MLflow LangChain flavor supports autologging, a powerful feature that allows you to log crucial details about the LangChain model and execution without the need for explicit logging statements. MLflow LangChain autologging covers various aspects of the model, including traces, models, signatures and more.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>MLflow’s LangChain Autologging feature has been overhauled in the <code class="docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">2.14.0</span></code> release. If you are using the earlier version of MLflow, please refer to the legacy documentation <a class="reference external" href="#documentation-for-old-versions">here</a> for applicable autologging documentation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MLflow LangChain Autologging is verified to be compatible with LangChain versions between 0.1.0 and 0.2.3. Outside of this range, the feature may not work as expected. To install the compatible version of LangChain, please run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">mlflow</span><span class="p">[</span><span class="n">langchain</span><span class="p">]</span> <span class="o">--</span><span class="n">upgrade</span>
</pre></div>
</div>
</div>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#quickstart" id="id4">Quickstart</a></p></li>
<li><p><a class="reference internal" href="#id1" id="id5">Configure Autologging</a></p></li>
<li><p><a class="reference internal" href="#example-code-of-langchain-autologging" id="id6">Example Code of LangChain Autologging</a></p></li>
<li><p><a class="reference internal" href="#how-it-works" id="id7">How It Works</a></p></li>
<li><p><a class="reference internal" href="#troubleshooting" id="id8">Troubleshooting</a></p></li>
<li><p><a class="reference internal" href="#documentation-for-old-versions" id="id9">Documentation for Old Versions</a></p></li>
</ul>
</div>
<div class="section" id="quickstart">
<h2><a class="toc-backref" href="#id4">Quickstart</a><a class="headerlink" href="#quickstart" title="Permalink to this headline"> </a></h2>
<p>To enable autologging for LangChain models, call <a class="reference internal" href="../../python_api/mlflow.langchain.html#mlflow.langchain.autolog" title="mlflow.langchain.autolog"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.autolog()</span></code></a> at the beginning of your script or notebook. This will automatically log the traces by default as well as other artifacts such as models, input examples, and model signatures if you explicitly enable them. For more information about the configuration, please refer to the <a class="reference external" href="#configure-autologging">Configure Autologging</a> section.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">autolog</span><span class="p">()</span>

<span class="c1"># Enable other optional logging</span>
<span class="c1"># mlflow.langchain.autolog(log_models=True, log_input_examples=True)</span>

<span class="c1"># Your LangChain model code here</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Once you have invoked the chain, you can view the logged traces and artifacts in the MLflow UI.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/langchain-tracing.gif"><img alt="LangChain Tracing via autolog" src="../../_images/langchain-tracing.gif" style="width: 100%;" /></a>
</div>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id5">Configure Autologging</a><a class="headerlink" href="#id1" title="Permalink to this headline"> </a></h2>
<p>MLflow LangChain autologging can log various information about the model and its inference. <strong>By default, only trace logging is enabled</strong>, but you can enable autologging of other information by setting the corresponding parameters when calling <a class="reference internal" href="../../python_api/mlflow.langchain.html#mlflow.langchain.autolog" title="mlflow.langchain.autolog"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.autolog()</span></code></a>. For other configurations, please refer to the API documentation.</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 30%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Target</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Traces</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_traces</span></code></p></td>
<td><p>Whether to generate and log traces for the model. See <a class="reference external" href="../tracing/index.html">MLflow Tracing</a> for more details about tracing feature.</p></td>
</tr>
<tr class="row-odd"><td><p>Model Artifacts</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_models</span></code></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the LangChain model will be logged when it is invoked. Supported models are <cite>Chain</cite>, <cite>AgentExecutor</cite>, <cite>BaseRetriever</cite>, <cite>SimpleChatModel</cite>, <cite>ChatPromptTemplate</cite>, and subset of <cite>Runnable</cite> types. Please refer to the <a class="reference external" href="https://github.com/mlflow/mlflow/blob/d2955cc90b6c5d7c931a8476b85f66e63990ca96/mlflow/langchain/utils.py#L183">MLflow repository</a> for the full list of supported models.</p></td>
</tr>
<tr class="row-even"><td><p>Model Signatures</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_model_signatures</span></code></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, <a class="reference internal" href="../../python_api/mlflow.models.html#mlflow.models.ModelSignature" title="mlflow.models.ModelSignature"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSignatures</span></code></a> describing model inputs and outputs are collected and logged along with Langchain model artifacts during inference. This option is only available when <code class="docutils literal notranslate"><span class="pre">log_models</span></code> is enabled.</p></td>
</tr>
<tr class="row-odd"><td><p>Input Example</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_input_examples</span></code></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, input examples from inference data are collected and logged along with LangChain model artifacts during inference. This option is only available when <code class="docutils literal notranslate"><span class="pre">log_models</span></code> is enabled.</p></td>
</tr>
<tr class="row-even"><td><p>Inputs and Outputs (<strong>Deprecated</strong>)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_inputs_outputs</span></code></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the inputs and outputs will be logged when the model is invoked. This feature is deprecated and will be removed in the future. Please use <code class="docutils literal notranslate"><span class="pre">log_traces</span></code> instead.</p></td>
</tr>
</tbody>
</table>
<p>For example, to disable logging of traces, and instead enable model logging, run the following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span>
    <span class="n">log_traces</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MLflow does not support automatic model logging for chains that contain retrievers. Saving retrievers requires additional <code class="docutils literal notranslate"><span class="pre">loader_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">persist_dir</span></code> information for loading the model. If you want to log the model with retrievers, please log the model manually as shown in the <a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/langchain/retriever_chain.py">retriever_chain</a> example.</p>
</div>
</div>
<div class="section" id="example-code-of-langchain-autologging">
<h2><a class="toc-backref" href="#id6">Example Code of LangChain Autologging</a><a class="headerlink" href="#example-code-of-langchain-autologging" title="Permalink to this headline"> </a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.schema.output_parser</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain.schema.runnable</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Uncomment the following to use the full abilities of langchain autologgin</span>
<span class="c1"># %pip install `langchain_community&gt;=0.0.16`</span>
<span class="c1"># These two libraries enable autologging to log text analysis related artifacts</span>
<span class="c1"># %pip install textstat spacy</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>

<span class="c1"># Enable mlflow langchain autologging</span>
<span class="c1"># Note: We only support auto-logging models that do not contain retrievers</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span>
    <span class="n">log_input_examples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">log_model_signatures</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">log_inputs_outputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="s2">&quot;lc_model&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">prompt_with_history_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Here is a history between you and a human: </span><span class="si">{chat_history}</span>

<span class="s2">Now, please answer this question: </span><span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">prompt_with_history</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;chat_history&quot;</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="n">prompt_with_history_str</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">extract_question</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">input</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">extract_history</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">input</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Build a chain with LCEL</span>
<span class="n">chain_with_history</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">extract_question</span><span class="p">),</span>
        <span class="s2">&quot;chat_history&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">extract_history</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="o">|</span> <span class="n">prompt_with_history</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Who owns MLflow?&quot;</span><span class="p">}]}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">chain_with_history</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
<span class="c1"># sample output:</span>
<span class="c1"># &quot;1. Databricks\n2. Microsoft\n3. Google\n4. Amazon\n\nEnter your answer: 1\n\n</span>
<span class="c1"># Correct! MLflow is an open source project developed by Databricks. ...</span>

<span class="c1"># We automatically log the model and trace related artifacts</span>
<span class="c1"># A model with name `lc_model` is registered, we can load it back as a PyFunc model</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;lc_model&quot;</span>
<span class="n">model_version</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;models:/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="how-it-works">
<h2><a class="toc-backref" href="#id7">How It Works</a><a class="headerlink" href="#how-it-works" title="Permalink to this headline"> </a></h2>
<p>MLflow LangChain Autologging uses two ways to log traces and other artifacts. Tracing is made possible via the <a class="reference external" href="https://python.langchain.com/v0.1/docs/modules/callbacks/">Callbacks</a> framework of LangChain. Other artifacts are recorded by patching <cite>the invocation functions</cite> of the supported models. In typical scenarios, you don’t need to care about the internal implementation details, but this section provides a brief overview of how it works under the hood.</p>
<div class="section" id="mlflow-tracing-callbacks">
<h3>MLflow Tracing Callbacks<a class="headerlink" href="#mlflow-tracing-callbacks" title="Permalink to this headline"> </a></h3>
<p><a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/mlflow/langchain/langchain_tracer.py">MlflowLangchainTracer</a> is a callback handler that is injected into the langchain model inference process to log traces automatically. It starts a new span upon a set of actions of the chain such as <code class="docutils literal notranslate"><span class="pre">on_chain_start</span></code>, <code class="docutils literal notranslate"><span class="pre">on_llm_start</span></code>, and concludes it when the action is finished. Various metadata such as span type, action name, input, output, latency, are automatically recorded to the span.</p>
</div>
<div class="section" id="customize-callback">
<h3>Customize Callback<a class="headerlink" href="#customize-callback" title="Permalink to this headline"> </a></h3>
<p>Sometimes you may want to customize what information is logged in the traces. You can achieve this by creating a custom callback handler that inherits from <a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/mlflow/langchain/langchain_tracer.py">MlflowLangchainTracer</a>. The following example demonstrates how to record an additional attribute to the span when a chat model starts running.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.langchain.langchain_tracer</span> <span class="kn">import</span> <span class="n">MlflowLangchainTracer</span>

<span class="k">class</span> <span class="nc">CustomLangchainTracer</span><span class="p">(</span><span class="n">MlflowLangchainTracer</span><span class="p">):</span>

    <span class="c1"># Override the handler functions to customize the behavior. The method signature is defined by LangChain Callbacks.</span>
    <span class="k">def</span> <span class="nf">on_chat_model_start</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">serialized</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">run_id</span><span class="p">:</span> <span class="n">UUID</span><span class="p">,</span>
        <span class="n">tags</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parent_run_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UUID</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run when a chat model starts running.&quot;&quot;&quot;</span>
        <span class="n">attributes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="o">**</span><span class="n">metadata</span><span class="p">,</span>
            <span class="c1"># Add additional attribute to the span</span>
            <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Call the _start_span method at the end of the handler function to start a new span.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_span</span><span class="p">(</span>
            <span class="n">span_name</span><span class="o">=</span><span class="n">name</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_span_name</span><span class="p">(</span><span class="n">serialized</span><span class="p">,</span> <span class="s2">&quot;chat model&quot;</span><span class="p">),</span>
            <span class="n">parent_run_id</span><span class="o">=</span><span class="n">parent_run_id</span><span class="p">,</span>
            <span class="n">span_type</span><span class="o">=</span><span class="n">SpanType</span><span class="o">.</span><span class="n">CHAT_MODEL</span><span class="p">,</span>
            <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">attributes</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="patch-functions-for-logging-artifacts">
<h3>Patch Functions for Logging Artifacts<a class="headerlink" href="#patch-functions-for-logging-artifacts" title="Permalink to this headline"> </a></h3>
<p>Other artifacts such as models are logged by patching the invocation functions of the supported models to insert the logging call. MLflow patches the following functions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">invoke</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stream</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_relevant_documents</span></code> (for retrievers)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__call__</span></code> (for Chains and AgentExecutors)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ainvoke</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">abatch</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">astream</span></code></p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>MLflow supports autologging for async functions (e.g., <code class="docutils literal notranslate"><span class="pre">ainvoke</span></code>, <code class="docutils literal notranslate"><span class="pre">abatch</span></code>, <code class="docutils literal notranslate"><span class="pre">astream</span></code>), however, the logging operation is not
asynchronous and may block the main thread. The invocation function itself is still not blocking and returns a coroutine object, but
the logging overhead may slow down the model inference process. Please be aware of this side effect when using async functions with autologging.</p>
</div>
</div>
</div>
<div class="section" id="troubleshooting">
<h2><a class="toc-backref" href="#id8">Troubleshooting</a><a class="headerlink" href="#troubleshooting" title="Permalink to this headline"> </a></h2>
<p>If you encounter any issues with MLflow LangChain flavor, please also refer to <cite>FAQ &lt;../index.html#faq&gt;</cite>. If you still have questions, please feel free to open an issue in <a class="reference external" href="https://github.com/mlflow/mlflow/issues">MLflow Github repo</a>.</p>
<div class="section" id="how-to-suppress-the-warning-messages-during-autologging">
<h3>How to suppress the warning messages during autologging?<a class="headerlink" href="#how-to-suppress-the-warning-messages-during-autologging" title="Permalink to this headline"> </a></h3>
<p>MLflow Langchain Autologging calls various logging functions and LangChain utilities under the hood. Some of them may
generate warning messages that are not critical to the autologging process. If you want to suppress these warning messages, pass <code class="docutils literal notranslate"><span class="pre">silent=True</span></code> to the <a class="reference internal" href="../../python_api/mlflow.langchain.html#mlflow.langchain.autolog" title="mlflow.langchain.autolog"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.autolog()</span></code></a> function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span><span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># No warning messages will be emitted from autologging</span>
</pre></div>
</div>
</div>
<div class="section" id="i-can-t-load-the-model-logged-by-mlflow-langchain-autologging">
<h3>I can’t load the model logged by mlflow langchain autologging<a class="headerlink" href="#i-can-t-load-the-model-logged-by-mlflow-langchain-autologging" title="Permalink to this headline"> </a></h3>
<p>There are a few type of models that MLflow LangChain autologging does not support native saving or loading.</p>
<ul>
<li><p><strong>Model contains langchain retrievers</strong></p>
<blockquote>
<div><p>LangChain retrievers are not supported by MLflow autologging. If your model contains a retriever, you will need to manually log the model using the <code class="docutils literal notranslate"><span class="pre">mlflow.langchain.log_model</span></code> API.
As loading those models requires specifying <cite>loader_fn</cite> and <cite>persist_dir</cite> parameters, please check examples in
<a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/langchain/retriever_chain.py">retriever_chain</a></p>
</div></blockquote>
</li>
<li><p><strong>Can’t pickle certain objects</strong></p>
<blockquote>
<div><p>For certain models that LangChain does not support native saving or loading, we will pickle the object when saving it. Due to this functionality, your cloudpickle version must be
consistent between the saving and loading environments to ensure that object references resolve properly. For further guarantees of correct object representation, you should ensure that your
environment has <cite>pydantic</cite> installed with at least version 2.</p>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="documentation-for-old-versions">
<h2><a class="toc-backref" href="#id9">Documentation for Old Versions</a><a class="headerlink" href="#documentation-for-old-versions" title="Permalink to this headline"> </a></h2>
<p>MLflow LangChain Autologging feature is largely renewed in <code class="docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">2.14.0</span></code>. If you are using the earlier version of MLflow, please refer to following documentation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use MLflow LangChain autologging, please upgrade langchain to <strong>version 0.1.0</strong> or higher.
Depending on your existing environment, you may need to manually install langchain_community&gt;=0.0.16 in order to enable the automatic logging of artifacts and metrics. (this behavior will be modified in the future to be an optional import)
If autologging doesn’t log artifacts as expected, please check the warning messages in <cite>stdout</cite> logs.
For langchain_community==0.0.16, you will need to install the <cite>textstat</cite> and <cite>spacy</cite> libraries manually, as well as restarting any active interactive environment (i.e., a notebook environment). On Databricks, you can achieve this via executing <cite>dbutils.library.restartPython()</cite> to force the Python REPL to restart, allowing the newly installed libraries to be available.</p>
</div>
<p>MLflow langchain autologging injects <a class="reference external" href="https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/callbacks/mlflow_callback.py">MlflowCallbackHandler</a> into the langchain model inference process to log
metrics and artifacts automatically. We will only log the model if both <cite>log_models</cite> is set to <cite>True</cite> when calling <a class="reference internal" href="../../python_api/mlflow.langchain.html#mlflow.langchain.autolog" title="mlflow.langchain.autolog"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.autolog()</span></code></a> and the objects being invoked are within the supported model types: <cite>Chain</cite>, <cite>AgentExecutor</cite>, <cite>BaseRetriever</cite>, <cite>RunnableSequence</cite>, <cite>RunnableParallel</cite>, <cite>RunnableBranch</cite>, <cite>SimpleChatModel</cite>, <cite>ChatPromptTemplate</cite>,
<cite>RunnableLambda</cite>, <cite>RunnablePassthrough</cite>. Additional model types will be supported in the future.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We patch the <cite>invoke</cite> function for all supported langchain models, the <cite>__call__</cite> function for Chains and AgentExecutors models, and <cite>get_relevant_documents</cite> function for BaseRetrievers so that only when those functions are called will MLflow automatically log metrics and artifacts.
If the model contains retrievers, we don’t support autologging the model because it requires saving <cite>loader_fn</cite> and <cite>persist_dir</cite> in order to load the model. Please log the model manually if you want to log the model with retrievers.</p>
</div>
<p>The following metrics and artifacts are logged by default (depending on the models involved):</p>
<dl>
<dt>Artifacts:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 39%" />
<col style="width: 61%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Artifact name</p></td>
<td><p>Explanation</p></td>
</tr>
<tr class="row-even"><td><p>table_action_records.html</p></td>
<td><p>Each action’s details, including chains, tools, llms, agents, retrievers.</p></td>
</tr>
<tr class="row-odd"><td><p>table_session_analysis.html</p></td>
<td><p>Details about prompt and output for each prompt step; token usages;
text analysis metrics</p></td>
</tr>
<tr class="row-even"><td><p>chat_html.html</p></td>
<td><p>LLM input and output details</p></td>
</tr>
<tr class="row-odd"><td><p>llm_start_x_prompt_y.json</p></td>
<td><p>Includes prompt and kwargs passed during llm <cite>generate</cite> call</p></td>
</tr>
<tr class="row-even"><td><p>llm_end_x_generation_y.json</p></td>
<td><p>Includes llm_output of the LLM result</p></td>
</tr>
<tr class="row-odd"><td><p>ent-&lt;hash string of generation.text&gt;.html</p></td>
<td><p>Visualization of the generation text using spacy “en_core_web_sm” model
with style ent (if spacy is installed and the model is downloaded)</p></td>
</tr>
<tr class="row-even"><td><p>dep-&lt;hash string of generation.text&gt;.html</p></td>
<td><p>Visualization of the generation text using spacy “en_core_web_sm” model
with style dep (if spacy is installed and the model is downloaded)</p></td>
</tr>
<tr class="row-odd"><td><p>llm_new_tokens_x.json</p></td>
<td><p>Records new tokens added to the LLM during inference</p></td>
</tr>
<tr class="row-even"><td><p>chain_start_x.json</p></td>
<td><p>Records the inputs and chain related information during inference</p></td>
</tr>
<tr class="row-odd"><td><p>chain_end_x.json</p></td>
<td><p>Records the chain outputs</p></td>
</tr>
<tr class="row-even"><td><p>tool_start_x.json</p></td>
<td><p>Records the tool’s name, descriptions information during inference</p></td>
</tr>
<tr class="row-odd"><td><p>tool_end_x.json</p></td>
<td><p>Records observation of the tool</p></td>
</tr>
<tr class="row-even"><td><p>retriever_start_x.json</p></td>
<td><p>Records the retriever’s information during inference</p></td>
</tr>
<tr class="row-odd"><td><p>retriever_end_x.json</p></td>
<td><p>Records the retriever’s result documents</p></td>
</tr>
<tr class="row-even"><td><p>agent_finish_x.json</p></td>
<td><p>Records final return value of the ActionAgent, including output and log</p></td>
</tr>
<tr class="row-odd"><td><p>agent_action_x.json</p></td>
<td><p>Records the ActionAgent’s action details</p></td>
</tr>
<tr class="row-even"><td><p>on_text_x.json</p></td>
<td><p>Records the text during inference</p></td>
</tr>
<tr class="row-odd"><td><p>inference_inputs_outputs.json</p></td>
<td><p>Input and output details for each inference call (logged by default, can
be turned off by setting <cite>log_inputs_outputs=False</cite> when turn on autolog)</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Metrics:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 39%" />
<col style="width: 61%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Metric types</p></td>
<td><p>Details</p></td>
</tr>
<tr class="row-even"><td><p>Basic Metrics</p></td>
<td><p>step, starts, ends, errors, text_ctr, chain_starts, chain_ends, llm_starts
llm_ends, llm_streams, tool_starts, tool_ends, agent_ends, retriever_ends
retriever_starts (they’re the count number of each component invocation)</p></td>
</tr>
<tr class="row-odd"><td><p>Text Analysis Metrics</p></td>
<td><p>flesch_reading_ease, flesch_kincaid_grade, smog_index, coleman_liau_index
automated_readability_index, dale_chall_readability_score,
difficult_words, linsear_write_formula, gunning_fog, fernandez_huerta,
szigriszt_pazos, gutierrez_polini, crawford, gulpease_index, osman
(they’re the text analysis metrics of the generation text if <cite>textstat</cite>
library is installed)</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each inference call logs those artifacts into a separate directory named <cite>artifacts-&lt;session_id&gt;-&lt;idx&gt;</cite>, where <cite>session_id</cite> is randomly generated uuid, and <cite>idx</cite> is the index of the inference call.
<cite>session_id</cite> is also preserved in the <cite>inference_inputs_outputs.json</cite> file, so you can easily find the corresponding artifacts for each inference call.</p>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="index.html" class="btn btn-neutral" title="MLflow LangChain Flavor" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="notebooks/langchain-quickstart.html" class="btn btn-neutral" title="Introduction to Using LangChain with MLflow" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.14.2.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>