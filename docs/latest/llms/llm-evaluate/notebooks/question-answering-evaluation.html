
  

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LLM Evaluation with MLflow Example Notebook &mdash; MLflow 2.9.3.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html">
  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="MLflow 2.9.3.dev0 documentation" href="../../../index.html"/>
        <link rel="up" title="LLM Evaluation Examples" href="index.html"/>
        <link rel="next" title="LLM RAG Evaluation with MLflow Example Notebook" href="/rag-evaluation.html"/>
        <link rel="prev" title="LLM Evaluation Examples" href="/index.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="../../../None"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.9.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../index.html" class="main-navigation-home"><img src="../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id1">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id2">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id3">Prompt Engineering UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id4">LLM Tracking in MLflow</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../custom-pyfunc-for-llms/index.html">Deploying Advanced LLMs with Custom PyFuncs in MLflow</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">LLM Evaluation Examples</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">LLM Evaluation with MLflow Example Notebook</a></li>
<li class="toctree-l4"><a class="reference internal" href="rag-evaluation.html">LLM RAG Evaluation with MLflow Example Notebook</a></li>
<li class="toctree-l4"><a class="reference internal" href="rag-evaluation-llama2.html">LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook</a></li>
<li class="toctree-l4"><a class="reference internal" href="huggingface-evaluation.html">Evaluate a Hugging Face LLM with <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#qa-evaluation-notebook">QA Evaluation Notebook</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#rag-evaluation-notebook-using-gpt-4-as-judge">RAG Evaluation Notebook (using gpt-4-as-judge)</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#rag-evaluation-notebook-using-llama2-70b-as-judge">RAG Evaluation Notebook (using llama2-70b-as-judge)</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#evaluating-a-hugging-face-llm-notebook-using-gpt-4-as-judge">Evaluating a ü§ó Hugging Face LLM Notebook (using gpt-4-as-judge)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../gateway/index.html">MLflow AI Gateway (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="index.html">LLM Evaluation Examples</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>LLM Evaluation with MLflow Example Notebook</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/llm-evaluate/notebooks/question-answering-evaluation.ipynb" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="LLM-Evaluation-with-MLflow-Example-Notebook">
<h1>LLM Evaluation with MLflow Example Notebook<a class="headerlink" href="#LLM-Evaluation-with-MLflow-Example-Notebook" title="Permalink to this headline"> </a></h1>
<p>In this notebook, we will demonstrate how to evaluate various LLMs and RAG systems with MLflow, leveraging simple metrics such as toxicity, as well as LLM-judged metrics such as relevance, and even custom LLM-judged metrics such as professionalism</p>
<p>We need to set our OpenAI API key, since we will be using GPT-4 for our LLM-judged metrics.</p>
<p>In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry:</p>
<p><code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY=&lt;your</span> <span class="pre">openai</span> <span class="pre">API</span> <span class="pre">key&gt;</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
</pre></div>
</div>
</div>
<div class="section" id="Basic-Question-Answering-Evaluation">
<h2>Basic Question-Answering Evaluation<a class="headerlink" href="#Basic-Question-Answering-Evaluation" title="Permalink to this headline"> </a></h2>
<p>Create a test case of <code class="docutils literal notranslate"><span class="pre">inputs</span></code> that will be passed into the model and <code class="docutils literal notranslate"><span class="pre">ground_truth</span></code> which will be used to compare against the generated output from the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;How does useEffect() work?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What does the static keyword in a function mean?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What does the &#39;finally&#39; block in Python do?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is the difference between multiprocessing and multithreading?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we‚Äôll refer to it as our ‚Äúeffect‚Äù), and call it later after performing the DOM updates.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don&#39;t create any. It will be shared by all objects.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&#39;Finally&#39; defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Create a simple OpenAI model that asks gpt-3.5 to answer the question in two sentences. Call <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> with the model and evaluation dataframe.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;Answer the following question in two sentences&quot;</span>
    <span class="n">basic_qa_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">basic_qa_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
        <span class="n">eval_df</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>  <span class="c1"># specify which column corresponds to the expected output</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>  <span class="c1"># model type indicates which metrics are relevant for this task</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">metrics</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023/10/27 00:56:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.
2023/10/27 00:56:56 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.
Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint
2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count
2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity
2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level
2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level
2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;toxicity/v1/mean&#39;: 0.00020573455913108774,
 &#39;toxicity/v1/variance&#39;: 3.4433758978645428e-09,
 &#39;toxicity/v1/p90&#39;: 0.00027067282790085303,
 &#39;toxicity/v1/ratio&#39;: 0.0,
 &#39;flesch_kincaid_grade_level/v1/mean&#39;: 15.149999999999999,
 &#39;flesch_kincaid_grade_level/v1/variance&#39;: 26.502499999999998,
 &#39;flesch_kincaid_grade_level/v1/p90&#39;: 20.85,
 &#39;ari_grade_level/v1/mean&#39;: 17.375,
 &#39;ari_grade_level/v1/variance&#39;: 42.92187499999999,
 &#39;ari_grade_level/v1/p90&#39;: 24.48,
 &#39;exact_match/v1&#39;: 0.0}
</pre></div></div>
</div>
<p>Inspect the evaluation results table as a dataframe to see row-by-row metrics to further assess model performance</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3354bdb0880642d0844b892575b3e916", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>inputs</th>
      <th>ground_truth</th>
      <th>outputs</th>
      <th>token_count</th>
      <th>toxicity/v1/score</th>
      <th>flesch_kincaid_grade_level/v1/score</th>
      <th>ari_grade_level/v1/score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>How does useEffect() work?</td>
      <td>The useEffect() hook tells React that your com...</td>
      <td>useEffect() is a React hook that allows you to...</td>
      <td>64</td>
      <td>0.000243</td>
      <td>14.2</td>
      <td>15.8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>What does the static keyword in a function mean?</td>
      <td>Static members belongs to the class, rather th...</td>
      <td>The static keyword in a function means that th...</td>
      <td>32</td>
      <td>0.000150</td>
      <td>12.6</td>
      <td>14.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>What does the 'finally' block in Python do?</td>
      <td>'Finally' defines a block of code to run when ...</td>
      <td>The 'finally' block in Python is used to speci...</td>
      <td>46</td>
      <td>0.000283</td>
      <td>10.1</td>
      <td>10.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>What is the difference between multiprocessing...</td>
      <td>Multithreading refers to the ability of a proc...</td>
      <td>The main difference between multiprocessing an...</td>
      <td>34</td>
      <td>0.000148</td>
      <td>23.7</td>
      <td>28.2</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</div>
<div class="section" id="LLM-judged-correctness-with-OpenAI-GPT-4">
<h2>LLM-judged correctness with OpenAI GPT-4<a class="headerlink" href="#LLM-judged-correctness-with-OpenAI-GPT-4" title="Permalink to this headline"> </a></h2>
<p>Construct an answer similarity metric using the <code class="docutils literal notranslate"><span class="pre">answer_similarity()</span></code> metric factory function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.metrics.genai</span> <span class="kn">import</span> <span class="n">EvaluationExample</span><span class="p">,</span> <span class="n">answer_similarity</span>

<span class="c1"># Create an example to describe what answer_similarity means like for this problem.</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">EvaluationExample</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">output</span><span class="o">=</span><span class="s2">&quot;MLflow is an open-source platform for managing machine &quot;</span>
    <span class="s2">&quot;learning workflows, including experiment tracking, model packaging, &quot;</span>
    <span class="s2">&quot;versioning, and deployment, simplifying the ML lifecycle.&quot;</span><span class="p">,</span>
    <span class="n">score</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">justification</span><span class="o">=</span><span class="s2">&quot;The definition effectively explains what MLflow is &quot;</span>
    <span class="s2">&quot;its purpose, and its developer. It could be more concise for a 5-score.&quot;</span><span class="p">,</span>
    <span class="n">grading_context</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;targets&quot;</span><span class="p">:</span> <span class="s2">&quot;MLflow is an open-source platform for managing &quot;</span>
        <span class="s2">&quot;the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, &quot;</span>
        <span class="s2">&quot;a company that specializes in big data and machine learning solutions. MLflow is &quot;</span>
        <span class="s2">&quot;designed to address the challenges that data scientists and machine learning &quot;</span>
        <span class="s2">&quot;engineers face when developing, training, and deploying machine learning models.&quot;</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Construct the metric using OpenAI GPT-4 as the judge</span>
<span class="n">answer_similarity_metric</span> <span class="o">=</span> <span class="n">answer_similarity</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai:/gpt-4&quot;</span><span class="p">,</span> <span class="n">examples</span><span class="o">=</span><span class="p">[</span><span class="n">example</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">answer_similarity_metric</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
EvaluationMetric(name=answer_similarity, greater_is_better=True, long_name=answer_similarity, version=v1, metric_details=
Task:
You are an impartial judge. You will be given an input that was sent to a machine
learning model, and you will be given an output that the model produced. You
may also be given additional information that was used by the model to generate the output.

Your task is to determine a numerical score called answer_similarity based on the input and output.
A definition of answer_similarity and a grading rubric are provided below.
You must use the grading rubric to determine your score. You must also justify your score.

Examples could be included below for reference. Make sure to use them as references and to
understand them before completing the task.

Input:
{input}

Output:
{output}

{grading_context_columns}

Metric definition:
Answer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.

Grading rubric:
Answer similarity: Below are the details for different scores:
- Score 1: the output has little to no semantic similarity to the provided targets.
- Score 2: the output displays partial semantic similarity to the provided targets on some aspects.
- Score 3: the output has moderate semantic similarity to the provided targets.
- Score 4: the output aligns with the provided targets in most aspects and has substantial semantic similarity.
- Score 5: the output closely aligns with the provided targets in all significant aspects.

Examples:

Input:
What is MLflow?

Output:
MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.

Additional information used by the model:
key: ground_truth
value:
MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.

score: 4
justification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.


You must return the following fields in your response one below the other:
score: Your numerical score for the model&#39;s answer_similarity based on the rubric
justification: Your step-by-step reasoning about the model&#39;s answer_similarity score
    )
</pre></div></div>
</div>
<p>Call <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> again but with your new <code class="docutils literal notranslate"><span class="pre">answer_similarity_metric</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">basic_qa_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
        <span class="n">eval_df</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">answer_similarity_metric</span><span class="p">],</span>  <span class="c1"># use the answer similarity metric created above</span>
    <span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">metrics</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023/10/27 00:57:07 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.
2023/10/27 00:57:07 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.
2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count
2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity
2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level
2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level
2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match
2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_similarity
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;toxicity/v1/mean&#39;: 0.00023413174494635314,
 &#39;toxicity/v1/variance&#39;: 4.211776498455113e-09,
 &#39;toxicity/v1/p90&#39;: 0.00029628578631673007,
 &#39;toxicity/v1/ratio&#39;: 0.0,
 &#39;flesch_kincaid_grade_level/v1/mean&#39;: 14.774999999999999,
 &#39;flesch_kincaid_grade_level/v1/variance&#39;: 21.546875000000004,
 &#39;flesch_kincaid_grade_level/v1/p90&#39;: 19.71,
 &#39;ari_grade_level/v1/mean&#39;: 17.0,
 &#39;ari_grade_level/v1/variance&#39;: 41.005,
 &#39;ari_grade_level/v1/p90&#39;: 23.92,
 &#39;exact_match/v1&#39;: 0.0,
 &#39;answer_similarity/v1/mean&#39;: 3.75,
 &#39;answer_similarity/v1/variance&#39;: 1.1875,
 &#39;answer_similarity/v1/p90&#39;: 4.7}
</pre></div></div>
</div>
<p>See the row-by-row LLM-judged answer similarity score and justifications</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7f1b9c85984d4261a9cbde6b2b7888ed", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>inputs</th>
      <th>ground_truth</th>
      <th>outputs</th>
      <th>token_count</th>
      <th>toxicity/v1/score</th>
      <th>flesch_kincaid_grade_level/v1/score</th>
      <th>ari_grade_level/v1/score</th>
      <th>answer_similarity/v1/score</th>
      <th>answer_similarity/v1/justification</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>How does useEffect() work?</td>
      <td>The useEffect() hook tells React that your com...</td>
      <td>useEffect() is a React hook that allows you to...</td>
      <td>53</td>
      <td>0.000299</td>
      <td>12.1</td>
      <td>12.1</td>
      <td>4</td>
      <td>The output provided by the model aligns well w...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>What does the static keyword in a function mean?</td>
      <td>Static members belongs to the class, rather th...</td>
      <td>In C/C++, the static keyword in a function mea...</td>
      <td>55</td>
      <td>0.000141</td>
      <td>12.5</td>
      <td>14.4</td>
      <td>2</td>
      <td>The output provided by the model does correctl...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>What does the 'finally' block in Python do?</td>
      <td>'Finally' defines a block of code to run when ...</td>
      <td>The 'finally' block in Python is used to defin...</td>
      <td>64</td>
      <td>0.000290</td>
      <td>11.7</td>
      <td>13.5</td>
      <td>5</td>
      <td>The output provided by the model aligns very c...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>What is the difference between multiprocessing...</td>
      <td>Multithreading refers to the ability of a proc...</td>
      <td>Multiprocessing involves the execution of mult...</td>
      <td>49</td>
      <td>0.000207</td>
      <td>22.8</td>
      <td>28.0</td>
      <td>4</td>
      <td>The output provided by the model aligns well w...</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</div>
<div class="section" id="Custom-LLM-judged-metric-for-professionalism">
<h2>Custom LLM-judged metric for professionalism<a class="headerlink" href="#Custom-LLM-judged-metric-for-professionalism" title="Permalink to this headline"> </a></h2>
<p>Create a custom metric that will be used to determine professionalism of the model outputs. Use <code class="docutils literal notranslate"><span class="pre">make_genai_metric</span></code> with a metric definition, grading prompt, grading example, and judge model configuration</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.metrics.genai</span> <span class="kn">import</span> <span class="n">EvaluationExample</span><span class="p">,</span> <span class="n">make_genai_metric</span>

<span class="n">professionalism_metric</span> <span class="o">=</span> <span class="n">make_genai_metric</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;professionalism&quot;</span><span class="p">,</span>
    <span class="n">definition</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language&quot;</span>
    <span class="p">),</span>
    <span class="n">grading_prompt</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;Professionalism: If the answer is written using a professional tone, below &quot;</span>
        <span class="s2">&quot;are the details for different scores: &quot;</span>
        <span class="s2">&quot;- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.&quot;</span>
        <span class="s2">&quot;- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.&quot;</span>
        <span class="s2">&quot;- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. &quot;</span>
        <span class="s2">&quot;- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. &quot;</span>
        <span class="s2">&quot;- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. &quot;</span>
    <span class="p">),</span>
    <span class="n">examples</span><span class="o">=</span><span class="p">[</span>
        <span class="n">EvaluationExample</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="n">output</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It&#39;s like your Swiss Army knife for machine learning!&quot;</span>
            <span class="p">),</span>
            <span class="n">score</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">justification</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;The response is written in a casual tone. It uses contractions, filler words such as &#39;like&#39;, and exclamation points, which make it sound less professional. &quot;</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="p">],</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">&quot;v1&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai:/gpt-4&quot;</span><span class="p">,</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
    <span class="n">grading_context_columns</span><span class="o">=</span><span class="p">[],</span>
    <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;p90&quot;</span><span class="p">],</span>
    <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">professionalism_metric</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
EvaluationMetric(name=professionalism, greater_is_better=True, long_name=professionalism, version=v1, metric_details=
Task:
You are an impartial judge. You will be given an input that was sent to a machine
learning model, and you will be given an output that the model produced. You
may also be given additional information that was used by the model to generate the output.

Your task is to determine a numerical score called professionalism based on the input and output.
A definition of professionalism and a grading rubric are provided below.
You must use the grading rubric to determine your score. You must also justify your score.

Examples could be included below for reference. Make sure to use them as references and to
understand them before completing the task.

Input:
{input}

Output:
{output}

{grading_context_columns}

Metric definition:
Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language

Grading rubric:
Professionalism: If the answer is written using a professional tone, below are the details for different scores: - Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. - Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. - Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks.

Examples:

Input:
What is MLflow?

Output:
MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It&#39;s like your Swiss Army knife for machine learning!



score: 2
justification: The response is written in a casual tone. It uses contractions, filler words such as &#39;like&#39;, and exclamation points, which make it sound less professional.


You must return the following fields in your response one below the other:
score: Your numerical score for the model&#39;s professionalism based on the rubric
justification: Your step-by-step reasoning about the model&#39;s professionalism score
    )
</pre></div></div>
</div>
<p>Call <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate</span></code> with your new professionalism metric.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">basic_qa_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
        <span class="n">eval_df</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">professionalism_metric</span><span class="p">],</span>  <span class="c1"># use the professionalism metric we created above</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023/10/27 00:57:20 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.
2023/10/27 00:57:20 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.
2023/10/27 00:57:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count
2023/10/27 00:57:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity
2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level
2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level
2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match
2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: professionalism
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;toxicity/v1/mean&#39;: 0.0002044261127593927, &#39;toxicity/v1/variance&#39;: 1.8580601275034412e-09, &#39;toxicity/v1/p90&#39;: 0.00025343164161313326, &#39;toxicity/v1/ratio&#39;: 0.0, &#39;flesch_kincaid_grade_level/v1/mean&#39;: 13.649999999999999, &#39;flesch_kincaid_grade_level/v1/variance&#39;: 33.927499999999995, &#39;flesch_kincaid_grade_level/v1/p90&#39;: 19.92, &#39;ari_grade_level/v1/mean&#39;: 16.25, &#39;ari_grade_level/v1/variance&#39;: 51.927499999999995, &#39;ari_grade_level/v1/p90&#39;: 23.900000000000002, &#39;professionalism/v1/mean&#39;: 4.0, &#39;professionalism/v1/variance&#39;: 0.0, &#39;professionalism/v1/p90&#39;: 4.0}
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "363638c468914c3cbc646b9714462540", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>inputs</th>
      <th>ground_truth</th>
      <th>outputs</th>
      <th>token_count</th>
      <th>toxicity/v1/score</th>
      <th>flesch_kincaid_grade_level/v1/score</th>
      <th>ari_grade_level/v1/score</th>
      <th>professionalism/v1/score</th>
      <th>professionalism/v1/justification</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>How does useEffect() work?</td>
      <td>The useEffect() hook tells React that your com...</td>
      <td>useEffect() is a hook in React that allows you...</td>
      <td>46</td>
      <td>0.000218</td>
      <td>11.1</td>
      <td>12.7</td>
      <td>4</td>
      <td>The language used in the output is formal and ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>What does the static keyword in a function mean?</td>
      <td>Static members belongs to the class, rather th...</td>
      <td>The static keyword in a function means that th...</td>
      <td>48</td>
      <td>0.000158</td>
      <td>9.7</td>
      <td>12.3</td>
      <td>4</td>
      <td>The language used in the output is formal and ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>What does the 'finally' block in Python do?</td>
      <td>'Finally' defines a block of code to run when ...</td>
      <td>The 'finally' block in Python is used to defin...</td>
      <td>45</td>
      <td>0.000269</td>
      <td>10.1</td>
      <td>11.3</td>
      <td>4</td>
      <td>The language used in the output is formal and ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>What is the difference between multiprocessing...</td>
      <td>Multithreading refers to the ability of a proc...</td>
      <td>Multiprocessing involves running multiple proc...</td>
      <td>33</td>
      <td>0.000173</td>
      <td>23.7</td>
      <td>28.7</td>
      <td>4</td>
      <td>The language used in the output is formal and ...</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Lets see if we can improve <code class="docutils literal notranslate"><span class="pre">basic_qa_model</span></code> by creating a new model that could perform better by changing the system prompt.</p>
<p>Call <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> using the new model. Observe that the professionalism score has increased!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;Answer the following question using extreme formality.&quot;</span>
    <span class="n">professional_qa_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">professional_qa_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
        <span class="n">eval_df</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">professionalism_metric</span><span class="p">],</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/sunish.sheth/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.
  warnings.warn(
/Users/sunish.sheth/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(&#34;Setuptools is replacing distutils.&#34;)
2023/10/27 00:57:30 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.
2023/10/27 00:57:30 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.
2023/10/27 00:57:37 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count
2023/10/27 00:57:37 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity
2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level
2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level
2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match
2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: professionalism
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;toxicity/v1/mean&#39;: 0.00030383203556993976, &#39;toxicity/v1/variance&#39;: 9.482036560896618e-09, &#39;toxicity/v1/p90&#39;: 0.0003866828687023372, &#39;toxicity/v1/ratio&#39;: 0.0, &#39;flesch_kincaid_grade_level/v1/mean&#39;: 17.625, &#39;flesch_kincaid_grade_level/v1/variance&#39;: 2.9068750000000003, &#39;flesch_kincaid_grade_level/v1/p90&#39;: 19.54, &#39;ari_grade_level/v1/mean&#39;: 21.425, &#39;ari_grade_level/v1/variance&#39;: 3.6168750000000007, &#39;ari_grade_level/v1/p90&#39;: 23.6, &#39;professionalism/v1/mean&#39;: 4.5, &#39;professionalism/v1/variance&#39;: 0.25, &#39;professionalism/v1/p90&#39;: 5.0}
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3fc51e72e330496394c7ed4a9cb8a111", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>inputs</th>
      <th>ground_truth</th>
      <th>outputs</th>
      <th>token_count</th>
      <th>toxicity/v1/score</th>
      <th>flesch_kincaid_grade_level/v1/score</th>
      <th>ari_grade_level/v1/score</th>
      <th>professionalism/v1/score</th>
      <th>professionalism/v1/justification</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>How does useEffect() work?</td>
      <td>The useEffect() hook tells React that your com...</td>
      <td>Certainly, I shall elucidate the mechanics of ...</td>
      <td>386</td>
      <td>0.000398</td>
      <td>16.3</td>
      <td>19.7</td>
      <td>5</td>
      <td>The response is written in an excessively form...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>What does the static keyword in a function mean?</td>
      <td>Static members belongs to the class, rather th...</td>
      <td>The static keyword utilized in the context of ...</td>
      <td>73</td>
      <td>0.000143</td>
      <td>16.4</td>
      <td>20.0</td>
      <td>4</td>
      <td>The language used in the output is formal and ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>What does the 'finally' block in Python do?</td>
      <td>'Finally' defines a block of code to run when ...</td>
      <td>The 'finally' block in Python serves as an int...</td>
      <td>97</td>
      <td>0.000313</td>
      <td>20.5</td>
      <td>24.5</td>
      <td>4</td>
      <td>The language used in the output is formal and ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>What is the difference between multiprocessing...</td>
      <td>Multithreading refers to the ability of a proc...</td>
      <td>Allow me to elucidate upon the distinction bet...</td>
      <td>324</td>
      <td>0.000361</td>
      <td>17.3</td>
      <td>21.5</td>
      <td>5</td>
      <td>The response is written in an excessively form...</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="index.html" class="btn btn-neutral" title="LLM Evaluation Examples" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="rag-evaluation.html" class="btn btn-neutral" title="LLM RAG Evaluation with MLflow Example Notebook" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../',
      VERSION:'2.9.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>