

<!DOCTYPE html>
<!-- source: docs/source/llms/llm-evaluate/index.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta content="LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MLflow LLM Evaluation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/llm-evaluate/index.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.19.1.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="LLMs" href="../index.html"/>
        <link rel="next" title="Prompt Engineering UI (Experimental)" href="/../prompt-engineering/index.html"/>
        <link rel="prev" title="Unity Catalog Integration" href="/../deployments/uc_integration.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/tabs.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.19.1.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-genai-applications-in-mlflow">Tutorials and Use Case Guides for GenAI applications in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow AI Gateway for LLMs</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">MLflow LLM Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#full-notebook-guides-and-examples">Full Notebook Guides and Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quickstart">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="#llm-evaluation-metrics">LLM Evaluation Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prepare-your-target-models">Prepare Your Target Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="#viewing-evaluation-results">Viewing Evaluation Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../index.html#benefits-of-mlflow-s-llm-evaluation">Benefits of MLflow’s LLM Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>MLflow LLM Evaluation</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/llm-evaluate/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="mlflow-llm-evaluation">
<span id="llm-eval"></span><h1>MLflow LLM Evaluation<a class="headerlink" href="#mlflow-llm-evaluation" title="Permalink to this headline"> </a></h1>
<p>With the emerging of ChatGPT, LLMs have shown its power of text generation in various fields, such as
question answering, translating and text summarization. Evaluating LLMs’ performance is slightly different
from traditional ML models, as very often there is no single ground truth to compare against.
MLflow provides an API <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to help evaluate your LLMs.</p>
<p>MLflow’s LLM evaluation functionality consists of 3 main components:</p>
<ol class="arabic simple">
<li><p><strong>A model to evaluate</strong>: it can be an MLflow <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model, a URI pointing to one registered
MLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline.</p></li>
<li><p><strong>Metrics</strong>: the metrics to compute, LLM evaluate will use LLM metrics.</p></li>
<li><p><strong>Evaluation data</strong>: the data your model is evaluated at, it can be a pandas Dataframe, a python list, a
numpy array or an <a class="reference internal" href="../../python_api/mlflow.data.html#mlflow.data.dataset.Dataset" title="mlflow.data.dataset.Dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.data.dataset.Dataset()</span></code></a> instance.</p></li>
</ol>
<div class="section" id="full-notebook-guides-and-examples">
<h2>Full Notebook Guides and Examples<a class="headerlink" href="#full-notebook-guides-and-examples" title="Permalink to this headline"> </a></h2>
<p>If you’re interested in thorough use-case oriented guides that showcase the simplicity and power of MLflow’s evaluate
functionality for LLMs, please navigate to the notebook collection below:</p>
<a href="notebooks/index.html" class="download-btn">View the Notebook Guides</a><br/></div>
<div class="section" id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline"> </a></h2>
<p>Below is a simple example that gives an quick overview of how MLflow LLM evaluation works. The example builds
a simple question-answering model by wrapping “openai/gpt-4” with custom prompt. You can paste it to
your IPython or local editor and execute it, and install missing dependencies as prompted. Running the code
requires OpenAI API key, if you don’t have an OpenAI key, you can set it up by following the <a class="reference external" href="https://platform.openai.com/account/api-keys">OpenAI guide</a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s1">&#39;your-api-key-here&#39;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">getpass</span> <span class="kn">import</span> <span class="n">getpass</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) &quot;</span>
            <span class="s2">&quot;lifecycle. It was developed by Databricks, a company that specializes in big data and &quot;</span>
            <span class="s2">&quot;machine learning solutions. MLflow is designed to address the challenges that data &quot;</span>
            <span class="s2">&quot;scientists and machine learning engineers face when developing, training, and deploying &quot;</span>
            <span class="s2">&quot;machine learning models.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Apache Spark is an open-source, distributed computing system designed for big data &quot;</span>
            <span class="s2">&quot;processing and analytics. It was developed in response to limitations of the Hadoop &quot;</span>
            <span class="s2">&quot;MapReduce computing model, offering improvements in speed and ease of use. Spark &quot;</span>
            <span class="s2">&quot;provides libraries for various tasks such as data ingestion, processing, and analysis &quot;</span>
            <span class="s2">&quot;through its components like Spark SQL for structured data, Spark Streaming for &quot;</span>
            <span class="s2">&quot;real-time data processing, and MLlib for machine learning tasks&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;Answer the following question in two sentences&quot;</span>
    <span class="c1"># Wrap &quot;gpt-4&quot; as an MLflow model.</span>
    <span class="n">logged_model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>

    <span class="c1"># Use predefined question-answering metrics to evaluate our model.</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">logged_model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
        <span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;See aggregated evaluation results below: </span><span class="se">\n</span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Evaluation result for each data record is available in `results.tables`.</span>
    <span class="n">eval_table</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;See evaluation table below: </span><span class="se">\n</span><span class="si">{</span><span class="n">eval_table</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="llm-evaluation-metrics">
<h2>LLM Evaluation Metrics<a class="headerlink" href="#llm-evaluation-metrics" title="Permalink to this headline"> </a></h2>
<p>There are two types of LLM evaluation metrics in MLflow:</p>
<ol class="arabic simple">
<li><p><strong>Heuristic-based metrics</strong>: These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe), based on certain functions, such as: Rouge (<a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.rougeL" title="mlflow.metrics.rougeL"><code class="xref py py-func docutils literal notranslate"><span class="pre">rougeL()</span></code></a>), Flesch Kincaid (<a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.flesch_kincaid_grade_level" title="mlflow.metrics.flesch_kincaid_grade_level"><code class="xref py py-func docutils literal notranslate"><span class="pre">flesch_kincaid_grade_level()</span></code></a>) or Bilingual Evaluation Understudy (BLEU) (<a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.bleu" title="mlflow.metrics.bleu"><code class="xref py py-func docutils literal notranslate"><span class="pre">bleu()</span></code></a>). These metrics are similar to traditional continuous value metrics. For the list of built-in heuristic metrics and how to define a custom metric with your own function definition, see the <a class="reference external" href="#heuristic-based-metrics">Heuristic-based Metrics</a> section.</p></li>
<li><p><strong>LLM-as-a-Judge metrics</strong>: LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs. It overcomes the limitations of heuristic-based metrics, which often miss nuances like context and semantic accuracy. LLM-as-a-Judge metrics provides a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation. MLflow provides various built-in LLM-as-a-Judge metrics and supports creating custom metrics with your own prompt, grading criteria, and reference examples. See the <a class="reference external" href="#llm-as-a-judge-metrics">LLM-as-a-Judge Metrics</a> section for more details.</p></li>
</ol>
<div class="section" id="id1">
<h3>Heuristic-based Metrics<a class="headerlink" href="#id1" title="Permalink to this headline"> </a></h3>
<div class="section" id="built-in-heuristic-metrics">
<h4>Built-in Heuristic Metrics<a class="headerlink" href="#built-in-heuristic-metrics" title="Permalink to this headline"> </a></h4>
<p>See <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.metrics.html">this page</a> for the full list of the built-in heuristic metrics.</p>
</div>
<div class="section" id="default-metrics-with-pre-defined-model-types">
<span id="llm-eval-default-metrics"></span><h4>Default Metrics with Pre-defined Model Types<a class="headerlink" href="#default-metrics-with-pre-defined-model-types" title="Permalink to this headline"> </a></h4>
<p>MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, “question-answering”. Depending on the
LLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluations. To use
defaults metrics for pre-selected tasks, specify the <code class="docutils literal notranslate"><span class="pre">model_type</span></code> argument in <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>, as shown by the example
below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">eval_data</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The supported LLM model types and associated metrics are listed below:</p>
<ul>
<li><p><strong>question-answering</strong>: <code class="docutils literal notranslate"><span class="pre">model_type=&quot;question-answering&quot;</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p>exact-match</p></li>
<li><p><a class="reference external" href="https://huggingface.co/spaces/evaluate-measurement/toxicity">toxicity</a> <sup>1</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Automated_readability_index">ari_grade_level</a> <sup>2</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level">flesch_kincaid_grade_level</a> <sup>2</sup></p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>text-summarization</strong>: <code class="docutils literal notranslate"><span class="pre">model_type=&quot;text-summarization&quot;</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/spaces/evaluate-metric/rouge">ROUGE</a> <sup>3</sup></p></li>
<li><p><a class="reference external" href="https://huggingface.co/spaces/evaluate-measurement/toxicity">toxicity</a> <sup>1</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Automated_readability_index">ari_grade_level</a> <sup>2</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level">flesch_kincaid_grade_level</a> <sup>2</sup></p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>text models</strong>: <code class="docutils literal notranslate"><span class="pre">model_type=&quot;text&quot;</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/spaces/evaluate-measurement/toxicity">toxicity</a> <sup>1</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Automated_readability_index">ari_grade_level</a> <sup>2</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level">flesch_kincaid_grade_level</a> <sup>2</sup></p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>retrievers</strong>: <code class="docutils literal notranslate"><span class="pre">model_type=&quot;retriever&quot;</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k">precision_at_k</a>  <sup>4</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Recall">recall_at_k</a> <sup>4</sup></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG">ndcg_at_k</a> <sup>4</sup></p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p><sup>1</sup> Requires packages <a class="reference external" href="https://pypi.org/project/evaluate">evaluate</a>, <a class="reference external" href="https://pytorch.org/get-started/locally/">torch</a>, and
<a class="reference external" href="https://huggingface.co/docs/transformers/installation">transformers</a></p>
<p><sup>2</sup> Requires package <a class="reference external" href="https://pypi.org/project/textstat">textstat</a></p>
<p><sup>3</sup> Requires packages <a class="reference external" href="https://pypi.org/project/evaluate">evaluate</a>, <a class="reference external" href="https://pypi.org/project/nltk">nltk</a>, and
<a class="reference external" href="https://pypi.org/project/rouge-score">rouge-score</a></p>
<p><sup>4</sup> All retriever metrics have a default <code class="docutils literal notranslate"><span class="pre">retriever_k</span></code> value of <code class="docutils literal notranslate"><span class="pre">3</span></code> that can be overridden by specifying <code class="docutils literal notranslate"><span class="pre">retriever_k</span></code> in the <code class="docutils literal notranslate"><span class="pre">evaluator_config</span></code> argument.</p>
</div>
<div class="section" id="use-a-custom-list-of-metrics">
<span id="llm-eval-custom-metrics"></span><h4>Use a Custom List of Metrics<a class="headerlink" href="#use-a-custom-list-of-metrics" title="Permalink to this headline"> </a></h4>
<p>Using the pre-defined metrics associated with a given model type is not the only way to generate scoring metrics
for LLM evaluation in MLflow. You can specify a custom list of metrics in the <cite>extra_metrics</cite> argument in <cite>mlflow.evaluate</cite>:</p>
<ul>
<li><p>To add additional metrics to the default metrics list of pre-defined model type, keep the <cite>model_type</cite> and add your metrics to <code class="docutils literal notranslate"><span class="pre">extra_metrics</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">eval_data</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">latency</span><span class="p">()],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The above code will evaluate your model using all metrics for “question-answering” model plus <a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.latency" title="mlflow.metrics.latency"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.metrics.latency()</span></code></a>.</p>
</li>
<li><p>To disable default metric calculation and only calculate your selected metrics, remove the <code class="docutils literal notranslate"><span class="pre">model_type</span></code> argument and define the desired metrics.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">eval_data</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">toxicity</span><span class="p">(),</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">latency</span><span class="p">()],</span>
<span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>The full reference for supported evaluation metrics can be found <a class="reference external" href="../../python_api/mlflow.html#mlflow.evaluate">here</a>.</p>
</div>
<div class="section" id="create-custom-heuristic-based-llm-evaluation-metrics">
<h4>Create Custom heuristic-based LLM Evaluation Metrics<a class="headerlink" href="#create-custom-heuristic-based-llm-evaluation-metrics" title="Permalink to this headline"> </a></h4>
<p>This is very similar to creating custom traditional metrics, with the exception of returning a <a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.MetricValue" title="mlflow.metrics.MetricValue"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.metrics.MetricValue()</span></code></a> instance.
Basically you need to:</p>
<ol class="arabic simple">
<li><p>Implement An <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> to define your scoring logic. This function must take in 2 args: <code class="docutils literal notranslate"><span class="pre">predictions</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code>.
<code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> must return a <a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.MetricValue" title="mlflow.metrics.MetricValue"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.metrics.MetricValue()</span></code></a> instance.</p></li>
<li><p>Pass <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> and other arguments to the <code class="docutils literal notranslate"><span class="pre">mlflow.metrics.make_metric</span></code> API to create the metric.</p></li>
</ol>
<p>The following code creates a dummy per-row metric called <code class="docutils literal notranslate"><span class="pre">&quot;over_10_chars&quot;</span></code>; if the model output is greater than 10,
the score is “yes”, otherwise it is “no”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;yes&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">10</span> <span class="k">else</span> <span class="s2">&quot;no&quot;</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">MetricValue</span><span class="p">(</span>
        <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
        <span class="n">aggregate_results</span><span class="o">=</span><span class="n">standard_aggregations</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
    <span class="p">)</span>


<span class="c1"># Create an EvaluationMetric object.</span>
<span class="n">passing_code_metric</span> <span class="o">=</span> <span class="n">make_metric</span><span class="p">(</span>
    <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;over_10_chars&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To create a custom metric that is dependent on other metrics, include those other metrics’ names as an argument after <code class="docutils literal notranslate"><span class="pre">predictions</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code>. This can be the name of a builtin metric or another custom metric.
Ensure that you do not accidentally have any circular dependencies in your metrics, or the evaluation will fail.</p>
<p>The following code creates a dummy per-row metric called <code class="docutils literal notranslate"><span class="pre">&quot;toxic_or_over_10_chars&quot;</span></code>: if the model output is greater than 10 or the toxicity score is greater than 0.5, the score is “yes”, otherwise it is “no”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">toxicity</span><span class="p">,</span> <span class="n">over_10_chars</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;yes&quot;</span> <span class="k">if</span> <span class="n">toxicity</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="ow">or</span> <span class="n">over_10_chars</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;no&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">len</span><span class="p">(</span><span class="n">toxicity</span><span class="o">.</span><span class="n">scores</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">MetricValue</span><span class="p">(</span><span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>


<span class="c1"># Create an EvaluationMetric object.</span>
<span class="n">toxic_and_over_10_chars_metric</span> <span class="o">=</span> <span class="n">make_metric</span><span class="p">(</span>
    <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;toxic_or_over_10_chars&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id9">
<h3>LLM-as-a-Judge Metrics<a class="headerlink" href="#id9" title="Permalink to this headline"> </a></h3>
<p>LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs, providing a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation.</p>
<p>MLflow supports several builtin LLM-as-a-judge metrics, as well as allowing you to create your own LLM-as-a-judge metrics with custom configurations and prompts.</p>
<div class="section" id="built-in-llm-as-a-judge-metrics">
<h4>Built-in LLM-as-a-Judge metrics<a class="headerlink" href="#built-in-llm-as-a-judge-metrics" title="Permalink to this headline"> </a></h4>
<p>To use built-in LLM-as-a-Judge metrics in MLflow, pass the list of metrics definitions to the <code class="docutils literal notranslate"><span class="pre">extra_metrics</span></code> argument in the <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> function.</p>
<p>The following example uses the built-in answer correctness metric for evaluation, in addition to the latency metric (heuristic):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.metrics</span> <span class="kn">import</span> <span class="n">latency</span>
<span class="kn">from</span> <span class="nn">mlflow.metrics.genai</span> <span class="kn">import</span> <span class="n">answer_correctness</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">eval_data</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span>
        <span class="n">answer_correctness</span><span class="p">(),</span>
        <span class="n">latency</span><span class="p">(),</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Here is the list of built-in LLM-as-a-Judge metrics. Click on the link to see the full documentation for each metric:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity" title="mlflow.metrics.genai.answer_similarity"><code class="xref py py-func docutils literal notranslate"><span class="pre">answer_similarity()</span></code></a>: Evaluate how similar a model’s generated output is compared to the information in the ground truth data.</p></li>
<li><p><a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness" title="mlflow.metrics.genai.answer_correctness"><code class="xref py py-func docutils literal notranslate"><span class="pre">answer_correctness()</span></code></a>: Evaluate how factually correct a model’s generated output is based on the information within the ground truth data.</p></li>
<li><p><a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_relevance" title="mlflow.metrics.genai.answer_relevance"><code class="xref py py-func docutils literal notranslate"><span class="pre">answer_relevance()</span></code></a>: Evaluate how relevant the model generated output is to the input (context is ignored).</p></li>
<li><p><a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.relevance" title="mlflow.metrics.genai.relevance"><code class="xref py py-func docutils literal notranslate"><span class="pre">relevance()</span></code></a>: Evaluate how relevant the model generated output is with respect to both the input and the context.</p></li>
<li><p><a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness" title="mlflow.metrics.genai.faithfulness"><code class="xref py py-func docutils literal notranslate"><span class="pre">faithfulness()</span></code></a>: Evaluate how faithful the model generated output is based on the context provided.</p></li>
</ul>
</div>
<div class="section" id="selecting-the-judge-model">
<h4>Selecting the Judge Model<a class="headerlink" href="#selecting-the-judge-model" title="Permalink to this headline"> </a></h4>
<p>By default, MLflow will use OpenAI’s GPT-4 model as the judge model that scores metrics. You can change the judge model by passing an override to the <code class="docutils literal notranslate"><span class="pre">model</span></code> argument within the metric definition.</p>
<div class="section" id="saas-llm-providers">
<h5>1. SaaS LLM Providers<a class="headerlink" href="#saas-llm-providers" title="Permalink to this headline"> </a></h5>
<p>To use SaaS LLM providers, such as OpenAI or Anthropic, set the <code class="docutils literal notranslate"><span class="pre">model</span></code> parameter in the metrics definition, in the format of <code class="docutils literal notranslate"><span class="pre">&lt;provider&gt;:/&lt;model-name&gt;</span></code>. Currently, MLflow supports <code class="docutils literal notranslate"><span class="pre">[&quot;openai&quot;,</span> <span class="pre">&quot;anthropic&quot;,</span> <span class="pre">&quot;bedrock&quot;,</span> <span class="pre">&quot;mistral&quot;,</span> <span class="pre">&quot;togetherai&quot;]</span></code> as viable LLM providers for any judge model.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">OpenAI / Azure OpenAI</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Anthropic</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Bedrock</button><button aria-controls="panel-0-0-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-3" name="0-3" role="tab" tabindex="-1">Mistral</button><button aria-controls="panel-0-0-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-4" name="0-4" role="tab" tabindex="-1">TogetherAI</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>OpenAI models can be accessed via the <code class="docutils literal notranslate"><span class="pre">openai:/&lt;model-name&gt;</span></code> URI.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-openai-api-key&gt;&quot;</span>

<span class="n">answer_correctness</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_correctness</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai:/gpt-4o&quot;</span><span class="p">)</span>

<span class="c1"># Test the metric definition</span>
<span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;MLflow is an innovative full self-driving airship.&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end ML lifecycle.&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Azure OpenAI endpoints can be accessed via the same <code class="docutils literal notranslate"><span class="pre">openai:/&lt;model-name&gt;</span></code> URI, by setting the environment variables, such as <code class="docutils literal notranslate"><span class="pre">OPENAI_API_BASE</span></code>, <code class="docutils literal notranslate"><span class="pre">OPENAI_API_TYPE</span></code>, etc.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_TYPE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;azure&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_BASE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;https:/my-azure-openai-endpoint.azure.com/&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_DEPLOYMENT_NAME&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o-mini&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_VERSION&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2024-08-01-preview&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-api-key-for-azure-openai-endpoint&gt;&quot;</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>Anthropic models can be accessed via the <code class="docutils literal notranslate"><span class="pre">anthropic:/&lt;model-name&gt;</span></code> URI. Note that the <cite>default judge parameters &lt;#overriding-default-judge-parameters&gt;</cite> need to be overridden by passing the <code class="docutils literal notranslate"><span class="pre">parameters</span></code> argument to the metrics definition, since the default parameters violates the Anthropic endpoint requirement (<code class="docutils literal notranslate"><span class="pre">temperature</span></code> and <code class="docutils literal notranslate"><span class="pre">top_p</span></code> cannot be specified together).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-anthropic-api-key&gt;&quot;</span>

<span class="n">answer_correctness</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;anthropic:/claude-3-5-sonnet-20241022&quot;</span><span class="p">,</span>
    <span class="c1"># Override default judge parameters to meet Claude endpoint requirements.</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Test the metric definition</span>
<span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;MLflow is an innovative full self-driving airship.&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end ML lifecycle.&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>Bedrock models can be accessed via the <code class="docutils literal notranslate"><span class="pre">bedrock:/&lt;model-name&gt;</span></code> URI. Make sure you have set the authentication information via the environment variables. You can use both role-based or API key-based authentication for accessing Bedrock models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AWS_REGION&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-aws-region&gt;&quot;</span>

<span class="c1"># Option 1. Role-based authentication</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AWS_ROLE_ARN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-aws-role-arn&gt;&quot;</span>

<span class="c1"># Option 2. API key-based authentication</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AWS_ACCESS_KEY_ID&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-aws-access-key-id&gt;&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AWS_SECRET_ACCESS_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-aws-secret-access-key&gt;&quot;</span>
<span class="c1"># You can also use session token for temporary credentials.</span>
<span class="c1"># os.environ[&quot;AWS_SESSION_TOKEN&quot;] = &quot;&lt;your-aws-session-token&gt;&quot;</span>

<span class="n">answer_correctness</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0&quot;</span><span class="p">,</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;anthropic_version&quot;</span><span class="p">:</span> <span class="s2">&quot;bedrock-2023-05-31&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Test the metric definition</span>
<span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;MLflow is an innovative full self-driving airship.&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end ML lifecycle.&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-3" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-3" name="0-3" role="tabpanel" tabindex="0"><p>Mistral models can be accessed via the <code class="docutils literal notranslate"><span class="pre">mistral:/&lt;model-name&gt;</span></code> URI.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MISTRAL_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-mistral-api-key&gt;&quot;</span>

<span class="n">answer_correctness</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;mistral:/mistral-small-latest&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Test the metric definition</span>
<span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;MLflow is an innovative full self-driving airship.&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end ML lifecycle.&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-4" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-4" name="0-4" role="tabpanel" tabindex="0"><p>TogetherAI models can be accessed via the <code class="docutils literal notranslate"><span class="pre">togetherai:/&lt;model-name&gt;</span></code> URI.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TOGETHERAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&lt;your-togetherai-api-key&gt;&quot;</span>

<span class="n">answer_correctness</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;togetherai:/togetherai-small-latest&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Test the metric definition</span>
<span class="n">answer_correctness</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;MLflow is an innovative full self-driving airship.&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end ML lifecycle.&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Your use of a third party LLM service (e.g., OpenAI) for evaluation may be subject to and governed by the LLM service’s terms of use.</p>
</div>
</div>
<div class="section" id="self-hosted-proxy-endpoints">
<h5>2. Self-hosted Proxy Endpoints<a class="headerlink" href="#self-hosted-proxy-endpoints" title="Permalink to this headline"> </a></h5>
<p>If you are accessing SaaS LLM providers via a proxy endpoint (e.g., for security compliance), you can set the <code class="docutils literal notranslate"><span class="pre">proxy_url</span></code> parameter in the metrics definition. Additionally, use the <code class="docutils literal notranslate"><span class="pre">extra_headers</span></code> parameters  to pass extra headers for the endpoint for authentication.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">answer_similarity</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai:/gpt-4o&quot;</span><span class="p">,</span>
    <span class="n">proxy_url</span><span class="o">=</span><span class="s2">&quot;https://my-proxy-endpoint/chat&quot;</span><span class="p">,</span>
    <span class="n">extra_headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Group-ID&quot;</span><span class="p">:</span> <span class="s2">&quot;my-group-id&quot;</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="mlflow-ai-gateway-endpoints">
<h5>3. MLflow AI Gateway Endpoints<a class="headerlink" href="#mlflow-ai-gateway-endpoints" title="Permalink to this headline"> </a></h5>
<p><a class="reference external" href="../deployments/index.html">MLflow AI Gateway</a> is a self-hosted solution that allows you to query various LLM providers in a unified interface. To use an endpoint hosted by MLflow AI Gateway:</p>
<ol class="arabic simple">
<li><p>Start the MLflow AI Gateway server with your LLM setting by following <a class="reference external" href="../deployments/index.html#quickstart">these steps</a>.</p></li>
<li><p>Set the MLflow deployment client to target the server address by using <a class="reference internal" href="../../python_api/mlflow.deployments.html#mlflow.deployments.set_deployments_target" title="mlflow.deployments.set_deployments_target"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_deployments_target()</span></code></a>.</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">endpoints:/&lt;endpoint-name&gt;</span></code> to the <code class="docutils literal notranslate"><span class="pre">model</span></code> parameter in the metrics definition.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.deployments</span> <span class="kn">import</span> <span class="n">set_deployments_target</span>

<span class="c1"># When the MLflow AI Gateway server is running at http://localhost:5000</span>
<span class="n">set_deployments_target</span><span class="p">(</span><span class="s2">&quot;http://localhost:5000&quot;</span><span class="p">)</span>
<span class="n">my_answer_similarity</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;endpoints:/my-endpoint&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="databricks-model-serving">
<h5>4. Databricks Model Serving<a class="headerlink" href="#databricks-model-serving" title="Permalink to this headline"> </a></h5>
<p>If you have a model hosted on Databricks, you can use it as a judge model by setting <code class="docutils literal notranslate"><span class="pre">endpoints:/&lt;endpoint-name&gt;</span></code> to the <code class="docutils literal notranslate"><span class="pre">model</span></code> parameter in the metrics definition. The following code uses a Llama 3.1 405B model available via the <a class="reference external" href="https://docs.databricks.com/en/machine-learning/model-serving/index.html">Foundation Model API</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.deployments</span> <span class="kn">import</span> <span class="n">set_deployments_target</span>

<span class="n">set_deployments_target</span><span class="p">(</span><span class="s2">&quot;databricks&quot;</span><span class="p">)</span>
<span class="n">llama3_answer_similarity</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;endpoints:/databricks-llama-3-1-405b-instruct&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="overriding-default-judge-parameters">
<h4>Overriding Default Judge Parameters<a class="headerlink" href="#overriding-default-judge-parameters" title="Permalink to this headline"> </a></h4>
<p>By default, MLflow queries the judge LLM model with the following parameters:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">temperature</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">max_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">200</span>
<span class="nt">top_p</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</pre></div>
</div>
<p>However, this might not be suitable for all LLM providers. For example, accessing Anthropic’s Claude models on Amazon Bedrock requires an <code class="docutils literal notranslate"><span class="pre">anthropic_version</span></code> parameter to be specified in the request payload. You can override these default parameters by passing the <code class="docutils literal notranslate"><span class="pre">parameters</span></code> argument to the metrics definition.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_answer_similarity</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0&quot;</span><span class="p">,</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;anthropic_version&quot;</span><span class="p">:</span> <span class="s2">&quot;bedrock-2023-05-31&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Note that the parameters dictionary you pass in the <code class="docutils literal notranslate"><span class="pre">parameters</span></code> argument will <strong>replace the default parameters</strong>, instead of being merged with them. For example, <code class="docutils literal notranslate"><span class="pre">top_p</span></code> will <strong>not</strong> be sent to the model in the above code example.</p>
</div>
<div class="section" id="creating-custom-llm-as-a-judge-metrics">
<h4>Creating Custom LLM-as-a-Judge Metrics<a class="headerlink" href="#creating-custom-llm-as-a-judge-metrics" title="Permalink to this headline"> </a></h4>
<p>You can also create your own LLM-as-a-Judge evaluation metrics with <a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric" title="mlflow.metrics.genai.make_genai_metric"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.metrics.genai.make_genai_metric()</span></code></a> API, which needs the following information:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code>: the name of your custom metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">definition</span></code>: describe what’s the metric doing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">grading_prompt</span></code>: describe the scoring criteria.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">examples</span></code> (Optional): a few input/output examples with scores provided; used as a reference for the LLM judge.</p></li>
</ul>
<p>See the <a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric" title="mlflow.metrics.genai.make_genai_metric"><code class="xref py py-func docutils literal notranslate"><span class="pre">API</span> <span class="pre">documentation</span></code></a> for the full list of the configurations.</p>
<p>Under the hood, <code class="docutils literal notranslate"><span class="pre">definition</span></code>, <code class="docutils literal notranslate"><span class="pre">grading_prompt</span></code>, <code class="docutils literal notranslate"><span class="pre">examples</span></code> together with evaluation data and model output will be
composed into a long prompt and sent to LLM. If you are familiar with the concept of prompt engineering,
SaaS LLM evaluation metric is basically trying to compose a “right” prompt containing instructions, data and model
output so that LLM, e.g., GPT4 can output the information we want.</p>
<p>Now let’s create a custom GenAI metrics called “professionalism”, which measures how professional our model output is.</p>
<p>Let’s first create a few examples with scores, these will be the reference samples LLM judge uses. To create such examples,
we will use <a class="reference internal" href="../../python_api/mlflow.metrics.html#mlflow.metrics.genai.EvaluationExample" title="mlflow.metrics.genai.EvaluationExample"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.metrics.genai.EvaluationExample()</span></code></a> class, which has 4 fields:</p>
<ul class="simple">
<li><p>input: input text.</p></li>
<li><p>output: output text.</p></li>
<li><p>score: the score for output in the context of input.</p></li>
<li><p>justification: why do we give the <cite>score</cite> for the data.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">professionalism_example_score_2</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">EvaluationExample</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">output</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps &quot;</span>
        <span class="s2">&quot;you track experiments, package your code and models, and collaborate with your team, making the whole ML &quot;</span>
        <span class="s2">&quot;workflow smoother. It&#39;s like your Swiss Army knife for machine learning!&quot;</span>
    <span class="p">),</span>
    <span class="n">score</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">justification</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;The response is written in a casual tone. It uses contractions, filler words such as &#39;like&#39;, and &quot;</span>
        <span class="s2">&quot;exclamation points, which make it sound less professional. &quot;</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">professionalism_example_score_4</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">EvaluationExample</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="n">output</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was &quot;</span>
        <span class="s2">&quot;developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is &quot;</span>
        <span class="s2">&quot;designed to address the challenges that data scientists and machine learning engineers face when &quot;</span>
        <span class="s2">&quot;developing, training, and deploying machine learning models.&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">score</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">justification</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;The response is written in a formal language and a neutral tone. &quot;</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Now let’s define the <code class="docutils literal notranslate"><span class="pre">professionalism</span></code> metric, you will see how each field is set up.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">professionalism</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">make_genai_metric</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;professionalism&quot;</span><span class="p">,</span>
    <span class="n">definition</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is &quot;</span>
        <span class="s2">&quot;tailored to the context and audience. It often involves avoiding overly casual language, slang, or &quot;</span>
        <span class="s2">&quot;colloquialisms, and instead using clear, concise, and respectful language.&quot;</span>
    <span class="p">),</span>
    <span class="n">grading_prompt</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;Professionalism: If the answer is written using a professional tone, below are the details for different scores: &quot;</span>
        <span class="s2">&quot;- Score 0: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for &quot;</span>
        <span class="s2">&quot;professional contexts.&quot;</span>
        <span class="s2">&quot;- Score 1: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in &quot;</span>
        <span class="s2">&quot;some informal professional settings.&quot;</span>
        <span class="s2">&quot;- Score 2: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.&quot;</span>
        <span class="s2">&quot;- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. &quot;</span>
        <span class="s2">&quot;- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal &quot;</span>
        <span class="s2">&quot;business or academic settings. &quot;</span>
    <span class="p">),</span>
    <span class="n">examples</span><span class="o">=</span><span class="p">[</span><span class="n">professionalism_example_score_2</span><span class="p">,</span> <span class="n">professionalism_example_score_4</span><span class="p">],</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai:/gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
    <span class="n">aggregations</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">],</span>
    <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="prepare-your-target-models">
<h2>Prepare Your Target Models<a class="headerlink" href="#prepare-your-target-models" title="Permalink to this headline"> </a></h2>
<p>In order to evaluate your model with <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code>, your model has to be one of the following types:</p>
<ol class="arabic simple">
<li><p>A <a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel" title="mlflow.pyfunc.PyFuncModel"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.PyFuncModel()</span></code></a> instance or a URI pointing to a logged <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.PyFuncModel</span></code> model. In
general we call that MLflow model. The</p></li>
<li><p>A python function that takes in string inputs and outputs a single string. Your callable must match the signature of
<a class="reference internal" href="../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel.predict" title="mlflow.pyfunc.PyFuncModel.predict"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.PyFuncModel.predict()</span></code></a> (without <code class="docutils literal notranslate"><span class="pre">params</span></code> argument), briefly it should:</p>
<ul class="simple">
<li><p>Has <code class="docutils literal notranslate"><span class="pre">data</span></code> as the only argument, which can be a <code class="docutils literal notranslate"><span class="pre">pandas.Dataframe</span></code>, <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>, python list, dictionary or scipy matrix.</p></li>
<li><p>Returns one of <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>, <code class="docutils literal notranslate"><span class="pre">pandas.Series</span></code>, <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> or list.</p></li>
</ul>
</li>
<li><p>An MLflow Deployments endpoint URI pointing to a local <a class="reference external" href="../deployments/index.html">MLflow AI Gateway</a>, <a class="reference external" href="https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html">Databricks Foundation Models API</a>, and <a class="reference external" href="https://docs.databricks.com/en/generative-ai/external-models/index.html">External Models in Databricks Model Serving</a>.</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">model=None</span></code>, and put model outputs in <code class="docutils literal notranslate"><span class="pre">data</span></code>. Only applicable when the data is a Pandas dataframe.</p></li>
</ol>
<div class="section" id="evaluating-with-an-mlflow-model">
<h3>Evaluating with an MLflow Model<a class="headerlink" href="#evaluating-with-an-mlflow-model" title="Permalink to this headline"> </a></h3>
<p>For detailed instruction on how to convert your model into a <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.PyFuncModel</span></code> instance, please read
<a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models">this doc</a>. But in short,
to evaluate your model as an MLflow model, we recommend following the steps below:</p>
<ol class="arabic">
<li><p>Log your model to MLflow server by <code class="docutils literal notranslate"><span class="pre">log_model</span></code>. Each flavor (<code class="docutils literal notranslate"><span class="pre">opeanai</span></code>, <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>, …)
has its own <code class="docutils literal notranslate"><span class="pre">log_model</span></code> API, e.g., <a class="reference internal" href="../../python_api/openai/index.html#mlflow.openai.log_model" title="mlflow.openai.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.openai.log_model()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;Answer the following question in two sentences&quot;</span>
    <span class="c1"># Wrap &quot;gpt-4o-mini&quot; as an MLflow model.</span>
    <span class="n">logged_model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Use the URI of logged model as the model instance in <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">logged_model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
    <span class="n">eval_data</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="evaluating-with-a-custom-function">
<span id="llm-eval-custom-function"></span><h3>Evaluating with a Custom Function<a class="headerlink" href="#evaluating-with-a-custom-function" title="Permalink to this headline"> </a></h3>
<p>As of MLflow 2.8.0, <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> supports evaluating a python function without requiring
logging the model to MLflow. This is useful when you don’t want to log the model and just want to evaluate
it. The following example uses <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to evaluate a function. You also need to set
up OpenAI authentication to run the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">openai_qa</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;Please answer the following question in formal language.&quot;</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]},</span>
            <span class="p">],</span>
        <span class="p">)</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predictions</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">openai_qa</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id17">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id17" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;flesch_kincaid_grade_level/v1/mean&quot;</span><span class="p">:</span> <span class="mf">14.75</span><span class="p">,</span>
    <span class="s2">&quot;flesch_kincaid_grade_level/v1/variance&quot;</span><span class="p">:</span> <span class="mf">0.5625</span><span class="p">,</span>
    <span class="s2">&quot;flesch_kincaid_grade_level/v1/p90&quot;</span><span class="p">:</span> <span class="mf">15.35</span><span class="p">,</span>
    <span class="s2">&quot;ari_grade_level/v1/mean&quot;</span><span class="p">:</span> <span class="mf">18.15</span><span class="p">,</span>
    <span class="s2">&quot;ari_grade_level/v1/variance&quot;</span><span class="p">:</span> <span class="mf">0.5625</span><span class="p">,</span>
    <span class="s2">&quot;ari_grade_level/v1/p90&quot;</span><span class="p">:</span> <span class="mf">18.75</span><span class="p">,</span>
    <span class="s2">&quot;exact_match/v1&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="evaluating-with-an-mlflow-deployments-endpoint">
<span id="llm-eval-model-endpoint"></span><h3>Evaluating with an MLflow Deployments Endpoint<a class="headerlink" href="#evaluating-with-an-mlflow-deployments-endpoint" title="Permalink to this headline"> </a></h3>
<p>For MLflow &gt;= 2.11.0, <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> supports evaluating a model endpoint by directly passing the MLflow Deployments endpoint URI to the <code class="docutils literal notranslate"><span class="pre">model</span></code> argument.
This is particularly useful when you want to evaluate a deployed model hosted by a local <a class="reference external" href="../deployments/index.html">MLflow AI Gateway</a>,  <a class="reference external" href="https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html">Databricks Foundation Models API</a>, and <a class="reference external" href="https://docs.databricks.com/en/generative-ai/external-models/index.html">External Models in Databricks Model Serving</a>, without implementing custom prediction logic to wrap it as an MLflow model or a python function.</p>
<p>Please don’t forget to set the target deployment client by using <a class="reference internal" href="../../python_api/mlflow.deployments.html#mlflow.deployments.set_deployments_target" title="mlflow.deployments.set_deployments_target"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.deployments.set_deployments_target()</span></code></a> before calling <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> with the endpoint URI, as shown in the example below. Otherwise, you will see an error message like <code class="docutils literal notranslate"><span class="pre">MlflowException:</span> <span class="pre">No</span> <span class="pre">deployments</span> <span class="pre">target</span> <span class="pre">has</span> <span class="pre">been</span> <span class="pre">set...</span></code>.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>When you want to use an endpoint <strong>not</strong> hosted by an MLflow AI Gateway or Databricks, you can create a custom Python function following the <a class="reference internal" href="#llm-eval-custom-function"><span class="std std-ref">Evaluating with a Custom Function</span></a> guide and use it as the <code class="docutils literal notranslate"><span class="pre">model</span></code> argument.</p>
</div>
<div class="section" id="supported-input-data-formats">
<h4>Supported Input Data Formats<a class="headerlink" href="#supported-input-data-formats" title="Permalink to this headline"> </a></h4>
<p>The input data can be either of the following format when using an URI of the MLflow Deployment Endpoint as the model:</p>
<table class="colwidths-given wrap-table docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Data Format</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Additional Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A pandas DataFrame with a string column.</p></td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</td>
<td><p>For this input format, MLflow will construct the appropriate request payload to the model endpoint type. For example, if your model is a chat endpoint (<code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code>), MLflow will wrap your input string with the chat messages format like <code class="docutils literal notranslate"><span class="pre">{&quot;messages&quot;:</span> <span class="pre">[{&quot;role&quot;:</span> <span class="pre">&quot;user&quot;,</span> <span class="pre">&quot;content&quot;:</span> <span class="pre">&quot;What</span> <span class="pre">is</span> <span class="pre">MLflow?&quot;}]}</span></code>. If you want to customize the request payload e.g. including system prompt, please use the next format.</p></td>
</tr>
<tr class="row-odd"><td><p>A pandas DataFrame with a dictionary column.</p></td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Please answer.&quot;</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">},</span>
                <span class="p">],</span>
                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="c1"># ... more dictionary records</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</td>
<td><p>In this format, the dictionary should have the correct request format for your model endpoint. Please refer to the <a class="reference external" href="../deployments/index.html#standard-query-parameters">MLflow Deployments documentation</a> for more information about the request format for different model endpoint types.</p></td>
</tr>
<tr class="row-even"><td><p>A list of input strings.</p></td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</td>
<td><p>The <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> also accepts a list input.</p></td>
</tr>
<tr class="row-odd"><td><p>A list of request payload (dictionary).</p></td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Please answer.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="c1"># ... more dictionary records</span>
<span class="p">]</span>
</pre></div>
</div>
</td>
<td><p>Similarly to Pandas DataFrame input, the dictionary should have the correct request format for your model endpoint.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="passing-inference-parameters">
<h4>Passing Inference Parameters<a class="headerlink" href="#passing-inference-parameters" title="Permalink to this headline"> </a></h4>
<p>You can pass additional inference parameters such as <code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>, <code class="docutils literal notranslate"><span class="pre">temperature</span></code>, <code class="docutils literal notranslate"><span class="pre">n</span></code>, to the model endpoint by setting the <code class="docutils literal notranslate"><span class="pre">inference_params</span></code> argument in <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>. The <code class="docutils literal notranslate"><span class="pre">inference_params</span></code> argument is a dictionary that contains the parameters to be passed to the model endpoint. The specified parameters are used for all the input record in the evaluation dataset.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When your input is a dictionary format that represents request payload, it can also include the parameters like <code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>. If there are overlapping parameters in both the <code class="docutils literal notranslate"><span class="pre">inference_params</span></code> and the input data, the values in the <code class="docutils literal notranslate"><span class="pre">inference_params</span></code> will take precedence.</p>
</div>
</div>
<div class="section" id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this headline"> </a></h4>
<p><strong>Chat Endpoint hosted by a local</strong> <a class="reference external" href="../deployments/index.html">MLflow AI Gateway</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.deployments</span> <span class="kn">import</span> <span class="n">set_deployments_target</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Point the client to the local MLflow AI Gateway</span>
<span class="n">set_deployments_target</span><span class="p">(</span><span class="s2">&quot;http://localhost:5000&quot;</span><span class="p">)</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="c1"># Input data must be a string column and named &quot;inputs&quot;.</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="c1"># Additional ground truth data for evaluating the answer</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;MLflow is an open-source platform ....&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Apache Spark is an open-source, ...&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;endpoints:/my-chat-endpoint&quot;</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">inference_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><strong>Completion Endpoint hosted on</strong> <a class="reference external" href="https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html">Databricks Foundation Models API</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.deployments</span> <span class="kn">import</span> <span class="n">set_deployments_target</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Point the client to Databricks Foundation Models API</span>
<span class="n">set_deployments_target</span><span class="p">(</span><span class="s2">&quot;databricks&quot;</span><span class="p">)</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="c1"># Input data must be a string column and named &quot;inputs&quot;.</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Write 3 reasons why you should use MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Can you explain the difference between classification and regression?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;endpoints:/databricks-mpt-7b-instruct&quot;</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">inference_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Evaluating <a class="reference external" href="https://docs.databricks.com/en/generative-ai/external-models/index.html">External Models in Databricks Model Serving</a> can be done in the same way, you just need to specify the different URI that points to the serving endpoint like <code class="docutils literal notranslate"><span class="pre">&quot;endpoints:/your-chat-endpoint&quot;</span></code>.</p>
</div>
</div>
<div class="section" id="evaluating-with-a-static-dataset">
<span id="llm-eval-static-dataset"></span><h3>Evaluating with a Static Dataset<a class="headerlink" href="#evaluating-with-a-static-dataset" title="Permalink to this headline"> </a></h3>
<p>For MLflow &gt;= 2.8.0, <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> supports evaluating a static dataset without specifying a model.
This is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and
want to evaluate the static dataset without re-running the model.</p>
<p>If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the
top-level <code class="docutils literal notranslate"><span class="pre">predictions</span></code> parameter in <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;What is MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is Spark?&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;ground_truth&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. &quot;</span>
            <span class="s2">&quot;It was developed by Databricks, a company that specializes in big data and machine learning solutions. &quot;</span>
            <span class="s2">&quot;MLflow is designed to address the challenges that data scientists and machine learning engineers &quot;</span>
            <span class="s2">&quot;face when developing, training, and deploying machine learning models.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Apache Spark is an open-source, distributed computing system designed for big data processing and &quot;</span>
            <span class="s2">&quot;analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, &quot;</span>
            <span class="s2">&quot;offering improvements in speed and ease of use. Spark provides libraries for various tasks such as &quot;</span>
            <span class="s2">&quot;data ingestion, processing, and analysis through its components like Spark SQL for structured data, &quot;</span>
            <span class="s2">&quot;Spark Streaming for real-time data processing, and MLlib for machine learning tasks&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;predictions&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;MLflow is an open-source platform that provides handy tools to manage Machine Learning workflow &quot;</span>
            <span class="s2">&quot;lifecycle in a simple way&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Spark is a popular open-source distributed computing system designed for big data processing and analytics.&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">()],</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;See aggregated evaluation results below: </span><span class="se">\n</span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">eval_table</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s2">&quot;eval_results_table&quot;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;See evaluation table below: </span><span class="se">\n</span><span class="si">{</span><span class="n">eval_table</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="viewing-evaluation-results">
<h2>Viewing Evaluation Results<a class="headerlink" href="#viewing-evaluation-results" title="Permalink to this headline"> </a></h2>
<div class="section" id="view-evaluation-results-via-code">
<h3>View Evaluation Results via Code<a class="headerlink" href="#view-evaluation-results-via-code" title="Permalink to this headline"> </a></h3>
<p><code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> returns the evaluation results as an <a class="reference internal" href="../../python_api/mlflow.models.html#mlflow.models.EvaluationResult" title="mlflow.models.EvaluationResult"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.EvaluationResult()</span></code></a> instance.
To see the score on selected metrics, you can check:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">metrics</span></code>: stores the aggregated results, like average/variance across the evaluation dataset. Let’s take a second
pass on the code example above and focus on printing out the aggregated results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">()],</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;See aggregated evaluation results below: </span><span class="se">\n</span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tables[&quot;eval_results_table&quot;]</span></code>: stores the per-row evaluation results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
        <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">genai</span><span class="o">.</span><span class="n">answer_similarity</span><span class="p">()],</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;See per-data evaluation results below: </span><span class="se">\n</span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="s1">&#39;eval_results_table&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="view-evaluation-results-via-the-mlflow-ui">
<h3>View Evaluation Results via the MLflow UI<a class="headerlink" href="#view-evaluation-results-via-the-mlflow-ui" title="Permalink to this headline"> </a></h3>
<p>Your evaluation result is automatically logged into MLflow server, so you can view your evaluation results directly from the
MLflow UI. To view the evaluation results on MLflow UI, please follow the steps below:</p>
<ol class="arabic simple">
<li><p>Go to the experiment view of your MLflow experiment.</p></li>
<li><p>Select the “Evaluation” tab.</p></li>
<li><p>Select the runs you want to check evaluation results.</p></li>
<li><p>Select the metrics from the dropdown menu on the right side.</p></li>
</ol>
<p>Please see the screenshot below for clarity:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/llm_evaluate_experiment_view.png"><img alt="Demo UI of MLflow evaluate" src="../../_images/llm_evaluate_experiment_view.png" style="width: 1024px;" /></a>
</div>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../deployments/uc_integration.html" class="btn btn-neutral" title="Unity Catalog Integration" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../prompt-engineering/index.html" class="btn btn-neutral" title="Prompt Engineering UI (Experimental)" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.19.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>