
  

<!DOCTYPE html>
<!-- source: docs/source/llms/openai/guide/index.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenAI within MLflow &mdash; MLflow 2.14.2.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/openai/guide/index.html">
  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="MLflow 2.14.2.dev0 documentation" href="../../../index.html"/>
        <link rel="up" title="MLflow OpenAI Flavor" href="../index.html"/>
        <link rel="next" title="MLflow Sentence-Transformers Flavor" href="/../../sentence-transformers/index.html"/>
        <link rel="prev" title="Advanced Tutorial: Embeddings Support with OpenAI in MLflow" href="/../notebooks/openai-embeddings-generation.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.14.2.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../index.html" class="main-navigation-home"><img src="../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id2">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../transformers/index.html">MLflow Transformers Flavor</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">MLflow OpenAI Flavor</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../index.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#what-makes-this-integration-so-special">What makes this Integration so Special?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#features">Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#getting-started-with-the-mlflow-openai-flavor-tutorials-and-guides">Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#id1">Detailed Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence-transformers/index.html">MLflow Sentence-Transformers Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../langchain/index.html">MLflow LangChain Flavor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../index.html#explore-the-native-llm-flavors">Explore the Native LLM Flavors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#id5">LLM Tracking in MLflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">MLflow OpenAI Flavor</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>OpenAI within MLflow</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/openai/guide/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="openai-within-mlflow">
<h1>OpenAI within MLflow<a class="headerlink" href="#openai-within-mlflow" title="Permalink to this headline"> </a></h1>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">openai</span></code> flavor is under active development and is marked as Experimental. Public APIs may change and new features are
subject to be added as additional functionality is brought to the flavor.</p>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"> </a></h2>
<p>The integration of OpenAI’s advanced language models within MLflow opens up new frontiers in creating and using NLP-based applications. It enables users to harness
the cutting-edge capabilities of models like GPT-4 for varied tasks, ranging from conversational AI to complex text analysis
and embeddings generation. This integration is a leap forward in making advanced NLP accessible and manageable within a robust framework like MLflow.</p>
</div>
<div class="section" id="beyond-simple-deployment-building-powerful-nlp-applications-with-openai-and-mlflow">
<h2>Beyond Simple Deployment: Building Powerful NLP Applications with OpenAI and MLflow<a class="headerlink" href="#beyond-simple-deployment-building-powerful-nlp-applications-with-openai-and-mlflow" title="Permalink to this headline"> </a></h2>
<p>While the openai flavor within MLflow simplifies the logging and deployment of OpenAI models, its true potential lies in unlocking the full power of NLP
applications. By seamlessly integrating with MLflow, you can:</p>
<div class="section" id="craft-task-specific-services">
<h3>Craft Task-Specific Services<a class="headerlink" href="#craft-task-specific-services" title="Permalink to this headline"> </a></h3>
<p>Raw access to a large language model doesn’t guarantee a valuable service. While powerful, unprompted models can be overly general, leading to unintended
outputs or inappropriate responses for the intent of the application. MLflow enables users to tailor models for specific tasks, achieving desired functionalities
while ensuring context and control.</p>
<p>This allows you to:</p>
<ul class="simple">
<li><p><strong>Define prompts and parameters</strong>: Instead of relying on open-ended inputs, you can define specific prompts and parameters that guide the model’s responses, focusing its capabilities on the desired task.</p></li>
<li><p><strong>Save and deploy customized models</strong>: The saved models, along with their prompts and parameters, can be easily deployed and shared, ensuring consistent behavior and performance.</p></li>
<li><p><strong>Perform champion/challenger evaluations</strong>: MLflow allows users to easily compare different prompts, parameters, and deployment configurations, facilitating the selection of the most effective model for a specific task.</p></li>
</ul>
</div>
<div class="section" id="simplify-deployment-and-comparison">
<h3>Simplify Deployment and Comparison<a class="headerlink" href="#simplify-deployment-and-comparison" title="Permalink to this headline"> </a></h3>
<p>MLflow streamlines the deployment process, enabling you to:</p>
<ul class="simple">
<li><p><strong>Package and deploy models as applications</strong>: The openai flavor simplifies model packaging, including prompts, configuration parameters, and inference parameters, into a single, portable artifact.</p></li>
<li><p><strong>Compare different approaches</strong>: With consistent packaging, you can easily compare different models, prompts, configurations, and deployment options, facilitating informed decision-making.</p></li>
<li><p><strong>Leverage MLflow’s ecosystem</strong>: MLflow integrates with various tools and platforms, allowing users to deploy models on diverse environments, from cloud platforms to local servers.</p></li>
</ul>
</div>
</div>
<div class="section" id="advanced-prompt-engineering-and-version-tracking-with-mlflow-and-openai-unleashing-the-true-potential-of-llms">
<h2>Advanced Prompt Engineering and Version Tracking with MLflow and OpenAI: Unleashing the True Potential of LLMs<a class="headerlink" href="#advanced-prompt-engineering-and-version-tracking-with-mlflow-and-openai-unleashing-the-true-potential-of-llms" title="Permalink to this headline"> </a></h2>
<p>The integration of MLflow and OpenAI marks a paradigm shift in the field of prompt engineering for large language models (LLMs). While basic prompts can
enable rudimentary functionalities, this powerful combination unlocks the full potential of LLMs, empowering developers and data scientists to meticulously
craft and refine prompts, ushering in a new era of targeted and impactful applications.</p>
<div class="section" id="beyond-the-basics-embracing-iterative-experimentation">
<h3>Beyond the Basics: Embracing Iterative Experimentation<a class="headerlink" href="#beyond-the-basics-embracing-iterative-experimentation" title="Permalink to this headline"> </a></h3>
<p>Forget static prompts and limited applications! MLflow and OpenAI revolutionize the process by facilitating iterative experimentation through:</p>
<ul class="simple">
<li><p><strong>Tracking and Comparison</strong>: MLflow logs and meticulously tracks every iteration of a prompt alongside its performance metrics. This allows for a granular comparison of different versions, enabling informed decisions and identification of the most effective prompts.</p></li>
<li><p><strong>Version Control for Reproducible Experimentation</strong>: Each prompt iteration is safely stored and version-controlled within MLflow. This allows for easy rollback and comparison, fostering experimentation and refinement while ensuring reproducibility, a crucial aspect of scientific advancement.</p></li>
<li><p><strong>Flexible Parameterization</strong>: MLflow enables control over which parameters are permitted to be modified at inference time, giving you the power to control creativity (temperature) and maximum token length (for cost).</p></li>
</ul>
</div>
<div class="section" id="refining-for-optimum-results-a-b-testing-and-fine-tuning">
<h3>Refining for Optimum Results: A/B Testing and Fine-Tuning<a class="headerlink" href="#refining-for-optimum-results-a-b-testing-and-fine-tuning" title="Permalink to this headline"> </a></h3>
<p>MLflow and OpenAI empower you to push the boundaries of LLM performance by:</p>
<ul class="simple">
<li><p><strong>A/B Testing for Optimal Prompt Selection</strong>: Perform efficient A/B testing of different prompt variations and parameter configurations. This allows for the identification of the most effective combination for specific tasks and user profiles, leading to remarkable performance gains.</p></li>
<li><p><strong>Tailoring Prompts for Desired Outcomes</strong>: Iterative and organized experimentation allows you to focus on what makes the most sense for your applications. Whether you prioritize factual accuracy, creative expression, or conversational fluency, MLflow and OpenAI empower you to tailor prompts to optimize specific performance metrics. This ensures that your LLM applications deliver the desired results, time and time again.</p></li>
</ul>
</div>
<div class="section" id="collaboration-and-sharing-fueling-innovation-and-progress">
<h3>Collaboration and Sharing: Fueling Innovation and Progress<a class="headerlink" href="#collaboration-and-sharing-fueling-innovation-and-progress" title="Permalink to this headline"> </a></h3>
<p>The power of MLflow and OpenAI extends beyond individual projects. By facilitating collaboration and sharing, they accelerate the advancement of LLM applications:</p>
<ul class="simple">
<li><p><strong>Shareable Artifacts for Collaborative Innovation</strong>: MLflow packages prompts, parameters, model versions, and performance metrics into shareable artifacts. This enables researchers and developers to collaborate seamlessly, leveraging each other’s insights and refined prompts to accelerate progress.</p></li>
</ul>
</div>
<div class="section" id="leveraging-mlflow-for-optimized-prompt-engineering">
<h3>Leveraging MLflow for Optimized Prompt Engineering<a class="headerlink" href="#leveraging-mlflow-for-optimized-prompt-engineering" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><strong>Iterative Improvement</strong>: MLflow’s tracking system supports an iterative approach to prompt engineering. By logging each experiment, users can incrementally refine their prompts, driving towards the most effective model interaction.</p></li>
<li><p><strong>Collaborative Experimentation</strong>: MLflow’s collaborative features enable teams to share and discuss prompt versions and experiment results, fostering a collaborative environment for prompt development.</p></li>
</ul>
</div>
<div class="section" id="real-world-impact">
<h3>Real-World Impact<a class="headerlink" href="#real-world-impact" title="Permalink to this headline"> </a></h3>
<p>In real-world applications, the ability to track and refine prompts using MLflow and OpenAI leads to more accurate, reliable, and efficient language model
implementations. Whether in customer service chatbots, content generation, or complex decision support systems, the meticulous management of prompts
and model versions directly translates to enhanced performance and user experience.</p>
<p>This integration not only simplifies the complexities of working with advanced LLMs but also opens up new avenues for innovation in NLP applications,
ensuring that each prompt-driven interaction is as effective and impactful as possible.</p>
</div>
</div>
<div class="section" id="direct-openai-service-usage">
<h2>Direct OpenAI Service Usage<a class="headerlink" href="#direct-openai-service-usage" title="Permalink to this headline"> </a></h2>
<p>Direct usage of OpenAI’s service through MLflow allows for seamless interaction with the latest GPT models for a variety of NLP tasks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models.signature</span> <span class="kn">import</span> <span class="n">ModelSignature</span>
<span class="kn">from</span> <span class="nn">mlflow.types.schema</span> <span class="kn">import</span> <span class="n">ColSpec</span><span class="p">,</span> <span class="n">ParamSchema</span><span class="p">,</span> <span class="n">ParamSpec</span><span class="p">,</span> <span class="n">Schema</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;mlflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="c1"># Uncomment the following lines to run this script without using a real OpenAI API key.</span>
<span class="c1"># os.environ[&quot;MLFLOW_TESTING&quot;] = &quot;true&quot;</span>
<span class="c1"># os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;test&quot;</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>


<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Single variable</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;cats&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;dogs&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="n">list_of_strings</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_strings</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Multiple variables</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a </span><span class="si">{adjective}</span><span class="s2"> joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;funny&quot;</span><span class="p">,</span> <span class="s2">&quot;scary&quot;</span><span class="p">],</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cats&quot;</span><span class="p">,</span> <span class="s2">&quot;dogs&quot;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>


<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="s2">&quot;funny&quot;</span><span class="p">,</span> <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;cats&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="s2">&quot;scary&quot;</span><span class="p">,</span> <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;dogs&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Multiple prompts</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are </span><span class="si">{person}</span><span class="s2">&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Let me hear your thoughts on </span><span class="si">{topic}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Elon Musk&quot;</span><span class="p">,</span> <span class="s2">&quot;Jeff Bezos&quot;</span><span class="p">],</span>
        <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;AI&quot;</span><span class="p">,</span> <span class="s2">&quot;ML&quot;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="s2">&quot;Elon Musk&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;AI&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="s2">&quot;Jeff Bezos&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;ML&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>


<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># No input variables</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are Elon Musk&quot;</span><span class="p">}],</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="n">list_of_strings</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_strings</span><span class="p">))</span>


<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Inference parameters with chat completions</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">ModelSignature</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">)]),</span>
            <span class="n">outputs</span><span class="o">=</span><span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">)]),</span>
            <span class="n">params</span><span class="o">=</span><span class="n">ParamSchema</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">ParamSpec</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">),</span>
                <span class="p">]</span>
            <span class="p">),</span>
        <span class="p">),</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}))</span>
</pre></div>
</div>
</div>
<div class="section" id="azure-openai-service-integration">
<h2>Azure OpenAI Service Integration<a class="headerlink" href="#azure-openai-service-integration" title="Permalink to this headline"> </a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">openai</span></code> flavor supports logging models that use the <a class="reference external" href="https://azure.microsoft.com/en-us/products/ai-services/openai-service">Azure OpenAI Service</a>.
There are a few notable differences between the Azure OpenAI Service and the OpenAI Service that need to be considered when logging models that target Azure endpoints.</p>
<div class="section" id="environment-configuration-for-azure-integration">
<h3>Environment Configuration for Azure Integration<a class="headerlink" href="#environment-configuration-for-azure-integration" title="Permalink to this headline"> </a></h3>
<p>To successfully log a model targeting Azure OpenAI Service, specific environment variables are essential for authentication and functionality.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following environment variables contain <strong>highly sensitive access keys</strong>. Ensure that you do not commit these values to source control or declare them in an interactive
environment. Environment variables should be set from within your terminal via an <code class="docutils literal notranslate"><span class="pre">export</span></code> command, an addition to your user profile configurations (i.e., .bashrc or .zshrc),
or set through your IDE’s environment variable configuration. Please do not leak your credentials.</p>
</div>
<ul class="simple">
<li><p><strong>OPENAI_API_KEY</strong>: The API key for the Azure OpenAI Service. This can be found in the Azure Portal under the “Keys and Endpoint” section of the “Keys and Endpoint” tab. You can use either <code class="docutils literal notranslate"><span class="pre">KEY1</span></code> or <code class="docutils literal notranslate"><span class="pre">KEY2</span></code>.</p></li>
<li><p><strong>OPENAI_API_BASE</strong>: The base endpoint for your Azure OpenAI resource (e.g., <code class="docutils literal notranslate"><span class="pre">https://&lt;your-service-name&gt;.openai.azure.com/</span></code>). Within the Azure OpenAI documentation and guides, this key is referred to as <code class="docutils literal notranslate"><span class="pre">AZURE_OPENAI_ENDPOINT</span></code> or simply <code class="docutils literal notranslate"><span class="pre">ENDPOINT</span></code>.</p></li>
<li><p><strong>OPENAI_API_VERSION</strong>: The API version to use for the Azure OpenAI Service. More information can be found in the <a class="reference external" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/reference">Azure OpenAI documentation</a>, including up-to-date lists of supported versions.</p></li>
<li><p><strong>OPENAI_API_TYPE</strong>: If using Azure OpenAI endpoints, this value should be set to <code class="docutils literal notranslate"><span class="pre">&quot;azure&quot;</span></code>.</p></li>
<li><p><strong>OPENAI_DEPLOYMENT_NAME</strong>: The deployment name that you chose when you deployed the model in Azure. To learn more, visit the <a class="reference external" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal">Azure OpenAI deployment documentation</a>.</p></li>
</ul>
</div>
<div class="section" id="azure-openai-service-in-mlflow">
<h3>Azure OpenAI Service in MLflow<a class="headerlink" href="#azure-openai-service-in-mlflow" title="Permalink to this headline"> </a></h3>
<p>Integrating Azure OpenAI models within MLflow follows similar procedures to direct OpenAI service usage, with additional Azure-specific configurations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Set environment variables for Azure OpenAI service</span>
<span class="sd">export OPENAI_API_KEY=&quot;&lt;AZURE OPENAI KEY&gt;&quot;</span>
<span class="sd"># OPENAI_API_BASE should be the endpoint of your Azure OpenAI resource</span>
<span class="sd"># e.g. https://&lt;service-name&gt;.openai.azure.com/</span>
<span class="sd">export OPENAI_API_BASE=&quot;&lt;AZURE OPENAI BASE&gt;&quot;</span>
<span class="sd"># OPENAI_API_VERSION e.g. 2023-05-15</span>
<span class="sd">export OPENAI_API_VERSION=&quot;&lt;AZURE OPENAI API VERSION&gt;&quot;</span>
<span class="sd">export OPENAI_API_TYPE=&quot;azure&quot;</span>
<span class="sd">export OPENAI_DEPLOYMENT_NAME=&quot;&lt;AZURE OPENAI DEPLOYMENT ID OR NAME&gt;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="c1"># Your Azure OpenAI model e.g. gpt-3.5-turbo</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;&lt;YOUR AZURE OPENAI MODEL&gt;&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
    <span class="p">)</span>

<span class="c1"># Load native OpenAI model</span>
<span class="n">native_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">deployment_id</span><span class="o">=</span><span class="n">native_model</span><span class="p">[</span><span class="s2">&quot;deployment_id&quot;</span><span class="p">],</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">native_model</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">completion</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>


<span class="c1"># Load as Pyfunc model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;cats&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;dogs&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="n">list_of_strings</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_strings</span><span class="p">))</span>

<span class="n">list_of_strings</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_strings</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="openai-autologging">
<h2>OpenAI Autologging<a class="headerlink" href="#openai-autologging" title="Permalink to this headline"> </a></h2>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Autologging is only supported for OpenAI &gt;= 1.17.</p>
</div>
<p>OpenAI autologging integration allows for seamless logging of OpenAI models within MLflow.
MLflow will automatically log a model upon calling the OpenAI API.
Each time a request is made, the inputs and outputs will be logged as artifacts.</p>
<p>Currently, the following tasks are supported for autologging:</p>
<ul class="simple">
<li><p>Chat completions</p></li>
<li><p>Completions</p></li>
<li><p>Embeddings</p></li>
</ul>
<p>If streaming is enabled on chat completions and completions, the output artifact will contain
all streaming chunks as a list of JSON objects.</p>
<p>Note that async clients like <cite>openai.AsyncOpenAI()</cite> are not supported for autologging.</p>
<p>Below is an example script that logs the model, inputs and outputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">openai</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span>
    <span class="n">log_input_examples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">log_model_signatures</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="s2">&quot;openai_model&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;your-api-key&quot;</span><span class="p">)</span>

<span class="c1"># First inference.</span>
<span class="c1"># The input and output will be logged as artifacts in `artifacts/artifacts-&lt;session_id&gt;-0/`.</span>
<span class="c1"># Also, the model will be logged as `models:/openai_model/1`.</span>
<span class="n">response1</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;tell me a joke&quot;</span><span class="p">}],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Second inference.</span>
<span class="c1"># The input and output will be logged as artifacts in `artifacts/artifacts-&lt;session_id&gt;-1/`.</span>
<span class="n">response2</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;tell me the capital of France&quot;</span><span class="p">}],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For more examples, please click <a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/openai/autologging">here</a>.</p>
</div>
<div class="section" id="next-steps-in-your-nlp-journey">
<h2>Next Steps in Your NLP Journey<a class="headerlink" href="#next-steps-in-your-nlp-journey" title="Permalink to this headline"> </a></h2>
<p>We invite you to harness the combined power of MLflow and OpenAI for developing innovative NLP applications. Whether it’s creating interactive
AI-driven platforms, enhancing data analysis with deep NLP insights, or exploring new frontiers in AI, this integration serves as a robust foundation
for your explorations</p>
</div>
<div class="section" id="supplementary-learnings">
<h2>Supplementary Learnings<a class="headerlink" href="#supplementary-learnings" title="Permalink to this headline"> </a></h2>
<p>If you’re a bit curious about what really sets apart OpenAI’s GPT models from other language models, we’ve included a brief (and heavily simplified) overview of
their training process below.
This is but one small aspect of why they’re so good and capable of responding in such a human-like manner, but it’s a fascintating insight into how different the
fine-tuning process is for these models as compared to the more familiar process of traditional supervised machine learning.</p>
<div class="section" id="rlhf-in-gpt-models">
<h3>RLHF in GPT Models<a class="headerlink" href="#rlhf-in-gpt-models" title="Permalink to this headline"> </a></h3>
<p>One of the defining features of OpenAI’s GPT models is their training process, particularly the use of Reinforcement Learning from Human Feedback
(RLHF). This methodology sets GPT models apart from traditional language models in several ways (although they are not the only organization to use this
strategy, it is a key process component that greatly helps to enhance the quality of their services).</p>
<div class="section" id="the-rlhf-process">
<h4>The RLHF Process<a class="headerlink" href="#the-rlhf-process" title="Permalink to this headline"> </a></h4>
<ol class="arabic simple">
<li><p><strong>Supervised Fine-Tuning (SFT)</strong>: Initially, GPT models undergo supervised fine-tuning using a large dataset of text. This process imparts the basic understanding of language and context.</p></li>
<li><p><strong>Reward Modeling (RM)</strong>: Human trainers review the model’s outputs and rate them based on criteria such as relevance, accuracy, and safety. This feedback is used to create a ‘reward model’—a system that evaluates the quality of the model’s responses.</p></li>
<li><p><strong>Proximal Policy Optimization (PPO)</strong>: In this stage, the model is trained using reinforcement learning techniques, guided by the reward model. The model learns to generate responses that are more aligned with the values and preferences as judged by human trainers.</p></li>
<li><p><strong>Iterative Improvement</strong>: The model undergoes continuous refinement through human feedback, ensuring that it evolves and adapts to produce responses that are aligned with the feedback preferences provided by the human reviewers.</p></li>
</ol>
</div>
<div class="section" id="why-rlhf-matters">
<h4>Why RLHF Matters<a class="headerlink" href="#why-rlhf-matters" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p><strong>Human-Like Responses</strong>: RLHF enables GPT models to generate responses that closely mimic human thought processes, making them more relatable and effective in practical applications.</p></li>
<li><p><strong>Safety and Relevance</strong>: Through human feedback, GPT models learn to avoid generating harmful or irrelevant content, thereby increasing their reliability and applicability.</p></li>
<li><p><strong>Cost-Effective Training</strong>: RLHF allows for more efficient and cost-effective training compared to extensively curating the training dataset to ensure that only desired outputs are generated.</p></li>
</ul>
<div class="figure align-center" id="id1">
<a class="reference internal image-reference" href="../../../_images/RLHF-architecture.png"><img alt="A primer on RLHF for sophisticated LLM training" src="../../../_images/RLHF-architecture.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-text">Simplified overview of RLHF</span><a class="headerlink" href="#id1" title="Permalink to this image"> </a></p>
</div>
</div>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../notebooks/openai-embeddings-generation.html" class="btn btn-neutral" title="Advanced Tutorial: Embeddings Support with OpenAI in MLflow" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../../sentence-transformers/index.html" class="btn btn-neutral" title="MLflow Sentence-Transformers Flavor" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../',
      VERSION:'2.14.2.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>