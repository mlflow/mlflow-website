

<!DOCTYPE html>
<!-- source: docs/source/llms/notebooks/chat-model-tool-calling.ipynb -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Build a tool-calling model with mlflow.pyfunc.ChatModel</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/notebooks/chat-model-tool-calling.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.17.3.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="LLMs" href="../index.html"/>
        <link rel="next" title="Introduction to MLflow Tracing" href="/../tracing/index.html"/>
        <link rel="prev" title="Tutorial: Custom GenAI Models using ChatModel" href="/../chat-model-guide/index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="../../None"></script>
<script type="text/javascript" src="../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-genai-applications-in-mlflow">Tutorials and Use Case Guides for GenAI applications in MLflow</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../custom-pyfunc-for-llms/index.html">Deploying Advanced LLMs with Custom PyFuncs in MLflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm-evaluate/notebooks/index.html">LLM Evaluation Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chat-model-guide/index.html">Tutorial: Custom GenAI Models using ChatModel</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Build a tool-calling model with <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.ChatModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Environment-setup">Environment setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Step-1:-Creating-the-tool-definition">Step 1: Creating the tool definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Step-2:-Implementing-the-tool">Step 2: Implementing the tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Step-3:-Implementing-the-predict-method">Step 3: Implementing the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Step-4-(optional,-but-recommended):-Enable-tracing-for-the-model">Step 4 (optional, but recommended): Enable tracing for the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Step-5:-Logging-the-model">Step 5: Logging the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Using-the-model-for-inference">Using the model for inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Serving-the-model">Serving the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">MLflow AI Gateway for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">Prompt Engineering UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id5">LLM Tracking in MLflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Build a tool-calling model with <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.ChatModel</span></code></li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/notebooks/chat-model-tool-calling.ipynb" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Build-a-tool-calling-model-with-mlflow.pyfunc.ChatModel">
<h1>Build a tool-calling model with <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.ChatModel</span></code><a class="headerlink" href="#Build-a-tool-calling-model-with-mlflow.pyfunc.ChatModel" title="Permalink to this headline"> </a></h1>
<a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/notebooks/chat-model-tool-calling.ipynb" class="notebook-download-btn"><i class="fas fa-download"></i>Download this Notebook</a><p>Welcome to the notebook tutorial on building a simple tool calling model using the <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel">mlflow.pyfunc.ChatModel</a> wrapper. ChatModel is a subclass of MLflow’s highly customizable <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel">PythonModel</a>, which was specifically designed to make creating GenAI workflows easier.</p>
<p>Briefly, here are some of the benefits of using ChatModel: 1. No need to define a complex signature! Chat models often accept complex inputs with many levels of nesting, and this can be cumbersome to define yourself. 2. Support for JSON / dict inputs (no need to wrap inputs or convert to Pandas DataFrame) 3. Includes the use of Dataclasses for defining expected inputs / outputs for a simplified development experience</p>
<p>For a more in-depth exploration of ChatModel, please check out the <a class="reference external" href="https://mlflow.org/docs/latest/llms/chat-model-guide/index.html">detailed guide</a>.</p>
<p>In this tutorial, we’ll be building a simple OpenAI wrapper that makes use of the tool calling support (released in MLflow 2.17.0).</p>
<div class="section" id="Environment-setup">
<h2>Environment setup<a class="headerlink" href="#Environment-setup" title="Permalink to this headline"> </a></h2>
<p>First, let’s set up the environment. We’ll need the OpenAI Python SDK, as well as MLflow &gt;= 2.17.0. We’ll also need to set our OpenAI API key in order to use the SDK.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install &#39;mlflow&gt;=2.17.0&#39; &#39;openai&gt;=1.0&#39; -qq
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Note: you may need to restart the kernel to use updated packages.
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">getpass</span> <span class="kn">import</span> <span class="n">getpass</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">getpass</span><span class="p">(</span><span class="s2">&quot;Enter your OpenAI API key: &quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-1:-Creating-the-tool-definition">
<h2>Step 1: Creating the tool definition<a class="headerlink" href="#Step-1:-Creating-the-tool-definition" title="Permalink to this headline"> </a></h2>
<p>Let’s begin to define our model! As mentioned in the introduction, we’ll be subclassing <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.ChatModel</span></code>. For this example, we’ll build a toy model that uses a tool to retrieve the weather for a given city.</p>
<p>The first step is to create a tool definition that we can pass to OpenAI. We do this by using <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.FunctionToolDefinition">mlflow.types.llm.FunctionToolDefinition</a> to describe the parameters that our tool accepts. The format of this dataclass is aligned with the OpenAI spec:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.types.llm</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FunctionToolDefinition</span><span class="p">,</span>
    <span class="n">ParamProperty</span><span class="p">,</span>
    <span class="n">ToolParamsSchema</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">class</span> <span class="nc">WeatherModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">ChatModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># a sample tool definition. we use the `FunctionToolDefinition`</span>
        <span class="c1"># class to describe the name and expected params for the tool.</span>
        <span class="c1"># for this example, we&#39;re defining a simple tool that returns</span>
        <span class="c1"># the weather for a given city.</span>
        <span class="n">weather_tool</span> <span class="o">=</span> <span class="n">FunctionToolDefinition</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Get weather information&quot;</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">ToolParamsSchema</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="n">ParamProperty</span><span class="p">(</span>
                        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
                        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;City name to get weather information for&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
            <span class="p">),</span>
            <span class="c1"># make sure to call `to_tool_definition()` to convert the `FunctionToolDefinition`</span>
            <span class="c1"># to a `ToolDefinition` object. this step is necessary to normalize the data format,</span>
            <span class="c1"># as multiple types of tools (besides just functions) might be available in the future.</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_tool_definition</span><span class="p">()</span>

        <span class="c1"># OpenAI expects tools to be provided as a list of dictionaries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">weather_tool</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-2:-Implementing-the-tool">
<h2>Step 2: Implementing the tool<a class="headerlink" href="#Step-2:-Implementing-the-tool" title="Permalink to this headline"> </a></h2>
<p>Now that we have a definition for the tool, we need to actually implement it. For the purposes of this tutorial, we’re just going to mock a response, but the implementation can be arbitrary—you might make an API call to an actual weather service, for example.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">WeatherModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">ChatModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weather_tool</span> <span class="o">=</span> <span class="n">FunctionToolDefinition</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Get weather information&quot;</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">ToolParamsSchema</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="n">ParamProperty</span><span class="p">(</span>
                        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
                        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;City name to get weather information for&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
            <span class="p">),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_tool_definition</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">weather_tool</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()]</span>

        <span class="k">def</span> <span class="nf">get_weather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
            <span class="c1"># in a real-world scenario, the implementation might be more complex</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;It&#39;s sunny in </span><span class="si">{</span><span class="n">city</span><span class="si">}</span><span class="s2">, with a temperature of 20C&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-3:-Implementing-the-predict-method">
<h2>Step 3: Implementing the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method<a class="headerlink" href="#Step-3:-Implementing-the-predict-method" title="Permalink to this headline"> </a></h2>
<p>The next thing we need to do is define a <code class="docutils literal notranslate"><span class="pre">predict()</span></code> function that accepts the following arguments:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModelContext">PythonModelContext</a> (not used in this tutorial)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">messages</span></code>: List[<a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatMessage">ChatMessage</a>]. This is the chat input that the model uses for generation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>: <a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatParams">ChatParams</a>. These are commonly used params used to configure the chat model, e.g. <code class="docutils literal notranslate"><span class="pre">temperature</span></code>, <code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>, etc. This is where the tool specifications can be found.</p></li>
</ol>
<p>This is the function that will ultimately be called during inference.</p>
<p>For the implementation, we’ll simply forward the user’s input to OpenAI, and provide the <code class="docutils literal notranslate"><span class="pre">get_weather</span></code> tool as an option for the LLM to use if it chooses to do so. If we receive a tool call request, we’ll call the <code class="docutils literal notranslate"><span class="pre">get_weather()</span></code> function and return the response back to OpenAI. We’ll need to use what we’ve defined in the previous two steps in order to do this.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.types.llm</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ChatMessage</span><span class="p">,</span>
    <span class="n">ChatParams</span><span class="p">,</span>
    <span class="n">ChatResponse</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">class</span> <span class="nc">WeatherModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">ChatModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weather_tool</span> <span class="o">=</span> <span class="n">FunctionToolDefinition</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Get weather information&quot;</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">ToolParamsSchema</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="n">ParamProperty</span><span class="p">(</span>
                        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
                        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;City name to get weather information for&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
            <span class="p">),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_tool_definition</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">weather_tool</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()]</span>

    <span class="k">def</span> <span class="nf">get_weather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;It&#39;s sunny in </span><span class="si">{}</span><span class="s2">, with a temperature of 20C&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>

    <span class="c1"># the core method that needs to be implemented. this function</span>
    <span class="c1"># will be called every time a user sends messages to our model</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">],</span> <span class="n">params</span><span class="p">:</span> <span class="n">ChatParams</span><span class="p">):</span>
        <span class="c1"># instantiate the OpenAI client</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

        <span class="c1"># convert the messages to a format that the OpenAI API expects</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span>

        <span class="c1"># call the OpenAI API</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="c1"># pass the tools in the request</span>
            <span class="n">tools</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># if OpenAI returns a tool_calling response, then we call</span>
        <span class="c1"># our tool. otherwise, we just return the response as is</span>
        <span class="n">tool_calls</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span>
        <span class="k">if</span> <span class="n">tool_calls</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Received a tool call, calling the weather tool...&quot;</span><span class="p">)</span>

            <span class="c1"># for this example, we only provide the model with one tool,</span>
            <span class="c1"># so we can assume the tool call is for the weather tool. if</span>
            <span class="c1"># we had more, we&#39;d need to check the name of the tool that</span>
            <span class="c1"># was called</span>
            <span class="n">city</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">tool_calls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">arguments</span><span class="p">)[</span><span class="s2">&quot;city&quot;</span><span class="p">]</span>
            <span class="n">tool_call_id</span> <span class="o">=</span> <span class="n">tool_calls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">id</span>

            <span class="c1"># call the tool and construct a new chat message</span>
            <span class="n">tool_response</span> <span class="o">=</span> <span class="n">ChatMessage</span><span class="p">(</span>
                <span class="n">role</span><span class="o">=</span><span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_weather</span><span class="p">(</span><span class="n">city</span><span class="p">),</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call_id</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

            <span class="c1"># send another request to the API, making sure to append</span>
            <span class="c1"># the assistant&#39;s tool call along with the tool response.</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool_response</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># return the result as a ChatResponse, as this</span>
        <span class="c1"># is the expected output of the predict method</span>
        <span class="k">return</span> <span class="n">ChatResponse</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-4-(optional,-but-recommended):-Enable-tracing-for-the-model">
<h2>Step 4 (optional, but recommended): Enable tracing for the model<a class="headerlink" href="#Step-4-(optional,-but-recommended):-Enable-tracing-for-the-model" title="Permalink to this headline"> </a></h2>
<p>This step is optional, but highly recommended to improve observability in your app. We’ll be using <a class="reference external" href="https://mlflow.org/docs/latest/llms/tracing/index.html">MLflow Tracing</a> to log the inputs and outputs of our model’s internal functions, so we can easily debug when things go wrong. Agent-style tool calling models can make many layers of function calls during the lifespan of a single request, so tracing is invaluable in helping us understand what’s going on at each step.</p>
<p>Integrating tracing is easy, we simply decorate the functions we’re interested in (<code class="docutils literal notranslate"><span class="pre">get_weather()</span></code> and <code class="docutils literal notranslate"><span class="pre">predict()</span></code>) with <code class="docutils literal notranslate"><span class="pre">&#64;mlflow.trace</span></code>! MLflow Tracing also has integrations with many popular GenAI frameworks, such as LangChain, OpenAI, LlamaIndex, and more. For the full list, check out this <a class="reference external" href="https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing">documentation page</a>. In this tutorial, we’re using the OpenAI SDK to make API calls, so we can enable tracing for this by
calling <code class="docutils literal notranslate"><span class="pre">mlflow.openai.autolog()</span></code>.</p>
<p>To view the traces in the UI, run <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">ui</span></code> in a separate terminal shell, and navigate to the <code class="docutils literal notranslate"><span class="pre">Traces</span></code> tab after using the model for inference below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.entities.span</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SpanType</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># automatically trace OpenAI SDK calls</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">autolog</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">WeatherModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">ChatModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weather_tool</span> <span class="o">=</span> <span class="n">FunctionToolDefinition</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Get weather information&quot;</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">ToolParamsSchema</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="n">ParamProperty</span><span class="p">(</span>
                        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
                        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;City name to get weather information for&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
            <span class="p">),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_tool_definition</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">weather_tool</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()]</span>

    <span class="nd">@mlflow</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">span_type</span><span class="o">=</span><span class="n">SpanType</span><span class="o">.</span><span class="n">TOOL</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_weather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;It&#39;s sunny in </span><span class="si">{}</span><span class="s2">, with a temperature of 20C&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>

    <span class="nd">@mlflow</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">span_type</span><span class="o">=</span><span class="n">SpanType</span><span class="o">.</span><span class="n">AGENT</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">],</span> <span class="n">params</span><span class="p">:</span> <span class="n">ChatParams</span><span class="p">):</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

        <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span>

        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">tool_calls</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span>
        <span class="k">if</span> <span class="n">tool_calls</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Received a tool call, calling the weather tool...&quot;</span><span class="p">)</span>

            <span class="n">city</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">tool_calls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">arguments</span><span class="p">)[</span><span class="s2">&quot;city&quot;</span><span class="p">]</span>
            <span class="n">tool_call_id</span> <span class="o">=</span> <span class="n">tool_calls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">id</span>

            <span class="n">tool_response</span> <span class="o">=</span> <span class="n">ChatMessage</span><span class="p">(</span>
                <span class="n">role</span><span class="o">=</span><span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_weather</span><span class="p">(</span><span class="n">city</span><span class="p">),</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call_id</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool_response</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">ChatResponse</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-5:-Logging-the-model">
<h2>Step 5: Logging the model<a class="headerlink" href="#Step-5:-Logging-the-model" title="Permalink to this headline"> </a></h2>
<p>Finally, we need to log the model. This saves the model as an artifact in MLflow Tracking, and allows us to load and serve it later on.</p>
<p>(Note: this is a fundamental pattern in MLflow. To learn more, check out the <a class="reference external" href="https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html">Quickstart guide</a>!)</p>
<p>In order to do this, we need to do a few things:</p>
<ol class="arabic simple">
<li><p>Define an input example to inform users about the input we expect</p></li>
<li><p>Instantiate the model</p></li>
<li><p>Call <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.log_model()</span></code> with the above as arguments</p></li>
</ol>
<p>Take note of the Model URI printed out at the end of the cell—we’ll need it when serving the model later!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># messages to use as input examples</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Please use the provided tools to answer user queries.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in Singapore?&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">input_example</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># instantiate the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">WeatherModel</span><span class="p">()</span>

<span class="c1"># log the model</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;weather-model&quot;</span><span class="p">,</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Successfully logged the model at the following URI: &quot;</span><span class="p">,</span> <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/10/29 09:30:14 INFO mlflow.pyfunc: Predicting on input example to validate output
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Received a tool call, calling the weather tool...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7efce7f1f8e64673ab381052e5b02499", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Received a tool call, calling the weather tool...
Successfully logged the model at the following URI:  runs:/8051850efa194a3b8b2450c4c9f4d42f/weather-model
</pre></div></div>
</div>
</div>
<div class="section" id="Using-the-model-for-inference">
<h2>Using the model for inference<a class="headerlink" href="#Using-the-model-for-inference" title="Permalink to this headline"> </a></h2>
<p>Now that the model is logged, our work is more or less done! In order to use the model for inference, let’s load it back using <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Load the previously logged ChatModel</span>
<span class="n">tool_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">system_prompt</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
    <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Please use the provided tools to answer user queries.&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">system_prompt</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in Singapore?&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="c1"># Call the model&#39;s predict method</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">tool_model</span><span class="o">.</span><span class="n">predict</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">system_prompt</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in San Francisco?&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="c1"># Generating another response</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">tool_model</span><span class="o">.</span><span class="n">predict</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024/10/29 09:30:27 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id &#39;0&#39;. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Received a tool call, calling the weather tool...
The weather in Singapore is sunny, with a temperature of 20°C.
Received a tool call, calling the weather tool...
The weather in San Francisco is sunny, with a temperature of 20°C.
</pre></div></div>
</div>
</div>
<div class="section" id="Serving-the-model">
<h2>Serving the model<a class="headerlink" href="#Serving-the-model" title="Permalink to this headline"> </a></h2>
<p>MLflow also allows you to serve models, using the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span> <span class="pre">serve</span></code> CLI tool. In another terminal shell, run the following from the same folder as this notebook:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span>&lt;YOUR<span class="w"> </span>OPENAI<span class="w"> </span>API<span class="w"> </span>KEY&gt;
$<span class="w"> </span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-m<span class="w"> </span>&lt;MODEL_URI&gt;
</pre></div>
</div>
<p>This will start serving the model on <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:5000</span></code>, and the model can be queried via POST request to the <code class="docutils literal notranslate"><span class="pre">/invocations</span></code> route.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">system_prompt</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in Tokyo?&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;http://127.0.0.1:5000/invocations&quot;</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>
<span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
<span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;choices&#39;: [{&#39;index&#39;: 0,
   &#39;message&#39;: {&#39;role&#39;: &#39;assistant&#39;,
    &#39;content&#39;: &#39;The weather in Tokyo is sunny, with a temperature of 20°C.&#39;},
   &#39;finish_reason&#39;: &#39;stop&#39;}],
 &#39;usage&#39;: {&#39;prompt_tokens&#39;: 100, &#39;completion_tokens&#39;: 16, &#39;total_tokens&#39;: 116},
 &#39;id&#39;: &#39;chatcmpl-ANVOhWssEiyYNFwrBPxp1gmQvZKsy&#39;,
 &#39;model&#39;: &#39;gpt-4o-mini-2024-07-18&#39;,
 &#39;object&#39;: &#39;chat.completion&#39;,
 &#39;created&#39;: 1730165599}
</pre></div></div>
</div>
</div>
<div class="section" id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline"> </a></h2>
<p>In this tutorial, we covered how to use MLflow’s <code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> class to create a convenient OpenAI wrapper that supports tool calling. Though the use-case was simple, the concepts covered here can be easily extended to support more complex functionality.</p>
<p>If you’re looking to dive deeper into building quality GenAI apps, you might be also be interested in checking out <a class="reference external" href="https://mlflow.org/docs/latest/llms/tracing/index.html">MLflow Tracing</a>, an observability tool you can use to trace the execution of arbitrary functions (such as your tool calls, for example).</p>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../chat-model-guide/index.html" class="btn btn-neutral" title="Tutorial: Custom GenAI Models using ChatModel" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../tracing/index.html" class="btn btn-neutral" title="Introduction to MLflow Tracing" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.17.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>