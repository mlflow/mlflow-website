
  

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MLflow AI Gateway (Experimental) &mdash; MLflow 2.9.3.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/llms/gateway/index.html">
  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="MLflow 2.9.3.dev0 documentation" href="../../index.html"/>
        <link rel="up" title="LLMs" href="../index.html"/>
        <link rel="next" title="Getting Started with the MLflow AI Gateway" href="/guides/index.html"/>
        <link rel="prev" title="Evaluate a Hugging Face LLM with mlflow.evaluate()" href="/../llm-evaluate/notebooks/huggingface-evaluation.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../_static/jquery.js"></script>
<script type="text/javascript" src="../../_static/underscore.js"></script>
<script type="text/javascript" src="../../_static/doctools.js"></script>
<script type="text/javascript" src="../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../index.html" class="wy-nav-top-logo"
      ><img src="../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.9.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home"><img src="../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../new-features/index.html">New Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">LLMs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#id1">MLflow Deployments Server for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id2">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id3">Prompt Engineering UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#native-mlflow-flavors-for-llms">Native MLflow Flavors for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#id4">LLM Tracking in MLflow</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#tutorials-and-use-case-guides-for-llms-in-mlflow">Tutorials and Use Case Guides for LLMs in MLflow</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../custom-pyfunc-for-llms/index.html">Deploying Advanced LLMs with Custom PyFuncs in MLflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm-evaluate/notebooks/index.html">LLM Evaluation Examples</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">MLflow AI Gateway (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="guides/index.html">Getting Started with the MLflow AI Gateway</a></li>
<li class="toctree-l4"><a class="reference internal" href="migration.html">MLflow AI Gateway Migration Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tutorials-and-guides">Tutorials and Guides</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quickstart">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="#concepts">Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-ai-gateway">Configuring the AI Gateway</a></li>
<li class="toctree-l4"><a class="reference internal" href="#querying-the-ai-gateway">Querying the AI Gateway</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlflow-ai-gateway-api-documentation">MLflow AI Gateway API Documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ai-gateway-security-considerations">AI Gateway Security Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">LangChain Integration</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../index.html">LLMs</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>MLflow AI Gateway (Experimental)</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/llms/gateway/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="mlflow-ai-gateway-experimental">
<span id="gateway"></span><h1>MLflow AI Gateway (Experimental)<a class="headerlink" href="#mlflow-ai-gateway-experimental" title="Permalink to this headline"> </a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>MLflow AI gateway is deprecated and has been replaced by <cite>the deployments API &lt;deployments&gt;</cite>
for generative AI. See <a class="reference internal" href="migration.html#gateway-migration"><span class="std std-ref">MLflow AI Gateway Migration Guide</span></a> for migration.</p>
</div>
<p>The MLflow AI Gateway service is a powerful tool designed to streamline the usage and management of
various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization.
It offers a high-level interface that simplifies the interaction with these services by providing
a unified endpoint to handle specific LLM related requests.</p>
<p>A major advantage of using the MLflow AI Gateway service is its centralized management of API keys.
By storing these keys in one secure location, organizations can significantly enhance their
security posture by minimizing the exposure of sensitive API keys throughout the system. It also
helps to prevent exposing these keys within code or requiring end-users to manage keys safely.</p>
<p>The gateway is designed to be flexible and adaptable, capable of easily defining and managing routes by updating the
configuration file. This enables the easy incorporation
of new LLM providers or provider LLM types into the system without necessitating changes to
applications that interface with the gateway. This level of adaptability makes the MLflow AI Gateway
Service an invaluable tool in environments that require agility and quick response to changes.</p>
<p>This simplification and centralization of language model interactions, coupled with the added
layer of security for API key management, make the MLflow AI Gateway service an ideal choice for
organizations that use LLMs on a regular basis.</p>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="tutorials-and-guides">
<h2>Tutorials and Guides<a class="headerlink" href="#tutorials-and-guides" title="Permalink to this headline"> </a></h2>
<p>If you’re interested in diving right in to a step by step guide that will get you up and running with the MLflow AI Gateway
as fast as possible, the guides below will be your best first stop.</p>
<a href="guides/index.html" class="download-btn">View the AI Gateway Getting Started Guide</a><br/></div>
<div class="section" id="quickstart">
<span id="gateway-quickstart"></span><h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline"> </a></h2>
<p>The following guide will assist you in getting up and running, using a 3-route configuration to
OpenAI services for chat, completions, and embeddings.</p>
<div class="section" id="step-1-install-the-mlflow-ai-gateway-service">
<h3>Step 1: Install the MLflow AI Gateway service<a class="headerlink" href="#step-1-install-the-mlflow-ai-gateway-service" title="Permalink to this headline"> </a></h3>
<p>First, you need to install the MLflow AI Gateway service on your machine. You can do this using pip from PyPI or from the MLflow repository.</p>
<div class="section" id="installing-from-pypi">
<h4>Installing from PyPI<a class="headerlink" href="#installing-from-pypi" title="Permalink to this headline"> </a></h4>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s1">&#39;mlflow[gateway]&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="step-2-set-the-openai-api-key-s-for-each-provider">
<h3>Step 2: Set the OpenAI API Key(s) for each provider<a class="headerlink" href="#step-2-set-the-openai-api-key-s-for-each-provider" title="Permalink to this headline"> </a></h3>
<p>The Gateway service needs to communicate with the OpenAI API. To do this, it requires an API key.
You can create an API key from the OpenAI dashboard.</p>
<p>For this example, we’re only connecting with OpenAI. If there are additional providers within the
configuration, these keys will need to be set as well.</p>
<p>Once you have the key, you can set it as an environment variable in your terminal:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span>your_api_key_here
</pre></div>
</div>
<p>This sets a temporary session-based environment variable. For production use cases, it is advisable
to store this key in the <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> or <code class="docutils literal notranslate"><span class="pre">.zshrc</span></code> files so that the key doesn’t have to be re-entered upon
system restart.</p>
</div>
<div class="section" id="step-3-create-a-gateway-configuration-file">
<h3>Step 3: Create a Gateway Configuration File<a class="headerlink" href="#step-3-create-a-gateway-configuration-file" title="Permalink to this headline"> </a></h3>
<p>Next, you need to create a Gateway configuration file. This is a YAML file where you specify the
routes that the Gateway service should expose. Let’s create a file with three routes using OpenAI as a provider: completions, chat, and embeddings.</p>
<p>For details about the configuration file’s parameters (including parameters for other providers besides OpenAI), see the <a class="reference internal" href="#gateway-configuration-details"><span class="std std-ref">AI Gateway Configuration Details</span></a> section below.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">completions</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/completions</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-3.5-turbo</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>

<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chat</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/chat</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-3.5-turbo</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>

<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">embeddings</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/embeddings</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text-embedding-ada-002</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
</pre></div>
</div>
<p>Save this file to a location on the system that is going to be running the MLflow AI Gateway server.</p>
</div>
<div class="section" id="step-4-start-the-gateway-service">
<h3>Step 4: Start the Gateway Service<a class="headerlink" href="#step-4-start-the-gateway-service" title="Permalink to this headline"> </a></h3>
<p>You’re now ready to start the Gateway service!</p>
<p>Use the MLflow AI Gateway <code class="docutils literal notranslate"><span class="pre">start</span></code> command and specify the path to your configuration file:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>gateway<span class="w"> </span>start<span class="w"> </span>--config-path<span class="w"> </span>config.yaml<span class="w"> </span>--port<span class="w"> </span><span class="o">{</span>port<span class="o">}</span><span class="w"> </span>--host<span class="w"> </span><span class="o">{</span>host<span class="o">}</span><span class="w"> </span>--workers<span class="w"> </span><span class="o">{</span>worker<span class="w"> </span>count<span class="o">}</span>
</pre></div>
</div>
<p>The configuration file can also be set using the <code class="docutils literal notranslate"><span class="pre">MLFLOW_GATEWAY_CONFIG_PATH</span></code> environment variable:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MLFLOW_GATEWAY_CONFIG_PATH</span><span class="o">=</span>/path/to/config.yaml
</pre></div>
</div>
<p>If you do not specify the host, a localhost address will be used.</p>
<p>If you do not specify the port, port 5000 will be used.</p>
<p>The worker count for gunicorn defaults to 2 workers.</p>
</div>
<div class="section" id="step-5-access-the-interactive-api-documentation">
<h3>Step 5: Access the Interactive API Documentation<a class="headerlink" href="#step-5-access-the-interactive-api-documentation" title="Permalink to this headline"> </a></h3>
<p>The MLflow AI Gateway service provides an interactive API documentation endpoint that you can use to explore
and test the exposed routes. Navigate to <code class="docutils literal notranslate"><span class="pre">http://{host}:{port}/</span></code> (or <code class="docutils literal notranslate"><span class="pre">http://{host}:{port}/docs</span></code>) in your browser to access it.</p>
<p>The docs endpoint allow for direct interaction with the routes and permits submitting actual requests to the
provider services by click on the “try it now” option within the endpoint definition entry.</p>
</div>
<div class="section" id="step-6-send-requests-using-the-fluent-api">
<h3>Step 6: Send Requests Using the Fluent API<a class="headerlink" href="#step-6-send-requests-using-the-fluent-api" title="Permalink to this headline"> </a></h3>
<p>For information on formatting requirements and how to pass parameters, see <a class="reference internal" href="#gateway-query"><span class="std std-ref">Querying the AI Gateway</span></a>.</p>
<p>Here’s an example of how to send a chat request using the <a class="reference internal" href="#gateway-fluent-api"><span class="std std-ref">Fluent API</span></a> :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.gateway</span> <span class="kn">import</span> <span class="n">query</span><span class="p">,</span> <span class="n">set_gateway_uri</span>

<span class="n">set_gateway_uri</span><span class="p">(</span><span class="n">gateway_uri</span><span class="o">=</span><span class="s2">&quot;http://localhost:5000&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">query</span><span class="p">(</span>
    <span class="s2">&quot;chat&quot;</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the best day of the week?&quot;</span><span class="p">}]},</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note:</strong> Remember to change the uri definition to the actual uri of your Gateway server.</p>
<p>The returned response will be in this data structure (the actual content and token values will likely be different):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;candidates&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;message&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">It&#39;s hard to say what the best day of the week is.&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;finish_reason&quot;</span><span class="p">:</span> <span class="s2">&quot;stop&quot;</span><span class="p">},</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;input_tokens&quot;</span><span class="p">:</span> <span class="mi">13</span><span class="p">,</span>
        <span class="s2">&quot;output_tokens&quot;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span>
        <span class="s2">&quot;total_tokens&quot;</span><span class="p">:</span> <span class="mi">28</span><span class="p">,</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt-3.5-turbo-0301&quot;</span><span class="p">,</span>
        <span class="s2">&quot;route_type&quot;</span><span class="p">:</span> <span class="s2">&quot;llm/v1/chat&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="step-7-send-requests-using-the-client-api">
<h3>Step 7: Send Requests Using the Client API<a class="headerlink" href="#step-7-send-requests-using-the-client-api" title="Permalink to this headline"> </a></h3>
<p>See the <a class="reference internal" href="#gateway-client-api"><span class="std std-ref">Client API</span></a> section for further information.</p>
</div>
<div class="section" id="step-8-send-requests-to-routes-via-rest-api">
<h3>Step 8: Send Requests to Routes via REST API<a class="headerlink" href="#step-8-send-requests-to-routes-via-rest-api" title="Permalink to this headline"> </a></h3>
<p>You can now send requests to the exposed routes.
See the <a class="reference internal" href="#gateway-rest-api"><span class="std std-ref">REST examples</span></a> for guidance on request formatting.</p>
</div>
<div class="section" id="step-9-compare-provider-models">
<h3>Step 9: Compare Provider Models<a class="headerlink" href="#step-9-compare-provider-models" title="Permalink to this headline"> </a></h3>
<p>Here’s an example of adding a new model from a provider to determine which model instance is better for a given use case.</p>
<p>Firstly, update the <a class="reference internal" href="#gateway-configuration"><span class="std std-ref">MLflow AI Gateway config</span></a> YAML file with the additional route definition to test:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">completions</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/completions</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-3.5-turbo</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">completions-gpt4</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/completions</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-4</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
</pre></div>
</div>
<p>This updated configuration adds a new completions route <code class="docutils literal notranslate"><span class="pre">completions-gpt4</span></code> while still preserving the original <code class="docutils literal notranslate"><span class="pre">completions</span></code>
route that was configured with the <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code>  model.</p>
<p>Once the configuration file is updated, simply save your changes. The Gateway will automatically create the new route with zero downtime.</p>
<p>At this point, you may use the <a class="reference internal" href="#gateway-fluent-api"><span class="std std-ref">Fluent API</span></a> to query both routes with similar prompts to decide which model performs best for your use case.</p>
<p>If you no longer need a route, you can delete it from the configuration YAML and save your changes. The AI Gateway will automatically remove the route.</p>
</div>
<div class="section" id="step-10-use-ai-gateway-routes-for-model-development">
<h3>Step 10: Use AI Gateway routes for model development<a class="headerlink" href="#step-10-use-ai-gateway-routes-for-model-development" title="Permalink to this headline"> </a></h3>
<p>Now that you have created several AI Gateway routes, you can create MLflow Models that query these
routes to build application-specific logic using techniques like prompt engineering. For more
information, see <a class="reference internal" href="#gateway-mlflow-models"><span class="std std-ref">AI Gateway and MLflow Models</span></a>.</p>
</div>
</div>
<div class="section" id="concepts">
<span id="gateway-concepts"></span><h2>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline"> </a></h2>
<p>There are several concepts that are referred to within the MLflow AI Gateway APIs, the configuration definitions, examples, and documentation.
Becoming familiar with these terms will help in configuring new endpoints (routes) and ease the use of the interface APIs for the AI Gateway.</p>
<div class="section" id="providers">
<span id="id1"></span><h3>Providers<a class="headerlink" href="#providers" title="Permalink to this headline"> </a></h3>
<p>The MLflow AI Gateway is designed to support a variety of model providers.
A provider represents the source of the machine learning models, such as OpenAI, Anthropic, and so on.
Each provider has its specific characteristics and configurations that are encapsulated within the model part of a route in the MLflow AI Gateway.</p>
<div class="section" id="supported-provider-models">
<h4>Supported Provider Models<a class="headerlink" href="#supported-provider-models" title="Permalink to this headline"> </a></h4>
<p>The table below presents a non-exhaustive list of models and a corresponding route type within the MLflow AI Gateway.
With the rapid development of LLMs, there is no guarantee that this list will be up to date at all times. However, the associations listed
below can be used as a helpful guide when configuring a given route for any newly released model types as they become available with a given provider.
<code class="docutils literal notranslate"><span class="pre">N/A</span></code> means that the provider or the AI Gateway implementation currently doesn’t support the route type.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Provider</p></th>
<th class="head" colspan="3"><p>Routes</p></th>
</tr>
<tr class="row-even"><th class="head"></th>
<th class="head"><p>llm/v1/completions</p></th>
<th class="head"><p>llm/v1/chat</p></th>
<th class="head"><p>llm/v1/embeddings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>OpenAI</p></td>
<td><ul class="simple">
<li><p>gpt-3.5-turbo</p></li>
<li><p>gpt-4</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>gpt-3.5-turbo</p></li>
<li><p>gpt-4</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>text-embedding-ada-002</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>MosaicML</p></td>
<td><ul class="simple">
<li><p>mpt-7b-instruct</p></li>
<li><p>mpt-30b-instruct</p></li>
<li><p>llama2-70b-chat†</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>llama2-70b-chat†</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>instructor-large</p></li>
<li><p>instructor-xl</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Anthropic</p></td>
<td><ul class="simple">
<li><p>claude-1</p></li>
<li><p>claude-1.3-100k</p></li>
<li><p>claude-2</p></li>
</ul>
</td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>Cohere</p></td>
<td><ul class="simple">
<li><p>command</p></li>
<li><p>command-light-nightly</p></li>
</ul>
</td>
<td><p>N/A</p></td>
<td><ul class="simple">
<li><p>embed-english-v2.0</p></li>
<li><p>embed-multilingual-v2.0</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Azure OpenAI</p></td>
<td><ul class="simple">
<li><p>text-davinci-003</p></li>
<li><p>gpt-35-turbo</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>gpt-35-turbo</p></li>
<li><p>gpt-4</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>text-embedding-ada-002</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>PaLM</p></td>
<td><ul class="simple">
<li><p>text-bison-001</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>chat-bison-001</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>embedding-gecko-001</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>MLflow</p></td>
<td><ul class="simple">
<li><p>MLflow served models*</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>MLflow served models*</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>MLflow served models**</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>HuggingFace TGI</p></td>
<td><p>N/A</p></td>
<td><ul class="simple">
<li><p>HF TGI Models</p></li>
</ul>
</td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>AI21 Labs</p></td>
<td><ul class="simple">
<li><p>j2-ultra</p></li>
<li><p>j2-mid</p></li>
<li><p>j2-light</p></li>
</ul>
</td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>AWS Bedrock</p></td>
<td><ul class="simple">
<li><p>Amazon Titan</p></li>
<li><p>Third-party providers</p></li>
</ul>
</td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>
<p>† Llama 2 is licensed under the <a class="reference external" href="https://ai.meta.com/llama/license/">LLAMA 2 Community License</a>, Copyright © Meta Platforms, Inc. All Rights Reserved.</p>
<p>Within each model block in the configuration file, the provider field is used to specify the name
of the provider for that model. This is a string value that needs to correspond to a provider the MLflow AI Gateway supports.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>*</cite> MLflow Model Serving will only work for chat or completions if the output return is in a route-compatible format. The
response must conform to either an output of <code class="docutils literal notranslate"><span class="pre">{&quot;predictions&quot;:</span> <span class="pre">str}</span></code> or <code class="docutils literal notranslate"><span class="pre">{&quot;predictions&quot;:</span> <span class="pre">{&quot;candidates&quot;:</span> <span class="pre">str}}</span></code>. Any complex return type from a model that
does not conform to these structures will raise an exception at query time.</p>
<p><cite>**</cite> Embeddings support is only available for models whose response signatures conform to the structured format of <code class="docutils literal notranslate"><span class="pre">{&quot;predictions&quot;:</span> <span class="pre">List[float]}</span></code>
or <code class="docutils literal notranslate"><span class="pre">{&quot;predictions&quot;:</span> <span class="pre">List[List[float]]}</span></code>. Any other return type will raise an exception at query time. <code class="docutils literal notranslate"><span class="pre">FeatureExtractionPipeline</span></code> in <code class="docutils literal notranslate"><span class="pre">transformers</span></code> and
models using the <code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code> flavor will return the correct data structures for the embeddings route.</p>
</div>
<p>Here’s an example of a provider configuration within a route:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chat</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/chat</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-4</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
</pre></div>
</div>
<p>In the above configuration, <code class="docutils literal notranslate"><span class="pre">openai</span></code> is the <cite>provider</cite> for the model.</p>
<p>As of now, the MLflow AI Gateway supports the following providers:</p>
<ul class="simple">
<li><p><strong>mosaicml</strong>: This is used for models offered by <a class="reference external" href="https://docs.mosaicml.com/en/latest/">MosaicML</a>.</p></li>
<li><p><strong>openai</strong>: This is used for models offered by <a class="reference external" href="https://platform.openai.com/">OpenAI</a> and the <a class="reference external" href="https://learn.microsoft.com/en-gb/azure/cognitive-services/openai/">Azure</a> integrations for Azure OpenAI and Azure OpenAI with AAD.</p></li>
<li><p><strong>anthropic</strong>: This is used for models offered by <a class="reference external" href="https://docs.anthropic.com/claude/docs">Anthropic</a>.</p></li>
<li><p><strong>cohere</strong>: This is used for models offered by <a class="reference external" href="https://docs.cohere.com/docs">Cohere</a>.</p></li>
<li><p><strong>palm</strong>: This is used for models offered by <a class="reference external" href="https://developers.generativeai.google/api/rest/generativelanguage/models/">PaLM</a>.</p></li>
<li><p><strong>huggingface text generation inference</strong>: This is used for models deployed using <a class="reference external" href="https://huggingface.co/docs/text-generation-inference/index">Huggingface Text Generation Inference</a>.</p></li>
<li><p><strong>ai21labs</strong>: This is used for models offered by <a class="reference external" href="https://studio.ai21.com/foundation-models">AI21 Labs</a>.</p></li>
<li><p><strong>bedrock</strong>: This is used for models offered by <a class="reference external" href="https://aws.amazon.com/bedrock/">AWS Bedrock</a>.</p></li>
</ul>
<p>More providers are being added continually. Check the latest version of the MLflow AI Gateway Docs for the
most up-to-date list of supported providers.</p>
<p>Remember, the provider you specify must be one that the MLflow AI Gateway supports. If the provider
is not supported, the Gateway will return an error when trying to route requests to that provider.</p>
</div>
</div>
<div class="section" id="routes">
<span id="id2"></span><h3>Routes<a class="headerlink" href="#routes" title="Permalink to this headline"> </a></h3>
<p><cite>Routes</cite> are central to how the MLflow AI Gateway functions. Each route acts as a proxy endpoint for the
user, forwarding requests to the underlying <a class="reference internal" href="#gateway-models"><span class="std std-ref">Models</span></a> and <a class="reference internal" href="#providers"><span class="std std-ref">Providers</span></a> specified in the configuration file.</p>
<p>A route in the MLflow AI Gateway consists of the following fields:</p>
<ul>
<li><p><strong>name</strong>: This is the unique identifier for the route. This will be part of the URL when making API calls via the MLflow AI Gateway.</p></li>
<li><p><strong>type</strong>: The type of the route corresponds to the type of language model interaction you desire. For instance, <code class="docutils literal notranslate"><span class="pre">llm/v1/completions</span></code> for text completion operations, <code class="docutils literal notranslate"><span class="pre">llm/v1/embeddings</span></code> for text embeddings, and <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> for chat operations.</p></li>
<li><p><strong>model</strong>: Defines the model to which this route will forward requests. The model contains the following details:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>provider</strong>: Specifies the name of the <a class="reference internal" href="#providers"><span class="std std-ref">provider</span></a> for this model. For example, <code class="docutils literal notranslate"><span class="pre">openai</span></code> for OpenAI’s <code class="docutils literal notranslate"><span class="pre">GPT-3.5</span></code> models.</p></li>
<li><p><strong>name</strong>: The name of the model to use. For example, <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> for OpenAI’s <code class="docutils literal notranslate"><span class="pre">GPT-3.5-Turbo</span></code> model.</p></li>
<li><p><strong>config</strong>: Contains any additional configuration details required for the model. This includes specifying the API base URL and the API key.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Here’s an example of a route configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">completions</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chat/completions</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-3.5-turbo</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
</pre></div>
</div>
<p>In the example above, a request sent to the completions route would be forwarded to the
<code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> model provided by <code class="docutils literal notranslate"><span class="pre">openai</span></code>.</p>
<p>The routes in the configuration file can be updated at any time, and the MLflow AI Gateway will
automatically update its available routes without requiring a restart. This feature provides you
with the flexibility to add, remove, or modify routes as your needs change. It enables ‘hot-swapping’
of routes, providing a seamless experience for any applications or services that interact with the MLflow AI Gateway.</p>
<p>When defining routes in the configuration file, ensure that each name is unique to prevent conflicts.
Duplicate route names will raise an <code class="docutils literal notranslate"><span class="pre">MlflowException</span></code>.</p>
</div>
<div class="section" id="models">
<span id="gateway-models"></span><h3>Models<a class="headerlink" href="#models" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">model</span></code> section within a <code class="docutils literal notranslate"><span class="pre">route</span></code> specifies which model to use for generating responses.
This configuration block needs to contain a <code class="docutils literal notranslate"><span class="pre">name</span></code> field which is used to specify the exact model instance to be used.
Additionally, a <a class="reference internal" href="#providers"><span class="std std-ref">provider</span></a> needs to be specified, one that you have an authenticated access api key for.</p>
<p>Different endpoint types are often associated with specific models.
For instance, the <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> and <code class="docutils literal notranslate"><span class="pre">llm/v1/completions</span></code> endpoints are generally associated with
conversational models, while <code class="docutils literal notranslate"><span class="pre">llm/v1/embeddings</span></code> endpoints would typically be associated with
embedding or transformer models. The model you choose should be appropriate for the type of endpoint specified.</p>
<p>Here’s an example of a model name configuration within a route:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">embeddings</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/embeddings</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text-embedding-ada-002</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
</pre></div>
</div>
<p>In the above configuration, <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code> is the model used for the embeddings endpoint.</p>
<p>When specifying a model, it is critical that the provider supports the model you are requesting.
For instance, <code class="docutils literal notranslate"><span class="pre">openai</span></code> as a provider supports models like <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code>, but other providers
may not. If the model is not supported by the provider, the MLflow AI Gateway will return an HTTP 4xx error
when trying to route requests to that model.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Always check the latest documentation of the specified provider to ensure that the model you want
to use is supported for the type of endpoint you’re configuring.</p>
</div>
<p>Remember, the model you choose directly affects the results of the responses you’ll get from the
API calls. Therefore, choose a model that fits your use-case requirements. For instance,
for generating conversational responses, you would typically choose a chat model.
Conversely, for generating embeddings of text, you would choose an embedding model.</p>
</div>
</div>
<div class="section" id="configuring-the-ai-gateway">
<span id="gateway-configuration"></span><h2>Configuring the AI Gateway<a class="headerlink" href="#configuring-the-ai-gateway" title="Permalink to this headline"> </a></h2>
<p>The MLflow AI Gateway service relies on a user-provided configuration file, written in YAML,
that defines the routes and providers available to the service. The configuration file dictates
how the gateway interacts with various language model providers and determines the end-points that
users can access.</p>
<div class="section" id="ai-gateway-configuration">
<h3>AI Gateway Configuration<a class="headerlink" href="#ai-gateway-configuration" title="Permalink to this headline"> </a></h3>
<p>The configuration file includes a series of sections, each representing a unique route.
Each route section has a name, a type, and a model specification, which includes the model
provider, name, and configuration details. The configuration section typically contains the base
URL for the API and an environment variable for the API key.</p>
<p>Here is an example of a single-route configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chat</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/chat</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-3.5-turbo</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$OPENAI_API_KEY</span>
</pre></div>
</div>
<p>In this example, we define a route named <code class="docutils literal notranslate"><span class="pre">chat</span></code> that corresponds to the <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> type, which
will use the <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> model from OpenAI to return query responses from the OpenAI service.</p>
<p>The Gateway configuration is very easy to update.
Simply edit the configuration file and save your changes, and the MLflow AI Gateway service will automatically
update the routes with zero disruption or down time. This allows you to try out new providers or model types while keeping your applications steady and reliable.</p>
<p>In order to define an API key for a given provider, there are three primary options:</p>
<ol class="arabic simple">
<li><p>Directly include it in the YAML configuration file.</p></li>
<li><p>Use an environment variable to store the API key and reference it in the YAML configuration file.</p></li>
<li><p>Define your API key in a file and reference the location of that key-bearing file within the YAML configuration file.</p></li>
</ol>
<p>If you choose to include the API key directly, replace <code class="docutils literal notranslate"><span class="pre">$OPENAI_API_KEY</span></code> in the YAML file with your
actual API key.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The MLflow AI Gateway service provides direct access to billed external LLM services. It is strongly recommended to restrict access to this server. See the section on <a class="reference internal" href="#gateway-security"><span class="std std-ref">security</span></a> for guidance.</p>
</div>
<p>If you prefer to use an environment variable (recommended), you can define it in your shell
environment. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your_openai_api_key&quot;</span>
</pre></div>
</div>
<p><strong>Note:</strong> Replace “your_openai_api_key” with your actual OpenAI API key.</p>
<div class="section" id="ai-gateway-configuration-details">
<span id="gateway-configuration-details"></span><h4>AI Gateway Configuration Details<a class="headerlink" href="#ai-gateway-configuration-details" title="Permalink to this headline"> </a></h4>
<p>The MLflow AI Gateway service relies on a user-provided configuration file. It defines how the gateway interacts with various language model providers and dictates the routes that users can access.</p>
<p>The configuration file is written in YAML and includes a series of sections, each representing a unique route. Each route section has a name, a type, and a model specification, which includes the provider, model name, and provider-specific configuration details.</p>
<p>Here are the details of each configuration parameter:</p>
<div class="section" id="general-configuration-parameters">
<h5>General Configuration Parameters<a class="headerlink" href="#general-configuration-parameters" title="Permalink to this headline"> </a></h5>
<ul class="simple">
<li><p><strong>routes</strong>: This is a list of route configurations. Each route represents a unique endpoint that maps to a particular language model service.</p></li>
</ul>
<p>Each route has the following configuration parameters:</p>
<ul class="simple">
<li><p><strong>name</strong>: This is the name of the route. It needs to be a unique name without spaces or any non-alphanumeric characters other than hyphen and underscore.</p></li>
<li><p><strong>route_type</strong>: This specifies the type of service offered by this route. This determines the interface for inputs to a route and the returned outputs. Current supported route types are:</p>
<ul>
<li><p>“llm/v1/completions”</p></li>
<li><p>“llm/v1/chat”</p></li>
<li><p>“llm/v1/embeddings”</p></li>
</ul>
</li>
<li><p><strong>model</strong>: This defines the provider-specific details of the language model. It contains the following fields:</p>
<ul>
<li><p><strong>provider</strong>: This indicates the provider of the AI model. It accepts the following values:</p>
<ul>
<li><p>“openai”</p></li>
<li><p>“mosaicml”</p></li>
<li><p>“anthropic”</p></li>
<li><p>“cohere”</p></li>
<li><p>“palm”</p></li>
<li><p>“azure” / “azuread”</p></li>
<li><p>“mlflow-model-serving”</p></li>
<li><p>“huggingface-text-generation-inference”</p></li>
<li><p>“ai21labs”</p></li>
<li><p>“bedrock”</p></li>
</ul>
</li>
<li><p><strong>name</strong>: This is an optional field to specify the name of the model.</p></li>
<li><p><strong>config</strong>: This contains provider-specific configuration details.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="provider-specific-configuration-parameters">
<h5>Provider-Specific Configuration Parameters<a class="headerlink" href="#provider-specific-configuration-parameters" title="Permalink to this headline"> </a></h5>
<div class="section" id="id3">
<h6>OpenAI<a class="headerlink" href="#id3" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 24%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>openai_api_key</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>This is the API key for the OpenAI service.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>openai_api_type</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>This is an optional field to specify the type of OpenAI API
to use.</p></td>
</tr>
<tr class="row-even"><td><p><strong>openai_api_base</strong></p></td>
<td><p>No</p></td>
<td><p><cite>https://api.openai.com/v1</cite></p></td>
<td><p>This is the base URL for the OpenAI API.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>openai_api_version</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>This is an optional field to specify the OpenAI API
version.</p></td>
</tr>
<tr class="row-even"><td><p><strong>openai_organization</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>This is an optional field to specify the organization in
OpenAI.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id4">
<h6>MosaicML<a class="headerlink" href="#id4" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>mosaicml_api_key</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the API key for the MosaicML service.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id5">
<h6>Cohere<a class="headerlink" href="#id5" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>cohere_api_key</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the API key for the Cohere service.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id6">
<h6>HuggingFace Text Generation Inference<a class="headerlink" href="#id6" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>hf_server_url</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the url of the Huggingface TGI Server.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id7">
<h6>PaLM<a class="headerlink" href="#id7" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>palm_api_key</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the API key for the PaLM service.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id8">
<h6>AI21 Labs<a class="headerlink" href="#id8" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>ai21labs_api_key</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the API key for the AI21 Labs service.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id9">
<h6>Anthropic<a class="headerlink" href="#id9" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>anthropic_api_key</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the API key for the Anthropic service.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id10">
<h6>AWS Bedrock<a class="headerlink" href="#id10" title="Permalink to this headline"> </a></h6>
<p>Top-level model configuration for AWS Bedrock routes must be one of the following two supported authentication modes: <cite>key-based</cite> or <cite>role-based</cite>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 8%" />
<col style="width: 25%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>aws_config</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>An object with either the key-based or role-based
schema below.</p></td>
</tr>
</tbody>
</table>
<p>To use key-based authentication, define an AWS Bedrock route with the required fields below.
.. note:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">If</span> <span class="n">using</span> <span class="n">a</span> <span class="n">configured</span> <span class="n">route</span> <span class="n">purely</span> <span class="k">for</span> <span class="n">development</span> <span class="ow">or</span> <span class="n">testing</span><span class="p">,</span> <span class="n">utilizing</span> <span class="n">an</span> <span class="n">IAM</span> <span class="n">User</span> <span class="n">role</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">temporary</span> <span class="n">short</span><span class="o">-</span><span class="n">lived</span> <span class="n">standard</span> <span class="n">IAM</span> <span class="n">role</span> <span class="n">are</span> <span class="n">recommended</span><span class="p">;</span> <span class="k">while</span> <span class="k">for</span> <span class="n">production</span> <span class="n">deployments</span><span class="p">,</span> <span class="n">a</span> <span class="n">standard</span> <span class="n">long</span><span class="o">-</span><span class="n">expiry</span> <span class="n">IAM</span> <span class="n">role</span> <span class="ow">is</span> <span class="n">recommended</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">route</span> <span class="ow">is</span> <span class="n">capable</span> <span class="n">of</span> <span class="n">handling</span> <span class="n">authentication</span> <span class="k">for</span> <span class="n">a</span> <span class="n">long</span> <span class="n">period</span><span class="o">.</span> <span class="n">If</span> <span class="n">the</span> <span class="n">authentication</span> <span class="n">expires</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">new</span> <span class="nb">set</span> <span class="n">of</span> <span class="n">keys</span> <span class="n">need</span> <span class="n">to</span> <span class="n">be</span> <span class="n">supplied</span><span class="p">,</span> <span class="n">the</span> <span class="n">route</span> <span class="n">must</span> <span class="n">be</span> <span class="n">recreated</span> <span class="ow">in</span> <span class="n">order</span> <span class="n">to</span> <span class="n">persist</span> <span class="n">the</span> <span class="n">new</span> <span class="n">keys</span><span class="o">.</span>
</pre></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 8%" />
<col style="width: 25%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>aws_region</strong></p></td>
<td><p>No</p></td>
<td><p>AWS_REGION/AWS_DEFAULT_REGION</p></td>
<td><p>The AWS Region to use for bedrock access.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>aws_secret_access_key</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>AWS secret access key for the IAM user/role
authorized to use bedrock</p></td>
</tr>
<tr class="row-even"><td><p><strong>aws_access_key_id</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>AWS access key ID for the IAM user/role
authorized to use Bedrock</p></td>
</tr>
<tr class="row-odd"><td><p><strong>aws_session_token</strong></p></td>
<td><p>No</p></td>
<td><p>None</p></td>
<td><p>Optional session token, if required</p></td>
</tr>
</tbody>
</table>
<p>Alternatively, for role-based authentication, an AWS Bedrock route can be defined and initialized with an a IAM Role  ARN that is authorized to access Bedrock.  The MLflow AI Gateway will attempt to assume this role with using the standard credential provider chain and will renew the role credentials if they have expired.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 8%" />
<col style="width: 25%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>aws_region</strong></p></td>
<td><p>No</p></td>
<td><p>AWS_REGION/AWS_DEFAULT_REGION</p></td>
<td><p>The AWS Region to use for bedrock access.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>aws_role_arn</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>An AWS role authorized to use Bedrock.  The standard
credential provider chain <em>must</em> be able to find
credentials authorized to assume this role.</p></td>
</tr>
<tr class="row-even"><td><p><strong>session_length_seconds</strong></p></td>
<td><p>No</p></td>
<td><p>900</p></td>
<td><p>The length of session to request.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="mlflow-model-serving">
<h6>MLflow Model Serving<a class="headerlink" href="#mlflow-model-serving" title="Permalink to this headline"> </a></h6>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>model_server_url</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>This is the url of the MLflow Model Server.</p></td>
</tr>
</tbody>
</table>
<p>Note that with MLflow model serving, the <code class="docutils literal notranslate"><span class="pre">name</span></code> parameter for the <code class="docutils literal notranslate"><span class="pre">model</span></code> definition is not used for validation and is only present for reference purposes. This alias can be
useful for understanding a particular version or route definition that was used that can be referenced back to a deployed model. You may choose any name that you wish, provided that
it is JSON serializable.</p>
</div>
<div class="section" id="azure-openai">
<h6>Azure OpenAI<a class="headerlink" href="#azure-openai" title="Permalink to this headline"> </a></h6>
<p>Azure provides two different mechanisms for integrating with OpenAI, each corresponding to a different type of security validation. One relies on an access token for validation, referred to as <code class="docutils literal notranslate"><span class="pre">azure</span></code>, while the other uses Azure Active Directory (Azure AD) integration for authentication, termed as <code class="docutils literal notranslate"><span class="pre">azuread</span></code>.</p>
<p>To match your user’s interaction and security access requirements, adjust the <code class="docutils literal notranslate"><span class="pre">openai_api_type</span></code> parameter to represent the preferred security validation model. This will ensure seamless interaction and reliable security for your Azure-OpenAI integration.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 67%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>openai_api_key</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>This is the API key for the Azure OpenAI service.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>openai_api_type</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>This field must be either <code class="docutils literal notranslate"><span class="pre">azure</span></code> or <code class="docutils literal notranslate"><span class="pre">azuread</span></code> depending on the security access protocol.</p></td>
</tr>
<tr class="row-even"><td><p><strong>openai_api_base</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>This is the base URL for the Azure OpenAI API service provided by Azure.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>openai_api_version</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>The version of the Azure OpenAI service to utilize, specified by a date.</p></td>
</tr>
<tr class="row-even"><td><p><strong>openai_deployment_name</strong></p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>This is the name of the deployment resource for the Azure OpenAI service.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>openai_organization</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>This is an optional field to specify the organization in OpenAI.</p></td>
</tr>
</tbody>
</table>
<p>An example configuration for Azure OpenAI is:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">routes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">completions</span>
<span class="w">    </span><span class="nt">route_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm/v1/completions</span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span>
<span class="w">      </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpt-35-turbo</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">openai_api_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;azuread&quot;</span>
<span class="w">        </span><span class="nt">openai_api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">$AZURE_AAD_TOKEN</span>
<span class="w">        </span><span class="nt">openai_deployment_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;{your_deployment_name}&quot;</span>
<span class="w">        </span><span class="nt">openai_api_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;https://{your_resource_name}-azureopenai.openai.azure.com/&quot;</span>
<span class="w">        </span><span class="nt">openai_api_version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2023-05-15&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Azure OpenAI has distinct features as compared with the direct OpenAI service. For an overview, please see <a class="reference external" href="https://learn.microsoft.com/en-gb/azure/cognitive-services/openai/how-to/switching-endpoints">the comparison documentation</a>.</p>
</div>
<p>For specifying an API key, there are three options:</p>
<ol class="arabic simple">
<li><p>(Preferred) Use an environment variable to store the API key and reference it in the YAML configuration file. This is denoted by a <code class="docutils literal notranslate"><span class="pre">$</span></code> symbol before the name of the environment variable.</p></li>
<li><p>(Preferred) Define the API key in a file and reference the location of that key-bearing file within the YAML configuration file.</p></li>
<li><p>Directly include it in the YAML configuration file.</p></li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The use of environment variables or file-based keys is recommended for better security practices. If the API key is directly included in the configuration file, it should be ensured that the file is securely stored and appropriately access controlled.
Please ensure that the configuration file is stored in a secure location as it contains sensitive API keys.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="querying-the-ai-gateway">
<span id="gateway-query"></span><h2>Querying the AI Gateway<a class="headerlink" href="#querying-the-ai-gateway" title="Permalink to this headline"> </a></h2>
<p>Once the MLflow AI Gateway server has been configured and started, it is ready to receive traffic from users.</p>
<div class="section" id="standard-query-parameters">
<span id="id11"></span><h3>Standard Query Parameters<a class="headerlink" href="#standard-query-parameters" title="Permalink to this headline"> </a></h3>
<p>The MLflow AI Gateway defines standard parameters for chat, completions, and embeddings that can be
used when querying any route regardless of its provider. Each parameter has a standard range and
default value. When querying a route with a particular provider, the MLflow AI Gateway automatically
scales parameter values according to the provider’s value ranges for that parameter.</p>
<div class="section" id="completions">
<h4>Completions<a class="headerlink" href="#completions" title="Permalink to this headline"> </a></h4>
<p>The standard parameters for completions routes with type <code class="docutils literal notranslate"><span class="pre">llm/v1/completions</span></code> are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 13%" />
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Query Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>prompt</strong></p></td>
<td><p>string</p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>The prompt for which to generate completions.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>n</strong></p></td>
<td><p>integer</p></td>
<td><p>No</p></td>
<td><p>1</p></td>
<td><p>The number of completions to generate for the
specified prompt, between 1 and 5.</p></td>
</tr>
<tr class="row-even"><td><p><strong>temperature</strong></p></td>
<td><p>float</p></td>
<td><p>No</p></td>
<td><p>0.0</p></td>
<td><p>The sampling temperature to use, between 0 and 1.
Higher values will make the output more random, and
lower values will make the output more deterministic.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>max_tokens</strong></p></td>
<td><p>integer</p></td>
<td><p>No</p></td>
<td><p>None</p></td>
<td><p>The maximum completion length, between 1 and infinity
(unlimited).</p></td>
</tr>
<tr class="row-even"><td><p><strong>stop</strong></p></td>
<td><p>array[string]</p></td>
<td><p>No</p></td>
<td><p>None</p></td>
<td><p>Sequences where the model should stop generating
tokens and return the completion.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="chat">
<h4>Chat<a class="headerlink" href="#chat" title="Permalink to this headline"> </a></h4>
<p>The standard parameters for chat routes with type <code class="docutils literal notranslate"><span class="pre">llm/v1/chat</span></code> are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 13%" />
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Query Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>messages</strong></p></td>
<td><p>array[message]</p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>A list of messages in a conversation from which to
a new message (chat completion). For information
about the message structure, see
<a class="reference internal" href="#chat-message-structure"><span class="std std-ref">Messages</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>n</strong></p></td>
<td><p>integer</p></td>
<td><p>No</p></td>
<td><p>1</p></td>
<td><p>The number of chat completions to generate for the
specified prompt, between 1 and 5.</p></td>
</tr>
<tr class="row-even"><td><p><strong>temperature</strong></p></td>
<td><p>float</p></td>
<td><p>No</p></td>
<td><p>0.0</p></td>
<td><p>The sampling temperature to use, between 0 and 1.
Higher values will make the output more random, and
lower values will make the output more deterministic.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>max_tokens</strong></p></td>
<td><p>integer</p></td>
<td><p>No</p></td>
<td><p>None</p></td>
<td><p>The maximum completion length, between 1 and infinity
(unlimited).</p></td>
</tr>
<tr class="row-even"><td><p><strong>stop</strong></p></td>
<td><p>array[string]</p></td>
<td><p>No</p></td>
<td><p>None</p></td>
<td><p>Sequences where the model should stop generating
tokens and return the chat completion.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="messages">
<span id="chat-message-structure"></span><h5>Messages<a class="headerlink" href="#messages" title="Permalink to this headline"> </a></h5>
<p>Each chat message is a string dictionary containing the following fields:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 8%" />
<col style="width: 21%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Field Name</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>role</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>The role of the conversation participant who sent the
message. Must be one of: <code class="docutils literal notranslate"><span class="pre">&quot;system&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;user&quot;</span></code>, or
<code class="docutils literal notranslate"><span class="pre">&quot;assistant&quot;</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>content</strong></p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>The message content.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="embeddings">
<h4>Embeddings<a class="headerlink" href="#embeddings" title="Permalink to this headline"> </a></h4>
<p>The standard parameters for completions routes with type <code class="docutils literal notranslate"><span class="pre">llm/v1/embeddings</span></code> are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 13%" />
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Query Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>input</strong></p></td>
<td><p>string
or
array[string]</p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
<td><p>A string or list of strings for which to generate
embeddings.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="additional-query-parameters">
<h3>Additional Query Parameters<a class="headerlink" href="#additional-query-parameters" title="Permalink to this headline"> </a></h3>
<p>In addition to the <a class="reference internal" href="#standard-query-parameters"><span class="std std-ref">Standard Query Parameters</span></a>, you can pass any additional parameters supported by the route’s provider as part of your query. For example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">logit_bias</span></code> (supported by OpenAI, Cohere)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_k</span></code> (supported by MosaicML, Anthropic, PaLM, Cohere)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">frequency_penalty</span></code> (supported by OpenAI, Cohere, AI21 Labs)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">presence_penalty</span></code> (supported by OpenAI, Cohere, AI21 Labs)</p></li>
</ul>
<p>The following parameters are not allowed:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">stream</span></code> is not supported. Setting this parameter on any provider will not work currently.</p></li>
</ul>
<p>Below is an example of submitting a query request to an MLflow AI Gateway route using additional parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="p">(</span>
        <span class="s2">&quot;What would happen if an asteroid the size of &quot;</span>
        <span class="s2">&quot;a basketball encountered the Earth traveling at 0.5c? &quot;</span>
        <span class="s2">&quot;Please provide your answer in .rst format for the purposes of documentation.&quot;</span>
    <span class="p">),</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;frequency_penalty&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s2">&quot;presence_penalty&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">query</span><span class="p">(</span><span class="n">route</span><span class="o">=</span><span class="s2">&quot;completions-gpt4&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The results of the query are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;chatcmpl-8Pr33fsCAtD2L4oZHlyfOkiYHLapc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;object&quot;</span><span class="p">:</span> <span class="s2">&quot;text_completion&quot;</span><span class="p">,</span>
    <span class="s2">&quot;created&quot;</span><span class="p">:</span> <span class="mi">1701172809</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt-4-0613&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choices&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;If an asteroid the size of a basketball ...&quot;</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="s2">&quot;usage&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;prompt_tokens&quot;</span><span class="p">:</span> <span class="mi">43</span><span class="p">,</span>
        <span class="s2">&quot;completion_tokens&quot;</span><span class="p">:</span> <span class="mi">592</span><span class="p">,</span>
        <span class="s2">&quot;total_tokens&quot;</span><span class="p">:</span> <span class="mi">635</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="fastapi-documentation-docs">
<h3>FastAPI Documentation (“/docs”)<a class="headerlink" href="#fastapi-documentation-docs" title="Permalink to this headline"> </a></h3>
<p>FastAPI, the framework used for building the MLflow AI Gateway, provides an automatic interactive API
documentation interface, which is accessible at the “/docs” endpoint (e.g., <code class="docutils literal notranslate"><span class="pre">http://my.gateway:9000/docs</span></code>).
This interactive interface is very handy for exploring and testing the available API endpoints.</p>
<p>As a convenience, accessing the root URL (e.g., <code class="docutils literal notranslate"><span class="pre">http://my.gateway:9000</span></code>) redirects to this “/docs” endpoint.</p>
</div>
<div class="section" id="mlflow-python-client-apis">
<h3>MLflow Python Client APIs<a class="headerlink" href="#mlflow-python-client-apis" title="Permalink to this headline"> </a></h3>
<p><a class="reference internal" href="../../python_api/mlflow.gateway.html#mlflow.gateway.client.MlflowGatewayClient" title="mlflow.gateway.client.MlflowGatewayClient"><code class="xref py py-class docutils literal notranslate"><span class="pre">MlflowGatewayClient</span></code></a> is the user-facing client API that is used to interact with the MLflow AI Gateway.
It abstracts the HTTP requests to the Gateway via a simple, easy-to-use Python API.</p>
<p>The fluent API is a higher-level interface that supports setting the Gateway URI once and using simple functions to interact with the AI Gateway Server.</p>
<div class="section" id="fluent-api">
<span id="gateway-fluent-api"></span><h4>Fluent API<a class="headerlink" href="#fluent-api" title="Permalink to this headline"> </a></h4>
<p>For the <code class="docutils literal notranslate"><span class="pre">fluent</span></code> API, here are some examples:</p>
<ol class="arabic">
<li><p>Set the Gateway URI:</p>
<p>Before using the Fluent API, the gateway URI must be set via <a class="reference internal" href="../../python_api/mlflow.gateway.html#mlflow.gateway.set_gateway_uri" title="mlflow.gateway.set_gateway_uri"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_gateway_uri()</span></code></a>.</p>
<p>Alternatively to directly calling the <code class="docutils literal notranslate"><span class="pre">set_gateway_uri</span></code> function, the environment variable <code class="docutils literal notranslate"><span class="pre">MLFLOW_GATEWAY_URI</span></code> can be set
directly, achieving the same session-level persistence for all <code class="docutils literal notranslate"><span class="pre">fluent</span></code> API usages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.gateway</span> <span class="kn">import</span> <span class="n">set_gateway_uri</span>

<span class="n">set_gateway_uri</span><span class="p">(</span><span class="n">gateway_uri</span><span class="o">=</span><span class="s2">&quot;http://my.gateway:7000&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Query a route:</p>
<p>The <a class="reference internal" href="../../python_api/mlflow.gateway.html#mlflow.gateway.query" title="mlflow.gateway.query"><code class="xref py py-func docutils literal notranslate"><span class="pre">query()</span></code></a> function queries the specified route and returns the response from the provider
in a standardized format. The data structure you send in the query depends on the route.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.gateway</span> <span class="kn">import</span> <span class="n">query</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">query</span><span class="p">(</span>
    <span class="s2">&quot;embeddings&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It was the best of times&quot;</span><span class="p">,</span> <span class="s2">&quot;It was the worst of times&quot;</span><span class="p">]}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="client-api">
<span id="gateway-client-api"></span><h4>Client API<a class="headerlink" href="#client-api" title="Permalink to this headline"> </a></h4>
<p>To use the <code class="docutils literal notranslate"><span class="pre">MlflowGatewayClient</span></code> API, see the below examples for the available API methods:</p>
<ol class="arabic">
<li><p>Create an <code class="docutils literal notranslate"><span class="pre">MlflowGatewayClient</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.gateway</span> <span class="kn">import</span> <span class="n">MlflowGatewayClient</span>

<span class="n">gateway_client</span> <span class="o">=</span> <span class="n">MlflowGatewayClient</span><span class="p">(</span><span class="s2">&quot;http://my.gateway:8888&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>List all routes:</p>
<p>The <a class="reference internal" href="../../python_api/mlflow.gateway.html#mlflow.gateway.client.MlflowGatewayClient.search_routes" title="mlflow.gateway.client.MlflowGatewayClient.search_routes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">search_routes()</span></code></a> method returns a list of all routes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">routes</span> <span class="o">=</span> <span class="n">gateway_client</span><span class="o">.</span><span class="n">search_routes</span><span class="p">()</span>
<span class="k">for</span> <span class="n">route</span> <span class="ow">in</span> <span class="n">routes</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">route</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Query a route:</p>
<p>The <a class="reference internal" href="../../python_api/mlflow.gateway.html#mlflow.gateway.client.MlflowGatewayClient.query" title="mlflow.gateway.client.MlflowGatewayClient.query"><code class="xref py py-meth docutils literal notranslate"><span class="pre">query()</span></code></a> method submits a query to a configured provider route.
The data structure you send in the query depends on the route.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">gateway_client</span><span class="o">.</span><span class="n">query</span><span class="p">(</span>
    <span class="s2">&quot;chat&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about rabbits&quot;</span><span class="p">}]}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="langchain-integration">
<h4>LangChain Integration<a class="headerlink" href="#langchain-integration" title="Permalink to this headline"> </a></h4>
<p><a class="reference external" href="https://github.com/hwchase17/langchain">LangChain</a> supports <a class="reference external" href="https://python.langchain.com/docs/ecosystem/integrations/mlflow_ai_gateway">an integration for MLflow AI Gateway</a>.
This integration enable users to use prompt engineering, retrieval augmented generation, and other techniques with LLMs in the gateway.</p>
<div class="literal-block-wrapper docutils container" id="id14">
<div class="code-block-caption"><span class="caption-text">Example</span><a class="headerlink" href="#id14" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">LLMChain</span><span class="p">,</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">MlflowAIGateway</span>

<span class="n">gateway</span> <span class="o">=</span> <span class="n">MlflowAIGateway</span><span class="p">(</span>
    <span class="n">gateway_uri</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:5000&quot;</span><span class="p">,</span>
    <span class="n">route</span><span class="o">=</span><span class="s2">&quot;completions&quot;</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">llm_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">gateway</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">PromptTemplate</span><span class="p">(</span>
        <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;adjective&quot;</span><span class="p">],</span>
        <span class="n">template</span><span class="o">=</span><span class="s2">&quot;Tell me a </span><span class="si">{adjective}</span><span class="s2"> joke&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">llm_chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">adjective</span><span class="o">=</span><span class="s2">&quot;funny&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([{</span><span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="s2">&quot;funny&quot;</span><span class="p">}]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="mlflow-models">
<span id="gateway-mlflow-models"></span><h4>MLflow Models<a class="headerlink" href="#mlflow-models" title="Permalink to this headline"> </a></h4>
<p>Interfacing with MLflow Models can be done in two ways. With the use of a custom PyFunc Model, a query can be issued directly to an AI Gateway endpoint and used in a broader context within a model.
Data may be augmented, manipulated, or used in a mixture of experts paradigm. The other means of utilizing the AI Gateway along with MLflow Models is to define a served MLflow model directly as a
route within the AI Gateway.</p>
<div class="section" id="using-the-ai-gateway-to-query-a-served-mlflow-model">
<h5>Using the AI Gateway to Query a served MLflow Model<a class="headerlink" href="#using-the-ai-gateway-to-query-a-served-mlflow-model" title="Permalink to this headline"> </a></h5>
<p>For a full walkthrough and example of using the MLflow serving integration to query a model directly through the MLflow AI Gateway, please see <a class="reference external" href="https://github.com/mlflow/mlflow/tree/master/examples/gateway/mlflow_serving/README.md">the full example</a>.
Within the guide, you will see the entire end-to-end process of serving multiple models from different servers and configuring an MLflow AI Gateway server instance to provide a single unified point to handle queries from.</p>
</div>
<div class="section" id="using-an-mlflow-model-to-query-the-ai-gateway">
<h5>Using an MLflow Model to Query the AI Gateway<a class="headerlink" href="#using-an-mlflow-model-to-query-the-ai-gateway" title="Permalink to this headline"> </a></h5>
<p>You can also build and deploy MLflow Models that call the MLflow AI Gateway.
The example below demonstrates how to use an AI Gateway server from within a custom <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The custom <code class="docutils literal notranslate"><span class="pre">Model</span></code> shown in the example below is utilizing environment variables for the AI Gateway server’s uri. These values can also be set manually within the
definition or can be applied via <a class="reference internal" href="../../python_api/mlflow.gateway.html#mlflow.gateway.get_gateway_uri" title="mlflow.gateway.get_gateway_uri"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.gateway.get_gateway_uri()</span></code></a> after the uri has been set. For the example below, the value for <code class="docutils literal notranslate"><span class="pre">MLFLOW_GATEWAY_URI</span></code> is
<code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:5000/</span></code>. For an actual deployment use case, this value would be set to the configured and production deployment server.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">mlflow</span>


<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">mlflow.gateway</span> <span class="kn">import</span> <span class="n">MlflowGatewayClient</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">MlflowGatewayClient</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MLFLOW_GATEWAY_URI&quot;</span><span class="p">])</span>

    <span class="n">payload</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">client</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">route</span><span class="o">=</span><span class="s2">&quot;completions-claude&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">query</span><span class="p">)[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">payload</span>
    <span class="p">]</span>


<span class="n">input_example</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Where is the moon?&quot;</span><span class="p">,</span> <span class="s2">&quot;What is a comet made of?&quot;</span><span class="p">]}</span>
<span class="p">)</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">input_example</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Above our heads.&quot;</span><span class="p">,</span> <span class="s2">&quot;It&#39;s mostly ice and rocks.&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span>
        <span class="n">registered_model_name</span><span class="o">=</span><span class="s2">&quot;anthropic_completions&quot;</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;anthropic_completions&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Tell me about Jupiter&quot;</span><span class="p">,</span> <span class="s2">&quot;Tell me about Saturn&quot;</span><span class="p">],</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>
        <span class="s2">&quot;max_records&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</pre></div>
</div>
<p>This custom MLflow model can be used in the same way as any other MLflow model. It can be used within a <code class="docutils literal notranslate"><span class="pre">spark_udf</span></code>, used with <a class="reference internal" href="../../python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>, or <a class="reference external" href="https://mlflow.org/docs/latest/models.html#built-in-deployment-tools">deploy</a> like any other model.</p>
</div>
</div>
<div class="section" id="rest-api">
<span id="gateway-rest-api"></span><h4>REST API<a class="headerlink" href="#rest-api" title="Permalink to this headline"> </a></h4>
<p>The REST API allows you to send HTTP requests directly to the MLflow AI Gateway server. This is useful if you’re not using Python or if you prefer to interact with the Gateway using HTTP directly.</p>
<p>Here are some examples for how you might use curl to interact with the Gateway:</p>
<ol class="arabic">
<li><p>Get information about a particular route: <code class="docutils literal notranslate"><span class="pre">GET</span> <span class="pre">/api/2.0/gateway/routes/{name}</span></code>
This endpoint returns a serialized representation of the Route data structure.
This provides information about the name and type, as well as the model details for the requested route endpoint.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>-X<span class="w"> </span>GET<span class="w"> </span>http://my.gateway:8888/api/2.0/gateway/routes/embeddings
</pre></div>
</div>
</li>
<li><p>List all routes: <code class="docutils literal notranslate"><span class="pre">GET</span> <span class="pre">/api/2.0/gateway/routes/</span></code></p>
<p>This endpoint returns a list of all routes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>-X<span class="w"> </span>GET<span class="w"> </span>http://my.gateway:8888/api/2.0/gateway/routes/
</pre></div>
</div>
</li>
<li><p>Query a route: <code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/gateway/{route}/invocations</span></code></p>
<p>This endpoint allows you to submit a query to a configured provider route. The data structure you send in the query depends on the route. Here are examples for the “completions”, “chat”, and “embeddings” routes:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Completions</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://my.gateway:8888/gateway/completions/invocations<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;prompt&quot;: &quot;Describe the probability distribution of the decay chain of U-235&quot;}&#39;</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Chat</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://my.gateway:8888/gateway/chat/invocations<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Can you write a limerick about orange flavored popsicles?&quot;}]}&#39;</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Embeddings</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://my.gateway:8888/gateway/embeddings/invocations<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;input&quot;: [&quot;I would like to return my shipment of beanie babies, please&quot;, &quot;Can I please speak to a human now?&quot;]}&#39;</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
<p><strong>Note:</strong> Remember to replace <code class="docutils literal notranslate"><span class="pre">http://my.gateway:8888</span></code> with the URL of your actual MLflow AI Gateway Server.</p>
</div>
</div>
</div>
<div class="section" id="mlflow-ai-gateway-api-documentation">
<h2>MLflow AI Gateway API Documentation<a class="headerlink" href="#mlflow-ai-gateway-api-documentation" title="Permalink to this headline"> </a></h2>
<p><a class="reference external" href="../deployments/api.html">API documentation</a></p>
</div>
<div class="section" id="ai-gateway-security-considerations">
<span id="gateway-security"></span><h2>AI Gateway Security Considerations<a class="headerlink" href="#ai-gateway-security-considerations" title="Permalink to this headline"> </a></h2>
<p>Remember to ensure secure access to the system that the MLflow AI Gateway service is running in to protect access to these keys.</p>
<p>An effective way to secure your MLflow AI Gateway service is by placing it behind a reverse proxy. This will allow the reverse proxy to handle incoming requests and forward them to the MLflow AI Gateway. The reverse proxy effectively shields your application from direct exposure to Internet traffic.</p>
<p>A popular choice for a reverse proxy is <cite>Nginx</cite>. In addition to handling the traffic to your application, <cite>Nginx</cite> can also serve static files and load balance the traffic if you have multiple instances of your application running.</p>
<p>Furthermore, to ensure the integrity and confidentiality of data between the client and the server, it’s highly recommended to enable HTTPS on your reverse proxy.</p>
<p>In addition to the reverse proxy, it’s also recommended to add an authentication layer before the requests reach the MLflow AI Gateway. This could be HTTP Basic Authentication, OAuth, or any other method that suits your needs.</p>
<p>For example, here’s a simple configuration for Nginx with Basic Authentication:</p>
<div class="highlight-nginx notranslate"><div class="highlight"><pre><span></span><span class="k">http</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kn">server</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kn">listen</span><span class="w"> </span><span class="mi">80</span><span class="p">;</span>

<span class="w">        </span><span class="kn">location</span><span class="w"> </span><span class="s">/</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kn">auth_basic</span><span class="w"> </span><span class="s">&quot;Restricted</span><span class="w"> </span><span class="s">Content&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="kn">auth_basic_user_file</span><span class="w"> </span><span class="s">/etc/nginx/.htpasswd</span><span class="p">;</span>

<span class="w">            </span><span class="kn">proxy_pass</span><span class="w"> </span><span class="s">http://localhost:5000</span><span class="p">;</span><span class="w">  </span><span class="c1"># Replace with the MLflow AI Gateway service port</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, <cite>/etc/nginx/.htpasswd</cite> is a file that contains the username and password for authentication.</p>
<p>These measures, together with a proper network setup, can significantly improve the security of your system and ensure that only authorized users have access to submit requests to your LLM services.</p>
</div>
<div class="section" id="id12">
<h2>LangChain Integration<a class="headerlink" href="#id12" title="Permalink to this headline"> </a></h2>
<p><a class="reference external" href="https://github.com/hwchase17/langchain">LangChain</a> supports an integration for MLflow AI Gateway. See <a class="reference external" href="https://python.langchain.com/docs/ecosystem/integrations/mlflow_ai_gateway">https://python.langchain.com/docs/ecosystem/integrations/mlflow_ai_gateway</a> for more information.</p>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../llm-evaluate/notebooks/huggingface-evaluation.html" class="btn btn-neutral" title="Evaluate a Hugging Face LLM with mlflow.evaluate()" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="guides/index.html" class="btn btn-neutral" title="Getting Started with the MLflow AI Gateway" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../',
      VERSION:'2.9.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>