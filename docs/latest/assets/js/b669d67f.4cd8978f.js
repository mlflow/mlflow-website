"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2937],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var a=i(96540);const t={},l=a.createContext(t);function o(e){const n=a.useContext(l);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),a.createElement(l.Provider,{value:n},e.children)}},49374:(e,n,i)=>{i.d(n,{B:()=>s});i(96540);const a=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var t=i(86025),l=i(28774),o=i(74848);const r=e=>{const n=e.split(".");for(let i=n.length;i>0;i--){const e=n.slice(0,i).join(".");if(a[e])return e}return null};function s({fn:e,children:n}){const i=r(e);if(!i)return(0,o.jsx)(o.Fragment,{children:n});const s=(0,t.Ay)(`/${a[i]}#${e}`);return(0,o.jsx)(l.A,{to:s,target:"_blank",children:n??(0,o.jsxs)("code",{children:[e,"()"]})})}},78333:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"flavors/langchain/guide/index","title":"LangChain within MLflow (Experimental)","description":"The langchain flavor is currently under active development and is marked as Experimental. Public APIs are evolving, and new features are being added to enhance its functionality.","source":"@site/docs/genai/flavors/langchain/guide/index.mdx","sourceDirName":"flavors/langchain/guide","slug":"/flavors/langchain/guide/","permalink":"/docs/latest/genai/flavors/langchain/guide/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"MLflow LangChain Flavor","permalink":"/docs/latest/genai/flavors/langchain/"},"next":{"title":"LangChain Quickstart","permalink":"/docs/latest/genai/flavors/langchain/notebooks/langchain-quickstart"}}');var t=i(74848),l=i(28453),o=i(49374);const r={},s="LangChain within MLflow (Experimental)",h={},d=[{value:"Overview",id:"overview",level:2},{value:"LangChain&#39;s Technical Essence",id:"langchains-technical-essence",level:2},{value:"Building Chains with LangChain",id:"building-chains-with-langchain",level:2},{value:"Integration with MLflow",id:"integration-with-mlflow",level:2},{value:"Basic Example: Logging a LangChain <code>LLMChain</code> in MLflow",id:"basic-example-logging-a-langchain-llmchain-in-mlflow",level:3},{value:"What the Simple LLMChain Example Showcases",id:"what-the-simple-llmchain-example-showcases",level:4},{value:"Logging a LangChain Agent with MLflow",id:"logging-a-langchain-agent-with-mlflow",level:3},{value:"What is an Agent?",id:"what-is-an-agent",level:4},{value:"Key Components of Agents",id:"key-components-of-agents",level:4},{value:"Agent",id:"agent",level:5},{value:"Tools",id:"tools",level:5},{value:"Toolkits",id:"toolkits",level:5},{value:"AgentExecutor",id:"agentexecutor",level:5},{value:"Additional Agent Runtimes",id:"additional-agent-runtimes",level:5},{value:"An Example of Logging an LangChain Agent",id:"an-example-of-logging-an-langchain-agent",level:4},{value:"What the Simple Agent Example Showcases",id:"what-the-simple-agent-example-showcases",level:4},{value:"Real-Time Streaming Outputs with LangChain and GenAI LLMs",id:"real-time-streaming-outputs-with-langchain-and-genai-llms",level:2},{value:"Overview of Streaming Output Capabilities",id:"overview-of-streaming-output-capabilities",level:3},{value:"Supported Streaming Models",id:"supported-streaming-models",level:3},{value:"Using <code>predict_stream</code> for Streaming Outputs",id:"using-predict_stream-for-streaming-outputs",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Advanced Integration with Callbacks",id:"advanced-integration-with-callbacks",level:3},{value:"Enhanced Management of RetrievalQA Chains with MLflow",id:"enhanced-management-of-retrievalqa-chains-with-mlflow",level:2},{value:"Key Insights into RetrievalQA Chains",id:"key-insights-into-retrievalqa-chains",level:3},{value:"Detailed Overview of the RAG Process",id:"detailed-overview-of-the-rag-process",level:3},{value:"Clarifying Vector Database Management with MLflow",id:"clarifying-vector-database-management-with-mlflow",level:3},{value:"Key MLflow Components and VectorDB Logging",id:"key-mlflow-components-and-vectordb-logging",level:3},{value:"Important Considerations",id:"important-considerations",level:3},{value:"An Example of logging a LangChain RetrievalQA Chain",id:"an-example-of-logging-a-langchain-retrievalqa-chain",level:3},{value:"Logging and Evaluating a LangChain Retriever in MLflow",id:"logging-and-evaluating-a-langchain-retriever-in-mlflow",level:3},{value:"Purpose of Logging Individual Retrievers",id:"purpose-of-logging-individual-retrievers",level:4},{value:"Requirements for Logging Retrievers in MLflow",id:"requirements-for-logging-retrievers-in-mlflow",level:4},{value:"An example of logging a LangChain Retriever",id:"an-example-of-logging-a-langchain-retriever",level:4},{value:"MLflow Langchain Autologging",id:"mlflow-langchain-autologging",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"langchain-within-mlflow-experimental",children:"LangChain within MLflow (Experimental)"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"langchain"})," flavor is currently under active development and is marked as Experimental. Public APIs are evolving, and new features are being added to enhance its functionality."]})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://www.langchain.com/",children:"LangChain"})," is a Python framework for creating applications powered by language models. It offers unique features for developing context-aware\napplications that utilize language models for reasoning and generating responses. This integration with MLflow streamlines the development and\ndeployment of complex NLP applications."]}),"\n",(0,t.jsx)(n.h2,{id:"langchains-technical-essence",children:"LangChain's Technical Essence"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context-Aware Applications"}),": LangChain specializes in connecting language models to various sources of context, enabling them to produce more relevant and accurate outputs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reasoning Capabilities"}),": It uses the power of language models to reason about the given context and take appropriate actions based on it."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible Chain Composition"}),": The LangChain Expression Language (LCEL) allows for easy construction of complex chains from basic components, supporting functionalities like streaming, parallelism, and logging."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"building-chains-with-langchain",children:"Building Chains with LangChain"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Basic Components"}),": LangChain facilitates chaining together components like prompt templates, models, and output parsers to create complex workflows."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Example - Joke Generator"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A basic chain can take a topic and generate a joke using a combination of a prompt template, a ChatOpenAI model, and an output parser."}),"\n",(0,t.jsxs)(n.li,{children:["The components are chained using the ",(0,t.jsx)(n.code,{children:"|"})," operator, similar to a Unix pipe, allowing the output of one component to feed into the next."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Advanced Use Cases"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"LangChain also supports more complex setups, like Retrieval-Augmented Generation (RAG) chains, which can add context when responding to questions."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-mlflow",children:"Integration with MLflow"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simplified Logging and Loading"}),": MLflow's ",(0,t.jsx)(n.code,{children:"langchain"})," flavor provides functions like ",(0,t.jsx)(n.code,{children:"log_model()"})," and ",(0,t.jsx)(n.code,{children:"load_model()"}),", enabling easy logging and retrieval of LangChain models within the MLflow ecosystem."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simplified Deployment"}),": LangChain models logged in MLflow can be interpreted as generic Python functions, simplifying their deployment and use in diverse applications. With dependency management incorporated directly into your logged model, you can deploy your application knowing that the environment that you used to train the model is what will be used to serve it."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Versatile Model Interaction"}),": The integration allows developers to leverage LangChain's unique features in conjunction with MLflow's robust model tracking and management capabilities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Autologging"}),": MLflow's ",(0,t.jsx)(n.code,{children:"langchain"})," flavor provides autologging of LangChain models, which automatically logs artifacts, metrics and models for inference."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"langchain"})," model flavor enables logging of ",(0,t.jsx)(n.a,{href:"https://github.com/hwchase17/langchain",children:"LangChain models"})," in MLflow format via\nthe ",(0,t.jsx)(o.B,{fn:"mlflow.langchain.save_model"})," and ",(0,t.jsx)(o.B,{fn:"mlflow.langchain.log_model"})," functions. Use of these\nfunctions also adds the ",(0,t.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the model to be\ninterpreted as a generic Python function for inference via ",(0,t.jsx)(o.B,{fn:"mlflow.pyfunc.load_model"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["You can also use the ",(0,t.jsx)(o.B,{fn:"mlflow.langchain.load_model"})," function to load a saved or logged MLflow\nModel with the ",(0,t.jsx)(n.code,{children:"langchain"})," flavor as a dictionary of the model's attributes."]}),"\n",(0,t.jsxs)(n.h3,{id:"basic-example-logging-a-langchain-llmchain-in-mlflow",children:["Basic Example: Logging a LangChain ",(0,t.jsx)(n.code,{children:"LLMChain"})," in MLflow"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nimport mlflow\n\n# Ensure the OpenAI API key is set in the environment\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\n\n# Initialize the OpenAI model and the prompt template\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=["product"],\n    template="What is a good name for a company that makes {product}?",\n)\n\n# Create the LLMChain with the specified model and prompt\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Log the LangChain LLMChain in an MLflow run\nwith mlflow.start_run():\n    logged_model = mlflow.langchain.log_model(chain, name="langchain_model")\n\n# Load the logged model using MLflow\'s Python function flavor\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n\n# Predict using the loaded model\nprint(loaded_model.predict([{"product": "colorful socks"}]))\n'})}),"\n",(0,t.jsx)(n.p,{children:"The output of the example is shown below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'["\\n\\nColorful Cozy Creations."]\n'})}),"\n",(0,t.jsx)(n.h4,{id:"what-the-simple-llmchain-example-showcases",children:"What the Simple LLMChain Example Showcases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration Flexibility"}),": The example highlights how LangChain's LLMChain, consisting of an OpenAI model and a custom prompt template, can be easily logged in MLflow."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simplified Model Management"}),": Through MLflow's ",(0,t.jsx)(n.code,{children:"langchain"})," flavor, the chain is logged, enabling version control, tracking, and easy retrieval."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ease of Deployment"}),": The logged LangChain model is loaded using MLflow's ",(0,t.jsx)(n.code,{children:"pyfunc"})," module, illustrating the straightforward deployment process for LangChain models within MLflow."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Practical Application"}),": The final prediction step demonstrates the model's functionality in a real-world scenario, generating a company name based on a given product."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"logging-a-langchain-agent-with-mlflow",children:"Logging a LangChain Agent with MLflow"}),"\n",(0,t.jsx)(n.h4,{id:"what-is-an-agent",children:"What is an Agent?"}),"\n",(0,t.jsxs)(n.p,{children:["Agents in LangChain leverage language models to dynamically determine and execute a sequence of actions, contrasting with the hardcoded sequences in chains.\nTo learn more about Agents and see additional examples within LangChain, you can ",(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/agents/",children:"read the LangChain docs on Agents"}),"."]}),"\n",(0,t.jsx)(n.h4,{id:"key-components-of-agents",children:"Key Components of Agents"}),"\n",(0,t.jsx)(n.h5,{id:"agent",children:"Agent"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The core chain driving decision-making, utilizing a language model and a prompt."}),"\n",(0,t.jsx)(n.li,{children:"Receives inputs like tool descriptions, user objectives, and previously executed steps."}),"\n",(0,t.jsx)(n.li,{children:"Outputs the next action set (AgentActions) or the final response (AgentFinish)."}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"tools",children:"Tools"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Functions invoked by agents to fulfill tasks."}),"\n",(0,t.jsx)(n.li,{children:"Essential to provide appropriate tools and accurately describe them for effective use."}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"toolkits",children:"Toolkits"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Collections of tools tailored for specific tasks."}),"\n",(0,t.jsx)(n.li,{children:"LangChain offers a range of built-in toolkits and supports custom toolkit creation."}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"agentexecutor",children:"AgentExecutor"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The runtime environment executing agent decisions."}),"\n",(0,t.jsx)(n.li,{children:"Handles complexities such as tool errors and agent output parsing."}),"\n",(0,t.jsx)(n.li,{children:"Ensures comprehensive logging and observability."}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"additional-agent-runtimes",children:"Additional Agent Runtimes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Beyond AgentExecutor, LangChain supports experimental runtimes like Plan-and-execute Agent, Baby AGI, and Auto GPT."}),"\n",(0,t.jsx)(n.li,{children:"Custom runtime logic creation is also facilitated."}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"an-example-of-logging-an-langchain-agent",children:"An Example of Logging an LangChain Agent"}),"\n",(0,t.jsx)(n.p,{children:"This example illustrates the process of logging a LangChain Agent in MLflow, highlighting the integration of LangChain's complex agent functionalities with MLflow's robust model management."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\nimport mlflow\n\n# Note: Ensure that the package \'google-search-results\' is installed via pypi to run this example\n# and that you have a accounts with SerpAPI and OpenAI to use their APIs.\n\n# Ensuring necessary API keys are set\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\nassert (\n    "SERPAPI_API_KEY" in os.environ\n), "Please set the SERPAPI_API_KEY environment variable."\n\n# Load the language model for agent control\nllm = OpenAI(temperature=0)\n\n# Next, let\'s load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools(["serpapi", "llm-math"], llm=llm)\n\n# Finally, let\'s initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n\n# Log the agent in an MLflow run\nwith mlflow.start_run():\n    logged_model = mlflow.langchain.log_model(agent, name="langchain_model")\n\n# Load the logged agent model for prediction\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n\n# Generate an inference result using the loaded model\nquestion = "What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?"\n\nanswer = loaded_model.predict([{"input": question}])\n\nprint(answer)\n'})}),"\n",(0,t.jsx)(n.p,{children:"The output of the example above is shown below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'["1.1044000282035853"]\n'})}),"\n",(0,t.jsx)(n.h4,{id:"what-the-simple-agent-example-showcases",children:"What the Simple Agent Example Showcases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex Agent Logging"}),": Demonstrates how LangChain's sophisticated agent, which utilizes multiple tools and a language model, can be logged in MLflow."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration of Advanced Tools"}),": Showcases the use of additional tools like 'serpapi' and 'llm-math' with a LangChain agent, emphasizing the framework's capability to integrate complex functionalities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Agent Initialization and Usage"}),": Details the initialization process of a LangChain agent with specific tools and model settings, and how it can be used to perform complex queries."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Efficient Model Management and Deployment"}),": Illustrates the ease with which complex LangChain agents can be managed and deployed using MLflow, from logging to prediction."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"real-time-streaming-outputs-with-langchain-and-genai-llms",children:"Real-Time Streaming Outputs with LangChain and GenAI LLMs"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Stream responses via the ",(0,t.jsx)(n.code,{children:"predict_stream"})," API are only available in MLflow versions >= 2.12.2. Previous versions of MLflow do not support streaming responses."]})}),"\n",(0,t.jsx)(n.h3,{id:"overview-of-streaming-output-capabilities",children:"Overview of Streaming Output Capabilities"}),"\n",(0,t.jsx)(n.p,{children:"LangChain integration within MLflow enables real-time streaming outputs from various GenAI language models (LLMs) that support such functionality.\nThis feature is essential for applications that require immediate, incremental responses, facilitating dynamic interactions such as conversational\nagents or live content generation."}),"\n",(0,t.jsx)(n.h3,{id:"supported-streaming-models",children:"Supported Streaming Models"}),"\n",(0,t.jsx)(n.p,{children:"LangChain is designed to work seamlessly with any LLM that offers streaming output capabilities. This includes certain models from providers\nlike OpenAI (e.g., specific versions of ChatGPT), as well as other LLMs from different vendors that support similar functionalities."}),"\n",(0,t.jsxs)(n.h3,{id:"using-predict_stream-for-streaming-outputs",children:["Using ",(0,t.jsx)(n.code,{children:"predict_stream"})," for Streaming Outputs"]}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"predict_stream"})," method within the MLflow pyfunc LangChain flavor is designed to handle synchronous inputs and provide outputs in a streaming manner. This method is particularly\nuseful for maintaining an engaging user experience by delivering parts of the model's response as they become available, rather than waiting for the\nentire completion of the response generation."]}),"\n",(0,t.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,t.jsxs)(n.p,{children:["The following example demonstrates setting up and using the ",(0,t.jsx)(n.code,{children:"predict_stream"})," function with a LangChain model managed in MLflow, highlighting\nthe real-time response generation:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\nimport mlflow\n\n\ntemplate_instructions = "Provide brief answers to technical questions about {topic} and do not answer non-technical questions."\nprompt = PromptTemplate(\n    input_variables=["topic"],\n    template=template_instructions,\n)\nchain = LLMChain(llm=OpenAI(temperature=0.05), prompt=prompt)\n\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(chain, name="tech_chain")\n\n# Assuming the model is already logged in MLflow and loaded\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# Simulate a single synchronous input\ninput_data = "Hello, can you explain streaming outputs?"\n\n# Generate responses in a streaming fashion\nresponse_stream = loaded_model.predict_stream(input_data)\nfor response_part in response_stream:\n    print("Streaming Response Part:", response_part)\n    # Each part of the response is handled as soon as it is generated\n'})}),"\n",(0,t.jsx)(n.h3,{id:"advanced-integration-with-callbacks",children:"Advanced Integration with Callbacks"}),"\n",(0,t.jsx)(n.p,{children:"LangChain's architecture also supports the use of callbacks within the streaming output context. These callbacks can be used to enhance\nfunctionality by allowing actions to be triggered during the streaming process, such as logging intermediate responses or modifying them before delivery."}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Most uses of callback handlers involve logging of traces involved in the various calls to services and tools within a Chain or Retriever. For purposes\nof simplicity, a simple ",(0,t.jsx)(n.code,{children:"stdout"})," callback handler is shown below. Real-world callback handlers must be subclasses of the ",(0,t.jsx)(n.code,{children:"BaseCallbackHandler"})," class\nfrom LangChain."]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain_core.callbacks import StdOutCallbackHandler\n\nhandler = StdOutCallbackHandler()\n\n# Attach callback to enhance the streaming process\nresponse_stream = loaded_model.predict_stream(input_data, callback_handlers=[handler])\nfor enhanced_response in response_stream:\n    print("Enhanced Streaming Response:", enhanced_response)\n'})}),"\n",(0,t.jsx)(n.p,{children:"These examples and explanations show how developers can utilize the real-time streaming output capabilities of LangChain models within MLflow,\nenabling the creation of highly responsive and interactive applications."}),"\n",(0,t.jsx)(n.h2,{id:"enhanced-management-of-retrievalqa-chains-with-mlflow",children:"Enhanced Management of RetrievalQA Chains with MLflow"}),"\n",(0,t.jsxs)(n.p,{children:["LangChain's integration with MLflow introduces a more efficient way to manage and utilize the ",(0,t.jsx)(n.code,{children:"RetrievalQA"})," chains, a key aspect of LangChain's capabilities.\nThese chains adeptly combine data retrieval with question-answering processes, leveraging the strength of language models."]}),"\n",(0,t.jsx)(n.h3,{id:"key-insights-into-retrievalqa-chains",children:"Key Insights into RetrievalQA Chains"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"RetrievalQA Chain Functionality"}),": These chains represent a sophisticated LangChain feature where information retrieval is seamlessly blended with language\nmodel-based question answering. They excel in scenarios requiring the language\nmodel to consult specific data or documents for accurate responses."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Role of the Retrieval Object"}),": At the core of RetrievalQA chains lies the retriever object, tasked with sourcing relevant documents or data in response\nto queries."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"detailed-overview-of-the-rag-process",children:"Detailed Overview of the RAG Process"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Document Loaders"}),": Facilitate loading documents from a diverse array of sources, boasting over 100 loaders and integrations."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Document Transformers"}),": Prepare documents for retrieval by transforming and segmenting them into manageable parts."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Text Embedding Models"}),": Generate semantic embeddings of texts, enhancing the relevance and efficiency of data retrieval."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vector Stores"}),": Specialized databases that store and facilitate the search of text embeddings."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Retrievers"}),": Employ various retrieval techniques, ranging from simple semantic searches to more sophisticated methods like the Parent Document Retriever and\nEnsemble Retriever."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"clarifying-vector-database-management-with-mlflow",children:"Clarifying Vector Database Management with MLflow"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Traditional LangChain Serialization"}),": LangChain typically requires manual management for the serialization of retriever objects, including handling of the vector database."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"MLflow's Simplification"}),": The ",(0,t.jsx)(n.code,{children:"langchain"})," flavor in MLflow substantially simplifies this process. It automates serialization, managing the contents of\nthe ",(0,t.jsx)(n.code,{children:"persist_dir"})," and the pickling of the ",(0,t.jsx)(n.code,{children:"loader_fn"})," function."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-mlflow-components-and-vectordb-logging",children:"Key MLflow Components and VectorDB Logging"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"persist_dir"}),": The directory where the retriever object, including the vector database, is stored."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"loader_fn"}),": The function for loading the retriever object from its storage location."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"important-considerations",children:"Important Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VectorDB Logging"}),": MLflow, through its ",(0,t.jsx)(n.code,{children:"langchain"})," flavor, does manage the vector database as part of the retriever object. However, the vector\ndatabase itself is not explicitly logged as a separate entity in MLflow."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Runtime VectorDB Maintenance"}),": It's essential to maintain consistency in the vector database between the training and runtime environments.\nWhile MLflow manages the serialization of the retriever object, ensuring that the same vector database is accessible during runtime remains crucial\nfor consistent performance."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"an-example-of-logging-a-langchain-retrievalqa-chain",children:"An Example of logging a LangChain RetrievalQA Chain"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nimport tempfile\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\nimport mlflow\n\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    persist_dir = os.path.join(temp_dir, "faiss_index")\n\n    # Create the vector db, persist the db to a local fs folder\n    loader = TextLoader("tests/langchain/state_of_the_union.txt")\n    documents = loader.load()\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    docs = text_splitter.split_documents(documents)\n    embeddings = OpenAIEmbeddings()\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(persist_dir)\n\n    # Create the RetrievalQA chain\n    retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=db.as_retriever())\n\n    # Log the retrievalQA chain\n    def load_retriever(persist_directory):\n        embeddings = OpenAIEmbeddings()\n        vectorstore = FAISS.load_local(persist_directory, embeddings)\n        return vectorstore.as_retriever()\n\n    with mlflow.start_run() as run:\n        logged_model = mlflow.langchain.log_model(\n            retrievalQA,\n            name="retrieval_qa",\n            loader_fn=load_retriever,\n            persist_dir=persist_dir,\n        )\n\n# Load the retrievalQA chain\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nprint(\n    loaded_model.predict(\n        [{"query": "What did the president say about Ketanji Brown Jackson"}]\n    )\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"The output of the example above is shown below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'[" The president said..."]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"logging-and-evaluating-a-langchain-retriever-in-mlflow",children:"Logging and Evaluating a LangChain Retriever in MLflow"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"langchain"})," flavor in MLflow extends its functionalities to include the logging and individual evaluation of retriever objects. This capability is particularly valuable for assessing the quality of documents retrieved by a retriever without needing to process them through a large language model (LLM)."]}),"\n",(0,t.jsx)(n.h4,{id:"purpose-of-logging-individual-retrievers",children:"Purpose of Logging Individual Retrievers"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Independent Evaluation"}),": Allows for the assessment of a retriever's performance in fetching relevant documents, independent of their subsequent use in LLMs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Quality Assurance"}),": Facilitates the evaluation of the retriever's effectiveness in sourcing accurate and contextually appropriate documents."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"requirements-for-logging-retrievers-in-mlflow",children:"Requirements for Logging Retrievers in MLflow"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"persist_dir"}),": Specifies where the retriever object is stored."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"loader_fn"}),": Details the function used to load the retriever object from its storage location."]}),"\n",(0,t.jsx)(n.li,{children:"These requirements align with those for logging RetrievalQA chains, ensuring consistency in the process."}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"an-example-of-logging-a-langchain-retriever",children:"An example of logging a LangChain Retriever"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nimport tempfile\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\nimport mlflow\n\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    persist_dir = os.path.join(temp_dir, "faiss_index")\n\n    # Create the vector database and persist it to a local filesystem folder\n    loader = TextLoader("tests/langchain/state_of_the_union.txt")\n    documents = loader.load()\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    docs = text_splitter.split_documents(documents)\n    embeddings = OpenAIEmbeddings()\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(persist_dir)\n\n    # Define a loader function to recall the retriever from the persisted vectorstore\n    def load_retriever(persist_directory):\n        embeddings = OpenAIEmbeddings()\n        vectorstore = FAISS.load_local(persist_directory, embeddings)\n        return vectorstore.as_retriever()\n\n    # Log the retriever with the loader function\n    with mlflow.start_run() as run:\n        logged_model = mlflow.langchain.log_model(\n            db.as_retriever(),\n            name="retriever",\n            loader_fn=load_retriever,\n            persist_dir=persist_dir,\n        )\n\n# Load the retriever chain\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nprint(\n    loaded_model.predict(\n        [{"query": "What did the president say about Ketanji Brown Jackson"}]\n    )\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"The output of the example above is shown below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'[\n    [\n        {\n            "page_content": "Tonight. I call...",\n            "metadata": {"source": "/state.txt"},\n        },\n        {\n            "page_content": "A former top...",\n            "metadata": {"source": "/state.txt"},\n        },\n    ]\n]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"mlflow-langchain-autologging",children:"MLflow Langchain Autologging"}),"\n",(0,t.jsxs)(n.p,{children:["Please refer to the ",(0,t.jsx)(n.a,{href:"/genai/flavors/langchain/autologging/",children:"MLflow Langchain Autologging "})," documentation for more details on how to enable autologging for Langchain models."]})]})}function m(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);