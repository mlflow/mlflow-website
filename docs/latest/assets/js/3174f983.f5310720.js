"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6174],{11470:(e,n,t)=>{t.d(n,{A:()=>w});var a=t(96540),s=t(34164),l=t(23104),r=t(56347),o=t(205),i=t(57485),c=t(31682),d=t(70679);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return u(e).map((({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a})))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h({value:e,tabValues:n}){return n.some((n=>n.value===e))}function m({queryString:e=!1,groupId:n}){const t=(0,r.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})}),[s,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,l=p(e),[r,i]=(0,a.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:l}))),[c,u]=m({queryString:t,groupId:s}),[g,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,d.Dv)(n);return[t,(0,a.useCallback)((e=>{n&&s.set(e)}),[n,s])]}({groupId:s}),x=(()=>{const e=c??g;return h({value:e,tabValues:l})?e:null})();(0,o.A)((()=>{x&&i(x)}),[x]);return{selectedValue:r,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),f(e)}),[u,f,l]),tabValues:l}}var f=t(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=t(74848);function j({className:e,block:n,selectedValue:t,selectValue:a,tabValues:r}){const o=[],{blockElementScrollPositionUntilNextRender:i}=(0,l.a_)(),c=e=>{const n=e.currentTarget,s=o.indexOf(n),l=r[s].value;l!==t&&(i(n),a(l))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:r.map((({value:e,label:n,attributes:a})=>(0,v.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...a,className:(0,s.A)("tabs__item",x.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function y({lazy:e,children:n,selectedValue:t}){const l=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=l.find((e=>e.props.value===t));return e?(0,a.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:l.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function b(e){const n=g(e);return(0,v.jsxs)("div",{className:(0,s.A)("tabs-container",x.tabList),children:[(0,v.jsx)(j,{...n,...e}),(0,v.jsx)(y,{...n,...e})]})}function w(e){const n=(0,f.A)();return(0,v.jsx)(b,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(34164);const s={tabItem:"tabItem_Ymn6"};var l=t(74848);function r({children:e,hidden:n,className:t}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,a.A)(s.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(96540);const s={},l=a.createContext(s);function r(e){const n=a.useContext(l);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(l.Provider,{value:n},e.children)}},86077:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"data-model/index","title":"MLflow Data Model","description":"MLflow\'s data model provides a structured approach to developing and managing GenAI applications by organizing how you log, debug, and evaluate them to achieve quality, cost, and latency goals. This structured approach addresses key challenges in reproducibility, quality assessment, and iterative development.","source":"@site/docs/genai/data-model/index.mdx","sourceDirName":"data-model","slug":"/data-model/","permalink":"/docs/latest/genai/data-model/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Unity Catalog","permalink":"/docs/latest/genai/governance/unity-catalog"},"next":{"title":"Experiments","permalink":"/docs/latest/genai/data-model/experiments"}}');var s=t(74848),l=t(28453),r=t(11470),o=t(19365);const i={},c="MLflow Data Model",d={},u=[{value:"Overview",id:"overview",level:2},{value:"MLflow Experiment",id:"mlflow-experiment",level:2},{value:"Setting up an Experiment",id:"setting-up-an-experiment",level:3},{value:"MLflow LoggedModel: Model Management",id:"mlflow-loggedmodel-model-management",level:2},{value:"Key Features of LoggedModel",id:"key-features-of-loggedmodel",level:3},{value:"Creating LoggedModels",id:"creating-loggedmodels",level:3},{value:"LoggedModel Benefits",id:"loggedmodel-benefits",level:3},{value:"MLflow Traces: The Foundation",id:"mlflow-traces-the-foundation",level:2},{value:"How Traces are Generated",id:"how-traces-are-generated",level:3},{value:"Purpose of Traces",id:"purpose-of-traces",level:3},{value:"Assessments: Quality Judgments",id:"assessments-quality-judgments",level:2},{value:"Feedback Assessments",id:"feedback-assessments",level:3},{value:"Expectation Assessments",id:"expectation-assessments",level:3},{value:"Scorers: Automated Quality Measurement",id:"scorers-automated-quality-measurement",level:2},{value:"Code-based Heuristics",id:"code-based-heuristics",level:3},{value:"LLM Judges",id:"llm-judges",level:3},{value:"Evaluation Datasets and Runs",id:"evaluation-datasets-and-runs",level:2},{value:"Evaluation Datasets",id:"evaluation-datasets",level:3},{value:"Evaluation Runs",id:"evaluation-runs",level:3},{value:"Labeling Sessions: Human Review",id:"labeling-sessions-human-review",level:2},{value:"Complete Data Model Structure",id:"complete-data-model-structure",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"mlflow-data-model",children:"MLflow Data Model"})}),"\n",(0,s.jsx)(n.p,{children:"MLflow's data model provides a structured approach to developing and managing GenAI applications by organizing how you log, debug, and evaluate them to achieve quality, cost, and latency goals. This structured approach addresses key challenges in reproducibility, quality assessment, and iterative development."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The MLflow data model consists of several interconnected entities that work together to support your GenAI application development workflow:"}),"\n",(0,s.jsxs)(n.p,{children:["\ud83e\uddea ",(0,s.jsx)(n.strong,{children:"Experiment"})," - The root container for your GenAI application"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83e\udd16 ",(0,s.jsx)(n.strong,{children:"LoggedModel"})," - A first-class entity representing your AI model or agent with integrated tracking"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd0d ",(0,s.jsx)(n.strong,{children:"Trace"})," - A log of inputs, outputs, and intermediate steps from a single application execution"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udcca ",(0,s.jsx)(n.strong,{children:"Assessments"})," - Quality judgments on a Trace, categorized as Feedback or Expectations"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,s.jsx)(n.strong,{children:"Scorers"})," - Definitions of automated evaluation functions that produce Feedback"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udccb ",(0,s.jsx)(n.strong,{children:"Evaluation Datasets"})," - Curated sets of inputs (and optional Expectations) for offline testing"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\ude80 ",(0,s.jsx)(n.strong,{children:"Evaluation Runs"})," - Results from running app versions against Evaluation Datasets, containing new, scored Traces"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83c\udff7\ufe0f ",(0,s.jsx)(n.strong,{children:"Labeling Sessions"})," - Collections of Traces organized for human review"]}),"\n",(0,s.jsx)(n.h2,{id:"mlflow-experiment",children:"MLflow Experiment"}),"\n",(0,s.jsxs)(n.p,{children:["An ",(0,s.jsx)(n.strong,{children:"Experiment"})," is the top-level container for each distinct application or use case. It contains all Traces from development and production alongside all other entities in the data model. We recommend creating a single Experiment for each application."]}),"\n",(0,s.jsx)(n.h3,{id:"setting-up-an-experiment",children:"Setting up an Experiment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Create or set an experiment\nmlflow.set_experiment("my-genai-app")\n\n# Or create explicitly\nexperiment = mlflow.create_experiment("my-genai-app")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"mlflow-loggedmodel-model-management",children:"MLflow LoggedModel: Model Management"}),"\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.strong,{children:"LoggedModel"})," is a first-class entity that represents your AI model, agent, or GenAI application within an Experiment. It provides unified tracking of model artifacts, execution traces, evaluation metrics, and metadata throughout the development lifecycle."]}),"\n",(0,s.jsx)(n.h3,{id:"key-features-of-loggedmodel",children:"Key Features of LoggedModel"}),"\n",(0,s.jsx)(n.p,{children:"LoggedModel serves as the central hub that connects:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model artifacts"})," and configuration parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution traces"})," from development and production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Evaluation metrics"})," and performance assessments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Version history"})," and deployment tracking"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"creating-loggedmodels",children:"Creating LoggedModels"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(o.A,{value:"direct",label:"Direct Model Logging",default:!0,children:[(0,s.jsx)(n.p,{children:"Create a LoggedModel by logging your model directly:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Log a model with comprehensive metadata\nlogged_model = mlflow.langchain.log_model(\n    lc_model=your_chain,\n    name="customer_support_agent",\n    params={"temperature": 0.1, "max_tokens": 2000},\n    model_type="agent",\n    input_example={"messages": "How can I help you?"},\n)\n\nprint(f"Model ID: {logged_model.model_id}")\n'})})]}),(0,s.jsxs)(o.A,{value:"active",label:"Active Model Pattern",children:[(0,s.jsx)(n.p,{children:"Use the active model pattern for automatic trace linking:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Set active model for automatic trace association\nmlflow.set_active_model(name="customer_support_agent")\n\n# Enable autologging\nmlflow.langchain.autolog()\n\n# All traces will be automatically linked to the active model\nresponse = your_model.invoke({"messages": "Hello!"})\n'})})]}),(0,s.jsxs)(o.A,{value:"external",label:"External Model Reference",children:[(0,s.jsx)(n.p,{children:"Reference models stored outside MLflow:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Create external model reference\nexternal_model = mlflow.create_external_model(\n    name="production_model_v2",\n    model_type="agent",\n    params={"version": "2.1", "endpoint": "api.example.com"},\n)\n'})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"loggedmodel-benefits",children:"LoggedModel Benefits"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Unified Tracking"}),": All model-related artifacts, traces, and metrics are organized under a single entity, providing complete visibility into model behavior and performance."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Automatic Trace Linking"}),": When using the active model pattern, all execution traces are automatically associated with the LoggedModel, eliminating manual tracking overhead."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Version Management"}),": LoggedModel supports systematic versioning and comparison across different model iterations, enabling data-driven model selection."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Evaluation Integration"}),": Evaluation metrics and results are directly linked to LoggedModel, providing comprehensive performance assessment."]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    LM[\ud83e\udd16 LoggedModel]\n\n    subgraph ARTIFACTS[\ud83d\udce6 Model Components]\n        MODEL_FILE[\ud83d\udd27 Model Artifacts]\n        PARAMS[\u2699\ufe0f Parameters]\n        METADATA[\ud83d\udccb Metadata]\n    end\n\n    subgraph TRACES[\ud83d\udcdd Execution History]\n        DEV_TRACES[\ud83d\udd27 Development Traces]\n        EVAL_TRACES[\ud83e\uddea Evaluation Traces]\n        PROD_TRACES[\ud83d\ude80 Production Traces]\n    end\n\n    subgraph METRICS[\ud83d\udcca Performance Data]\n        EVAL_METRICS[\ud83d\udcc8 Evaluation Metrics]\n        QUALITY_SCORES[\u2b50 Quality Scores]\n        PERF_DATA[\u26a1 Performance Data]\n    end\n\n    LM --\x3e ARTIFACTS\n    LM --\x3e TRACES\n    LM --\x3e METRICS\n\n    classDef modelStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000\n    classDef artifactStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef traceStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000\n    classDef metricStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n\n    class LM modelStyle\n    class ARTIFACTS,MODEL_FILE,PARAMS,METADATA artifactStyle\n    class TRACES,DEV_TRACES,EVAL_TRACES,PROD_TRACES traceStyle\n    class METRICS,EVAL_METRICS,QUALITY_SCORES,PERF_DATA metricStyle"}),"\n",(0,s.jsx)(n.h2,{id:"mlflow-traces-the-foundation",children:"MLflow Traces: The Foundation"}),"\n",(0,s.jsxs)(n.p,{children:["The foundational concept is the ",(0,s.jsx)(n.strong,{children:"Trace"}),": a single, complete execution of your GenAI application (e.g., a user request or API call)."]}),"\n",(0,s.jsx)(n.h3,{id:"how-traces-are-generated",children:"How Traces are Generated"}),"\n",(0,s.jsx)(n.p,{children:"Traces are generated through:"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(o.A,{value:"automatic",label:"Automatic Instrumentation",default:!0,children:[(0,s.jsx)(n.p,{children:"Automatic tracing is enabled with a single line of code for 20+ popular LLM SDKs:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Enable automatic tracing for OpenAI\nmlflow.openai.autolog()\n\n# Your existing code works unchanged\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model="gpt-4", messages=[{"role": "user", "content": "Hello!"}]\n)\n'})})]}),(0,s.jsxs)(o.A,{value:"custom",label:"Decorators and Context Managers",default:!0,children:[(0,s.jsx)(n.p,{children:"Using the MLflow tracing APIs for fine-grained control:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"@mlflow.trace\ndef my_custom_function(input_data):\n    # Your custom logic here\n    result = process_data(input_data)\n    return result\n"})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"purpose-of-traces",children:"Purpose of Traces"}),"\n",(0,s.jsx)(n.p,{children:"Traces enable:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observability"}),": Gain insights into application performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debugging"}),": Understand execution flow to resolve issues"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quality Evaluation"}),": Assess response quality over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human Review"}),": Provide data for expert annotation"]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    A[\ud83d\udc64 User Request] --\x3e B[\ud83e\udd16 GenAI App]\n    B --\x3e C[\ud83d\udcdd MLflow Trace]\n    C --\x3e D[\ud83d\udc41\ufe0f Observability]\n    C --\x3e E[\u2b50 Quality Evaluation]\n    C --\x3e F[\ud83d\udc65 Human Review]\n\n    classDef userStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef appStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n    classDef traceStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:3px,color:#000\n    classDef outputStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000\n\n    class A userStyle\n    class B appStyle\n    class C traceStyle\n    class D,E,F outputStyle"}),"\n",(0,s.jsx)(n.h2,{id:"assessments-quality-judgments",children:"Assessments: Quality Judgments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Assessments"})," are qualitative or quantitative judgments attached to Traces to understand and improve GenAI application quality. A Trace can have multiple Assessments, primarily Feedback or Expectations."]}),"\n",(0,s.jsx)(n.h3,{id:"feedback-assessments",children:"Feedback Assessments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Feedback"})," captures evaluations of a Trace and includes:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83c\udff7\ufe0f ",(0,s.jsx)(n.strong,{children:"Name"})," - Developer-defined category (e.g., ",(0,s.jsx)(n.code,{children:"relevance"}),", ",(0,s.jsx)(n.code,{children:"correctness"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2b50 ",(0,s.jsx)(n.strong,{children:"Score"})," - Evaluation (e.g., thumbs up/down, numerical rating)"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udcad ",(0,s.jsx)(n.strong,{children:"Rationale"})," - Optional textual explanation for the score"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Log feedback programmatically\nmlflow.log_feedback(\n    request_id="trace-123",\n    feedback_name="relevance",\n    feedback_score=4,\n    feedback_rationale="Response was highly relevant to the question",\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"expectation-assessments",children:"Expectation Assessments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expectations"})," are ground truth labels for a Trace (e.g., ",(0,s.jsx)(n.code,{children:"expected_facts"}),", ",(0,s.jsx)(n.code,{children:"expected_response"}),"). These are primarily used in offline evaluation to compare app output against known correct answers."]}),"\n",(0,s.jsx)(n.h2,{id:"scorers-automated-quality-measurement",children:"Scorers: Automated Quality Measurement"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scorers"})," are functions that programmatically assess Trace quality, producing Feedback. They can be:"]}),"\n",(0,s.jsx)(n.h3,{id:"code-based-heuristics",children:"Code-based Heuristics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def check_response_length(trace):\n    """Custom scorer to check if response is appropriate length"""\n    response = trace.outputs.get("response", "")\n    if 50 <= len(response) <= 500:\n        return {"score": 1, "rationale": "Response length is appropriate"}\n    else:\n        return {"score": 0, "rationale": "Response too short or too long"}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"llm-judges",children:"LLM Judges"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from mlflow.metrics import genai\n\n# Use built-in LLM judge\nrelevance_metric = genai.relevance()\n\n# Evaluate traces with the metric\nresults = mlflow.genai.evaluate(\n    predict_fn=your_model, data=evaluation_data, scorers=[relevance_metric]\n)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-datasets-and-runs",children:"Evaluation Datasets and Runs"}),"\n",(0,s.jsx)(n.p,{children:"MLflow provides systematic offline testing through Evaluation Datasets and Evaluation Runs."}),"\n",(0,s.jsx)(n.h3,{id:"evaluation-datasets",children:"Evaluation Datasets"}),"\n",(0,s.jsxs)(n.p,{children:["An ",(0,s.jsx)(n.strong,{children:"Evaluation Dataset"})," is a curated collection of example inputs used to evaluate and improve app performance:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Create an evaluation dataset\ndataset = mlflow.data.from_dict(\n    {\n        "inputs": ["What is MLflow?", "How do I log metrics?"],\n        "expectations": {\n            "expected_outputs": ["MLflow is...", "To log metrics..."],\n        },\n    }\n)\n\n# Register the dataset\nmlflow.log_input(dataset, context="evaluation")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"evaluation-runs",children:"Evaluation Runs"}),"\n",(0,s.jsxs)(n.p,{children:["An ",(0,s.jsx)(n.strong,{children:"Evaluation Run"})," stores results from running a new app version against an Evaluation Dataset:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Run evaluation\nresults = mlflow.genai.evaluate(\n    predict_fn=your_model,\n    data=evaluation_dataset,\n    scorers=[relevance_metric, accuracy_metric],\n)\n"})}),"\n",(0,s.jsx)(n.p,{children:"The evaluation process:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"New app version processes inputs from Evaluation Dataset"}),"\n",(0,s.jsx)(n.li,{children:"MLflow generates a new Trace for each input"}),"\n",(0,s.jsx)(n.li,{children:"Configured Scorers annotate Traces with Feedback"}),"\n",(0,s.jsx)(n.li,{children:"All annotated Traces are stored in the Evaluation Run"}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    A[\ud83d\ude80 Production Traces] --\x3e B[\ud83d\udd0d Curate Examples]\n    B --\x3e C[\ud83d\udccb Evaluation Dataset]\n    C --\x3e D[\u2705 Add Expectations]\n    E[\ud83d\udd04 New App Version] --\x3e F[\u2699\ufe0f MLflow Evaluation]\n    C --\x3e F\n    G[\ud83c\udfaf Scorers] --\x3e F\n    F --\x3e H[\ud83d\udcca New Traces]\n    H --\x3e I[\ud83d\udcc8 Evaluation Run]\n\n    classDef sourceStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef processStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px,color:#000\n    classDef dataStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    classDef resultStyle fill:#fff8e1,stroke:#f57c00,stroke-width:2px,color:#000\n    classDef coreStyle fill:#e8eaf6,stroke:#3f51b5,stroke-width:3px,color:#000\n\n    class A,E sourceStyle\n    class B,G processStyle\n    class C,D dataStyle\n    class H,I resultStyle\n    class F coreStyle"}),"\n",(0,s.jsx)(n.h2,{id:"labeling-sessions-human-review",children:"Labeling Sessions: Human Review"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Labeling Sessions"})," organize Traces for human review, typically through the MLflow UI. Domain experts can browse these Traces and attach Feedback as Assessments."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Create a labeling session\nsession = mlflow.create_labeling_session(\n    name="quality-review-session", trace_ids=["trace-1", "trace-2", "trace-3"]\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"complete-data-model-structure",children:"Complete Data Model Structure"}),"\n",(0,s.jsx)(n.p,{children:"All components operate within an Experiment, forming a comprehensive hierarchy:"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    subgraph EXP[\ud83e\uddea Experiment Container]\n        direction TB\n        LM[\ud83e\udd16 LoggedModel] --\x3e A[\ud83d\udcdd Trace]\n        A --\x3e B[\ud83d\udcca Assessment]\n        B --\x3e C[\ud83d\udc4d Feedback]\n        B --\x3e D[\ud83c\udfaf Expectation]\n        E[\u2699\ufe0f Scorer]\n        F[\ud83d\ude80 Evaluation Run] --\x3e G[\ud83d\udcc8 Scored Traces]\n        H[\ud83d\udccb Evaluation Dataset] --\x3e I[\ud83d\udcc4 Input Data + Expectations]\n        J[\ud83c\udff7\ufe0f Labeling Session] --\x3e K[\ud83d\udcdd Curated Traces]\n        LM --\x3e L[\ud83d\udcca Model Metrics]\n        LM --\x3e M[\ud83d\udd27 Model Artifacts]\n    end\n\n    classDef modelStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000\n    classDef traceStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000\n    classDef assessmentStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000\n    classDef feedbackStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef expectationStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef toolStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    classDef dataStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px,color:#000\n    classDef containerStyle fill:#f5f5f5,stroke:#424242,stroke-width:3px,color:#000\n\n    class LM,L,M modelStyle\n    class A,K,G traceStyle\n    class B assessmentStyle\n    class C feedbackStyle\n    class D expectationStyle\n    class E,F toolStyle\n    class H,I,J dataStyle\n    class EXP containerStyle"}),"\n",(0,s.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsx)(n.p,{children:"To begin using the MLflow data model:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83e\uddea ",(0,s.jsx)(n.strong,{children:"Set up an experiment"})," for your GenAI application"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83e\udd16 ",(0,s.jsx)(n.strong,{children:"Create or set a LoggedModel"})," to organize your model tracking"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,s.jsx)(n.strong,{children:"Enable automatic tracing"})," for your LLM library"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,s.jsx)(n.strong,{children:"Define custom scorers"})," for your quality metrics"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udccb ",(0,s.jsx)(n.strong,{children:"Create evaluation datasets"})," from representative examples"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\ude80 ",(0,s.jsx)(n.strong,{children:"Run evaluations"})," to compare different versions"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udc65 ",(0,s.jsx)(n.strong,{children:"Review traces"})," and add human feedback as needed"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# 1. Set up experiment\nmlflow.set_experiment("my-genai-app")\n\n# 2. Create LoggedModel\nlogged_model = mlflow.langchain.log_model(\n    lc_model=your_model, name="my_agent", params={"temperature": 0.1}\n)\n\n# 3. Enable tracing with active model\nmlflow.set_active_model(name="my_agent")\nmlflow.langchain.autolog()\n\n# 4. Your app code runs normally\n# Traces are automatically captured and linked to LoggedModel\n\n# 5. Evaluate and iterate\nresults = mlflow.genai.evaluate(\n    predict_fn=your_model, data=evaluation_data, scorers=[your_custom_scorers]\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83e\udd16 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/genai/data-model/logged-model",children:"LoggedModel Guide"})}),": Learn comprehensive model lifecycle management"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/genai/tracing/app-instrumentation/automatic",children:"Automatic Tracing"})}),": Learn how to enable automatic tracing for your LLM library"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udee0\ufe0f ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/genai/tracing/app-instrumentation/manual-tracing",children:"Custom Tracing"})}),": Add manual instrumentation to your application"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udcca ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/genai/eval-monitor",children:"Evaluation Guide"})}),": Dive deeper into evaluation workflows"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udda5\ufe0f ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/genai/tracing/observe-with-traces/ui",children:"MLflow UI"})}),": Explore traces and results in the web interface"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"MLflow's comprehensive data model empowers systematic observation, debugging, evaluation, and improvement of GenAI applications, providing the foundation for building high-quality, reliable, and maintainable GenAI systems."})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}}}]);