"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8698],{11470:(e,n,t)=>{t.d(n,{A:()=>w});var i=t(96540),a=t(34164),r=t(23104),s=t(56347),l=t(205),o=t(57485),c=t(31682),p=t(70679);function m(e){return i.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??function(e){return m(e).map((({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i})))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function d({value:e,tabValues:n}){return n.some((n=>n.value===e))}function _({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,o.aZ)(a),(0,i.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})}),[a,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,r=u(e),[s,o]=(0,i.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!d({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:r}))),[c,m]=_({queryString:t,groupId:a}),[f,h]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,p.Dv)(n);return[t,(0,i.useCallback)((e=>{n&&a.set(e)}),[n,a])]}({groupId:a}),g=(()=>{const e=c??f;return d({value:e,tabValues:r})?e:null})();(0,l.A)((()=>{g&&o(g)}),[g]);return{selectedValue:s,selectValue:(0,i.useCallback)((e=>{if(!d({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);o(e),m(e),h(e)}),[m,h,r]),tabValues:r}}var h=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=t(74848);function y({className:e,block:n,selectedValue:t,selectValue:i,tabValues:s}){const l=[],{blockElementScrollPositionUntilNextRender:o}=(0,r.a_)(),c=e=>{const n=e.currentTarget,a=l.indexOf(n),r=s[a].value;r!==t&&(o(n),i(r))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:s.map((({value:e,label:n,attributes:i})=>(0,v.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:p,onClick:c,...i,className:(0,a.A)("tabs__item",g.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function b({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find((e=>e.props.value===t));return e?(0,i.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function x(e){const n=f(e);return(0,v.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,v.jsx)(y,{...n,...e}),(0,v.jsx)(b,{...n,...e})]})}function w(e){const n=(0,h.A)();return(0,v.jsx)(x,{...e,children:m(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var i=t(34164);const a={tabItem:"tabItem_Ymn6"};var r=t(74848);function s({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,i.A)(a.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var i=t(96540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},47718:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"evaluation/metrics-visualizations","title":"Custom Metrics & Visualizations","description":"MLflow\'s evaluation framework allows you to define custom metrics and create specialized visualizations tailored to your specific business requirements. This capability is essential when standard metrics don\'t capture your domain\'s unique success criteria or when you need custom visual analysis for stakeholder communication.","source":"@site/docs/classic-ml/evaluation/metrics-visualizations.mdx","sourceDirName":"evaluation","slug":"/evaluation/metrics-visualizations","permalink":"/docs/latest/ml/evaluation/metrics-visualizations","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Custom Metrics & Visualizations","sidebar_position":4},"sidebar":"classicMLSidebar","previous":{"title":"Model Evaluation","permalink":"/docs/latest/ml/evaluation/model-eval"},"next":{"title":"SHAP Integration","permalink":"/docs/latest/ml/evaluation/shap"}}');var a=t(74848),r=t(28453),s=(t(49374),t(11470)),l=t(19365);const o={title:"Custom Metrics & Visualizations",sidebar_position:4},c="Custom Metrics & Visualizations",p={},m=[{value:"Quick Start: Creating Custom Metrics",id:"quick-start-creating-custom-metrics",level:2},{value:"Custom Metric Patterns",id:"custom-metric-patterns",level:2},{value:"Custom Visualizations",id:"custom-visualizations",level:2},{value:"Advanced Custom Metrics",id:"advanced-custom-metrics",level:2},{value:"Best Practices and Guidelines",id:"best-practices-and-guidelines",level:2},{value:"Metric Design Principles",id:"metric-design-principles",level:3},{value:"Visualization Best Practices",id:"visualization-best-practices",level:3},{value:"Integration Example",id:"integration-example",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"custom-metrics--visualizations",children:"Custom Metrics & Visualizations"})}),"\n",(0,a.jsx)(n.p,{children:"MLflow's evaluation framework allows you to define custom metrics and create specialized visualizations tailored to your specific business requirements. This capability is essential when standard metrics don't capture your domain's unique success criteria or when you need custom visual analysis for stakeholder communication."}),"\n",(0,a.jsx)(n.h2,{id:"quick-start-creating-custom-metrics",children:"Quick Start: Creating Custom Metrics"}),"\n",(0,a.jsx)(n.p,{children:"Define domain-specific metrics using MLflow's metric builder:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom mlflow.models import make_metric\nfrom mlflow.metrics.base import MetricValue\n\n# Generate sample data\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = pd.DataFrame(X_test)\neval_data["target"] = y_test\n\n\n# Define a custom business metric\ndef business_value_metric(predictions, targets, metrics):\n    """\n    Custom metric calculating business value impact.\n    True Positives = $100 value, False Positives = -$20 cost\n    """\n    tp = np.sum((predictions == 1) & (targets == 1))\n    fp = np.sum((predictions == 1) & (targets == 0))\n    tn = np.sum((predictions == 0) & (targets == 0))\n    fn = np.sum((predictions == 0) & (targets == 1))\n\n    # Business logic: TP worth $100, FP costs $20\n    business_value = (tp * 100) - (fp * 20)\n    total_possible_value = np.sum(targets == 1) * 100\n\n    return MetricValue(\n        scores=[business_value],  # Total business value\n        aggregate_results={\n            "total_business_value": business_value,\n            "value_per_prediction": business_value / len(predictions),\n            "value_efficiency": business_value / total_possible_value\n            if total_possible_value > 0\n            else 0,\n        },\n    )\n\n\n# Create the metric\nbusiness_metric = make_metric(\n    eval_fn=business_value_metric, greater_is_better=True, name="business_value"\n)\n\nwith mlflow.start_run():\n    # Log model\n    mlflow.sklearn.log_model(model, name="model")\n    model_uri = mlflow.get_artifact_uri("model")\n\n    # Evaluate with custom metric\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n        extra_metrics=[business_metric],\n    )\n\n    print(f"Standard Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(\n        f"Business Value: ${result.metrics[\'business_value/total_business_value\']:.2f}"\n    )\n    print(\n        f"Value per Prediction: ${result.metrics[\'business_value/value_per_prediction\']:.2f}"\n    )\n'})}),"\n",(0,a.jsx)(n.h2,{id:"custom-metric-patterns",children:"Custom Metric Patterns"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"financial-metrics",label:"Financial Impact Metrics",default:!0,children:[(0,a.jsx)(n.p,{children:"Create metrics that translate model performance into business terms:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_profit_loss_metric(cost_per_fp=50, revenue_per_tp=200):\n    """Calculate profit/loss impact of model predictions."""\n\n    def eval_fn(predictions, targets, metrics):\n        tp = np.sum((predictions == 1) & (targets == 1))\n        fp = np.sum((predictions == 1) & (targets == 0))\n        fn = np.sum((predictions == 0) & (targets == 1))\n        tn = np.sum((predictions == 0) & (targets == 0))\n\n        # Calculate financial impact\n        revenue = tp * revenue_per_tp\n        costs = fp * cost_per_fp\n        missed_opportunity = fn * revenue_per_tp\n        net_profit = revenue - costs\n\n        return MetricValue(\n            aggregate_results={\n                "net_profit": net_profit,\n                "total_revenue": revenue,\n                "total_costs": costs,\n                "missed_revenue": missed_opportunity,\n                "roi": (net_profit / max(costs, 1)) * 100,\n            }\n        )\n\n    return make_metric(eval_fn=eval_fn, greater_is_better=True, name="profit_loss")\n\n\n# Usage\nprofit_metric = create_profit_loss_metric(cost_per_fp=30, revenue_per_tp=150)\n\nwith mlflow.start_run():\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n        extra_metrics=[profit_metric],\n    )\n\n    print(f"Net Profit: ${result.metrics[\'profit_loss/net_profit\']:.2f}")\n    print(f"ROI: {result.metrics[\'profit_loss/roi\']:.1f}%")\n'})})]}),(0,a.jsxs)(l.A,{value:"threshold-metrics",label:"Threshold-Based Metrics",children:[(0,a.jsx)(n.p,{children:"Create metrics that evaluate performance at specific business thresholds:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_threshold_precision_metric(threshold=0.8):\n    """Precision metric for high-confidence predictions only."""\n\n    def eval_fn(predictions, targets, metrics):\n        # This assumes predictions are probabilities; adjust for your use case\n        high_confidence_mask = np.abs(predictions - 0.5) >= (threshold - 0.5)\n\n        if np.sum(high_confidence_mask) == 0:\n            return MetricValue(aggregate_results={"high_confidence_precision": 0.0})\n\n        hc_predictions = predictions[high_confidence_mask]\n        hc_targets = targets[high_confidence_mask]\n\n        # Convert probabilities to binary predictions\n        binary_predictions = (hc_predictions > 0.5).astype(int)\n\n        tp = np.sum((binary_predictions == 1) & (hc_targets == 1))\n        fp = np.sum((binary_predictions == 1) & (hc_targets == 0))\n\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        coverage = np.sum(high_confidence_mask) / len(predictions)\n\n        return MetricValue(\n            aggregate_results={\n                "high_confidence_precision": precision,\n                "high_confidence_coverage": coverage,\n                "high_confidence_count": np.sum(high_confidence_mask),\n            }\n        )\n\n    return make_metric(\n        eval_fn=eval_fn, greater_is_better=True, name="threshold_precision"\n    )\n\n\n# Create threshold-based metric\nthreshold_metric = create_threshold_precision_metric(threshold=0.8)\n\nwith mlflow.start_run():\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n        extra_metrics=[threshold_metric],\n    )\n\n    print(\n        f"High Confidence Precision: {result.metrics[\'threshold_precision/high_confidence_precision\']:.3f}"\n    )\n    print(\n        f"Coverage: {result.metrics[\'threshold_precision/high_confidence_coverage\']:.3f}"\n    )\n'})})]}),(0,a.jsxs)(l.A,{value:"domain-specific",label:"Domain-Specific Metrics",children:[(0,a.jsx)(n.p,{children:"Create metrics tailored to specific business domains:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Healthcare/Medical Domain\ndef create_medical_safety_metric(false_negative_penalty=10, false_positive_penalty=1):\n    """Safety-focused metric for medical predictions where FN is more critical than FP."""\n\n    def eval_fn(predictions, targets, metrics):\n        tp = np.sum((predictions == 1) & (targets == 1))\n        fp = np.sum((predictions == 1) & (targets == 0))\n        fn = np.sum((predictions == 0) & (targets == 1))\n        tn = np.sum((predictions == 0) & (targets == 0))\n\n        # Safety score: heavily penalize missed positive cases\n        safety_score = (\n            tp - (fn * false_negative_penalty) - (fp * false_positive_penalty)\n        )\n        max_possible_score = np.sum(\n            targets == 1\n        )  # All true positives, no false negatives\n\n        # Normalized safety score\n        normalized_safety = (\n            safety_score / max_possible_score if max_possible_score > 0 else 0\n        )\n\n        return MetricValue(\n            aggregate_results={\n                "safety_score": safety_score,\n                "normalized_safety": normalized_safety,\n                "missed_critical_cases": fn,\n                "false_alarms": fp,\n            }\n        )\n\n    return make_metric(eval_fn=eval_fn, greater_is_better=True, name="medical_safety")\n\n\n# Example: E-commerce/Recommendation Domain\ndef create_recommendation_diversity_metric():\n    """Diversity metric for recommendation systems."""\n\n    def eval_fn(predictions, targets, metrics):\n        # Assumes predictions contain recommendation scores or categories\n        unique_predictions = len(np.unique(predictions))\n        total_predictions = len(predictions)\n\n        diversity_ratio = unique_predictions / total_predictions\n\n        # Calculate entropy as diversity measure\n        pred_counts = np.bincount(predictions.astype(int))\n        pred_probs = pred_counts / len(predictions)\n        entropy = -np.sum(pred_probs * np.log2(pred_probs + 1e-10))\n\n        return MetricValue(\n            aggregate_results={\n                "diversity_ratio": diversity_ratio,\n                "prediction_entropy": entropy,\n                "unique_predictions": unique_predictions,\n            }\n        )\n\n    return make_metric(\n        eval_fn=eval_fn, greater_is_better=True, name="recommendation_diversity"\n    )\n\n\n# Usage examples\nmedical_metric = create_medical_safety_metric(\n    false_negative_penalty=5, false_positive_penalty=1\n)\ndiversity_metric = create_recommendation_diversity_metric()\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"custom-visualizations",children:"Custom Visualizations"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"business-dashboards",label:"Business Impact Visualizations",default:!0,children:[(0,a.jsx)(n.p,{children:"Generate custom visual analysis beyond standard plots:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n\ndef create_business_impact_visualization(eval_df, builtin_metrics, artifacts_dir):\n    """Create custom business impact visualization."""\n\n    # Calculate business segments\n    eval_df["prediction_confidence"] = np.abs(eval_df["prediction"] - 0.5)\n    eval_df["confidence_segment"] = pd.cut(\n        eval_df["prediction_confidence"],\n        bins=[0, 0.1, 0.3, 0.5],\n        labels=["Low", "Medium", "High"],\n    )\n\n    # Create subplots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n    # 1. Accuracy by confidence segment\n    accuracy_by_segment = eval_df.groupby("confidence_segment").apply(\n        lambda x: (x["prediction"] == x["target"]).mean()\n    )\n\n    axes[0, 0].bar(\n        accuracy_by_segment.index, accuracy_by_segment.values, color="skyblue"\n    )\n    axes[0, 0].set_title("Accuracy by Confidence Segment")\n    axes[0, 0].set_ylabel("Accuracy")\n    axes[0, 0].set_ylim(0, 1)\n\n    # 2. Prediction distribution\n    axes[0, 1].hist(\n        eval_df["prediction"],\n        bins=20,\n        alpha=0.7,\n        label="Predictions",\n        color="lightgreen",\n    )\n    axes[0, 1].axvline(\n        eval_df["prediction"].mean(), color="red", linestyle="--", label="Mean"\n    )\n    axes[0, 1].set_title("Prediction Distribution")\n    axes[0, 1].legend()\n\n    # 3. Confusion matrix heatmap\n    from sklearn.metrics import confusion_matrix\n\n    cm = confusion_matrix(eval_df["target"], eval_df["prediction"])\n    sns.heatmap(cm, annot=True, fmt="d", ax=axes[1, 0], cmap="Blues")\n    axes[1, 0].set_title("Confusion Matrix")\n    axes[1, 0].set_xlabel("Predicted")\n    axes[1, 0].set_ylabel("Actual")\n\n    # 4. Business value by segment\n    def calculate_segment_value(segment_data):\n        tp = np.sum((segment_data["prediction"] == 1) & (segment_data["target"] == 1))\n        fp = np.sum((segment_data["prediction"] == 1) & (segment_data["target"] == 0))\n        return (tp * 100) - (fp * 20)  # Business value calculation\n\n    value_by_segment = eval_df.groupby("confidence_segment").apply(\n        calculate_segment_value\n    )\n\n    colors = ["lightcoral" if v < 0 else "lightgreen" for v in value_by_segment.values]\n    axes[1, 1].bar(value_by_segment.index, value_by_segment.values, color=colors)\n    axes[1, 1].set_title("Business Value by Confidence Segment")\n    axes[1, 1].set_ylabel("Business Value ($)")\n    axes[1, 1].axhline(y=0, color="black", linestyle="-", alpha=0.3)\n\n    plt.tight_layout()\n\n    # Save visualization\n    viz_path = os.path.join(artifacts_dir, "business_impact_analysis.png")\n    plt.savefig(viz_path, dpi=300, bbox_inches="tight")\n    plt.close()\n\n    return {"business_impact_analysis": viz_path}\n'})})]}),(0,a.jsxs)(l.A,{value:"performance-analysis",label:"Advanced Performance Analysis",children:[(0,a.jsx)(n.p,{children:"Create detailed performance analysis visualizations:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_performance_breakdown_visualization(eval_df, builtin_metrics, artifacts_dir):\n    """Create detailed performance breakdown visualization."""\n\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n    # 1. Performance by prediction probability\n    eval_df["prob_bin"] = pd.cut(eval_df["prediction"], bins=10, labels=False)\n    perf_by_prob = eval_df.groupby("prob_bin").apply(\n        lambda x: (x["prediction"] == x["target"]).mean() if len(x) > 0 else 0\n    )\n\n    axes[0, 0].plot(perf_by_prob.index, perf_by_prob.values, marker="o")\n    axes[0, 0].set_title("Accuracy by Prediction Probability Bin")\n    axes[0, 0].set_xlabel("Probability Bin")\n    axes[0, 0].set_ylabel("Accuracy")\n\n    # 2. Calibration plot\n    true_probs = eval_df.groupby("prob_bin")["target"].mean()\n    pred_probs = eval_df.groupby("prob_bin")["prediction"].mean()\n\n    axes[0, 1].plot([0, 1], [0, 1], "k--", alpha=0.5, label="Perfect Calibration")\n    axes[0, 1].scatter(pred_probs, true_probs, alpha=0.7, label="Model")\n    axes[0, 1].set_title("Calibration Plot")\n    axes[0, 1].set_xlabel("Mean Predicted Probability")\n    axes[0, 1].set_ylabel("Fraction of Positives")\n    axes[0, 1].legend()\n\n    # 3. Error distribution\n    errors = eval_df["target"] - eval_df["prediction"]\n    axes[0, 2].hist(errors, bins=20, alpha=0.7, color="orange")\n    axes[0, 2].set_title("Prediction Error Distribution")\n    axes[0, 2].set_xlabel("Error (Actual - Predicted)")\n    axes[0, 2].set_ylabel("Frequency")\n\n    # 4. Feature importance correlation (if available)\n    if "feature_0" in eval_df.columns:\n        feature_cols = [col for col in eval_df.columns if col.startswith("feature_")][\n            :5\n        ]\n        corr_with_error = []\n\n        for feature in feature_cols:\n            corr = np.corrcoef(eval_df[feature], errors)[0, 1]\n            corr_with_error.append(abs(corr))\n\n        axes[1, 0].bar(range(len(corr_with_error)), corr_with_error)\n        axes[1, 0].set_title("Feature Correlation with Prediction Errors")\n        axes[1, 0].set_xlabel("Feature Index")\n        axes[1, 0].set_ylabel("Absolute Correlation")\n        axes[1, 0].set_xticks(range(len(feature_cols)))\n        axes[1, 0].set_xticklabels([f"F{i}" for i in range(len(feature_cols))])\n\n    # 5. Class distribution\n    class_dist = eval_df["target"].value_counts()\n    axes[1, 1].pie(\n        class_dist.values,\n        labels=[f"Class {i}" for i in class_dist.index],\n        autopct="%1.1f%%",\n    )\n    axes[1, 1].set_title("Target Class Distribution")\n\n    # 6. Precision-Recall by threshold\n    from sklearn.metrics import precision_recall_curve\n\n    precision, recall, thresholds = precision_recall_curve(\n        eval_df["target"], eval_df["prediction"]\n    )\n\n    axes[1, 2].plot(recall, precision, marker=".", markersize=2)\n    axes[1, 2].set_title("Precision-Recall Curve")\n    axes[1, 2].set_xlabel("Recall")\n    axes[1, 2].set_ylabel("Precision")\n    axes[1, 2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n\n    # Save visualization\n    viz_path = os.path.join(artifacts_dir, "performance_breakdown_analysis.png")\n    plt.savefig(viz_path, dpi=300, bbox_inches="tight")\n    plt.close()\n\n    return {"performance_breakdown_analysis": viz_path}\n'})})]}),(0,a.jsxs)(l.A,{value:"interactive-plots",label:"Interactive Visualizations",children:[(0,a.jsx)(n.p,{children:"Create interactive visualizations for deeper analysis:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_interactive_analysis_artifacts(eval_df, builtin_metrics, artifacts_dir):\n    """Create interactive HTML visualizations using Plotly."""\n\n    try:\n        import plotly.graph_objects as go\n        import plotly.express as px\n        from plotly.subplots import make_subplots\n        import plotly.offline as pyo\n\n        # Create subplot figure\n        fig = make_subplots(\n            rows=2,\n            cols=2,\n            subplot_titles=(\n                "Prediction Distribution",\n                "Accuracy by Confidence",\n                "ROC Curve",\n                "Feature Analysis",\n            ),\n            specs=[\n                [{"secondary_y": False}, {"secondary_y": False}],\n                [{"secondary_y": False}, {"secondary_y": False}],\n            ],\n        )\n\n        # 1. Interactive prediction distribution\n        fig.add_trace(\n            go.Histogram(x=eval_df["prediction"], name="Predictions", nbinsx=20),\n            row=1,\n            col=1,\n        )\n\n        # 2. Confidence-based accuracy\n        eval_df["confidence_level"] = pd.cut(\n            np.abs(eval_df["prediction"] - 0.5),\n            bins=5,\n            labels=["Very Low", "Low", "Medium", "High", "Very High"],\n        )\n\n        conf_accuracy = eval_df.groupby("confidence_level").apply(\n            lambda x: (x["prediction"] == x["target"]).mean()\n        )\n\n        fig.add_trace(\n            go.Bar(x=conf_accuracy.index, y=conf_accuracy.values, name="Accuracy"),\n            row=1,\n            col=2,\n        )\n\n        # 3. ROC Curve\n        from sklearn.metrics import roc_curve\n\n        fpr, tpr, _ = roc_curve(eval_df["target"], eval_df["prediction"])\n\n        fig.add_trace(\n            go.Scatter(x=fpr, y=tpr, mode="lines", name="ROC Curve"), row=2, col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[0, 1], y=[0, 1], mode="lines", name="Random", line=dict(dash="dash")\n            ),\n            row=2,\n            col=1,\n        )\n\n        # 4. Feature analysis (if features available)\n        if "feature_0" in eval_df.columns:\n            fig.add_trace(\n                go.Scatter(\n                    x=eval_df["feature_0"],\n                    y=eval_df["prediction"],\n                    mode="markers",\n                    marker=dict(color=eval_df["target"], colorscale="Viridis"),\n                    name="Feature vs Prediction",\n                ),\n                row=2,\n                col=2,\n            )\n\n        # Update layout\n        fig.update_layout(\n            title_text="Interactive Model Performance Analysis",\n            showlegend=True,\n            height=800,\n        )\n\n        # Save interactive plot\n        interactive_path = os.path.join(artifacts_dir, "interactive_analysis.html")\n        pyo.plot(fig, filename=interactive_path, auto_open=False)\n\n        return {"interactive_analysis": interactive_path}\n\n    except ImportError:\n        # Fallback to matplotlib if plotly not available\n        print("Plotly not available, creating static visualization instead")\n        return create_business_impact_visualization(\n            eval_df, builtin_metrics, artifacts_dir\n        )\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"advanced-custom-metrics",children:"Advanced Custom Metrics"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"business-metric",label:"Multi-Metric Combinations",default:!0,children:[(0,a.jsx)(n.p,{children:"Create composite metrics that combine multiple evaluation criteria:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_composite_business_metric():\n    """Composite metric combining accuracy, profit, and risk measures."""\n\n    def eval_fn(predictions, targets, metrics):\n        # Get standard accuracy\n        accuracy = metrics.get("accuracy_score", 0)\n\n        # Calculate profit (reuse previous logic)\n        tp = np.sum((predictions == 1) & (targets == 1))\n        fp = np.sum((predictions == 1) & (targets == 0))\n        fn = np.sum((predictions == 0) & (targets == 1))\n\n        profit = (tp * 100) - (fp * 20) - (fn * 50)  # Include missed opportunity cost\n\n        # Calculate risk (variance in performance across segments)\n        n_segments = 5\n        segment_size = len(predictions) // n_segments\n        segment_accuracies = []\n\n        for i in range(n_segments):\n            start_idx = i * segment_size\n            end_idx = (\n                start_idx + segment_size if i < n_segments - 1 else len(predictions)\n            )\n\n            seg_pred = predictions[start_idx:end_idx]\n            seg_target = targets[start_idx:end_idx]\n            seg_accuracy = np.mean(seg_pred == seg_target) if len(seg_pred) > 0 else 0\n            segment_accuracies.append(seg_accuracy)\n\n        risk_measure = np.std(segment_accuracies)  # Lower std = lower risk\n\n        # Composite score: balance accuracy, profit, and risk\n        composite_score = (\n            (accuracy * 0.4) + (profit / 1000 * 0.4) + ((1 - risk_measure) * 0.2)\n        )\n\n        return MetricValue(\n            aggregate_results={\n                "composite_score": composite_score,\n                "accuracy_component": accuracy,\n                "profit_component": profit,\n                "risk_component": risk_measure,\n                "segment_consistency": 1 - risk_measure,\n            }\n        )\n\n    return make_metric(\n        eval_fn=eval_fn, greater_is_better=True, name="composite_business"\n    )\n\n\n# Usage\ncomposite_metric = create_composite_business_metric()\n'})})]}),(0,a.jsxs)(l.A,{value:"time-metrics",label:"Time-Aware Metrics",default:!0,children:[(0,a.jsx)(n.p,{children:"Create metrics that consider temporal aspects of predictions:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_time_decay_metric(decay_rate=0.1):\n    """Metric that gives more weight to recent predictions."""\n\n    def eval_fn(predictions, targets, metrics):\n        # Assume predictions are ordered by time (most recent last)\n        n_predictions = len(predictions)\n\n        # Create time weights (exponential decay)\n        time_weights = np.exp(decay_rate * np.arange(n_predictions))\n        time_weights = time_weights / np.sum(time_weights)  # Normalize\n\n        # Calculate weighted accuracy\n        correct_predictions = (predictions == targets).astype(float)\n        weighted_accuracy = np.sum(correct_predictions * time_weights)\n\n        # Calculate recency bias\n        recent_accuracy = np.mean(\n            correct_predictions[-len(predictions) // 4 :]\n        )  # Last 25%\n        overall_accuracy = np.mean(correct_predictions)\n        recency_bias = recent_accuracy - overall_accuracy\n\n        return MetricValue(\n            aggregate_results={\n                "time_weighted_accuracy": weighted_accuracy,\n                "recent_accuracy": recent_accuracy,\n                "overall_accuracy": overall_accuracy,\n                "recency_bias": recency_bias,\n            }\n        )\n\n    return make_metric(eval_fn=eval_fn, greater_is_better=True, name="time_decay")\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-and-guidelines",children:"Best Practices and Guidelines"}),"\n",(0,a.jsx)(n.h3,{id:"metric-design-principles",children:"Metric Design Principles"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Make Metrics Business-Relevant"})," - Translate technical performance into business impact using domain-specific terminology and thresholds that align with actual business objectives."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Ensure Metric Reliability"})," - Handle edge cases like division by zero and empty datasets, validate calculations with known test cases, and include confidence intervals when appropriate."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Optimize for Interpretability"})," - Provide multiple aggregation levels, include intermediate calculations for transparency, use descriptive naming, and add context through ratio and percentage metrics."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Documentation and Validation"})," - Document metric assumptions and limitations, test with synthetic data where ground truth is known, and provide clear interpretations of metric values."]}),"\n",(0,a.jsx)(n.h3,{id:"visualization-best-practices",children:"Visualization Best Practices"}),"\n",(0,a.jsxs)("ul",{children:[(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Target Your Audience"}),": Create technical plots for data scientists and executive dashboards for business stakeholders"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Make It Interactive"}),": Use interactive visualizations for exploration and static ones for reports"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Focus on Insights"}),": Highlight key findings and actionable insights rather than just displaying data"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Consistent Styling"}),": Use consistent color schemes and formatting across all visualizations"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Performance Considerations"}),": Optimize large visualizations for rendering speed and file size"]})]}),"\n",(0,a.jsx)(n.h2,{id:"integration-example",children:"Integration Example"}),"\n",(0,a.jsx)(n.p,{children:"Here's a comprehensive example combining custom metrics and visualizations:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def comprehensive_custom_evaluation():\n    """Complete example of custom metrics and visualizations."""\n\n    # Define multiple custom metrics\n    business_metric = create_profit_loss_metric(cost_per_fp=30, revenue_per_tp=150)\n    threshold_metric = create_threshold_precision_metric(threshold=0.8)\n    composite_metric = create_composite_business_metric()\n\n    # Define custom visualizations\n    custom_artifacts = [\n        create_business_impact_visualization,\n        create_performance_breakdown_visualization,\n        create_interactive_analysis_artifacts,\n    ]\n\n    with mlflow.start_run(run_name="Comprehensive_Custom_Evaluation"):\n        # Evaluate with all custom components\n        result = mlflow.evaluate(\n            model_uri,\n            eval_data,\n            targets="target",\n            model_type="classifier",\n            extra_metrics=[business_metric, threshold_metric, composite_metric],\n            custom_artifacts=custom_artifacts,\n        )\n\n        # Log additional business context\n        mlflow.log_params(\n            {\n                "evaluation_focus": "business_impact",\n                "custom_metrics_count": 3,\n                "custom_visualizations_count": len(custom_artifacts),\n            }\n        )\n\n        print("Custom Evaluation Results:")\n        print(f"Profit/Loss: ${result.metrics.get(\'profit_loss/net_profit\', 0):.2f}")\n        print(\n            f"High Confidence Precision: {result.metrics.get(\'threshold_precision/high_confidence_precision\', 0):.3f}"\n        )\n        print(\n            f"Composite Score: {result.metrics.get(\'composite_business/composite_score\', 0):.3f}"\n        )\n\n        print("\\nCustom Artifacts Created:")\n        for name, path in result.artifacts.items():\n            if any(\n                keyword in name.lower()\n                for keyword in ["business", "performance", "interactive"]\n            ):\n                print(f"  {name}: {path}")\n\n\n# Run comprehensive evaluation\n# comprehensive_custom_evaluation()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"Custom metrics and visualizations in MLflow enable you to create evaluation frameworks perfectly tailored to your business needs. By defining domain-specific success criteria and creating targeted visual analysis, you can bridge the gap between technical model performance and business value."}),"\n",(0,a.jsx)(n.p,{children:"Key benefits include:"}),"\n",(0,a.jsxs)("ul",{children:[(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Business Alignment"}),": Metrics that directly reflect business impact and priorities"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Stakeholder Communication"}),": Visual dashboards that make model performance accessible"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Domain Expertise"}),": Evaluation criteria that capture industry-specific requirements"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Decision Support"}),": Clear insights that drive informed model selection and deployment"]})]}),"\n",(0,a.jsx)(n.p,{children:"Whether you're optimizing for profit, minimizing risk, or meeting regulatory requirements, custom metrics and visualizations ensure your model evaluation process aligns with what truly matters for your use case."})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},49374:(e,n,t)=>{t.d(n,{B:()=>o});t(96540);const i=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var a=t(86025),r=t(28774),s=t(74848);const l=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(i[e])return e}return null};function o({fn:e,children:n}){const t=l(e);if(!t)return(0,s.jsx)(s.Fragment,{children:n});const o=(0,a.Ay)(`/${i[t]}#${e}`);return(0,s.jsx)(r.A,{to:o,target:"_blank",children:n??(0,s.jsxs)("code",{children:[e,"()"]})})}}}]);