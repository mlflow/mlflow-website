"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[5293],{6789:(e,l,n)=>{n.d(l,{A:()=>m});n(96540);var a=n(28774),t=n(34164);const i={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var r=n(86025),o=n(15626),s=n(74848);function m({icon:e,image:l,imageDark:n,imageWidth:m,imageHeight:p,iconSize:c=32,containerHeight:d,title:f,description:h,href:g,linkText:_="Learn more \u2192",className:w}){if(!e&&!l)throw new Error("TileCard requires either an icon or image prop");const u=d?{height:`${d}px`}:{},y={};return m&&(y.width=`${m}px`),p&&(y.height=`${p}px`),(0,s.jsxs)(a.A,{href:g,className:(0,t.A)(i.tileCard,w),children:[(0,s.jsx)("div",{className:i.tileIcon,style:u,children:e?(0,s.jsx)(e,{size:c}):n?(0,s.jsx)(o.A,{sources:{light:(0,r.default)(l),dark:(0,r.default)(n)},alt:f,className:i.tileImage,style:y}):(0,s.jsx)("img",{src:(0,r.default)(l),alt:f,className:i.tileImage,style:y})}),(0,s.jsx)("h3",{children:f}),(0,s.jsx)("p",{children:h}),(0,s.jsx)("div",{className:i.tileLink,children:_})]})}},49374:(e,l,n)=>{n.d(l,{B:()=>o});n(96540);const a=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var t=n(86025),i=n(74848);const r=e=>{const l=e.split(".");for(let n=l.length;n>0;n--){const e=l.slice(0,n).join(".");if(a[e])return e}return null};function o({fn:e,children:l,hash:n}){const o=r(e);if(!o)return(0,i.jsx)(i.Fragment,{children:l});const s=(0,t.default)(`/${a[o]}#${n??e}`);return(0,i.jsx)("a",{href:s,target:"_blank",children:l??(0,i.jsxs)("code",{children:[e,"()"]})})}},65592:(e,l,n)=>{n.d(l,{A:()=>r});n(96540);var a=n(34164);const t={tilesGrid:"tilesGrid_hB9N"};var i=n(74848);function r({children:e,className:l}){return(0,i.jsx)("div",{className:(0,a.A)(t.tilesGrid,l),children:e})}},76252:(e,l,n)=>{n.r(l),n.d(l,{assets:()=>k,contentTitle:()=>y,default:()=>j,frontMatter:()=>u,metadata:()=>a,toc:()=>x});const a=JSON.parse('{"id":"traditional-ml/sparkml/index","title":"MLflow Spark MLlib Integration","description":"Apache Spark MLlib provides distributed machine learning algorithms for processing large-scale datasets across clusters. MLflow integrates with Spark MLlib to track distributed ML pipelines, manage models, and enable flexible deployment from cluster training to standalone inference.","source":"@site/docs/classic-ml/traditional-ml/sparkml/index.mdx","sourceDirName":"traditional-ml/sparkml","slug":"/traditional-ml/sparkml/","permalink":"/mlflow-website/docs/latest/ml/traditional-ml/sparkml/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"XGBoost within MLflow","permalink":"/mlflow-website/docs/latest/ml/traditional-ml/xgboost/guide/"},"next":{"title":"Prophet","permalink":"/mlflow-website/docs/latest/ml/traditional-ml/prophet/"}}');var t=n(74848),i=n(28453),r=(n(49374),n(11470)),o=n(19365),s=n(82238),m=n(65592),p=n(6789),c=n(51004),d=n(61878),f=n(22356),h=n(99653),g=n(17133),_=n(56064),w=n(60665);const u={},y="MLflow Spark MLlib Integration",k={},x=[{value:"Why MLflow + Spark MLlib?",id:"why-mlflow--spark-mllib",level:2},{value:"Basic Model Logging",id:"basic-model-logging",level:2},{value:"Model Formats and Loading",id:"model-formats-and-loading",level:2},{value:"Datasource Autologging",id:"datasource-autologging",level:2},{value:"Model Signatures",id:"model-signatures",level:2},{value:"ONNX Conversion",id:"onnx-conversion",level:2},{value:"Model Registry",id:"model-registry",level:2},{value:"Learn More",id:"learn-more",level:2}];function v(e){const l={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(l.header,{children:(0,t.jsx)(l.h1,{id:"mlflow-spark-mllib-integration",children:"MLflow Spark MLlib Integration"})}),"\n",(0,t.jsx)(l.p,{children:"Apache Spark MLlib provides distributed machine learning algorithms for processing large-scale datasets across clusters. MLflow integrates with Spark MLlib to track distributed ML pipelines, manage models, and enable flexible deployment from cluster training to standalone inference."}),"\n",(0,t.jsx)(l.h2,{id:"why-mlflow--spark-mllib",children:"Why MLflow + Spark MLlib?"}),"\n",(0,t.jsx)(s.A,{features:[{icon:c.A,title:"Pipeline Tracking",description:"Automatically log Spark ML pipelines with all stages, transformers, and estimators. Track parameters from each pipeline component and maintain complete lineage."},{icon:d.A,title:"Format Flexibility",description:"Save models in native Spark format for distributed batch processing or PyFunc format for inference outside a Spark cluster with automatic DataFrame conversion."},{icon:f.A,title:"Datasource Autologging",description:"Track data sources automatically with paths, formats, and versions. Maintain complete data lineage for distributed ML workflows."},{icon:h.A,title:"Cross-Platform Deployment",description:"Deploy Spark models with PyFunc wrappers for REST APIs and edge computing, or convert to ONNX for platform-independent inference."}]}),"\n",(0,t.jsx)(l.h2,{id:"basic-model-logging",children:"Basic Model Logging"}),"\n",(0,t.jsxs)(l.p,{children:["Log Spark MLlib models with ",(0,t.jsx)(l.code,{children:"mlflow.spark.log_model()"}),":"]}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'import mlflow\nimport mlflow.spark\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import Tokenizer, HashingTF\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder.appName("MLflowSparkExample").getOrCreate()\n\n# Prepare training data\ntraining = spark.createDataFrame(\n    [\n        (0, "a b c d e spark", 1.0),\n        (1, "b d", 0.0),\n        (2, "spark f g h", 1.0),\n        (3, "hadoop mapreduce", 0.0),\n    ],\n    ["id", "text", "label"],\n)\n\n# Create ML Pipeline\ntokenizer = Tokenizer(inputCol="text", outputCol="words")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")\nlr = LogisticRegression(maxIter=10, regParam=0.001)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# Train and log the model\nwith mlflow.start_run():\n    model = pipeline.fit(training)\n\n    # Log the entire pipeline\n    model_info = mlflow.spark.log_model(\n        spark_model=model, artifact_path="spark-pipeline"\n    )\n\n    # Log parameters manually\n    mlflow.log_params(\n        {\n            "max_iter": lr.getMaxIter(),\n            "reg_param": lr.getRegParam(),\n            "num_features": hashingTF.getNumFeatures(),\n        }\n    )\n\nprint(f"Model logged with URI: {model_info.model_uri}")\n'})}),"\n",(0,t.jsx)(l.p,{children:"Automatically logs the complete pipeline with all stages, parameters, and model in both Spark native and PyFunc formats."}),"\n",(0,t.jsx)(l.h2,{id:"model-formats-and-loading",children:"Model Formats and Loading"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsxs)(o.A,{value:"native",label:"Native Spark Format",children:[(0,t.jsx)(l.p,{children:"Preserves full Spark ML functionality for distributed processing:"}),(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'# Load as native Spark model (requires Spark session)\nspark_model = mlflow.spark.load_model(model_info.model_uri)\n\n# Use for distributed batch scoring\ntest_data = spark.createDataFrame(\n    [(4, "spark i j k"), (5, "l m n"), (6, "spark hadoop spark"), (7, "apache hadoop")],\n    ["id", "text"],\n)\n\npredictions = spark_model.transform(test_data)\npredictions.show()\n'})})]}),(0,t.jsxs)(o.A,{value:"pyfunc",label:"PyFunc Format",children:[(0,t.jsx)(l.p,{children:"Enables inference outside a Spark cluster:"}),(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'import pandas as pd\n\n# Load as PyFunc model\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Use with pandas DataFrame\ntest_data = pd.DataFrame(\n    {"text": ["spark machine learning", "hadoop distributed computing"]}\n)\n\npredictions = pyfunc_model.predict(test_data)\nprint(predictions)\n'})}),(0,t.jsx)(l.p,{children:"PyFunc automatically converts pandas DataFrames to Spark format and creates a local Spark session for inference. Note that the Apache Spark library is still required as a dependency."})]})]}),"\n",(0,t.jsx)(l.h2,{id:"datasource-autologging",children:"Datasource Autologging"}),"\n",(0,t.jsx)(l.p,{children:"Track data sources automatically during model training:"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'import mlflow.spark\n\nmlflow.spark.autolog()\n\nwith mlflow.start_run():\n    raw_data = spark.read.parquet("s3://my-bucket/training-data/")\n    model = pipeline.fit(raw_data)\n    mlflow.spark.log_model(model, artifact_path="model")\n'})}),"\n",(0,t.jsx)(l.p,{children:"Requires Spark 3.0+, MLflow-Spark JAR configuration, and is not supported on Databricks shared/serverless clusters. Logs paths, formats, and versions for all datasource reads."}),"\n",(0,t.jsx)(l.h2,{id:"model-signatures",children:"Model Signatures"}),"\n",(0,t.jsx)(l.p,{children:"Infer signatures automatically for Spark ML models:"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'from mlflow.models import infer_signature\nfrom pyspark.ml.functions import array_to_vector\n\nvector_data = spark.createDataFrame(\n    [([3.0, 4.0], 0.0), ([5.0, 6.0], 1.0)], ["features_array", "label"]\n).select(array_to_vector("features_array").alias("features"), "label")\n\nlr = LogisticRegression(featuresCol="features", labelCol="label")\nmodel = lr.fit(vector_data)\n\npredictions = model.transform(vector_data)\n\n# Infer signature from pandas DataFrames\nsignature = infer_signature(\n    vector_data.limit(2).toPandas(),\n    predictions.select("prediction").limit(2).toPandas(),\n)\n\nwith mlflow.start_run():\n    mlflow.spark.log_model(\n        spark_model=model,\n        artifact_path="vector_model",\n        signature=signature,\n    )\n'})}),"\n",(0,t.jsx)(l.h2,{id:"onnx-conversion",children:"ONNX Conversion"}),"\n",(0,t.jsx)(l.p,{children:"Convert Spark models to ONNX (experimental):"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'import onnxmltools\n\nwith mlflow.start_run():\n    model = pipeline.fit(training_data)\n    mlflow.spark.log_model(spark_model=model, artifact_path="spark_model")\n\n    onnx_model = onnxmltools.convert_sparkml(model, name="SparkMLPipeline")\n    onnxmltools.utils.save_model(onnx_model, "model.onnx")\n    mlflow.log_artifact("model.onnx")\n'})}),"\n",(0,t.jsx)(l.h2,{id:"model-registry",children:"Model Registry"}),"\n",(0,t.jsx)(l.p,{children:"Register and promote Spark models:"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'from mlflow import MlflowClient\n\nclient = MlflowClient()\n\nwith mlflow.start_run():\n    model = pipeline.fit(train_data)\n\n    mlflow.spark.log_model(\n        spark_model=model,\n        artifact_path="production_candidate",\n        registered_model_name="CustomerSegmentationModel",\n    )\n\n    mlflow.set_tags(\n        {\n            "validation_passed": "true",\n            "deployment_target": "batch_scoring",\n        }\n    )\n\nmodel_version = client.get_latest_versions(\n    "CustomerSegmentationModel", stages=["None"]\n)[0]\n\nclient.transition_model_version_stage(\n    name="CustomerSegmentationModel", version=model_version.version, stage="Staging"\n)\n'})}),"\n",(0,t.jsx)(l.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,t.jsxs)(m.A,{children:[(0,t.jsx)(p.A,{icon:g.A,iconSize:48,title:"Model Registry",description:"Manage model versions, aliases, and lifecycle stages for production deployment workflows.",href:"/ml/model-registry",linkText:"View registry docs \u2192",containerHeight:64}),(0,t.jsx)(p.A,{icon:_.A,iconSize:48,title:"Model Signatures",description:"Define input and output schemas for model validation and type checking.",href:"/ml/model/signatures",linkText:"Learn about signatures \u2192",containerHeight:64}),(0,t.jsx)(p.A,{icon:h.A,iconSize:48,title:"Model Deployment",description:"Deploy Spark models with MLflow serving, batch inference, and cloud platforms.",href:"/ml/deployment",linkText:"Deploy models \u2192",containerHeight:64}),(0,t.jsx)(p.A,{icon:w.A,iconSize:48,title:"MLflow Tracking",description:"Track experiments, parameters, metrics, and artifacts across ML workflows.",href:"/ml/tracking",linkText:"View tracking docs \u2192",containerHeight:64})]})]})}function j(e={}){const{wrapper:l}={...(0,i.R)(),...e.components};return l?(0,t.jsx)(l,{...e,children:(0,t.jsx)(v,{...e})}):v(e)}},82238:(e,l,n)=>{n.d(l,{A:()=>i});n(96540);const a={featureHighlights:"featureHighlights_Ardf",highlightItem:"highlightItem_XPnN",highlightIcon:"highlightIcon_SUR8",highlightContent:"highlightContent_d0XP"};var t=n(74848);function i({features:e,col:l=2}){return(0,t.jsx)("div",{className:a.featureHighlights,style:{gridTemplateColumns:`repeat(${l}, 1fr)`},children:e.map((e,l)=>(0,t.jsxs)("div",{className:a.highlightItem,children:[e.icon&&(0,t.jsx)("div",{className:a.highlightIcon,children:(0,t.jsx)(e.icon,{size:24})}),(0,t.jsxs)("div",{className:a.highlightContent,children:[(0,t.jsx)("h4",{children:e.title}),(0,t.jsx)("p",{children:e.description})]})]},l))})}}}]);