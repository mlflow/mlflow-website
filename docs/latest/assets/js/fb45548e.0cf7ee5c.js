"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[80],{10493:(e,t,o)=>{o.d(t,{Zp:()=>s,AC:()=>i,WO:()=>m,_C:()=>p,$3:()=>h,jK:()=>d});var n=o(34164);const l={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var a=o(28774),r=o(74848);const i=({children:e,isSmall:t,cols:o})=>(0,r.jsx)("div",{className:(0,n.A)(l.CardGroup,t?l.AutofillColumns:o?l[`Cols${o}`]:l.MaxThreeColumns),children:e}),s=({children:e,link:t=""})=>t?(0,r.jsx)(a.A,{className:(0,n.A)(l.Link,l.Card,l.CardBordered),to:t,children:e}):(0,r.jsx)("div",{className:(0,n.A)(l.Card,l.CardBordered),children:e}),p=({headerText:e,link:t,text:o})=>(0,r.jsx)(s,{link:t,children:(0,r.jsxs)("span",{children:[(0,r.jsx)("div",{className:(0,n.A)(l.CardTitle,l.BoxRoot,l.PaddingBottom4),style:{pointerEvents:"none"},children:(0,r.jsx)("div",{className:(0,n.A)(l.BoxRoot,l.FlexFlex,l.FlexAlignItemsCenter,l.FlexDirectionRow,l.FlexJustifyContentFlexStart,l.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,r.jsx)("div",{className:(0,n.A)(l.BoxRoot,l.BoxHideIfEmpty,l.MarginTop4,l.MarginLeft4),style:{pointerEvents:"auto"},children:(0,r.jsx)("span",{className:"",children:e})})})}),(0,r.jsx)("span",{className:(0,n.A)(l.TextColor,l.CardBody),children:(0,r.jsx)("p",{children:o})})]})}),m=({description:e,children:t,link:o})=>(0,r.jsx)(s,{link:o,children:(0,r.jsxs)("div",{className:l.LogoCardContent,children:[(0,r.jsx)("div",{className:l.LogoCardImage,children:t}),(0,r.jsx)("p",{className:l.TextColor,children:e})]})}),h=({children:e,link:t})=>(0,r.jsx)(s,{link:t,children:(0,r.jsx)("div",{className:l.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,n.A)("max-height-img-container",l.SmallLogoCardImage),children:e})})}),d=({title:e,description:t,link:o=""})=>(0,r.jsx)(s,{link:o,children:(0,r.jsxs)("div",{className:l.TitleCardContent,children:[(0,r.jsx)("div",{className:(0,n.A)(l.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,r.jsx)("hr",{className:(0,n.A)(l.TitleCardSeparator),style:{margin:"12px 0"}}),(0,r.jsx)("p",{className:(0,n.A)(l.TextColor),children:t})]})})},25800:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/prompt-logged-trace-f531e466499e24d2b8541d655237ea91.png"},45576:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/prompt-logged-model-links-b4c073f2c15e203ec34d7ef35c840112.png"},49374:(e,t,o)=>{o.d(t,{B:()=>s});o(96540);const n=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=o(86025),a=o(28774),r=o(74848);const i=e=>{const t=e.split(".");for(let o=t.length;o>0;o--){const e=t.slice(0,o).join(".");if(n[e])return e}return null};function s({fn:e,children:t}){const o=i(e);if(!o)return(0,r.jsx)(r.Fragment,{children:t});const s=(0,l.Ay)(`/${n[o]}#${e}`);return(0,r.jsx)(a.A,{to:s,target:"_blank",children:t??(0,r.jsxs)("code",{children:[e,"()"]})})}},54255:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/prompt-logged-graph-0b4cd4a6c6e28f2afeffea5a9bcee188.png"},54548:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/prompt-logged-model-5db16e3e39f3bbd4ca3138c96dff045e.png"},80944:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"prompt-version-mgmt/prompt-registry/log-with-model","title":"Log Prompts with Models","description":"Prompts are often used as a part of GenAI applications. Managing the association between prompts and models is crucial for tracking the evolution of models and ensuring consistency across different environments. MLflow Prompt Registry is integrated with MLflow\'s model tracking capability, allowing you to track which prompts (and versions) are used by your models and applications.","source":"@site/docs/genai/prompt-version-mgmt/prompt-registry/log-with-model.mdx","sourceDirName":"prompt-version-mgmt/prompt-registry","slug":"/prompt-version-mgmt/prompt-registry/log-with-model","permalink":"/docs/latest/genai/prompt-version-mgmt/prompt-registry/log-with-model","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Use Prompts in Apps","permalink":"/docs/latest/genai/prompt-version-mgmt/prompt-registry/use-prompts-in-apps"},"next":{"title":"Use Prompts in Deployed Apps","permalink":"/docs/latest/genai/prompt-version-mgmt/prompt-registry/use-prompts-in-deployed-apps"}}');var l=o(74848),a=o(28453);o(49374),o(10493),o(14252),o(11470),o(19365);const r={},i="Log Prompts with Models",s={},p=[{value:"Basic Usage",id:"basic-usage",level:2},{value:"Example 1: Logging Prompts with LangChain",id:"example-1-logging-prompts-with-langchain",level:2},{value:"1. Create a prompt",id:"1-create-a-prompt",level:3},{value:"2. Define a Chain using the registered prompts",id:"2-define-a-chain-using-the-registered-prompts",level:3},{value:"3. Log the Chain to MLflow",id:"3-log-the-chain-to-mlflow",level:3},{value:"Example 2: Automatic Prompt Logging with Models-from-Code",id:"example-2-automatic-prompt-logging-with-models-from-code",level:2},{value:"1. Create a prompt",id:"1-create-a-prompt-1",level:3},{value:"2. Define a Graph using the registered prompt",id:"2-define-a-graph-using-the-registered-prompt",level:3},{value:"3. Log the Graph to MLflow",id:"3-log-the-graph-to-mlflow",level:3},{value:"4. Load the graph back and invoke",id:"4-load-the-graph-back-and-invoke",level:3}];function m(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(t.header,{children:(0,l.jsx)(t.h1,{id:"log-prompts-with-models",children:"Log Prompts with Models"})}),"\n",(0,l.jsx)(t.p,{children:"Prompts are often used as a part of GenAI applications. Managing the association between prompts and models is crucial for tracking the evolution of models and ensuring consistency across different environments. MLflow Prompt Registry is integrated with MLflow's model tracking capability, allowing you to track which prompts (and versions) are used by your models and applications."}),"\n",(0,l.jsx)(t.h2,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,l.jsxs)(t.p,{children:["To log a model with associated prompts, use the ",(0,l.jsx)(t.code,{children:"prompts"})," parameter in the ",(0,l.jsx)(t.code,{children:"log_model"})," method. The ",(0,l.jsx)(t.code,{children:"prompts"})," parameter accepts a list of prompt URLs or prompt objects that are associated with the model. The associated prompts are displayed in the MLflow UI for the model run."]}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{children:'import mlflow\n\nwith mlflow.start_run():\n    mlflow.<flavor>.log_model(\n        model,\n        ...\n        # Specify a list of prompt URLs or prompt objects.\n        prompts=["prompts:/summarization-prompt/2"]\n    )\n'})}),"\n",(0,l.jsx)(t.admonition,{type:"warning",children:(0,l.jsxs)(t.p,{children:["The ",(0,l.jsx)(t.code,{children:"prompts"})," parameter for associating prompts with models is only supported for GenAI flavors such as OpenAI, LangChain, LlamaIndex, DSPy, etc. Please refer to the ",(0,l.jsx)(t.a,{href:"/genai/flavors",children:"GenAI flavors"})," for the full list of supported flavors."]})}),"\n",(0,l.jsx)(t.h2,{id:"example-1-logging-prompts-with-langchain",children:"Example 1: Logging Prompts with LangChain"}),"\n",(0,l.jsx)(t.h3,{id:"1-create-a-prompt",children:"1. Create a prompt"}),"\n",(0,l.jsxs)(t.p,{children:["If you haven't already created a prompt, follow ",(0,l.jsx)(t.a,{href:"/genai/prompt-version-mgmt/prompt-registry/create-and-edit-prompts",children:"the instructions in this page"})," to create a new prompt."]}),"\n",(0,l.jsx)(t.h3,{id:"2-define-a-chain-using-the-registered-prompts",children:"2. Define a Chain using the registered prompts"}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-python",children:'from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Load registered prompt\nprompt = mlflow.genai.load_prompt("prompts:/summarization-prompt/2")\n\n# Create LangChain prompt object\nlangchain_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            # IMPORTANT: Convert prompt template from double to single curly braces format\n            "system",\n            prompt.to_single_brace_format(),\n        ),\n        ("placeholder", "{messages}"),\n    ]\n)\n\n# Define the LangChain chain\nllm = ChatOpenAI()\nchain = langchain_prompt | llm\n\n# Invoke the chain\nresponse = chain.invoke({"num_sentences": 1, "sentences": "This is a test sentence."})\nprint(response)\n'})}),"\n",(0,l.jsx)(t.h3,{id:"3-log-the-chain-to-mlflow",children:"3. Log the Chain to MLflow"}),"\n",(0,l.jsxs)(t.p,{children:["Then log the chain to MLflow and specify the prompt URL in the ",(0,l.jsx)(t.code,{children:"prompts"})," parameter:"]}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-python",children:'with mlflow.start_run(run_name="summarizer-model"):\n    mlflow.langchain.log_model(\n        chain, name="model", prompts=["prompts:/summarization-prompt/2"]\n    )\n'})}),"\n",(0,l.jsx)(t.p,{children:"Now you can view the associated prompts to the model in MLflow UI:"}),"\n",(0,l.jsx)(t.p,{children:(0,l.jsx)(t.img,{alt:"Associated Prompts",src:o(54548).A+"",width:"888",height:"382"})}),"\n",(0,l.jsx)(t.p,{children:"Moreover, you can view the list of models (runs) that use a specific prompt in the prompt details page:"}),"\n",(0,l.jsx)(t.p,{children:(0,l.jsx)(t.img,{alt:"Associated Prompts",src:o(45576).A+"",width:"3364",height:"1244"})}),"\n",(0,l.jsx)(t.h2,{id:"example-2-automatic-prompt-logging-with-models-from-code",children:"Example 2: Automatic Prompt Logging with Models-from-Code"}),"\n",(0,l.jsxs)(t.p,{children:[(0,l.jsx)(t.a,{href:"/ml/model/models-from-code",children:"Models-from-Code"})," is a feature that allows you to define and log models in code.\nLogging a model with code brings several benefits, such as portability, readability, avoiding serialization, and more."]}),"\n",(0,l.jsxs)(t.p,{children:["Combining with MLflow Prompt Registry, the feature unlocks even more flexibility to manage prompt versions. Notably,\nif your model code uses a prompt from MLflow Prompt Registry, MLflow ",(0,l.jsx)(t.strong,{children:"automatically"})," logs it with the model for you."]}),"\n",(0,l.jsx)(t.p,{children:"In the following example, we use LangGraph to define a very simple chat bot using the registered prompt."}),"\n",(0,l.jsx)(t.h3,{id:"1-create-a-prompt-1",children:"1. Create a prompt"}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{children:'import mlflow\n\n# Register a new prompt\nprompt = mlflow.genai.register_prompt(\n    name="chat-prompt",\n    template="You are an expert in programming. Please answer the user\'s question about programming.",\n)\n'})}),"\n",(0,l.jsx)(t.h3,{id:"2-define-a-graph-using-the-registered-prompt",children:"2. Define a Graph using the registered prompt"}),"\n",(0,l.jsxs)(t.p,{children:["Create a Python script ",(0,l.jsx)(t.code,{children:"chatbot.py"})," with the following content."]}),"\n",(0,l.jsx)(t.admonition,{type:"tip",children:(0,l.jsxs)(t.p,{children:["If you are using Jupyter notebook, you can uncomment the ",(0,l.jsx)(t.code,{children:"%writefile"})," magic\ncommand and run the following code in a cell to generate the script."]})}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-python",children:'# %%writefile chatbot.py\n\nimport mlflow\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: list\n\n\nllm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)\nsystem_prompt = mlflow.genai.load_prompt("prompts:/chat-prompt/1")\n\n\ndef add_system_message(state: State):\n    return {\n        "messages": [\n            {\n                "role": "system",\n                "content": system_prompt.to_single_brace_format(),\n            },\n            *state["messages"],\n        ]\n    }\n\n\ndef chatbot(state: State):\n    return {"messages": [llm.invoke(state["messages"])]}\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node("add_system_message", add_system_message)\ngraph_builder.add_node("chatbot", chatbot)\ngraph_builder.add_edge(START, "add_system_message")\ngraph_builder.add_edge("add_system_message", "chatbot")\ngraph_builder.add_edge("chatbot", END)\n\ngraph = graph_builder.compile()\n\nmlflow.models.set_model(graph)\n'})}),"\n",(0,l.jsx)(t.h3,{id:"3-log-the-graph-to-mlflow",children:"3. Log the Graph to MLflow"}),"\n",(0,l.jsxs)(t.p,{children:["Specify the file path to the script in the ",(0,l.jsx)(t.code,{children:"model"})," parameter:"]}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-python",children:'with mlflow.start_run():\n    model_info = mlflow.langchain.log_model(\n        lc_model="./chatbot.py",\n        name="graph",\n    )\n'})}),"\n",(0,l.jsxs)(t.p,{children:["We didn't specify the ",(0,l.jsx)(t.code,{children:"prompts"})," parameter this time, but MLflow automatically logs the prompt loaded within the script to the logged model. Now you can view the associated prompt in MLflow UI:"]}),"\n",(0,l.jsx)(t.p,{children:(0,l.jsx)(t.img,{alt:"Associated Prompts",src:o(54255).A+"",width:"878",height:"413"})}),"\n",(0,l.jsx)(t.h3,{id:"4-load-the-graph-back-and-invoke",children:"4. Load the graph back and invoke"}),"\n",(0,l.jsx)(t.p,{children:"Finally, let's load the graph back and invoke it to see the chatbot in action."}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-python",children:'# Enable MLflow tracing for LangChain to view the prompt passed to LLM.\nmlflow.langchain.autolog()\n\n# Load the graph\ngraph = mlflow.langchain.load_model(model_info.model_uri)\n\ngraph.invoke(\n    {\n        "messages": [\n            {\n                "role": "user",\n                "content": "What is the difference between multi-threading and multi-processing?",\n            }\n        ]\n    }\n)\n'})}),"\n",(0,l.jsx)(t.p,{children:(0,l.jsx)(t.img,{alt:"Chatbot",src:o(25800).A+"",width:"1751",height:"797"})})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,l.jsx)(t,{...e,children:(0,l.jsx)(m,{...e})}):m(e)}}}]);