"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4487],{3760:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>I,contentTitle:()=>D,default:()=>E,frontMatter:()=>T,metadata:()=>n,toc:()=>C});const n=JSON.parse('{"id":"datasets/index","title":"Evaluation Datasets","description":"Transform Your GenAI Testing with Structured Evaluation Data","source":"@site/docs/genai/datasets/index.mdx","sourceDirName":"datasets","slug":"/datasets/","permalink":"/mlflow-website/docs/latest/genai/datasets/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Versioning Scorers","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/versioning"},"next":{"title":"End-to-End Workflow","permalink":"/mlflow-website/docs/latest/genai/datasets/end-to-end-workflow"}}');var i=a(74848),s=a(28453),o=a(11470),r=a(19365),l=(a(49374),a(66927),a(47020)),c=a(82238),p=a(79206),d=a(65592),m=a(6789),h=a(56955),u=a(51004),f=a(15977),w=a(98236),_=a(93893),g=a(93164),x=a(96844),y=a(47792),v=a(22864),j=a(76316),b=a(61878),A=a(80827),N=a(70099),k=a(96393);const T={},D="Evaluation Datasets",I={},C=[{value:"Transform Your GenAI Testing with Structured Evaluation Data",id:"transform-your-genai-testing-with-structured-evaluation-data",level:2},{value:"Quickstart: Build Your First Evaluation Dataset",id:"quickstart-build-your-first-evaluation-dataset",level:2},{value:"Why Evaluation Datasets?",id:"why-evaluation-datasets",level:2},{value:"The Evaluation Loop",id:"the-evaluation-loop",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Next Steps",id:"next-steps",level:2}];function S(e){const t={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"evaluation-datasets",children:"Evaluation Datasets"})}),"\n",(0,i.jsx)(t.h2,{id:"transform-your-genai-testing-with-structured-evaluation-data",children:"Transform Your GenAI Testing with Structured Evaluation Data"}),"\n",(0,i.jsx)(t.p,{children:"Evaluation datasets are the foundation of systematic GenAI application testing. They provide a centralized way to manage test data, ground truth expectations, and evaluation results\u2014enabling you to measure and improve the quality of your AI applications with confidence."}),"\n",(0,i.jsx)(t.h2,{id:"quickstart-build-your-first-evaluation-dataset",children:"Quickstart: Build Your First Evaluation Dataset"}),"\n",(0,i.jsxs)(t.p,{children:["There are several ways to create evaluation datasets, each suited to different stages of your GenAI development process. ",(0,i.jsx)(t.strong,{children:"Expectations are the cornerstone of effective evaluation"}),"\u2014they define the ground truth against which your AI's outputs are measured, enabling systematic quality assessment across iterations."]}),"\n",(0,i.jsx)(l.A,{children:(0,i.jsxs)(o.A,{children:[(0,i.jsx)(r.A,{value:"from-traces",label:"Build from Traces",default:!0,children:(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.datasets import create_dataset\n\n# Create your evaluation dataset\ndataset = create_dataset(\n    name="production_validation_set",\n    experiment_id=["0"],  # "0" is the default experiment\n    tags={"team": "ml-platform", "stage": "validation"},\n)\n\n# First, retrieve traces that will become the basis of the dataset\n# Request list format to work with individual Trace objects\ntraces = mlflow.search_traces(\n    experiment_ids=["0"],\n    max_results=50,\n    filter_string="attributes.trace_name = \'chat_completion\'",\n    return_type="list",  # Returns list[Trace] for direct manipulation\n)\n\n# Add expectations to the traces\nfor trace in traces[:20]:\n    # Expectations can be structured metrics\n    mlflow.log_expectation(\n        trace_id=trace.info.trace_id,\n        name="output_quality",\n        value={"relevance": 0.95, "accuracy": 1.0, "contains_citation": True},\n    )\n\n    # They can also be specific expected text\n    mlflow.log_expectation(\n        trace_id=trace.info.trace_id,\n        name="expected_answer",\n        value="The correct answer should include step-by-step instructions for password reset with email verification",\n    )\n\n# Retrieve the traces with added expectations\nannotated_traces = mlflow.search_traces(\n    experiment_ids=["0"], max_results=100, return_type="list"  # Get list[Trace] objects\n)\n\n# Merge the list of Trace objects directly into your dataset\ndataset.merge_records(annotated_traces)\n'})})}),(0,i.jsx)(r.A,{value:"from-dicts",label:"From Dictionaries",children:(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.datasets import create_dataset\n\n# Create dataset with manual test cases\ndataset = create_dataset(\n    name="regression_test_suite",\n    experiment_id=["0", "1"],  # Multiple experiments\n    tags={"type": "regression", "priority": "critical"},\n)\n\n# Define test cases with expected outputs\ntest_cases = [\n    {\n        "inputs": {\n            "question": "How do I reset my password?",\n            "context": "user_support",\n        },\n        "expectations": {\n            "answer": "To reset your password, click \'Forgot Password\' on the login page, enter your email, and follow the link sent to your inbox",\n            "contains_steps": True,\n            "tone": "helpful",\n            "max_response_time": 2.0,\n        },\n    },\n    {\n        "inputs": {\n            "question": "What are your refund policies?",\n            "context": "customer_service",\n        },\n        "expectations": {\n            "includes_timeframe": True,\n            "mentions_exceptions": True,\n            "accuracy": 1.0,\n        },\n    },\n]\n\ndataset.merge_records(test_cases)\n'})})}),(0,i.jsx)(r.A,{value:"from-pandas",label:"From DataFrame",children:(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import pandas as pd\nimport mlflow\nfrom mlflow.genai.datasets import create_dataset\n\n# Create dataset from existing test data\ndataset = create_dataset(\n    name="benchmark_dataset",\n    experiment_id=["0"],  # Use your experiment ID\n    tags={"source": "benchmark", "version": "2024.1"},\n)\n\n# Method 1: Use traces from search_traces (default returns DataFrame)\n# search_traces returns a pandas DataFrame by default when pandas is installed\ntraces_df = mlflow.search_traces(\n    experiment_ids=["0"],  # Search in your experiment\n    max_results=100\n    # No return_type specified - defaults to "pandas"\n)\n\n# The DataFrame from search_traces can be passed directly\ndataset.merge_records(traces_df)\n\n# Method 2: Create your own DataFrame with inputs and expectations\n# You can also create a DataFrame with the expected structure\ncustom_df = pd.DataFrame(\n    {\n        "inputs.question": [\n            "What is MLflow?",\n            "How do I track experiments?",\n            "Explain model versioning",\n        ],\n        "inputs.domain": ["general", "technical", "technical"],\n        "expectations.relevance": [1.0, 0.95, 0.9],\n        "expectations.technical_accuracy": [1.0, 1.0, 0.95],\n        "expectations.includes_examples": [True, True, False],\n        "tags.priority": ["high", "medium", "medium"],  # Optional tags\n        "tags.reviewed": [True, True, False],\n    }\n)\n\n# merge_records accepts DataFrames with inputs, expectations, and tags columns\ndataset.merge_records(custom_df)\n'})})})]})}),"\n",(0,i.jsx)(t.h2,{id:"why-evaluation-datasets",children:"Why Evaluation Datasets?"}),"\n",(0,i.jsx)(c.A,{features:[{icon:u.A,title:"Centralized Test Management",description:"Store all your test cases, expected outputs, and evaluation criteria in one place. No more scattered CSV files or hardcoded test data."},{icon:f.A,title:"Consistent Evaluation Source",description:"Maintain a concrete representation of test data that can be used repeatedly as your project evolves. Eliminate manual testing and avoid repeatedly assembling evaluation data for each iteration."},{icon:w.A,title:"Systematic Testing",description:"Move beyond ad-hoc testing to systematic evaluation. Define clear expectations and measure performance consistently across deployments."},{icon:_.A,title:"Collaborative Improvement",description:"Enable your entire team to contribute test cases and expectations. Share evaluation datasets across projects and teams."}]}),"\n",(0,i.jsx)(t.h2,{id:"the-evaluation-loop",children:"The Evaluation Loop"}),"\n",(0,i.jsxs)(t.p,{children:["Evaluation datasets bridge the critical gap between trace generation and evaluation execution in the GenAI development lifecycle. As you test your application and capture traces with expectations, ",(0,i.jsx)(t.strong,{children:"evaluation datasets transform these individual test cases into a materialized, reusable evaluation suite"}),". This creates a consistent and evolving collection of evaluation records that grows with your application\u2014each iteration adds new test cases while preserving the historical test coverage. Rather than losing valuable test scenarios after each development cycle, you build a comprehensive evaluation asset that can immediately assess the quality of changes and improvements to your implementation."]}),"\n",(0,i.jsx)(h.A,{title:"The Evaluation Loop",steps:[{icon:g.A,title:"Iterate on Code",description:"Build and improve your GenAI application",detailedDescription:"Whether starting fresh or improving existing code, implement changes to your GenAI application. Use MLflow's comprehensive tracing to monitor each iteration, capture execution details, and track how your modifications impact performance and quality."},{icon:w.A,title:"Test App",description:"Run initial tests and scenarios",detailedDescription:"Execute thorough testing with diverse prompts including edge cases, adversarial inputs, and typical user scenarios. Use MLflow's tracing to capture every interaction, response time, and token consumption for analysis."},{icon:x.A,title:"Collect Traces",description:"Capture real interactions",detailedDescription:"Systematically collect traces from both testing environments and production deployments. Each trace contains the complete request-response cycle, intermediate steps, and metadata that forms the foundation of your evaluation data."},{icon:y.A,title:"Add Expectations",description:"Define ground truth outputs",detailedDescription:"Annotate traces with expected outputs and quality criteria. Domain experts define what constitutes correct behavior, creating a gold standard for evaluation. These expectations become the benchmark against which all AI responses are measured."},{icon:u.A,title:"Create Dataset",description:"Organize evaluation data",detailedDescription:"Transform your annotated traces into structured evaluation datasets. These datasets become reusable test suites that can be versioned, shared across teams, and used to consistently measure performance across different model versions and configurations.",isFocus:!0},{icon:v.A,title:"Run Evaluation",description:"Execute systematic evaluation",detailedDescription:"Run comprehensive evaluations using MLflow's evaluation framework with automated scorers, LLM judges, and custom metrics. Generate detailed reports comparing actual outputs against expectations to quantify your AI's performance."},{icon:j.A,title:"Analyze Results",description:"Identify improvements",detailedDescription:"Deep dive into evaluation results to identify patterns, failure modes, and improvement opportunities. Use MLflow's visualization tools to compare performance across experiments and track progress over time."}],loopBackIcon:f.A,loopBackText:"Iterate & Improve",loopBackDescription:"After analyzing results, iterate on your application by refining prompts, adjusting model parameters, or enhancing your evaluation criteria. The cycle continues as you progressively improve quality.",circleSize:600}),"\n",(0,i.jsx)(t.h2,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsx)(p.A,{concepts:[{icon:y.A,title:"Ground Truth Management",description:"Define and maintain expected outputs for your test cases. Capture expert knowledge about what constitutes correct behavior for your AI system."},{icon:b.A,title:"Schema Evolution",description:"Automatically track the structure of your test data as it evolves. Add new fields and test dimensions without breaking existing evaluations."},{icon:f.A,title:"Incremental Updates",description:"Continuously improve your test suite by adding new cases from production. Update expectations as your understanding of correct behavior evolves."},{icon:A.A,title:"Flexible Tagging",description:"Organize datasets with tags for easy discovery and filtering. Track metadata like data sources, annotation guidelines, and quality levels."},{icon:j.A,title:"Performance Tracking",description:"Monitor how your application performs against the same test data over time. Identify regressions and improvements across deployments."},{icon:x.A,title:"Experiment Integration",description:"Link datasets to MLflow experiments for complete traceability. Understand which test data was used for each model evaluation."}]}),"\n",(0,i.jsx)(t.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsxs)(d.A,{children:[(0,i.jsx)(m.A,{icon:N.A,iconSize:48,title:"Dataset Structure",description:"Understand how evaluation datasets organize test inputs, expectations, and metadata",href:"/genai/concepts/evaluation-datasets",linkText:"Learn the concepts \u2192",containerHeight:64}),(0,i.jsx)(m.A,{icon:g.A,iconSize:48,title:"SDK Guide",description:"Complete guide to creating and managing evaluation datasets programmatically",href:"/genai/datasets/sdk-guide",linkText:"View SDK guide \u2192",containerHeight:64}),(0,i.jsx)(m.A,{icon:k.A,iconSize:48,title:"Evaluation Integration",description:"Learn how to use datasets with MLflow's evaluation framework",href:"/genai/eval-monitor",linkText:"Explore evaluation \u2192",containerHeight:64})]}),"\n",(0,i.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(t.p,{children:"Ready to improve your GenAI testing? Start with these resources:"}),"\n",(0,i.jsxs)(d.A,{children:[(0,i.jsx)(m.A,{icon:y.A,iconSize:48,title:"Setting Expectations",description:"Learn how to define ground truth and expected outputs for your AI system",href:"/genai/assessments/expectations",linkText:"Define expectations \u2192",containerHeight:64}),(0,i.jsx)(m.A,{icon:x.A,iconSize:48,title:"Tracing Guide",description:"Capture detailed execution data from your GenAI applications",href:"/genai/tracing",linkText:"Start tracing \u2192",containerHeight:64}),(0,i.jsx)(m.A,{icon:k.A,iconSize:48,title:"Evaluation Framework",description:"Run systematic evaluations using your datasets with automated scorers",href:"/genai/eval-monitor",linkText:"Learn evaluation \u2192",containerHeight:64})]})]})}function E(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(S,{...e})}):S(e)}},6789:(e,t,a)=>{a.d(t,{A:()=>c});a(96540);var n=a(28774),i=a(34164);const s={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var o=a(86025),r=a(21122),l=a(74848);function c({icon:e,image:t,imageDark:a,imageWidth:c,imageHeight:p,iconSize:d=32,containerHeight:m,title:h,description:u,href:f,linkText:w="Learn more \u2192",className:_}){if(!e&&!t)throw new Error("TileCard requires either an icon or image prop");const g=m?{height:`${m}px`}:{},x={};return c&&(x.width=`${c}px`),p&&(x.height=`${p}px`),(0,l.jsxs)(n.A,{href:f,className:(0,i.A)(s.tileCard,_),children:[(0,l.jsx)("div",{className:s.tileIcon,style:g,children:e?(0,l.jsx)(e,{size:d}):a?(0,l.jsx)(r.A,{sources:{light:(0,o.Ay)(t),dark:(0,o.Ay)(a)},alt:h,className:s.tileImage,style:x}):(0,l.jsx)("img",{src:(0,o.Ay)(t),alt:h,className:s.tileImage,style:x})}),(0,l.jsx)("h3",{children:h}),(0,l.jsx)("p",{children:u}),(0,l.jsx)("div",{className:s.tileLink,children:w})]})}},47020:(e,t,a)=>{a.d(t,{A:()=>s});a(96540);const n={wrapper:"wrapper_sf5q"};var i=a(74848);function s({children:e}){return(0,i.jsx)("div",{className:n.wrapper,children:e})}},49374:(e,t,a)=>{a.d(t,{B:()=>r});a(96540);const n=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var i=a(86025),s=a(74848);const o=e=>{const t=e.split(".");for(let a=t.length;a>0;a--){const e=t.slice(0,a).join(".");if(n[e])return e}return null};function r({fn:e,children:t,hash:a}){const r=o(e);if(!r)return(0,s.jsx)(s.Fragment,{children:t});const l=(0,i.Ay)(`/${n[r]}#${a??e}`);return(0,s.jsx)("a",{href:l,target:"_blank",children:t??(0,s.jsxs)("code",{children:[e,"()"]})})}},56955:(e,t,a)=>{a.d(t,{A:()=>R});var n=a(96540);const i="loopContainer_P7aD",s="loopTitle_JPUj",o="loopContent_d_OB",r="circleContainer_r3vu",l="svgCanvas_uDoP",c="arrowPath_C9al",p="arrowHead_pHvN",d="stepNode_dfTI",m="stepNodeContent_qttg",h="highlighted_oNtg",u="focusNode_z3RB",f="stepNumber_LNrP",w="stepLabel_vl8R",_="centerIcon_KAOa",g="loopIconWrapper_xBPW",x="loopText_T4eg",y="tooltip_UzKu",v="tooltipTitle_HAKW",j="tooltipDescription_EDYJ",b="tooltipArrow_WNhr",A="centerTooltip_R18b",N="centerTooltipDescription_ttXB",k="mobileLinearContent_PCYK",T="mobileStepItem_x9mX",D="mobileStepIndicator_zWzO",I="mobileStepNumber_HnjD",C="mobileFocusNode_FTRa",S="mobileStepConnector_kK9y",E="mobileStepContent_jmKx",L="mobileStepTitle_P2DM",M="mobileStepDescription_qbMN",z="mobileLoopBack_nXtn",F="mobileLoopIcon_FAGz",$="mobileLoopContent_BRFV",G="mobileLoopTitle_JcCt",H="mobileLoopDescription_5B8T";var q=a(74848);const R=({steps:e,title:t,loopBackIcon:a,loopBackText:R,loopBackDescription:B,circleSize:W=400})=>{const[O,P]=(0,n.useState)(null),[U,K]=(0,n.useState)(!1),[Y,J]=(0,n.useState)({x:0,y:0}),[X,V]=(0,n.useState)(!1),Q=(0,n.useRef)(null);n.useEffect(()=>{const e=()=>{V(window.innerWidth<=768)};return e(),window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)},[]);const Z=X?280:W,ee=(()=>{const t=W/2,a=X?100:140,n=X?130:220,i=Math.min(1.2,.8+.05*e.length),s=(t-(X?50:80))*i;return Math.max(a,Math.min(n,s))})(),te=Z/2,ae=Z/2,ne=t=>{const a=2*t*Math.PI/e.length-Math.PI/2;return{x:te+ee*Math.cos(a),y:ae+ee*Math.sin(a)}},ie=(e,t)=>{const a=ne(e),n=ne(t),i=n.x-a.x,s=n.y-a.y,o=(a.x+n.x)/2,r=(a.y+n.y)/2,l=Math.sqrt(i*i+s*s),c=i/l,p=s/l;return`M ${o-10*c} ${r-10*p} L ${o+10*c} ${r+10*p}`},se=()=>{P(null)};return X?(0,q.jsxs)("div",{className:i,children:[t&&(0,q.jsx)("h3",{className:s,children:t}),(0,q.jsxs)("div",{className:k,children:[e.map((t,a)=>(0,q.jsxs)("div",{className:T,children:[(0,q.jsxs)("div",{className:D,children:[(0,q.jsx)("div",{className:`${I} ${t.isFocus?C:""}`,children:t.icon?(0,q.jsx)(t.icon,{size:20}):(0,q.jsx)("span",{children:a+1})}),a<e.length-1&&(0,q.jsx)("div",{className:S})]}),(0,q.jsxs)("div",{className:E,children:[(0,q.jsx)("h4",{className:L,children:t.title}),(0,q.jsx)("p",{className:M,children:t.detailedDescription||t.description})]})]},a)),a&&B&&(0,q.jsxs)("div",{className:z,children:[(0,q.jsx)("div",{className:F,children:(0,q.jsx)(a,{size:24})}),(0,q.jsxs)("div",{className:$,children:[(0,q.jsx)("h4",{className:G,children:R||"Iterate"}),(0,q.jsx)("p",{className:H,children:B})]})]})]})]}):(0,q.jsxs)("div",{className:i,children:[t&&(0,q.jsx)("h3",{className:s,children:t}),(0,q.jsx)("div",{className:o,children:(0,q.jsxs)("div",{className:r,ref:Q,style:{width:`${Z}px`,height:`${Z}px`},children:[(0,q.jsxs)("svg",{width:Z,height:Z,className:l,children:[e.map((t,a)=>{const n=(a+1)%e.length;return(0,q.jsxs)("g",{children:[(0,q.jsx)("defs",{children:(0,q.jsx)("marker",{id:`arrowhead-${a}`,markerWidth:"6",markerHeight:"6",refX:"5",refY:"3",orient:"auto",children:(0,q.jsx)("path",{d:"M 0 0 L 6 3 L 0 6 L 1.5 3 Z",fill:"currentColor",opacity:"1",className:p})})}),(0,q.jsx)("path",{d:ie(a,n),fill:"none",stroke:"currentColor",strokeWidth:"2",strokeDasharray:"0",opacity:"0.9",markerEnd:`url(#arrowhead-${a})`,className:c})]},`arrow-${a}`)}),a&&(0,q.jsxs)("g",{className:_,onMouseEnter:()=>K(!0),onMouseLeave:()=>K(!1),style:{cursor:"pointer"},children:[(0,q.jsx)("foreignObject",{x:te-35,y:ae-35,width:"70",height:"70",children:(0,q.jsx)("div",{className:g,children:(0,q.jsx)(a,{size:32})})}),R&&(0,q.jsx)("text",{x:te,y:ae+50,textAnchor:"middle",className:x,children:R})]})]}),e.map((e,t)=>{const a=ne(t);return(0,q.jsxs)("div",{className:`${d} ${e.highlight?h:""} ${e.isFocus?u:""}`,style:{left:`${a.x}px`,top:`${a.y}px`,transform:"translate(-50%, -50%)"},onMouseEnter:e=>(e=>{if(P(e),Q.current){Q.current.getBoundingClientRect();const t=ne(e);J({x:t.x,y:t.y})}})(t),onMouseLeave:se,children:[(0,q.jsx)("div",{className:m,children:e.icon?(0,q.jsx)(e.icon,{size:24}):(0,q.jsx)("span",{className:f,children:t+1})}),(0,q.jsx)("div",{className:w,children:e.title})]},t)}),null!==O&&(0,q.jsxs)("div",{className:y,style:{left:`${Y.x}px`,top:`${Y.y}px`,transform:"translate(-50%, -120%)"},children:[(0,q.jsx)("h4",{className:v,children:e[O].title}),(0,q.jsx)("p",{className:j,children:e[O].detailedDescription||e[O].description}),(0,q.jsx)("div",{className:b})]}),U&&B&&(0,q.jsx)("div",{className:A,style:{left:`${te}px`,top:`${ae}px`,transform:"translate(-50%, -50%)"},children:(0,q.jsx)("p",{className:N,children:B})})]})})]})}},65592:(e,t,a)=>{a.d(t,{A:()=>o});a(96540);var n=a(34164);const i={tilesGrid:"tilesGrid_hB9N"};var s=a(74848);function o({children:e,className:t}){return(0,s.jsx)("div",{className:(0,n.A)(i.tilesGrid,t),children:e})}},66927:(e,t,a)=>{a.d(t,{A:()=>o});a(96540);const n={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var i=a(86025),s=a(74848);function o({src:e,alt:t,width:a,caption:o,className:r}){return(0,s.jsxs)("div",{className:`${n.container} ${r||""}`,children:[(0,s.jsx)("div",{className:n.imageWrapper,style:a?{width:a}:{},children:(0,s.jsx)("img",{src:(0,i.Ay)(e),alt:t,className:n.image})}),o&&(0,s.jsx)("p",{className:n.caption,children:o})]})}},79206:(e,t,a)=>{a.d(t,{A:()=>s});a(96540);const n={conceptOverview:"conceptOverview_x8T_",overviewTitle:"overviewTitle_HyAI",conceptGrid:"conceptGrid_uJNV",conceptCard:"conceptCard_oday",conceptHeader:"conceptHeader_HCk5",conceptIcon:"conceptIcon_gejw",conceptTitle:"conceptTitle_TGMM",conceptDescription:"conceptDescription_ZyDn"};var i=a(74848);function s({concepts:e,title:t}){return(0,i.jsxs)("div",{className:n.conceptOverview,children:[t&&(0,i.jsx)("h3",{className:n.overviewTitle,children:t}),(0,i.jsx)("div",{className:n.conceptGrid,children:e.map((e,t)=>(0,i.jsxs)("div",{className:n.conceptCard,children:[(0,i.jsxs)("div",{className:n.conceptHeader,children:[e.icon&&(0,i.jsx)("div",{className:n.conceptIcon,children:(0,i.jsx)(e.icon,{size:20})}),(0,i.jsx)("h4",{className:n.conceptTitle,children:e.title})]}),(0,i.jsx)("p",{className:n.conceptDescription,children:e.description})]},t))})]})}},82238:(e,t,a)=>{a.d(t,{A:()=>s});a(96540);const n={featureHighlights:"featureHighlights_Ardf",highlightItem:"highlightItem_XPnN",highlightIcon:"highlightIcon_SUR8",highlightContent:"highlightContent_d0XP"};var i=a(74848);function s({features:e}){return(0,i.jsx)("div",{className:n.featureHighlights,children:e.map((e,t)=>(0,i.jsxs)("div",{className:n.highlightItem,children:[e.icon&&(0,i.jsx)("div",{className:n.highlightIcon,children:(0,i.jsx)(e.icon,{size:24})}),(0,i.jsxs)("div",{className:n.highlightContent,children:[(0,i.jsx)("h4",{children:e.title}),(0,i.jsx)("p",{children:e.description})]})]},t))})}}}]);