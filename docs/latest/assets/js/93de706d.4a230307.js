"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["8190"],{38993(e,t,n){n.r(t),n.d(t,{metadata:()=>a,default:()=>v,frontMatter:()=>x,contentTitle:()=>b,toc:()=>y,assets:()=>j});var a=JSON.parse('{"id":"traditional-ml/xgboost/index","title":"MLflow XGBoost Integration","description":"Introduction","source":"@site/docs/classic-ml/traditional-ml/xgboost/index.mdx","sourceDirName":"traditional-ml/xgboost","slug":"/traditional-ml/xgboost/","permalink":"/mlflow-website/docs/latest/ml/traditional-ml/xgboost/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"XGBoost"},"sidebar":"classicMLSidebar","previous":{"title":"Scikit-learn","permalink":"/mlflow-website/docs/latest/ml/traditional-ml/sklearn/"},"next":{"title":"SparkML","permalink":"/mlflow-website/docs/latest/ml/traditional-ml/sparkml/"}}'),o=n(74848),r=n(28453),i=n(33508),l=n(10440),s=n(77541),d=n(78010),m=n(57250),c=n(46858),g=n(17133),p=n(76316),h=n(61878),u=n(10166),_=n(22864),f=n(99653);let x={sidebar_position:1,sidebar_label:"XGBoost"},b="MLflow XGBoost Integration",j={},y=[{value:"Introduction",id:"introduction",level:2},{value:"Why MLflow + XGBoost?",id:"why-mlflow--xgboost",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Autologging",id:"autologging",level:2},{value:"What Gets Logged",id:"what-gets-logged",level:3},{value:"Autolog Configuration",id:"autolog-configuration",level:3},{value:"Hyperparameter Tuning",id:"hyperparameter-tuning",level:2},{value:"Grid Search",id:"grid-search",level:3},{value:"Optuna Integration",id:"optuna-integration",level:3},{value:"Model Management",id:"model-management",level:2},{value:"Model Registry Integration",id:"model-registry-integration",level:2},{value:"Model Serving",id:"model-serving",level:2},{value:"Learn More",id:"learn-more",level:2}];function w(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"mlflow-xgboost-integration",children:"MLflow XGBoost Integration"})}),"\n",(0,o.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.strong,{children:"XGBoost"})," (eXtreme Gradient Boosting) is a popular gradient boosting library for structured data. MLflow provides native integration with XGBoost for experiment tracking, model management, and deployment."]}),"\n",(0,o.jsx)(t.p,{children:"This integration supports both the native XGBoost API and scikit-learn compatible interface, making it easy to track experiments and deploy models regardless of which API you prefer."}),"\n",(0,o.jsx)(t.h2,{id:"why-mlflow--xgboost",children:"Why MLflow + XGBoost?"}),"\n",(0,o.jsx)(i.A,{features:[{icon:c.A,title:"Automatic Logging",description:"Single line of code (mlflow.xgboost.autolog()) captures all parameters, metrics per boosting round, and feature importance without manual instrumentation."},{icon:g.A,title:"Complete Model Recording",description:"Logs trained models with serialization format, input/output signatures, model dependencies, and Python environment for reproducible deployments."},{icon:p.A,title:"Hyperparameter Tuning",description:"Automatically creates child runs for GridSearchCV and RandomizedSearchCV, tracking all parameter combinations and their performance metrics."},{icon:h.A,title:"Dual API Support",description:"Works with both native XGBoost API (xgb.train) and scikit-learn compatible estimators (XGBClassifier, XGBRegressor) with the same autologging functionality."}]}),"\n",(0,o.jsx)(t.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,o.jsx)(t.p,{children:"Get started with XGBoost and MLflow in just a few lines of code:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport xgboost as xgb\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\n# Enable autologging - captures everything automatically\nmlflow.xgboost.autolog()\n\n# Load and prepare data\ndata = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\n\n# Prepare data in XGBoost format\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Train model - MLflow automatically logs everything!\nwith mlflow.start_run():\n    model = xgb.train(\n        params={\n            "objective": "reg:squarederror",\n            "max_depth": 6,\n            "learning_rate": 0.1,\n        },\n        dtrain=dtrain,\n        num_boost_round=100,\n        evals=[(dtrain, "train"), (dtest, "test")],\n    )\n'})}),"\n",(0,o.jsx)(t.p,{children:"Autologging captures parameters, metrics per iteration, feature importance with visualizations, and the trained model."}),"\n",(0,o.jsx)(t.admonition,{title:"Tracking Server Setup",type:"tip",children:(0,o.jsxs)(t.p,{children:["Running locally? MLflow stores experiments in the current directory by default. For team collaboration or remote tracking, ",(0,o.jsx)(t.strong,{children:(0,o.jsx)(t.a,{href:"/ml/tracking/tutorials/remote-server",children:"set up a tracking server"})}),"."]})}),"\n",(0,o.jsx)(t.h2,{id:"autologging",children:"Autologging"}),"\n",(0,o.jsx)(t.p,{children:"Enable autologging to automatically track XGBoost experiments with a single line of code:"}),"\n",(0,o.jsxs)(d.A,{children:[(0,o.jsx)(m.A,{value:"native",label:"Native XGBoost API",default:!0,children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport xgboost as xgb\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ndata = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\n\n# Enable autologging\nmlflow.xgboost.autolog()\n\n# Train with native API\nwith mlflow.start_run():\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    model = xgb.train(\n        params={"objective": "reg:squarederror", "max_depth": 6},\n        dtrain=dtrain,\n        num_boost_round=100,\n    )\n'})})}),(0,o.jsx)(m.A,{value:"sklearn",label:"Scikit-learn API",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"import mlflow\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n\n# Load data\ndata = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\n\n# Enable sklearn autologging (works with XGBoost estimators)\nmlflow.sklearn.autolog()\n\n# Train with sklearn-compatible API\nwith mlflow.start_run():\n    model = XGBRegressor(n_estimators=100, max_depth=6)\n    model.fit(X_train, y_train)\n"})})})]}),"\n",(0,o.jsx)(t.h3,{id:"what-gets-logged",children:"What Gets Logged"}),"\n",(0,o.jsx)(t.p,{children:"When autologging is enabled, MLflow automatically captures:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Parameters"}),": All booster parameters and training configuration"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Metrics"}),": Training and validation metrics for each boosting round"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Feature Importance"}),": Multiple importance types (weight, gain, cover) with visualizations"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Model"}),": The trained model with proper serialization format"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Artifacts"}),": Feature importance plots and JSON data"]}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"autolog-configuration",children:"Autolog Configuration"}),"\n",(0,o.jsx)(t.p,{children:"Customize autologging behavior:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'mlflow.xgboost.autolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    log_datasets=True,\n    model_format="json",  # Recommended for portability\n    registered_model_name="XGBoostModel",\n    extra_tags={"team": "data-science"},\n)\n'})}),"\n",(0,o.jsx)(t.h2,{id:"hyperparameter-tuning",children:"Hyperparameter Tuning"}),"\n",(0,o.jsx)(t.h3,{id:"grid-search",children:"Grid Search"}),"\n",(0,o.jsx)(t.p,{children:"MLflow automatically creates child runs for hyperparameter tuning:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom xgboost import XGBClassifier\n\n# Load data\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\n\n# Enable autologging\nmlflow.sklearn.autolog()\n\n# Define parameter grid\nparam_grid = {\n    "n_estimators": [50, 100, 200],\n    "max_depth": [3, 6, 9],\n    "learning_rate": [0.01, 0.1, 0.3],\n}\n\n# Run grid search - MLflow logs each combination as a child run\nwith mlflow.start_run():\n    model = XGBClassifier(random_state=42)\n    grid_search = GridSearchCV(model, param_grid, cv=5, scoring="roc_auc", n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n\n    print(f"Best score: {grid_search.best_score_}")\n'})}),"\n",(0,o.jsx)(t.h3,{id:"optuna-integration",children:"Optuna Integration"}),"\n",(0,o.jsx)(t.p,{children:"For more advanced hyperparameter optimization:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport optuna\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\n# Load data\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\n\nmlflow.xgboost.autolog()\n\n\ndef objective(trial):\n    params = {\n        "n_estimators": trial.suggest_int("n_estimators", 50, 300),\n        "max_depth": trial.suggest_int("max_depth", 3, 10),\n        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),\n        "subsample": trial.suggest_float("subsample", 0.6, 1.0),\n        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),\n    }\n\n    with mlflow.start_run(nested=True):\n        model = XGBClassifier(**params, random_state=42)\n        model.fit(X_train, y_train)\n        score = model.score(X_test, y_test)\n        return score\n\n\nwith mlflow.start_run():\n    study = optuna.create_study(direction="maximize")\n    study.optimize(objective, n_trials=50)\n\n    mlflow.log_params({f"best_{k}": v for k, v in study.best_params.items()})\n    mlflow.log_metric("best_score", study.best_value)\n'})}),"\n",(0,o.jsx)(t.h2,{id:"model-management",children:"Model Management"}),"\n",(0,o.jsx)(t.p,{children:"Log models with specific configurations:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import mlflow.xgboost\nimport xgboost as xgb\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ndata = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\nwith mlflow.start_run():\n    params = {"objective": "reg:squarederror", "max_depth": 6}\n    model = xgb.train(params, dtrain, num_boost_round=100)\n\n    mlflow.xgboost.log_model(\n        xgb_model=model,\n        name="model",\n        model_format="json",  # Recommended for portability\n        registered_model_name="production_model",\n    )\n'})}),"\n",(0,o.jsx)(t.admonition,{title:"Model Format",type:"tip",children:(0,o.jsxs)(t.p,{children:["Use ",(0,o.jsx)(t.code,{children:'model_format="json"'})," for the best portability across XGBoost versions. The ",(0,o.jsx)(t.code,{children:"json"})," format is human-readable and cross-platform compatible."]})}),"\n",(0,o.jsx)(t.p,{children:"Load models for inference:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'# Load as native XGBoost model\nmodel = mlflow.xgboost.load_model("runs:/<run_id>/model")\n\n# Load as PyFunc for generic interface\npyfunc_model = mlflow.pyfunc.load_model("runs:/<run_id>/model")\n\n# Load from model registry using alias\nmodel = mlflow.pyfunc.load_model("models:/XGBoostModel@champion")\n'})}),"\n",(0,o.jsx)(t.h2,{id:"model-registry-integration",children:"Model Registry Integration"}),"\n",(0,o.jsx)(t.p,{children:"Register and manage model versions:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import mlflow.xgboost\nimport xgboost as xgb\nfrom mlflow import MlflowClient\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\n# Load and prepare data\ndata = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2, random_state=42\n)\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\n# Register model during training\nwith mlflow.start_run():\n    params = {"objective": "reg:squarederror", "max_depth": 6}\n    model = xgb.train(params, dtrain, num_boost_round=100)\n\n    mlflow.xgboost.log_model(\n        xgb_model=model,\n        name="model",\n        registered_model_name="XGBoostModel",\n    )\n\n# Set alias for deployment\nclient = MlflowClient()\nclient.set_registered_model_alias(\n    name="XGBoostModel",\n    alias="champion",\n    version=1,\n)\n\n# Load model by alias\nmodel = mlflow.pyfunc.load_model("models:/XGBoostModel@champion")\n'})}),"\n",(0,o.jsx)(t.h2,{id:"model-serving",children:"Model Serving"}),"\n",(0,o.jsx)(t.p,{children:"Serve models locally for testing:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:'mlflow models serve -m "models:/XGBoostModel@champion" -p 5000\n'})}),"\n",(0,o.jsx)(t.p,{children:"Make predictions via REST API:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import requests\nimport pandas as pd\n\ndata = pd.DataFrame(\n    {\n        "feature1": [1.2, 2.3],\n        "feature2": [0.8, 1.5],\n        "feature3": [3.4, 4.2],\n    }\n)\n\nresponse = requests.post(\n    "http://localhost:5000/invocations",\n    headers={"Content-Type": "application/json"},\n    json={"dataframe_split": data.to_dict(orient="split")},\n)\n\npredictions = response.json()\n'})}),"\n",(0,o.jsx)(t.p,{children:"Deploy to cloud platforms:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"# Deploy to AWS SageMaker\nmlflow deployments create \\\n    -t sagemaker \\\n    --name xgboost-endpoint \\\n    -m models:/XGBoostModel@champion\n\n# Deploy to Azure ML\nmlflow deployments create \\\n    -t azureml \\\n    --name xgboost-service \\\n    -m models:/XGBoostModel@champion\n"})}),"\n",(0,o.jsx)(t.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,o.jsxs)(l.A,{children:[(0,o.jsx)(s.A,{icon:u.A,title:"Tracking Server Setup",description:"Set up a self-hosted MLflow tracking server for team collaboration and remote experiment tracking.",href:"/ml/tracking/tutorials/remote-server"}),(0,o.jsx)(s.A,{icon:_.A,title:"Model Evaluation",description:"Evaluate XGBoost models using MLflow's comprehensive evaluation framework with built-in metrics and custom evaluators.",href:"/ml/evaluation"}),(0,o.jsx)(s.A,{icon:f.A,title:"Model Deployment",description:"Deploy XGBoost models locally, to Kubernetes, AWS SageMaker, or other cloud platforms using MLflow's deployment tools.",href:"/ml/deployment"})]})]})}function v(e={}){let{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(w,{...e})}):w(e)}},33508(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);function o({features:e,col:t=2}){return(0,a.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${t}, 1fr)`},children:e.map((e,t)=>(0,a.jsxs)("div",{className:"highlightItem_XPnN",children:[e.icon&&(0,a.jsx)("div",{className:"highlightIcon_SUR8",children:(0,a.jsx)(e.icon,{size:24})}),(0,a.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,a.jsx)("h4",{children:e.title}),(0,a.jsx)("p",{children:e.description})]})]},t))})}},77541(e,t,n){n.d(t,{A:()=>d});var a=n(74848);n(96540);var o=n(95310),r=n(34164);let i="tileImage_O4So";var l=n(66497),s=n(92802);function d({icon:e,image:t,imageDark:n,imageWidth:d,imageHeight:m,iconSize:c=32,containerHeight:g,title:p,description:h,href:u,linkText:_="Learn more \u2192",className:f}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let x=g?{height:`${g}px`}:{},b={};return d&&(b.width=`${d}px`),m&&(b.height=`${m}px`),(0,a.jsxs)(o.A,{href:u,className:(0,r.A)("tileCard_NHsj",f),children:[(0,a.jsx)("div",{className:"tileIcon_pyoR",style:x,children:e?(0,a.jsx)(e,{size:c}):n?(0,a.jsx)(s.A,{sources:{light:(0,l.default)(t),dark:(0,l.default)(n)},alt:p,className:i,style:b}):(0,a.jsx)("img",{src:(0,l.default)(t),alt:p,className:i,style:b})}),(0,a.jsx)("h3",{children:p}),(0,a.jsx)("p",{children:h}),(0,a.jsx)("div",{className:"tileLink_iUbu",children:_})]})}},10440(e,t,n){n.d(t,{A:()=>r});var a=n(74848);n(96540);var o=n(34164);function r({children:e,className:t}){return(0,a.jsx)("div",{className:(0,o.A)("tilesGrid_hB9N",t),children:e})}}}]);