"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["5439"],{76924(e,a,n){n.r(a),n.d(a,{metadata:()=>t,default:()=>p,frontMatter:()=>s,contentTitle:()=>l,toc:()=>d,assets:()=>i});var t=JSON.parse('{"id":"eval-monitor/running-evaluation/eval-examples","title":"MLflow evaluation examples for GenAI","description":"This page presents some common usage patterns for the evaluation harness, including data patterns and predict_fn patterns.","source":"@site/docs/genai/eval-monitor/running-evaluation/eval-examples.mdx","sourceDirName":"eval-monitor/running-evaluation","slug":"/eval-monitor/running-evaluation/eval-examples","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/running-evaluation/eval-examples","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Quickstart","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/quickstart"},"next":{"title":"Evaluate Prompts","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/running-evaluation/prompts"}}'),o=n(74848),r=n(28453);let s={},l="MLflow evaluation examples for GenAI",i={},d=[{value:"Common data input patterns",id:"common-data-input-patterns",level:2},{value:"Evaluate using an MLflow Evaluation Dataset (recommended)",id:"evaluate-using-an-mlflow-evaluation-dataset-recommended",level:3},{value:"Evaluate using a list of dictionaries",id:"evaluate-using-a-list-of-dictionaries",level:3},{value:"Evaluate using a Pandas DataFrame",id:"evaluate-using-a-pandas-dataframe",level:3},{value:"Evaluate using a Spark DataFrame",id:"evaluate-using-a-spark-dataframe",level:3},{value:"Common <code>predict_fn</code> patterns",id:"common-predict_fn-patterns",level:2},{value:"Call your app directly",id:"call-your-app-directly",level:3},{value:"Wrap your app in a callable",id:"wrap-your-app-in-a-callable",level:3},{value:"Evaluate a logged model",id:"evaluate-a-logged-model",level:3}];function c(e){let a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(a.header,{children:(0,o.jsx)(a.h1,{id:"mlflow-evaluation-examples-for-genai",children:"MLflow evaluation examples for GenAI"})}),"\n",(0,o.jsxs)(a.p,{children:["This page presents some common usage patterns for the evaluation harness, including data patterns and ",(0,o.jsx)(a.code,{children:"predict_fn"})," patterns."]}),"\n",(0,o.jsx)(a.h2,{id:"common-data-input-patterns",children:"Common data input patterns"}),"\n",(0,o.jsx)(a.h3,{id:"evaluate-using-an-mlflow-evaluation-dataset-recommended",children:"Evaluate using an MLflow Evaluation Dataset (recommended)"}),"\n",(0,o.jsx)(a.p,{children:"MLflow Evaluation Datasets are useful when you need to convert traces to evaluation records."}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Correctness, Safety\nfrom my_app import agent  # Your GenAI app with tracing\n\n# Load versioned evaluation dataset\ndataset = mlflow.genai.datasets.get_dataset(\n    dataset_id="d-7f2e3a9b8c1d4e5f6a7b8c9d0e1f2a3b"\n)\n\n# Run evaluation\nresults = mlflow.genai.evaluate(\n    data=dataset,\n    predict_fn=agent,\n    scorers=[Correctness(), Safety()],\n)\n'})}),"\n",(0,o.jsxs)(a.p,{children:["To create datasets from traces or scratch, see ",(0,o.jsx)(a.a,{href:"/genai/datasets/",children:"Build evaluation datasets"}),"."]}),"\n",(0,o.jsx)(a.h3,{id:"evaluate-using-a-list-of-dictionaries",children:"Evaluate using a list of dictionaries"}),"\n",(0,o.jsx)(a.p,{children:"Use a simple list of dictionaries for quick prototyping without creating a formal evaluation dataset. This is useful for quick prototyping, small datasets (fewer than 100 examples), and informal development testing."}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Correctness, RelevanceToQuery\nfrom my_app import agent  # Your GenAI app with tracing\n\n# Define test data as a list of dictionaries\neval_data = [\n    {\n        "inputs": {"question": "What is MLflow?"},\n        "expectations": {\n            "expected_facts": ["open-source platform", "ML lifecycle management"]\n        },\n    },\n    {\n        "inputs": {"question": "How do I track experiments?"},\n        "expectations": {\n            "expected_facts": ["mlflow.start_run()", "log metrics", "log parameters"]\n        },\n    },\n    {\n        "inputs": {"question": "What are MLflow\'s main components?"},\n        "expectations": {\n            "expected_facts": ["Tracking", "Projects", "Models", "Registry"]\n        },\n    },\n]\n\n# Run evaluation\nresults = mlflow.genai.evaluate(\n    data=eval_data,\n    predict_fn=agent,\n    scorers=[Correctness(), RelevanceToQuery()],\n)\n'})}),"\n",(0,o.jsxs)(a.p,{children:["For production, convert to an ",(0,o.jsx)(a.a,{href:"/genai/datasets/",children:"MLflow Evaluation Dataset"}),"."]}),"\n",(0,o.jsx)(a.h3,{id:"evaluate-using-a-pandas-dataframe",children:"Evaluate using a Pandas DataFrame"}),"\n",(0,o.jsx)(a.p,{children:"Use Pandas DataFrames for evaluation when working with CSV files or existing data science workflows. This is useful for quick prototyping, small datasets (fewer than 100 examples), and informal development testing."}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom mlflow.genai.scorers import Correctness, Safety\nfrom my_app import agent  # Your GenAI app with tracing\n\n# Create evaluation data as a Pandas DataFrame\neval_df = pd.DataFrame(\n    [\n        {\n            "inputs": {"question": "What is MLflow?"},\n            "expectations": {\n                "expected_response": "MLflow is an open-source platform for ML lifecycle management"\n            },\n        },\n        {\n            "inputs": {"question": "How do I log metrics?"},\n            "expectations": {\n                "expected_response": "Use mlflow.log_metric() to log metrics"\n            },\n        },\n    ]\n)\n\n# Run evaluation\nresults = mlflow.genai.evaluate(\n    data=eval_df,\n    predict_fn=agent,\n    scorers=[Correctness(), Safety()],\n)\n'})}),"\n",(0,o.jsx)(a.h3,{id:"evaluate-using-a-spark-dataframe",children:"Evaluate using a Spark DataFrame"}),"\n",(0,o.jsx)(a.p,{children:"Use Spark DataFrames for large-scale evaluations or if you need to filter the records in an MLflow Evaluation Dataset before running the evaluation."}),"\n",(0,o.jsxs)(a.p,{children:["The DataFrame must comply with the ",(0,o.jsx)(a.a,{href:"/genai/datasets/sdk-guide#evaluation-dataset-schema",children:"evaluation dataset schema"}),"."]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Safety, RelevanceToQuery\nfrom my_app import agent  # Your GenAI app with tracing\n\n# Load evaluation data from a Parquet file or any Spark-compatible source\neval_df = spark.read.parquet("path/to/evaluation/data")\n\n# Run evaluation\nresults = mlflow.genai.evaluate(\n    data=eval_df,\n    predict_fn=agent,\n    scorers=[Safety(), RelevanceToQuery()],\n)\n'})}),"\n",(0,o.jsxs)(a.h2,{id:"common-predict_fn-patterns",children:["Common ",(0,o.jsx)(a.code,{children:"predict_fn"})," patterns"]}),"\n",(0,o.jsx)(a.h3,{id:"call-your-app-directly",children:"Call your app directly"}),"\n",(0,o.jsxs)(a.p,{children:["Pass your app directly as ",(0,o.jsx)(a.code,{children:"predict_fn"})," when parameter names match your evaluation dataset keys. This is useful for apps that have parameter names that match the ",(0,o.jsx)(a.code,{children:"inputs"})," in your evaluation dataset."]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import RelevanceToQuery, Safety\n\n\n# Your GenAI app that accepts \'question\' as a parameter\n@mlflow.trace\ndef my_chatbot_app(question: str) -> dict:\n    # Your app logic here\n    response = f"I can help you with: {question}"\n    return {"response": response}\n\n\n# Evaluation data with \'question\' key matching the function parameter\neval_data = [\n    {"inputs": {"question": "What is MLflow?"}},\n    {"inputs": {"question": "How do I track experiments?"}},\n]\n\n# Pass your app directly since parameter names match\nresults = mlflow.genai.evaluate(\n    data=eval_data,\n    predict_fn=my_chatbot_app,  # Direct reference, no wrapper needed\n    scorers=[RelevanceToQuery(), Safety()],\n)\n'})}),"\n",(0,o.jsx)(a.h3,{id:"wrap-your-app-in-a-callable",children:"Wrap your app in a callable"}),"\n",(0,o.jsxs)(a.p,{children:["If your app expects different parameter names or data structures than your evaluation dataset's ",(0,o.jsx)(a.code,{children:"inputs"}),", wrap it in a callable function. This is useful when there are parameter name mismatches between your app's parameters and evaluation dataset ",(0,o.jsx)(a.code,{children:"input"})," keys (for example, ",(0,o.jsx)(a.code,{children:"user_input"})," vs ",(0,o.jsx)(a.code,{children:"question"}),"), or when data format conversions are required (for example, string to list or JSON parsing)."]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import RelevanceToQuery, Safety\n\n\n# Your existing GenAI app with different parameter names\n@mlflow.trace\ndef customer_support_bot(user_message: str, chat_history: list = None) -> dict:\n    # Your app logic here\n    context = f"History: {chat_history}" if chat_history else "New conversation"\n    return {\n        "bot_response": f"Helping with: {user_message}. {context}",\n        "confidence": 0.95,\n    }\n\n\n# Wrapper function to translate evaluation data to your app\'s interface\ndef evaluate_support_bot(question: str, history: str = None) -> dict:\n    # Convert evaluation dataset format to your app\'s expected format\n    chat_history = history.split("|") if history else []\n\n    # Call your app with the translated parameters\n    result = customer_support_bot(user_message=question, chat_history=chat_history)\n\n    # Translate output to standard format if needed\n    return {\n        "response": result["bot_response"],\n        "confidence_score": result["confidence"],\n    }\n\n\n# Evaluation data with different key names\neval_data = [\n    {"inputs": {"question": "Reset password", "history": "logged in|forgot email"}},\n    {"inputs": {"question": "Track my order"}},\n]\n\n# Use the wrapper function for evaluation\nresults = mlflow.genai.evaluate(\n    data=eval_data,\n    predict_fn=evaluate_support_bot,  # Wrapper handles translation\n    scorers=[RelevanceToQuery(), Safety()],\n)\n'})}),"\n",(0,o.jsx)(a.h3,{id:"evaluate-a-logged-model",children:"Evaluate a logged model"}),"\n",(0,o.jsx)(a.p,{children:"Wrap logged MLflow models to translate between evaluation's named parameters and the model's single-parameter interface."}),"\n",(0,o.jsxs)(a.p,{children:["Most logged models (such as those using PyFunc or logging flavors like LangChain) accept a single input parameter (for example, ",(0,o.jsx)(a.code,{children:"model_inputs"})," for PyFunc), while ",(0,o.jsx)(a.code,{children:"predict_fn"})," expects named parameters that correspond to the keys in your evaluation dataset."]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Safety\n\n# Make sure to load your logged model outside of the predict_fn so MLflow only loads it once!\nmodel = mlflow.pyfunc.load_model("models:/chatbot/staging")\n\n\ndef evaluate_model(question: str) -> dict:\n    return model.predict({"question": question})\n\n\nresults = mlflow.genai.evaluate(\n    data=[{"inputs": {"question": "Tell me about MLflow"}}],\n    predict_fn=evaluate_model,\n    scorers=[Safety()],\n)\n'})})]})}function p(e={}){let{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,o.jsx)(a,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},28453(e,a,n){n.d(a,{R:()=>s,x:()=>l});var t=n(96540);let o={},r=t.createContext(o);function s(e){let a=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:a},e.children)}}}]);