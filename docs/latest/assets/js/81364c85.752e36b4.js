"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[231],{2406:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"prompt-registry/optimize-prompts","title":"Optimize Prompts (Experimental)","description":"MLflow allows you to plug your prompts into advanced prompt optimization techniques through MLflow\'s unified interface using the  API.","source":"@site/docs/genai/prompt-registry/optimize-prompts.mdx","sourceDirName":"prompt-registry","slug":"/prompt-registry/optimize-prompts","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/optimize-prompts","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"sidebar_label":"Optimize Prompts \ud83c\udd95"},"sidebar":"genAISidebar","previous":{"title":"Structured Output","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/structured-output"},"next":{"title":"Prompt Engineering UI","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/prompt-engineering"}}');var r=n(74848),o=n(28453),a=n(49374);const s={sidebar_position:5,sidebar_label:"Optimize Prompts \ud83c\udd95"},l="Optimize Prompts (Experimental)",p={},m=[{value:"Optimization Overview",id:"optimization-overview",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"FAQ",id:"faq",level:2},{value:"What are the supported Dataset formats?",id:"what-are-the-supported-dataset-formats",level:3},{value:"How to combine multiple scorers?",id:"how-to-combine-multiple-scorers",level:3}];function c(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"optimize-prompts-experimental",children:"Optimize Prompts (Experimental)"})}),"\n",(0,r.jsxs)(t.p,{children:["MLflow allows you to plug your prompts into advanced prompt optimization techniques through MLflow's unified interface using the ",(0,r.jsx)(a.B,{fn:"mlflow.genai.optimize_prompt"})," API.\nThis feature helps you improve your prompts automatically by leveraging evaluation metrics and labeled data. Currently, ",(0,r.jsx)(t.a,{href:"https://dspy.ai/api/optimizers/MIPROv2/",children:"DSPy's MIPROv2 algorithm"})," is supported by this API."]}),"\n",(0,r.jsx)(t.admonition,{title:"Key Benefits",type:"tip",children:(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Unified Interface"}),": Access to the state-of-the-art prompt optimization algorithms through a neutral interface."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Prompt Management"}),": Integrate with MLflow Prompt Registry to gain reusability, version control and lineage."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Evaluation"}),": Evaluate prompt performance comprehensively with MLflow's evaluation features."]}),"\n"]})}),"\n",(0,r.jsx)(t.h2,{id:"optimization-overview",children:"Optimization Overview"}),"\n",(0,r.jsxs)(t.p,{children:["In order to use ",(0,r.jsx)(a.B,{fn:"mlflow.genai.optimize_prompt"})," API, you need to prepare the following:"]}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Component"}),(0,r.jsx)(t.th,{children:"Definition"}),(0,r.jsx)(t.th,{children:"Example"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"Registered Prompt"})}),(0,r.jsxs)(t.td,{children:["A prompt registered in MLflow. See ",(0,r.jsx)(t.a,{href:"/genai/prompt-registry/",children:"Prompt Management"})," for how to register a prompt."]}),(0,r.jsx)(t.td,{children:(0,r.jsx)("pre",{children:(0,r.jsx)("code",{children:'mlflow.register_prompt(name="qa", template="Answer {{question}}")'})})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"Scorer Objects"})}),(0,r.jsxs)(t.td,{children:["A set of ",(0,r.jsx)(a.B,{fn:"mlflow.genai.Scorer",children:"Scorer"})," objects that evaluate the quality of the prompt. See ",(0,r.jsx)(a.B,{fn:"mlflow.genai.scorer"})," for how to define a custom scorer."]}),(0,r.jsx)(t.td,{children:(0,r.jsx)("pre",{children:(0,r.jsxs)("code",{children:["@scorer",(0,r.jsx)("br",{}),"def exact_match(expectations: dict, outputs: dict)",(0,r.jsx)("br",{})," return expectations == outputs"]})})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"Training(+Validation) Data"})}),(0,r.jsx)(t.td,{children:"A set of training data and optionally validation data containing inputs and expected outputs."}),(0,r.jsx)(t.td,{children:'[{"inputs": {"question": "2+2"}, "expectations": {"answer": "4"}}, {"inputs": {"question": "2+3"}, "expectations": {"answer": "5"}}]'})]})]})]}),"\n",(0,r.jsx)(t.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsx)(t.p,{children:"Here's a simple example of optimizing a question-answering prompt:"}),"\n",(0,r.jsx)(t.p,{children:"As a prerequisite, you need to install DSPy."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"$ pip install dspy>=2.6.0 mlflow>=3.1.0\n"})}),"\n",(0,r.jsx)(t.p,{children:"Then, run the following code to register the initial prompt and optimize it."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import os\nfrom typing import Any\nimport mlflow\nfrom mlflow.genai.scorers import scorer\nfrom mlflow.genai.optimize import OptimizerConfig, LLMParams\n\nos.environ["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"\n\n\n# Define a custom scorer function to evaluate prompt performance with the @scorer decorator.\n# The scorer function for optimization can take inputs, outputs, and expectations.\n@scorer\ndef exact_match(expectations: dict[str, Any], outputs: dict[str, Any]) -> bool:\n    return expectations["answer"] == outputs["answer"]\n\n\n# Register the initial prompt\ninitial_template = """\nAnswer to this math question: {{question}}.\nReturn the result in a JSON string in the format of {"answer": "xxx"}.\n"""\n\nprompt = mlflow.genai.register_prompt(\n    name="math",\n    template=initial_template,\n)\n\n# The data can be a list of dictionaries, a pandas DataFrame, or an mlflow.genai.EvaluationDataset\n# It needs to contain inputs and expectations where each row is a dictionary.\ntrain_data = [\n    {\n        "inputs": {"question": "Given that $y=3$, evaluate $(1+y)^y$."},\n        "expectations": {"answer": "64"},\n    },\n    {\n        "inputs": {\n            "question": "The midpoint of the line segment between $(x,y)$ and $(-9,1)$ is $(3,-5)$. Find $(x,y)$."\n        },\n        "expectations": {"answer": "(15,-11)"},\n    },\n    {\n        "inputs": {\n            "question": "What is the value of $b$ if $5^b + 5^b + 5^b + 5^b + 5^b = 625^{(b-1)}$? Express your answer as a common fraction."\n        },\n        "expectations": {"answer": "\\\\frac{5}{3}"},\n    },\n    {\n        "inputs": {"question": "Evaluate the expression $a^3\\\\cdot a^2$ if $a= 5$."},\n        "expectations": {"answer": "3125"},\n    },\n    {\n        "inputs": {"question": "Evaluate $\\\\lceil 8.8 \\\\rceil+\\\\lceil -8.8 \\\\rceil$."},\n        "expectations": {"answer": "17"},\n    },\n]\n\neval_data = [\n    {\n        "inputs": {\n            "question": "The sum of 27 consecutive positive integers is $3^7$. What is their median?"\n        },\n        "expectations": {"answer": "81"},\n    },\n    {\n        "inputs": {"question": "What is the value of $x$ if $x^2 - 10x + 25 = 0$?"},\n        "expectations": {"answer": "5"},\n    },\n    {\n        "inputs": {\n            "question": "If $a\\\\ast b = 2a+5b-ab$, what is the value of $3\\\\ast10$?"\n        },\n        "expectations": {"answer": "26"},\n    },\n    {\n        "inputs": {\n            "question": "Given that $-4$ is a solution to $x^2 + bx -36 = 0$, what is the value of $b$?"\n        },\n        "expectations": {"answer": "-5"},\n    },\n]\n\n# Optimize the prompt\nresult = mlflow.genai.optimize_prompt(\n    target_llm_params=LLMParams(model_name="openai/gpt-4.1-mini"),\n    prompt=prompt,\n    train_data=train_data,\n    eval_data=eval_data,\n    scorers=[exact_match],\n    optimizer_config=OptimizerConfig(\n        num_instruction_candidates=8,\n        max_few_show_examples=2,\n    ),\n)\n\n# The optimized prompt is automatically registered as a new version\nprint(result.prompt.uri)\n'})}),"\n",(0,r.jsx)(t.p,{children:"In the example above the average performance score increased from 0 to 0.5.\nAfter the optimization process is completed, you can visit the MLflow Prompt Registry page and see the optimized prompt."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Optimized Prompt",src:n(44198).A+"",width:"3456",height:"1780"})}),"\n",(0,r.jsxs)(t.p,{children:["Note that the optimized prompt of ",(0,r.jsx)(a.B,{fn:"mlflow.genai.optimize_prompt"})," expects the output to be a JSON string.\nTherefore, you need to parse the output using ",(0,r.jsx)(t.code,{children:"json.loads"})," in your application. See ",(0,r.jsx)(t.a,{href:"/genai/prompt-registry/#4-load-and-use-the-prompt",children:"Load and Use the Prompt"})," for how to load the optimized prompt."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport json\nimport openai\n\n\ndef predict(question: str, prompt_uri: str) -> str:\n    prompt = mlflow.genai.load_prompt(prompt_uri)\n    content = prompt.format(question=question)\n    completion = openai.chat.completions.create(\n        model="gpt-4.1-mini",\n        messages=[{"role": "user", "content": content}],\n        temperature=0.1,\n    )\n\n    return json.loads(completion.choices[0].message.content)["answer"]\n'})}),"\n",(0,r.jsx)(t.h2,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsxs)(t.p,{children:["You can customize the optimization process using ",(0,r.jsx)(t.code,{children:"OptimizerConfig"}),", which includes the following parameters:"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"num_instruction_candidates"}),": The number of candidate instructions to try. Default: 6"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"max_few_show_examples"}),": The maximum number of examples to show in few-shot demonstrations. Default: 6"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"optimizer_llm"}),": The LLM to use for optimization. Default: None (uses target LLM)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"verbose"}),": Whether to show optimizer logs during optimization. Default: False"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"autolog"}),": Whether to log the optimization parameters, datasets and metrics. If set to True, a MLflow run is automatically created to store them. Default: False"]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["See ",(0,r.jsx)(a.B,{fn:"mlflow.genai.OptimizerConfig"})," for more details."]}),"\n",(0,r.jsx)(t.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,r.jsx)(t.admonition,{type:"info",children:(0,r.jsx)(t.p,{children:"We are actively working on the benchmarking.\nThese benchmarks results are preliminary and subject to change."})}),"\n",(0,r.jsx)(t.p,{children:"MLflow prompt optimization can improve your application's performance across various tasks. Here are the results from testing MLflow's optimization capabilities on several datasets:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.a,{href:"https://huggingface.co/datasets/allenai/ai2_arc",children:(0,r.jsx)(t.strong,{children:"ARC-Challenge"})}),": The ai2_arc dataset contains a set of multiple choice science questions"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.a,{href:"https://huggingface.co/datasets/openai/gsm8k",children:(0,r.jsx)(t.strong,{children:"GSM8K"})}),": The gsm8k dataset contains a set of linguistically diverse grade school math word problems"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.a,{href:"https://huggingface.co/datasets/DigitalLearningGmbH/MATH-lighteval",children:(0,r.jsx)(t.strong,{children:"MATH"})}),": Competition mathematics problems requiring advanced reasoning and problem-solving skills"]}),"\n"]}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Dataset"}),(0,r.jsx)(t.th,{children:"Model"}),(0,r.jsx)(t.th,{children:"Baseline"}),(0,r.jsx)(t.th,{children:"Optimized"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"MATH"})}),(0,r.jsx)(t.td,{children:"gpt-4.1o-nano"}),(0,r.jsx)(t.td,{children:"17.25%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"18.48%"})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"GSM8K"})}),(0,r.jsx)(t.td,{children:"gpt-4.1o-nano"}),(0,r.jsx)(t.td,{children:"21.46%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"49.89%"})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"ARC-Challenge"})}),(0,r.jsx)(t.td,{children:"gpt-4.1o-nano"}),(0,r.jsx)(t.td,{children:"71.42%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"89.25%"})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"MATH"})}),(0,r.jsx)(t.td,{children:"Llama4-maverick"}),(0,r.jsx)(t.td,{children:"33.06%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"33.26%"})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"GSM8K"})}),(0,r.jsx)(t.td,{children:"Llama4-maverick"}),(0,r.jsx)(t.td,{children:"55.80%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"58.22%"})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"ARC-Challenge"})}),(0,r.jsx)(t.td,{children:"Llama4-maverick"}),(0,r.jsx)(t.td,{children:"0.17%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"93.17%"})})]})]})]}),"\n",(0,r.jsxs)(t.p,{children:["The results above are benchmarks tested against ",(0,r.jsx)(t.code,{children:"gpt-4.1o-nano"})," and ",(0,r.jsx)(t.code,{children:"Llama4-maverick"})," with DSPy's MIPROv2 algorithm and default settings, using specific evaluation metrics for each task.\nThe results might change if you use a different model, configuration, dataset, or starting prompt(s).\nThese results show that MLflow's prompt optimization can solve many of the challenges, delivering measurable performance gains with minimal effort."]}),"\n",(0,r.jsx)(t.h2,{id:"faq",children:"FAQ"}),"\n",(0,r.jsx)(t.h3,{id:"what-are-the-supported-dataset-formats",children:"What are the supported Dataset formats?"}),"\n",(0,r.jsxs)(t.p,{children:["The training and evaluation data for the ",(0,r.jsx)(a.B,{fn:"mlflow.genai.optimize_prompt"})," API can be a list of dictionaries, a pandas DataFrame, a spark DataFrame, or an ",(0,r.jsx)(a.B,{fn:"mlflow.genai.EvaluationDataset",children:"mlflow.genai.EvaluationDataset"}),".\nIn any case, the data needs to contain inputs and expectations columns that contains a dictionary of input fields and expected output fields.\nEach inputs or expectations dictionary can contain primitive types, lists, nested dictionaries, and Pydantic models. Data types are inferred from the first row of the dataset."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# \u2705 OK\n[\n    {\n        "inputs": {"question": "What is the capital of France?"},\n        "expectations": {"answer": "Paris"},\n    },\n]\n\n# \u2705 OK\n[\n    {\n        "inputs": {"question": "What are the three largest cities of Japan?"},\n        "expectations": {"answer": ["Tokyo", "Osaka", "Nagoya"]},\n    },\n]\n\n# \u2705 OK\nfrom pydantic import BaseModel\n\n\nclass Country(BaseModel):\n    name: str\n    capital: str\n    population: int\n\n\n[\n    {\n        "inputs": {"question": "What is the capital of France?"},\n        "expectations": {\n            "answer": Country(name="France", capital="Paris", population=68000000)\n        },\n    },\n]\n\n# \u274c NG\n[\n    {\n        "inputs": "What is the capital of France?",\n        "expectations": "Paris",\n    },\n]\n'})}),"\n",(0,r.jsx)(t.h3,{id:"how-to-combine-multiple-scorers",children:"How to combine multiple scorers?"}),"\n",(0,r.jsxs)(t.p,{children:["While the ",(0,r.jsx)(a.B,{fn:"mlflow.genai.optimize_prompt"})," API accepts multiple scorers, the optimizer needs to combine them into a single score during the optimization process.\nBy default, the optimizer computes the total score of all scorers with numeric or boolean values.\nIf you want to use a custom aggregation function or use scorers that return non-numeric values, you can pass a custom aggregation function to the ",(0,r.jsx)(t.code,{children:"objective"})," parameter."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'@scorer\ndef safeness(outputs: dict[str, Any]) -> bool:\n    return "death" not in outputs["answer"].lower()\n\n\n@scorer\ndef relevance(expectations: dict[str, Any], outputs: dict[str, Any]) -> bool:\n    return expectations["answer"] in outputs["answer"]\n\n\ndef objective(scores: dict[str, Any]) -> float:\n    if not scores["safeness"]:\n        return -1\n    return scores["relevance"]\n\n\nresult = mlflow.genai.optimize_prompt(\n    target_llm_params=LLMParams(model_name="openai/gpt-4.1-mini"),\n    prompt=prompt,\n    train_data=train_data,\n    eval_data=eval_data,\n    scorers=[safeness, relevance],\n    objective=objective,\n)\n'})})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>s});var i=n(96540);const r={},o=i.createContext(r);function a(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(o.Provider,{value:t},e.children)}},44198:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/registered_prompt-979a351f289dc9b8c6ba24ea856e1398.png"},49374:(e,t,n)=>{n.d(t,{B:()=>s});n(96540);const i=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var r=n(86025),o=n(74848);const a=e=>{const t=e.split(".");for(let n=t.length;n>0;n--){const e=t.slice(0,n).join(".");if(i[e])return e}return null};function s({fn:e,children:t,hash:n}){const s=a(e);if(!s)return(0,o.jsx)(o.Fragment,{children:t});const l=(0,r.Ay)(`/${i[s]}#${n??e}`);return(0,o.jsx)("a",{href:l,target:"_blank",children:t??(0,o.jsxs)("code",{children:[e,"()"]})})}}}]);