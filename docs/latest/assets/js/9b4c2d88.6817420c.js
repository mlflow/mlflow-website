/*! For license information please see 9b4c2d88.6817420c.js.LICENSE.txt */
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4423],{6789:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var i=t(28774),a=t(34164);const o={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var s=t(86025),r=t(74848);function l({icon:e,image:n,iconSize:t=32,containerHeight:l,title:c,description:d,href:p,linkText:h="Learn more \u2192",className:g}){if(!e&&!n)throw new Error("TileCard requires either an icon or image prop");const m=l?{height:`${l}px`}:{};return(0,r.jsxs)(i.A,{href:p,className:(0,a.A)(o.tileCard,g),children:[(0,r.jsx)("div",{className:o.tileIcon,style:m,children:e?(0,r.jsx)(e,{size:t}):(0,r.jsx)("img",{src:(0,s.Ay)(n),alt:c,className:o.tileImage})}),(0,r.jsx)("h3",{children:c}),(0,r.jsx)("p",{children:d}),(0,r.jsx)("div",{className:o.tileLink,children:h})]})}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(96540);const a={},o=i.createContext(a);function s(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:n},e.children)}},45244:(e,n,t)=>{t.d(n,{A:()=>i});const i=(0,t(84722).A)("book",[["path",{d:"M4 19.5v-15A2.5 2.5 0 0 1 6.5 2H19a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1H6.5a1 1 0 0 1 0-5H20",key:"k3hazp"}]])},58013:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>g,contentTitle:()=>h,default:()=>y,frontMatter:()=>p,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"governance/ai-gateway/integration","title":"AI Gateway Integration","description":"Learn how to integrate the MLflow AI Gateway with applications, frameworks, and production systems.","source":"@site/docs/genai/governance/ai-gateway/integration.mdx","sourceDirName":"governance/ai-gateway","slug":"/governance/ai-gateway/integration","permalink":"/docs/latest/genai/governance/ai-gateway/integration","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Usage","permalink":"/docs/latest/genai/governance/ai-gateway/usage"},"next":{"title":"Getting Started with MLflow AI Gateway","permalink":"/docs/latest/genai/governance/ai-gateway/guides/"}}');var a=t(74848),o=t(28453),s=t(65592),r=t(6789),l=t(80964),c=t(85731),d=t(45244);const p={},h="AI Gateway Integration",g={},m=[{value:"Application Integrations",id:"application-integrations",level:2},{value:"FastAPI Integration",id:"fastapi-integration",level:3},{value:"Flask Integration",id:"flask-integration",level:3},{value:"Async/Await Support",id:"asyncawait-support",level:3},{value:"LangChain Integration",id:"langchain-integration",level:2},{value:"Setup",id:"setup",level:3},{value:"Chat Models",id:"chat-models",level:3},{value:"Embeddings",id:"embeddings",level:3},{value:"Complete RAG Example",id:"complete-rag-example",level:3},{value:"OpenAI Compatibility",id:"openai-compatibility",level:2},{value:"MLflow Models Integration",id:"mlflow-models-integration",level:2},{value:"Registering Models",id:"registering-models",level:3},{value:"Production Best Practices",id:"production-best-practices",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Security",id:"security",level:3},{value:"Monitoring and Logging",id:"monitoring-and-logging",level:3},{value:"Load Balancing",id:"load-balancing",level:3},{value:"Health and Monitoring",id:"health-and-monitoring",level:2},{value:"Next Steps",id:"next-steps",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"ai-gateway-integration",children:"AI Gateway Integration"})}),"\n","\n",(0,a.jsx)(n.p,{children:"Learn how to integrate the MLflow AI Gateway with applications, frameworks, and production systems."}),"\n",(0,a.jsx)(n.h2,{id:"application-integrations",children:"Application Integrations"}),"\n",(0,a.jsx)(n.h3,{id:"fastapi-integration",children:"FastAPI Integration"}),"\n",(0,a.jsx)(n.p,{children:"Build REST APIs that proxy requests to the AI Gateway, adding your own business logic, authentication, and data processing:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\nfrom mlflow.deployments import get_deploy_client\n\napp = FastAPI()\nclient = get_deploy_client("http://localhost:5000")\n\n\n@app.post("/chat")\nasync def chat_endpoint(message: str):\n    try:\n        response = client.predict(\n            endpoint="chat", inputs={"messages": [{"role": "user", "content": message}]}\n        )\n        return {"response": response["choices"][0]["message"]["content"]}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post("/embed")\nasync def embed_endpoint(text: str):\n    try:\n        response = client.predict(endpoint="embeddings", inputs={"input": text})\n        return {"embedding": response["data"][0]["embedding"]}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n'})}),"\n",(0,a.jsx)(n.h3,{id:"flask-integration",children:"Flask Integration"}),"\n",(0,a.jsx)(n.p,{children:"Create Flask applications that integrate AI capabilities using familiar request/response patterns:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from flask import Flask, request, jsonify\nfrom mlflow.deployments import get_deploy_client\n\napp = Flask(__name__)\nclient = get_deploy_client("http://localhost:5000")\n\n\n@app.route("/chat", methods=["POST"])\ndef chat():\n    try:\n        data = request.get_json()\n        response = client.predict(\n            endpoint="chat",\n            inputs={"messages": [{"role": "user", "content": data["message"]}]},\n        )\n        return jsonify({"response": response["choices"][0]["message"]["content"]})\n    except Exception as e:\n        return jsonify({"error": str(e)}), 500\n\n\nif __name__ == "__main__":\n    app.run(debug=True)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"asyncawait-support",children:"Async/Await Support"}),"\n",(0,a.jsx)(n.p,{children:"Handle multiple concurrent requests efficiently using asyncio for high-throughput applications:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport aiohttp\nimport json\n\n\nasync def async_query_gateway(endpoint, data):\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            f"http://localhost:5000/gateway/{endpoint}/invocations",\n            headers={"Content-Type": "application/json"},\n            data=json.dumps(data),\n        ) as response:\n            return await response.json()\n\n\nasync def main():\n    # Concurrent requests\n    tasks = [\n        async_query_gateway(\n            "chat", {"messages": [{"role": "user", "content": f"Question {i}"}]}\n        )\n        for i in range(5)\n    ]\n\n    responses = await asyncio.gather(*tasks)\n    for i, response in enumerate(responses):\n        print(f"Response {i}: {response[\'choices\'][0][\'message\'][\'content\']}")\n\n\n# Run async example\nasyncio.run(main())\n'})}),"\n",(0,a.jsx)(n.h2,{id:"langchain-integration",children:"LangChain Integration"}),"\n",(0,a.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,a.jsx)(n.p,{children:"LangChain provides pre-built components that work directly with the AI Gateway, enabling easy integration with LangChain's ecosystem of tools and chains:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain_community.llms import MLflowAIGateway\nfrom langchain_community.embeddings import MlflowAIGatewayEmbeddings\nfrom langchain_community.chat_models import ChatMLflowAIGateway\n\n# Configure LangChain to use your gateway\ngateway_uri = "http://localhost:5000"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"chat-models",children:"Chat Models"}),"\n",(0,a.jsx)(n.p,{children:"Create LangChain chat models that route through your gateway, allowing you to switch providers without changing your application code:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Chat model\nchat = ChatMLflowAIGateway(\n    gateway_uri=gateway_uri,\n    route="chat",\n    params={\n        "temperature": 0.7,\n        "top_p": 0.95,\n    },\n)\n\n# Generate response\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content="You are a helpful assistant."),\n    HumanMessage(content="What is LangChain?"),\n]\n\nresponse = chat(messages)\nprint(response.content)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"embeddings",children:"Embeddings"}),"\n",(0,a.jsx)(n.p,{children:"Use gateway-powered embeddings for vector search, semantic similarity, and RAG applications:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Embeddings\nembeddings = MlflowAIGatewayEmbeddings(gateway_uri=gateway_uri, route="embeddings")\n\n# Generate embeddings\ntext_embeddings = embeddings.embed_documents(\n    ["This is a document", "This is another document"]\n)\n\nquery_embedding = embeddings.embed_query("This is a query")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"complete-rag-example",children:"Complete RAG Example"}),"\n",(0,a.jsx)(n.p,{children:"Build a complete Retrieval-Augmented Generation (RAG) system using the gateway for both embeddings and chat completion:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import RetrievalQA\n\n# Load documents\nloader = TextLoader("path/to/document.txt")\ndocuments = loader.load()\n\n# Split documents\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\n# Create vector store\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Create QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=chat, chain_type="stuff", retriever=vectorstore.as_retriever()\n)\n\n# Query the system\nquestion = "What is the main topic of the document?"\nresult = qa_chain.run(question)\nprint(result)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"openai-compatibility",children:"OpenAI Compatibility"}),"\n",(0,a.jsx)(n.p,{children:"The AI Gateway provides OpenAI-compatible endpoints, allowing you to migrate existing OpenAI applications with minimal code changes:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\n\n# Configure OpenAI client to use the gateway\nopenai.api_base = "http://localhost:5000/gateway/chat"\nopenai.api_key = "not-needed"  # Gateway handles authentication\n\n# Use standard OpenAI client\nresponse = openai.ChatCompletion.create(\n    model="gpt-3.5-turbo",  # Endpoint name in your gateway config\n    messages=[{"role": "user", "content": "Hello, AI Gateway!"}],\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"mlflow-models-integration",children:"MLflow Models Integration"}),"\n",(0,a.jsx)(n.p,{children:"Deploy your own custom models alongside external providers for a unified interface to both proprietary and third-party models."}),"\n",(0,a.jsx)(n.h3,{id:"registering-models",children:"Registering Models"}),"\n",(0,a.jsx)(n.p,{children:"Train and register your models using MLflow's standard workflow, then expose them through the gateway:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.pyfunc\n\n# Log and register a model\nwith mlflow.start_run():\n    # Your model training code here\n    mlflow.pyfunc.log_model(\n        name="my_model",\n        python_model=MyCustomModel(),\n        registered_model_name="custom-chat-model",\n    )\n\n# Deploy the model\n# Then configure it in your gateway config.yaml:\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"endpoints:\n  - name: custom-model\n    endpoint_type: llm/v1/chat\n    model:\n      provider: mlflow-model-serving\n      name: custom-chat-model\n      config:\n        model_server_url: http://localhost:5001\n"})}),"\n",(0,a.jsx)(n.h2,{id:"production-best-practices",children:"Production Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Connection Pooling"}),": Use persistent HTTP connections for high-throughput applications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch Requests"}),": Group multiple requests when possible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Async Operations"}),": Use async/await for concurrent requests"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Caching"}),": Implement response caching for repeated queries"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import time\nfrom mlflow.deployments import get_deploy_client\nfrom mlflow.exceptions import MlflowException\n\n\ndef robust_query(client, endpoint, inputs, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.predict(endpoint=endpoint, inputs=inputs)\n        except MlflowException as e:\n            if attempt < max_retries - 1:\n                time.sleep(2**attempt)  # Exponential backoff\n                continue\n            raise e\n\n\n# Usage\nclient = get_deploy_client("http://localhost:5000")\nresponse = robust_query(\n    client, "chat", {"messages": [{"role": "user", "content": "Hello"}]}\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"security",children:"Security"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use HTTPS"})," in production"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement authentication"})," at the application level"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate inputs"})," before sending to the gateway"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitor usage"})," and implement rate limiting"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"monitoring-and-logging",children:"Monitoring and Logging"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import logging\nfrom mlflow.deployments import get_deploy_client\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef monitored_query(client, endpoint, inputs):\n    start_time = time.time()\n    try:\n        logger.info(f"Querying endpoint: {endpoint}")\n        response = client.predict(endpoint=endpoint, inputs=inputs)\n        duration = time.time() - start_time\n        logger.info(f"Query completed in {duration:.2f}s")\n        return response\n    except Exception as e:\n        duration = time.time() - start_time\n        logger.error(f"Query failed after {duration:.2f}s: {e}")\n        raise\n'})}),"\n",(0,a.jsx)(n.h3,{id:"load-balancing",children:"Load Balancing"}),"\n",(0,a.jsx)(n.p,{children:"For high-availability setups, consider running multiple gateway instances:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import random\nfrom mlflow.deployments import get_deploy_client\n\n# Multiple gateway instances\ngateway_urls = ["http://gateway1:5000", "http://gateway2:5000", "http://gateway3:5000"]\n\n\ndef get_client():\n    url = random.choice(gateway_urls)\n    return get_deploy_client(url)\n\n\n# Use with automatic failover\ndef resilient_query(endpoint, inputs, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            client = get_client()\n            return client.predict(endpoint=endpoint, inputs=inputs)\n        except Exception as e:\n            if attempt < max_retries - 1:\n                continue\n            raise e\n'})}),"\n",(0,a.jsx)(n.h2,{id:"health-and-monitoring",children:"Health and Monitoring"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Check gateway health via HTTP\nimport requests\n\n\ndef check_gateway_health(gateway_url):\n    try:\n        response = requests.get(f"{gateway_url}/health")\n        return {\n            "status": response.status_code,\n            "healthy": response.status_code == 200,\n            "response": response.json() if response.status_code == 200 else None,\n        }\n    except requests.RequestException as e:\n        return {"status": "error", "healthy": False, "error": str(e)}\n\n\n# Example usage\nhealth = check_gateway_health("http://localhost:5000")\nprint(f"Gateway Health: {health}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(r.A,{icon:l.A,title:"Configuration Guide",description:"Learn how to configure providers and advanced settings",href:"/genai/governance/ai-gateway/configuration",linkText:"Configure providers \u2192"}),(0,a.jsx)(r.A,{icon:c.A,title:"Usage Guide",description:"Master basic querying and client usage patterns",href:"/genai/governance/ai-gateway/usage",linkText:"Learn usage \u2192"}),(0,a.jsx)(r.A,{icon:d.A,title:"Tutorial",description:"Complete step-by-step walkthrough from setup to deployment",href:"/genai/governance/ai-gateway/guides",linkText:"Follow tutorial \u2192"})]})]})}function y(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},65592:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var i=t(34164);const a={tilesGrid:"tilesGrid_hB9N"};var o=t(74848);function s({children:e,className:n}){return(0,o.jsx)("div",{className:(0,i.A)(a.tilesGrid,n),children:e})}},80964:(e,n,t)=>{t.d(n,{A:()=>i});const i=(0,t(84722).A)("settings",[["path",{d:"M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 1 1.72v.51a2 2 0 0 1-1 1.74l-.15.09a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l.22-.39a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1-1-1.74v-.5a2 2 0 0 1 1-1.74l.15-.09a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z",key:"1qme2f"}],["circle",{cx:"12",cy:"12",r:"3",key:"1v7zrd"}]])},84722:(e,n,t)=>{t.d(n,{A:()=>c});var i=t(96540);const a=e=>{const n=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,((e,n,t)=>t?t.toUpperCase():n.toLowerCase())))(e);return n.charAt(0).toUpperCase()+n.slice(1)},o=(...e)=>e.filter(((e,n,t)=>Boolean(e)&&""!==e.trim()&&t.indexOf(e)===n)).join(" ").trim(),s=e=>{for(const n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0};var r={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const l=(0,i.forwardRef)((({color:e="currentColor",size:n=24,strokeWidth:t=2,absoluteStrokeWidth:a,className:l="",children:c,iconNode:d,...p},h)=>(0,i.createElement)("svg",{ref:h,...r,width:n,height:n,stroke:e,strokeWidth:a?24*Number(t)/Number(n):t,className:o("lucide",l),...!c&&!s(p)&&{"aria-hidden":"true"},...p},[...d.map((([e,n])=>(0,i.createElement)(e,n))),...Array.isArray(c)?c:[c]]))),c=(e,n)=>{const t=(0,i.forwardRef)((({className:t,...s},r)=>{return(0,i.createElement)(l,{ref:r,iconNode:n,className:o(`lucide-${c=a(e),c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,t),...s});var c}));return t.displayName=a(e),t}},85731:(e,n,t)=>{t.d(n,{A:()=>i});const i=(0,t(84722).A)("play",[["polygon",{points:"6 3 20 12 6 21 6 3",key:"1oa8hb"}]])}}]);