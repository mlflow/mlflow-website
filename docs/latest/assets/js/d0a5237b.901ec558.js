"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8739],{3203:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/model_evaluation_feature_importance-e3b91eb353333c150ec7b4e91f67f5f4.png"},6661:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>p,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"evaluation/shap","title":"SHAP Integration","description":"MLflow\'s built-in SHAP integration provides automatic model explanations and feature importance analysis during evaluation. SHAP (SHapley Additive exPlanations) values help you understand what drives your model\'s predictions, making your ML models more interpretable and trustworthy.","source":"@site/docs/classic-ml/evaluation/shap.mdx","sourceDirName":"evaluation","slug":"/evaluation/shap","permalink":"/docs/latest/ml/evaluation/shap","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"Custom Metrics & Visualizations","permalink":"/docs/latest/ml/evaluation/metrics-visualizations"},"next":{"title":"Plugin Evaluators","permalink":"/docs/latest/ml/evaluation/plugin-evaluators"}}');var r=a(74848),l=a(28453),i=(a(49374),a(11470)),s=a(19365);const o={},p="SHAP Integration",d={},m=[{value:"Quick Start: Automatic SHAP Explanations",id:"quick-start-automatic-shap-explanations",level:2},{value:"Understanding SHAP Outputs",id:"understanding-shap-outputs",level:2},{value:"Feature Importance Visualization",id:"feature-importance-visualization",level:3},{value:"Configuring SHAP Explanations",id:"configuring-shap-explanations",level:3},{value:"Explainer Types",id:"explainer-types",level:4},{value:"Output Control",id:"output-control",level:4},{value:"Working with SHAP Explainers",id:"working-with-shap-explainers",level:2},{value:"Interpreting SHAP Values",id:"interpreting-shap-values",level:3},{value:"Summary Plots",id:"summary-plots",level:4},{value:"Individual Explanations",id:"individual-explanations",level:4},{value:"Production SHAP Workflows",id:"production-shap-workflows",level:2},{value:"Best Practices and Use Cases",id:"best-practices-and-use-cases",level:2},{value:"When to Use SHAP Integration",id:"when-to-use-shap-integration",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Integration with MLflow Model Registry",id:"integration-with-mlflow-model-registry",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,l.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"shap-integration",children:"SHAP Integration"})}),"\n",(0,r.jsx)(n.p,{children:"MLflow's built-in SHAP integration provides automatic model explanations and feature importance analysis during evaluation. SHAP (SHapley Additive exPlanations) values help you understand what drives your model's predictions, making your ML models more interpretable and trustworthy."}),"\n",(0,r.jsx)(n.h2,{id:"quick-start-automatic-shap-explanations",children:"Quick Start: Automatic SHAP Explanations"}),"\n",(0,r.jsx)(n.p,{children:"Enable SHAP explanations during model evaluation with a simple configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport xgboost as xgb\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom mlflow.models import infer_signature\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Train model\nmodel = xgb.XGBClassifier().fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["label"] = y_test\n\nwith mlflow.start_run():\n    # Log model\n    signature = infer_signature(X_test, model.predict(X_test))\n    mlflow.sklearn.log_model(model, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n\n    # Evaluate with SHAP explanations enabled\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n        evaluators=["default"],\n        evaluator_config={"log_explainer": True},  # Enable SHAP logging\n    )\n\n    print("SHAP artifacts generated:")\n    for artifact_name in result.artifacts:\n        if "shap" in artifact_name.lower():\n            print(f"  - {artifact_name}")\n'})}),"\n",(0,r.jsx)(n.p,{children:"This automatically generates:"}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Feature importance plots"})," showing which features matter most"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"SHAP summary plots"})," displaying feature impact distributions"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"SHAP explainer model"})," saved for future use on new data"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Individual prediction explanations"})," for sample predictions"]})]}),"\n",(0,r.jsx)(n.h2,{id:"understanding-shap-outputs",children:"Understanding SHAP Outputs"}),"\n",(0,r.jsx)(n.h3,{id:"feature-importance-visualization",children:"Feature Importance Visualization"}),"\n",(0,r.jsx)(n.p,{children:"MLflow automatically creates SHAP-based feature importance charts:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# The evaluation generates several SHAP visualizations:\n# - shap_feature_importance_plot.png: Bar chart of average feature importance\n# - shap_summary_plot.png: Dot plot showing feature impact distribution\n# - explainer model: Saved SHAP explainer for generating new explanations\n\n# Access the results\nprint(f"Model accuracy: {result.metrics[\'accuracy_score\']:.3f}")\nprint("Generated SHAP artifacts:")\nfor name, path in result.artifacts.items():\n    if "shap" in name:\n        print(f"  {name}: {path}")\n'})}),"\n",(0,r.jsx)("figure",{children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{src:a(3203).A+"",width:"1835",height:"735"}),"\n",(0,r.jsx)("figcaption",{children:"Shap feature importances logged to MLflow when Shap evaluation is enabled"})]})}),"\n",(0,r.jsx)(n.h3,{id:"configuring-shap-explanations",children:"Configuring SHAP Explanations"}),"\n",(0,r.jsx)(n.p,{children:"Control how SHAP explanations are generated:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Advanced SHAP configuration\nshap_config = {\n    "log_explainer": True,  # Save the explainer model\n    "explainer_type": "exact",  # Use exact SHAP values (slower but precise)\n    "max_error_examples": 100,  # Number of error cases to explain\n    "log_model_explanations": True,  # Log individual prediction explanations\n}\n\nresult = mlflow.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    evaluators=["default"],\n    evaluator_config=shap_config,\n)\n'})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Configuration Options"}),(0,r.jsx)(n.h4,{id:"explainer-types",children:"Explainer Types"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:'"exact"'}),": Precise SHAP values using the exact algorithm (slower)"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:'"permutation"'}),": Permutation-based explanations (faster, approximate)"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:'"partition"'}),": Tree-based explanations for tree models"]})]}),(0,r.jsx)(n.h4,{id:"output-control",children:"Output Control"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:"log_explainer"}),": Whether to save the SHAP explainer as a model"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:"max_error_examples"}),": Number of misclassified examples to explain in detail"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("code",{children:"log_model_explanations"}),": Whether to log explanations for individual predictions"]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"working-with-shap-explainers",children:"Working with SHAP Explainers"}),"\n",(0,r.jsxs)(i.A,{children:[(0,r.jsxs)(s.A,{value:"loading-explainers",label:"Loading & Using Explainers",default:!0,children:[(0,r.jsx)(n.p,{children:"Once logged, you can load and use SHAP explainers on new data:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Load the saved SHAP explainer\nrun_id = "your_run_id_here"\nexplainer_uri = f"runs:/{run_id}/explainer"\n\n# Load explainer\nexplainer = mlflow.pyfunc.load_model(explainer_uri)\n\n# Generate explanations for new data\nnew_data = X_test[:10]  # Example: first 10 samples\nexplanations = explainer.predict(new_data)\n\nprint(f"Generated explanations shape: {explanations.shape}")\nprint(f"Feature contributions for first prediction: {explanations[0]}")\n\n# The explanations array contains SHAP values for each feature and prediction\n'})}),(0,r.jsx)(n.h3,{id:"interpreting-shap-values",children:"Interpreting SHAP Values"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def interpret_shap_explanations(explanations, feature_names, sample_idx=0):\n    """Interpret SHAP explanations for a specific prediction."""\n\n    sample_explanations = explanations[sample_idx]\n\n    # Sort features by absolute importance\n    feature_importance = list(zip(feature_names, sample_explanations))\n    feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n\n    print(f"SHAP explanation for sample {sample_idx}:")\n    print("Top 5 most important features:")\n\n    for i, (feature, importance) in enumerate(feature_importance[:5]):\n        direction = "increases" if importance > 0 else "decreases"\n        print(f"  {i+1}. {feature}: {importance:.3f} ({direction} prediction)")\n\n    return feature_importance\n\n\n# Usage\nfeature_names = X_test.columns.tolist()\ntop_features = interpret_shap_explanations(explanations, feature_names, sample_idx=0)\n'})})]}),(0,r.jsxs)(s.A,{value:"model-comparison",label:"Model Comparison with SHAP",children:[(0,r.jsx)(n.p,{children:"Compare feature importance across different models:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\ndef compare_models_with_shap(models_dict, eval_data, targets):\n    """Compare multiple models using SHAP explanations."""\n\n    model_results = {}\n\n    with mlflow.start_run(run_name="Model_Comparison_with_SHAP"):\n        for model_name, model in models_dict.items():\n            with mlflow.start_run(run_name=f"Model_{model_name}", nested=True):\n                # Train model\n                model.fit(X_train, y_train)\n\n                # Log model\n                signature = infer_signature(X_train, model.predict(X_train))\n                mlflow.sklearn.log_model(model, name="model", signature=signature)\n                model_uri = mlflow.get_artifact_uri("model")\n\n                # Evaluate with SHAP\n                result = mlflow.evaluate(\n                    model_uri,\n                    eval_data,\n                    targets=targets,\n                    model_type="classifier",\n                    evaluator_config={"log_explainer": True},\n                )\n\n                model_results[model_name] = {\n                    "accuracy": result.metrics["accuracy_score"],\n                    "artifacts": result.artifacts,\n                }\n\n                # Tag for easy comparison\n                mlflow.set_tag("model_type", model_name)\n\n        # Log comparison summary\n        best_model = max(\n            model_results.keys(), key=lambda k: model_results[k]["accuracy"]\n        )\n        mlflow.log_params(\n            {"best_model": best_model, "models_compared": len(models_dict)}\n        )\n\n    return model_results\n\n\n# Compare models\nmodels = {\n    "random_forest": RandomForestClassifier(n_estimators=100, random_state=42),\n    "xgboost": xgb.XGBClassifier(random_state=42),\n    "logistic": LogisticRegression(random_state=42),\n}\n\ncomparison_results = compare_models_with_shap(models, eval_data, "label")\n\nprint("Model Comparison Results:")\nfor model_name, results in comparison_results.items():\n    print(f"  {model_name}: {results[\'accuracy\']:.3f} accuracy")\n'})})]}),(0,r.jsxs)(s.A,{value:"custom-analysis",label:"Custom SHAP Analysis",children:[(0,r.jsx)(n.p,{children:"Perform custom SHAP analysis beyond automatic generation:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def custom_shap_analysis(model, data, feature_names):\n    """Perform custom SHAP analysis with detailed insights."""\n\n    with mlflow.start_run(run_name="Custom_SHAP_Analysis"):\n        # Create SHAP explainer\n        explainer = shap.Explainer(model)\n        shap_values = explainer(data)\n\n        # Global feature importance\n        feature_importance = np.abs(shap_values.values).mean(axis=0)\n        importance_dict = dict(zip(feature_names, feature_importance))\n\n        # Log feature importance metrics\n        for feature, importance in importance_dict.items():\n            mlflow.log_metric(f"importance_{feature}", importance)\n\n        # Create custom visualizations\n        import matplotlib.pyplot as plt\n\n        # Summary plot\n        plt.figure(figsize=(10, 8))\n        shap.summary_plot(shap_values, data, feature_names=feature_names, show=False)\n        plt.tight_layout()\n        plt.savefig("custom_shap_summary.png", dpi=300, bbox_inches="tight")\n        mlflow.log_artifact("custom_shap_summary.png")\n        plt.close()\n\n        # Waterfall plot for first prediction\n        plt.figure(figsize=(10, 6))\n        shap.waterfall_plot(shap_values[0], show=False)\n        plt.tight_layout()\n        plt.savefig("shap_waterfall_first_prediction.png", dpi=300, bbox_inches="tight")\n        mlflow.log_artifact("shap_waterfall_first_prediction.png")\n        plt.close()\n\n        # Log analysis summary\n        mlflow.log_params(\n            {\n                "top_feature": max(\n                    importance_dict.keys(), key=lambda k: importance_dict[k]\n                ),\n                "total_features": len(feature_names),\n                "samples_analyzed": len(data),\n            }\n        )\n\n        return shap_values, importance_dict\n\n\n# Usage\n# shap_values, importance = custom_shap_analysis(model, X_test[:100], X_test.columns.tolist())\n'})}),(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"SHAP Visualization Types"}),(0,r.jsx)(n.h4,{id:"summary-plots",children:"Summary Plots"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Bar plots"}),": Average feature importance across all predictions"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Dot plots"}),": Feature importance distribution showing positive/negative impacts"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Violin plots"}),": Distribution of SHAP values for each feature"]})]}),(0,r.jsx)(n.h4,{id:"individual-explanations",children:"Individual Explanations"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Waterfall plots"}),": Step-by-step breakdown of a single prediction"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Force plots"}),": Visual representation of feature contributions"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Decision plots"}),": Path through feature space for predictions"]})]})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"production-shap-workflows",children:"Production SHAP Workflows"}),"\n",(0,r.jsxs)(i.A,{children:[(0,r.jsxs)(s.A,{value:"batch-processing",label:"Batch Explanation Generation",default:!0,children:[(0,r.jsx)(n.p,{children:"Generate explanations for large datasets efficiently:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def batch_shap_explanations(model_uri, data_path, batch_size=1000):\n    """Generate SHAP explanations for large datasets in batches."""\n\n    import pandas as pd\n\n    with mlflow.start_run(run_name="Batch_SHAP_Generation"):\n        # Load model and create explainer\n        model = mlflow.pyfunc.load_model(model_uri)\n\n        # Process data in batches\n        batch_results = []\n        total_samples = 0\n\n        for chunk_idx, data_chunk in enumerate(\n            pd.read_parquet(data_path, chunksize=batch_size)\n        ):\n            # Generate explanations for batch\n            explanations = generate_explanations(model, data_chunk)\n\n            # Store results\n            batch_results.append(\n                {\n                    "batch_idx": chunk_idx,\n                    "explanations": explanations,\n                    "sample_count": len(data_chunk),\n                }\n            )\n\n            total_samples += len(data_chunk)\n\n            # Log progress\n            if chunk_idx % 10 == 0:\n                print(f"Processed {total_samples} samples...")\n\n        # Log batch processing summary\n        mlflow.log_params(\n            {\n                "total_batches": len(batch_results),\n                "total_samples": total_samples,\n                "batch_size": batch_size,\n            }\n        )\n\n        return batch_results\n\n\ndef generate_explanations(model, data):\n    """Generate SHAP explanations (placeholder - implement based on your model type)."""\n    # This would contain your actual SHAP explanation logic\n    # returning mock data for example\n    return np.random.random((len(data), data.shape[1]))\n'})})]}),(0,r.jsxs)(s.A,{value:"drift-monitoring",label:"Feature Importance Monitoring",children:[(0,r.jsx)(n.p,{children:"Track how feature importance changes over time:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def monitor_feature_importance_drift(current_explainer_uri, historical_importance_path):\n    """Monitor changes in feature importance over time."""\n\n    with mlflow.start_run(run_name="Feature_Importance_Monitoring"):\n        # Load current explainer\n        current_explainer = mlflow.pyfunc.load_model(current_explainer_uri)\n\n        # Generate current explanations\n        current_explanations = current_explainer.predict(X_test[:1000])\n        current_importance = np.abs(current_explanations).mean(axis=0)\n\n        # Load historical importance (would come from previous runs)\n        # historical_importance = load_historical_importance(historical_importance_path)\n        # For demo, create mock historical data\n        historical_importance = np.random.random(len(current_importance))\n\n        # Calculate importance drift\n        importance_drift = np.abs(current_importance - historical_importance)\n        relative_drift = importance_drift / (historical_importance + 1e-8)\n\n        # Log drift metrics\n        mlflow.log_metrics(\n            {\n                "max_importance_drift": np.max(importance_drift),\n                "avg_importance_drift": np.mean(importance_drift),\n                "max_relative_drift": np.max(relative_drift),\n                "features_with_high_drift": np.sum(relative_drift > 0.2),\n            }\n        )\n\n        # Log per-feature drift\n        for i, drift in enumerate(importance_drift):\n            mlflow.log_metric(f"feature_{i}_drift", drift)\n\n        # Alert if significant drift detected\n        high_drift_detected = np.max(relative_drift) > 0.5\n        mlflow.log_param("high_drift_alert", high_drift_detected)\n\n        if high_drift_detected:\n            print("WARNING: Significant feature importance drift detected!")\n\n        return {\n            "current_importance": current_importance,\n            "importance_drift": importance_drift,\n            "high_drift_detected": high_drift_detected,\n        }\n\n\n# Usage\n# drift_results = monitor_feature_importance_drift(\n#     "runs:/your_run_id/explainer",\n#     "path/to/historical/importance.npy"\n# )\n'})})]}),(0,r.jsxs)(s.A,{value:"performance-optimization",label:"Performance Optimization",children:[(0,r.jsx)(n.p,{children:"Optimize SHAP performance for large-scale applications:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Optimized configuration for large datasets\ndef get_optimized_shap_config(dataset_size):\n    """Get optimized SHAP configuration based on dataset size."""\n\n    if dataset_size < 1000:\n        # Small datasets - use exact methods\n        return {\n            "log_explainer": True,\n            "explainer_type": "exact",\n            "max_error_examples": 100,\n            "log_model_explanations": True,\n        }\n    elif dataset_size < 50000:\n        # Medium datasets - standard configuration\n        return {\n            "log_explainer": True,\n            "explainer_type": "permutation",\n            "max_error_examples": 50,\n            "log_model_explanations": True,\n        }\n    else:\n        # Large datasets - optimized for speed\n        return {\n            "log_explainer": True,\n            "explainer_type": "permutation",\n            "max_error_examples": 25,\n            "log_model_explanations": False,\n        }\n\n\n# Memory-efficient SHAP evaluation\ndef memory_efficient_shap_evaluation(model_uri, eval_data, targets, sample_size=5000):\n    """Perform SHAP evaluation with memory optimization for large datasets."""\n\n    # Sample data if too large\n    if len(eval_data) > sample_size:\n        sampled_data = eval_data.sample(n=sample_size, random_state=42)\n        print(f"Sampled {sample_size} rows from {len(eval_data)} for SHAP analysis")\n    else:\n        sampled_data = eval_data\n\n    # Get optimized configuration\n    config = get_optimized_shap_config(len(sampled_data))\n\n    with mlflow.start_run(run_name="Memory_Efficient_SHAP"):\n        result = mlflow.evaluate(\n            model_uri,\n            sampled_data,\n            targets=targets,\n            model_type="classifier",\n            evaluator_config=config,\n        )\n\n        # Log sampling information\n        mlflow.log_params(\n            {\n                "original_dataset_size": len(eval_data),\n                "sampled_dataset_size": len(sampled_data),\n                "sampling_ratio": len(sampled_data) / len(eval_data),\n            }\n        )\n\n        return result\n\n\n# Usage\n# result = memory_efficient_shap_evaluation(model_uri, large_eval_data, "target")\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Performance Guidelines:"})}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Small datasets (< 1,000 samples)"}),": Use exact SHAP methods for precision"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Medium datasets (1,000 - 50,000 samples)"}),": Standard SHAP analysis works well"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Large datasets (50,000+ samples)"}),": Consider sampling or approximate methods"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Very large datasets (100,000+ samples)"}),": Use batch processing with sampling"]})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices-and-use-cases",children:"Best Practices and Use Cases"}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-shap-integration",children:"When to Use SHAP Integration"}),"\n",(0,r.jsx)(n.p,{children:"SHAP integration provides the most value in these scenarios:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"High Interpretability Requirements"})," - Healthcare and medical diagnosis systems, financial services (credit scoring, loan approval), legal and compliance applications, hiring and HR decision systems, and fraud detection and risk assessment."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Complex Model Types"})," - XGBoost, Random Forest, and other ensemble methods, neural networks and deep learning models, custom ensemble approaches, and any model where feature relationships are non-obvious."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Regulatory and Compliance Needs"})," - Models requiring explainability for regulatory approval, systems where decisions must be justified to stakeholders, applications where bias detection is important, and audit trails requiring detailed decision explanations."]}),"\n",(0,r.jsx)(n.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dataset Size Guidelines:"})}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Small datasets (< 1,000 samples)"}),": Use exact SHAP methods for precision"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Medium datasets (1,000 - 50,000 samples)"}),": Standard SHAP analysis works well"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Large datasets (50,000+ samples)"}),": Consider sampling or approximate methods"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Very large datasets (100,000+ samples)"}),": Use batch processing with sampling"]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Memory Management:"})}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"Process explanations in batches for large datasets"}),(0,r.jsx)("li",{children:"Use approximate SHAP methods when exact precision isn't required"}),(0,r.jsx)("li",{children:"Clear intermediate results to manage memory usage"}),(0,r.jsx)("li",{children:"Consider model-specific optimizations (e.g., TreeExplainer for tree models)"})]}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-mlflow-model-registry",children:"Integration with MLflow Model Registry"}),"\n",(0,r.jsx)(n.p,{children:"SHAP explainers can be stored and versioned alongside your models:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def register_model_with_explainer(model_uri, explainer_uri, model_name):\n    """Register both model and explainer in MLflow Model Registry."""\n\n    from mlflow.tracking import MlflowClient\n\n    client = MlflowClient()\n\n    # Register the main model\n    model_version = mlflow.register_model(model_uri, model_name)\n\n    # Register the explainer as a separate model\n    explainer_name = f"{model_name}_explainer"\n    explainer_version = mlflow.register_model(explainer_uri, explainer_name)\n\n    # Add tags to link them\n    client.set_model_version_tag(\n        model_name, model_version.version, "explainer_model", explainer_name\n    )\n\n    client.set_model_version_tag(\n        explainer_name, explainer_version.version, "base_model", model_name\n    )\n\n    return model_version, explainer_version\n\n\n# Usage\n# model_ver, explainer_ver = register_model_with_explainer(\n#     model_uri, explainer_uri, "my_classifier"\n# )\n'})}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"MLflow's SHAP integration provides automatic model interpretability without additional setup complexity. By enabling SHAP explanations during evaluation, you gain valuable insights into feature importance and model behavior that are essential for building trustworthy ML systems."}),"\n",(0,r.jsx)(n.p,{children:"Key benefits include:"}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Automatic Generation"}),": SHAP explanations created during standard model evaluation"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Production Ready"}),": Saved explainers can generate explanations for new data"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Visual Insights"}),": Automatic generation of feature importance and summary plots"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Model Comparison"}),": Compare interpretability across different model types"]})]}),"\n",(0,r.jsx)(n.p,{children:'SHAP integration is particularly valuable for regulated industries, high-stakes decisions, and complex models where understanding "why" is as important as "what" the model predicts.'})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},11470:(e,n,a)=>{a.d(n,{A:()=>b});var t=a(96540),r=a(34164),l=a(23104),i=a(56347),s=a(205),o=a(57485),p=a(31682),d=a(70679);function m(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function c(e){const{values:n,children:a}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return m(e).map((({props:{value:e,label:n,attributes:a,default:t}})=>({value:e,label:n,attributes:a,default:t})))}(a);return function(e){const n=(0,p.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function u({value:e,tabValues:n}){return n.some((n=>n.value===e))}function h({queryString:e=!1,groupId:n}){const a=(0,i.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,o.aZ)(r),(0,t.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(a.location.search);n.set(r,e),a.replace({...a.location,search:n.toString()})}),[r,a])]}function f(e){const{defaultValue:n,queryString:a=!1,groupId:r}=e,l=c(e),[i,o]=(0,t.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:l}))),[p,m]=h({queryString:a,groupId:r}),[f,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[a,r]=(0,d.Dv)(n);return[a,(0,t.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:r}),g=(()=>{const e=p??f;return u({value:e,tabValues:l})?e:null})();(0,s.A)((()=>{g&&o(g)}),[g]);return{selectedValue:i,selectValue:(0,t.useCallback)((e=>{if(!u({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);o(e),m(e),_(e)}),[m,_,l]),tabValues:l}}var _=a(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=a(74848);function w({className:e,block:n,selectedValue:a,selectValue:t,tabValues:i}){const s=[],{blockElementScrollPositionUntilNextRender:o}=(0,l.a_)(),p=e=>{const n=e.currentTarget,r=s.indexOf(n),l=i[r].value;l!==a&&(o(n),t(l))},d=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const a=s.indexOf(e.currentTarget)+1;n=s[a]??s[0];break}case"ArrowLeft":{const a=s.indexOf(e.currentTarget)-1;n=s[a]??s[s.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:i.map((({value:e,label:n,attributes:t})=>(0,x.jsx)("li",{role:"tab",tabIndex:a===e?0:-1,"aria-selected":a===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:p,...t,className:(0,r.A)("tabs__item",g.tabItem,t?.className,{"tabs__item--active":a===e}),children:n??e},e)))})}function y({lazy:e,children:n,selectedValue:a}){const l=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=l.find((e=>e.props.value===a));return e?(0,t.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:l.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==a})))})}function v(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,r.A)("tabs-container",g.tabList),children:[(0,x.jsx)(w,{...n,...e}),(0,x.jsx)(y,{...n,...e})]})}function b(e){const n=(0,_.A)();return(0,x.jsx)(v,{...e,children:m(e.children)},String(n))}},19365:(e,n,a)=>{a.d(n,{A:()=>i});a(96540);var t=a(34164);const r={tabItem:"tabItem_Ymn6"};var l=a(74848);function i({children:e,hidden:n,className:a}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,t.A)(r.tabItem,a),hidden:n,children:e})}},28453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>s});var t=a(96540);const r={},l=t.createContext(r);function i(e){const n=t.useContext(l);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(l.Provider,{value:n},e.children)}},49374:(e,n,a)=>{a.d(n,{B:()=>o});a(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=a(86025),l=a(28774),i=a(74848);const s=e=>{const n=e.split(".");for(let a=n.length;a>0;a--){const e=n.slice(0,a).join(".");if(t[e])return e}return null};function o({fn:e,children:n}){const a=s(e);if(!a)return(0,i.jsx)(i.Fragment,{children:n});const o=(0,r.Ay)(`/${t[a]}#${e}`);return(0,i.jsx)(l.A,{to:o,target:"_blank",children:n??(0,i.jsxs)("code",{children:[e,"()"]})})}}}]);