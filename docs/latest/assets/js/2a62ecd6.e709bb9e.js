"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["3607"],{47419(e,t,n){n.r(t),n.d(t,{metadata:()=>r,default:()=>g,frontMatter:()=>s,contentTitle:()=>a,toc:()=>l,assets:()=>c});var r=JSON.parse('{"id":"tracing/integrations/listing/instructor","title":"Tracing Instructor","description":"Instructor Tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/instructor.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/instructor","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/instructor","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"sidebar_label":"Instructor"},"sidebar":"genAISidebar","previous":{"title":"Claude Code","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/claude_code"},"next":{"title":"MLflow AI Gateway","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/mlflow-ai-gateway"}}'),i=n(74848),o=n(28453);let s={sidebar_position:12,sidebar_label:"Instructor"},a="Tracing Instructor",c={},l=[{value:"Example Usage",id:"example-usage",level:3}];function u(e){let t={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"tracing-instructor",children:"Tracing Instructor"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Instructor Tracing via autolog",src:n(98305).A+"",width:"1364",height:"595"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://python.useinstructor.com/",children:"Instructor"})," is an open-source Python library built on top of Pydantic, simplifying structured LLM outputs with validation, retries, and streaming."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"/genai/tracing",children:"MLflow Tracing"})," works with Instructor by enabling auto-tracing for the underlying LLM libraries. For example, if you use Instructor for OpenAI LLMs, you can enable tracing with ",(0,i.jsx)(t.code,{children:"mlflow.openai.autolog()"})," and the generated traces will capture the structured outputs from Instructor."]}),"\n",(0,i.jsx)(t.p,{children:"Similarly, you can also trace Instructor with other LLM providers, such as Anthropic, Gemini, and LiteLLM, by enabling the corresponding autologging in MLflow."}),"\n",(0,i.jsx)(t.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,i.jsx)(t.p,{children:"The following example shows how to trace Instructor call that wraps an OpenAI API."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n# Use other autologging function e.g., mlflow.anthropic.autolog() if you are using Instructor with different LLM providers\nmlflow.openai.autolog()\n\n# Optional, create an experiment to store traces\nmlflow.set_experiment("Instructor")\n\n\n# Use Instructor as usual\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(OpenAI())\n\nres = client.chat.completions.create(\n    model="gpt-4o-mini",\n    response_model=ExtractUser,\n    messages=[{"role": "user", "content": "John Doe is 30 years old."}],\n)\nprint(f"Name: {res.name}, Age:{res.age}")\n'})})]})}function g(e={}){let{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},98305(e,t,n){n.d(t,{A:()=>r});let r=n.p+"assets/images/instructor-tracing-9d197095322fed09f9b5f0b28577542c.png"},28453(e,t,n){n.d(t,{R:()=>s,x:()=>a});var r=n(96540);let i={},o=r.createContext(i);function s(e){let t=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:t},e.children)}}}]);