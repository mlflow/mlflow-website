"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7651],{10493:(e,n,t)=>{t.d(n,{Zp:()=>s,AC:()=>r,WO:()=>c,_C:()=>p,$3:()=>d,jK:()=>m});var a=t(34164);const l={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var o=t(28774),i=t(74848);const r=({children:e,isSmall:n,cols:t})=>(0,i.jsx)("div",{className:(0,a.A)(l.CardGroup,n?l.AutofillColumns:t?l[`Cols${t}`]:l.MaxThreeColumns),children:e}),s=({children:e,link:n=""})=>n?(0,i.jsx)(o.A,{className:(0,a.A)(l.Link,l.Card,l.CardBordered),to:n,children:e}):(0,i.jsx)("div",{className:(0,a.A)(l.Card,l.CardBordered),children:e}),p=({headerText:e,link:n,text:t})=>(0,i.jsx)(s,{link:n,children:(0,i.jsxs)("span",{children:[(0,i.jsx)("div",{className:(0,a.A)(l.CardTitle,l.BoxRoot,l.PaddingBottom4),style:{pointerEvents:"none"},children:(0,i.jsx)("div",{className:(0,a.A)(l.BoxRoot,l.FlexFlex,l.FlexAlignItemsCenter,l.FlexDirectionRow,l.FlexJustifyContentFlexStart,l.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,i.jsx)("div",{className:(0,a.A)(l.BoxRoot,l.BoxHideIfEmpty,l.MarginTop4,l.MarginLeft4),style:{pointerEvents:"auto"},children:(0,i.jsx)("span",{className:"",children:e})})})}),(0,i.jsx)("span",{className:(0,a.A)(l.TextColor,l.CardBody),children:(0,i.jsx)("p",{children:t})})]})}),c=({description:e,children:n,link:t})=>(0,i.jsx)(s,{link:t,children:(0,i.jsxs)("div",{className:l.LogoCardContent,children:[(0,i.jsx)("div",{className:l.LogoCardImage,children:n}),(0,i.jsx)("p",{className:l.TextColor,children:e})]})}),d=({children:e,link:n})=>(0,i.jsx)(s,{link:n,children:(0,i.jsx)("div",{className:l.SmallLogoCardContent,children:(0,i.jsx)("div",{className:(0,a.A)("max-height-img-container",l.SmallLogoCardImage),children:e})})}),m=({title:e,description:n,link:t=""})=>(0,i.jsx)(s,{link:t,children:(0,i.jsxs)("div",{className:l.TitleCardContent,children:[(0,i.jsx)("div",{className:(0,a.A)(l.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,i.jsx)("hr",{className:(0,a.A)(l.TitleCardSeparator),style:{margin:"12px 0"}}),(0,i.jsx)("p",{className:(0,a.A)(l.TextColor),children:n})]})})},17974:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/pydanticai-mcp-tracing-539c1eadbc3bf818d47f3daec180391a.png"},49374:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const a=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=t(86025),o=t(28774),i=t(74848);const r=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(a[e])return e}return null};function s({fn:e,children:n}){const t=r(e);if(!t)return(0,i.jsx)(i.Fragment,{children:n});const s=(0,l.Ay)(`/${a[t]}#${e}`);return(0,i.jsx)(o.A,{to:s,target:"_blank",children:n??(0,i.jsxs)("code",{children:[e,"()"]})})}},53982:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"tracing/integrations/listing/pydantic_ai","title":"Tracing PydanticAI","description":"PydanticAI Tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/pydantic_ai.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/pydantic_ai","permalink":"/docs/latest/genai/tracing/integrations/listing/pydantic_ai","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"sidebar_label":"PydanticAI"},"sidebar":"genAISidebar","previous":{"title":"CrewAI","permalink":"/docs/latest/genai/tracing/integrations/listing/crewai"},"next":{"title":"Anthropic","permalink":"/docs/latest/genai/tracing/integrations/listing/anthropic"}}');var l=t(74848),o=t(28453),i=t(49374);t(10493),t(14252),t(11470),t(19365);const r={sidebar_position:7,sidebar_label:"PydanticAI"},s="Tracing PydanticAI",p={},c=[{value:"Example Usage",id:"example-usage",level:3},{value:"Advanced Example: Utilising MCP Server",id:"advanced-example-utilising-mcp-server",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"tracing-pydanticai",children:"Tracing PydanticAI"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"PydanticAI Tracing via autolog",src:t(71145).A+"",width:"2878",height:"1500"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"https://ai.pydantic.dev/",children:"\u200bPydanticAI"})," is a Python framework designed to simplify the development of production-grade generative AI applications. It brings type safety, ergonomic API design, and a developer-friendly experience to GenAI app development.\u200b"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"/genai/tracing",children:"MLflow Tracing"})," provides automatic tracing capability for ",(0,l.jsx)(n.a,{href:"https://ai.pydantic.dev/",children:"\u200bPydanticAI"}),", an open source framework for building multi-agent applications. By enabling auto tracing for \u200bPydanticAI by calling the ",(0,l.jsx)(i.B,{fn:"mlflow.pydantic_ai.autolog"})," function, , MLflow will capture nested traces for \u200bPydanticAI workflow execution and logged them to the active MLflow Experiment."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.pydantic_ai.autolog()\n"})}),"\n",(0,l.jsx)(n.p,{children:"MLflow trace automatically captures the following information about \u200bPydanticAI agents:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Agent calls with prompts, kwargs & output responses"}),"\n",(0,l.jsx)(n.li,{children:"LLM requests logging model name, prompt, parameters & response"}),"\n",(0,l.jsx)(n.li,{children:"Tool runs capturing tool name, arguments & usage metrics"}),"\n",(0,l.jsx)(n.li,{children:"MCP server calls & listings for tool-invocation tracing"}),"\n",(0,l.jsx)(n.li,{children:"Span metadata: latency, errors & run-ID linkage"}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"Currently, MLflow's PydanticAI integration supports tracing for both synchronous and asynchronous executions, but does not yet support streaming operations."})}),"\n",(0,l.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,l.jsx)(n.p,{children:"First, enable auto-tracing for PydanticAI, and optionally create an MLflow experiment to write traces to. This helps organizing your traces better."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Turn on auto tracing by calling mlflow.pydantic_ai.autolog()\nmlflow.pydantic_ai.autolog()\n\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("PydanticAI")\n'})}),"\n",(0,l.jsx)(n.p,{children:"Next, let\u2019s define a multi-agent workflow using PydanticAI. The example below sets up a weather agent where users can ask for the weather in multiple locations, and the agent will use the get_lat_lng tool to get the latitude and longitude of the locations, then use the get_weather tool to get the weather for those locations."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\n\nweather_agent = Agent(\n    # Switch to your favorite LLM\n    "google-gla:gemini-2.0-flash",\n    # \'Be concise, reply with one sentence.\' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    system_prompt=(\n        "Be concise, reply with one sentence."\n        "Use the `get_lat_lng` tool to get the latitude and longitude of the locations, "\n        "then use the `get_weather` tool to get the weather."\n    ),\n    deps_type=Deps,\n    retries=2,\n    instrument=True,\n)\n\n\n@weather_agent.tool\nasync def get_lat_lng(\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n    """Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    """\n    if ctx.deps.geo_api_key is None:\n        return {"lat": 51.1, "lng": -0.1}\n\n    params = {\n        "q": location_description,\n        "api_key": ctx.deps.geo_api_key,\n    }\n    r = await ctx.deps.client.get("https://geocode.maps.co/search", params=params)\n    r.raise_for_status()\n    data = r.json()\n\n    if data:\n        return {"lat": data[0]["lat"], "lng": data[0]["lon"]}\n    else:\n        raise ModelRetry("Could not find the location")\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    """Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    """\n\n    if ctx.deps.weather_api_key is None:\n        return {"temperature": "21 \xb0C", "description": "Sunny"}\n\n    params = {\n        "apikey": ctx.deps.weather_api_key,\n        "location": f"{lat},{lng}",\n        "units": "metric",\n    }\n    r = await ctx.deps.client.get(\n        "https://api.tomorrow.io/v4/weather/realtime", params=params\n    )\n    r.raise_for_status()\n    data = r.json()\n\n    values = data["data"]["values"]\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        1000: "Clear, Sunny",\n        1100: "Mostly Clear",\n        1101: "Partly Cloudy",\n        1102: "Mostly Cloudy",\n        1001: "Cloudy",\n        2000: "Fog",\n        2100: "Light Fog",\n        4000: "Drizzle",\n        4001: "Rain",\n        4200: "Light Rain",\n        4201: "Heavy Rain",\n        5000: "Snow",\n        5001: "Flurries",\n        5100: "Light Snow",\n        5101: "Heavy Snow",\n        6000: "Freezing Drizzle",\n        6001: "Freezing Rain",\n        6200: "Light Freezing Rain",\n        6201: "Heavy Freezing Rain",\n        7000: "Ice Pellets",\n        7101: "Heavy Ice Pellets",\n        7102: "Light Ice Pellets",\n        8000: "Thunderstorm",\n    }\n    return {\n        "temperature": f\'{values["temperatureApparent"]:0.0f}\xb0C\',\n        "description": code_lookup.get(values["weatherCode"], "Unknown"),\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        weather_api_key = os.getenv("WEATHER_API_KEY")\n        geo_api_key = os.getenv("GEO_API_KEY")\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            "What is the weather like in London and in Wiltshire?", deps=deps\n        )\n        print("Response:", result.output)\n\n\n# If you are running this on a notebook\nawait main()\n\n# Uncomment this is you are using an IDE or Python script.\n# asyncio.run(main())\n'})}),"\n",(0,l.jsx)(n.h2,{id:"advanced-example-utilising-mcp-server",children:"Advanced Example: Utilising MCP Server"}),"\n",(0,l.jsx)(n.p,{children:"MLflow Tracing automatically captures tool-related interactions from the MCP server in PydanticAI, including call_tool and list_tools operations. These actions are recorded as individual spans in the trace UI."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"PydanticAI MCP Server tracing via autolog",src:t(17974).A+"",width:"2880",height:"1462"})}),"\n",(0,l.jsx)(n.p,{children:"The example below demonstrates how to run an MCP server using PydanticAI with MLflow tracing enabled. All tool invocation and listing operations are automatically captured as trace spans in the UI, along with relevant metadata."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport asyncio\n\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("MCP Server")\nmlflow.pydantic_ai.autolog()\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    "deno",\n    args=[\n        "run",\n        "-N",\n        "-R=node_modules",\n        "-W=node_modules",\n        "--node-modules-dir=auto",\n        "jsr:@pydantic/mcp-run-python",\n        "stdio",\n    ],\n)\n\nagent = Agent("openai:gpt-4o", mcp_servers=[server], instrument=True)\n\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run("How many days between 2000-01-01 and 2025-03-18?")\n    print(result.output)\n    # > There are 9,208 days between January 1, 2000, and March 18, 2025.\n\n\n# If you are running this on a notebook\nawait main()\n\n# Uncomment this is you are using an IDE or Python script.\n# asyncio.run(main())\n'})}),"\n",(0,l.jsx)(n.h3,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,l.jsxs)(n.p,{children:["Auto tracing for PydanticAI can be disabled globally by calling ",(0,l.jsx)(n.code,{children:"mlflow.pydantic_ai.autolog(disable=True)"})," or ",(0,l.jsx)(n.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},71145:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/pydanticai-tracing-794fbcf11fdb1407e2be31fefaec5536.png"}}]);