"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["2316"],{18189(e,t,a){a.r(t),a.d(t,{metadata:()=>n,default:()=>w,frontMatter:()=>x,contentTitle:()=>g,toc:()=>y,assets:()=>v});var n=JSON.parse('{"id":"datasets/index","title":"Building MLflow evaluation datasets","description":"To systematically test and improve a GenAI application, you use an evaluation dataset. An evaluation dataset is a selected set of example inputs \u2014 either labeled (with known expected outputs, i.e. ground-truth expectations) or unlabeled (without ground-truth). Evaluation datasets help you improve your app\'s performance in the following ways:","source":"@site/docs/genai/datasets/index.mdx","sourceDirName":"datasets","slug":"/datasets/","permalink":"/mlflow-website/docs/latest/genai/datasets/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Versioning Scorers","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/versioning"},"next":{"title":"SDK Guide","permalink":"/mlflow-website/docs/latest/genai/datasets/sdk-guide"}}'),s=a(74848),r=a(28453),i=a(46077),o=a(78010),l=a(57250),d=a(95986),c=a(10440),u=a(77541);let h=(0,a(75689).A)("layers",[["path",{d:"M12.83 2.18a2 2 0 0 0-1.66 0L2.6 6.08a1 1 0 0 0 0 1.83l8.58 3.91a2 2 0 0 0 1.66 0l8.58-3.9a1 1 0 0 0 0-1.83z",key:"zw3jo"}],["path",{d:"M2 12a1 1 0 0 0 .58.91l8.6 3.91a2 2 0 0 0 1.65 0l8.58-3.9A1 1 0 0 0 22 12",key:"1wduqc"}],["path",{d:"M2 17a1 1 0 0 0 .58.91l8.6 3.91a2 2 0 0 0 1.65 0l8.58-3.9A1 1 0 0 0 22 17",key:"kqbvx6"}]]);var p=a(47792),m=a(96393),f=a(66497);let x={},g="Building MLflow evaluation datasets",v={},y=[{value:"Requirements",id:"requirements",level:2},{value:"Data sources for evaluation datasets",id:"data-sources-for-evaluation-datasets",level:2},{value:"Create or update a dataset using the UI",id:"create-or-update-a-dataset-using-the-ui",level:2},{value:"Create a dataset using the SDK and add records",id:"create-a-dataset-using-the-sdk-and-add-records",level:2},{value:"Select traces for evaluation datasets",id:"select-traces-for-evaluation-datasets",level:4},{value:"Preview the dataset",id:"preview-the-dataset",level:3},{value:"Understanding Source Types",id:"understanding-source-types",level:3},{value:"Update existing datasets",id:"update-existing-datasets",level:2},{value:"Next Steps",id:"next-steps",level:2}];function j(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"building-mlflow-evaluation-datasets",children:"Building MLflow evaluation datasets"})}),"\n",(0,s.jsx)(t.p,{children:"To systematically test and improve a GenAI application, you use an evaluation dataset. An evaluation dataset is a selected set of example inputs \u2014 either labeled (with known expected outputs, i.e. ground-truth expectations) or unlabeled (without ground-truth). Evaluation datasets help you improve your app's performance in the following ways:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Improve quality by testing fixes against known problematic examples from production."}),"\n",(0,s.jsx)(t.li,{children:'Prevent regressions. Create a "golden set" of examples that must always work correctly.'}),"\n",(0,s.jsx)(t.li,{children:"Compare app versions. Test different prompts, models, or app logic against the same data."}),"\n",(0,s.jsx)(t.li,{children:"Target specific features or isolate certain problems in your agent. Build specialized datasets for safety, domain knowledge, or edge cases."}),"\n",(0,s.jsx)(t.li,{children:"Validate the app across different environments (e.g., development vs. production) as part of LLMOps."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"You can think of them as test suites or benchmarks for your LLM functionality."}),"\n",(0,s.jsx)(t.h2,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsx)(t.admonition,{title:"SQL Backend Required",type:"warning",children:(0,s.jsxs)(t.p,{children:["Evaluation Datasets require an MLflow Tracking Server with a ",(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.a,{href:"/self-hosting/architecture/backend-store/#types-of-backend-stores",children:"SQL backend"})})," (PostgreSQL, MySQL, SQLite, or MSSQL).\nThis feature is ",(0,s.jsx)(t.strong,{children:"not available"})," in FileStore (local file system-based tracking). If you need\na simple local configuration for MLflow, use the sqlite option when starting MLflow."]})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["An evaluation dataset is attached to an MLflow experiment. If you do not already have an experiment, see ",(0,s.jsx)(t.a,{href:"/genai/tracing/quickstart/",children:"Create an MLflow Experiment"})," to create one."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"data-sources-for-evaluation-datasets",children:"Data sources for evaluation datasets"}),"\n",(0,s.jsx)(t.p,{children:"You can use any of the following to create an evaluation dataset:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Existing traces. If you have already captured traces from a GenAI application, you can use them to create an evaluation dataset based on real-world scenarios."}),"\n",(0,s.jsx)(t.li,{children:'Manually created examples. Define test cases by hand using dictionaries or DataFrames. This is useful for targeting specific edge cases or creating "golden" test cases that must always pass.'}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["This page describes how to create an MLflow evaluation dataset. You can create datasets from traces using either the MLflow Monitoring UI or the SDK. You can also use other types of datasets, such as Pandas DataFrames or a list of dictionaries. See ",(0,s.jsx)(t.a,{href:"/genai/eval-monitor/running-evaluation/eval-examples",children:"Evaluation examples"})," for more examples."]}),"\n",(0,s.jsx)(t.h2,{id:"create-or-update-a-dataset-using-the-ui",children:"Create or update a dataset using the UI"}),"\n",(0,s.jsx)(t.p,{children:"Follow these steps to use the UI to create a dataset or add to a dataset from existing traces."}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Click ",(0,s.jsx)(t.strong,{children:"Experiments"})," in the sidebar to display the Experiments page."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"In the table, click on the name of your experiment to open it."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["In the left sidebar, click ",(0,s.jsx)(t.strong,{children:"Traces"}),"."]}),"\n",(0,s.jsx)(i.A,{src:"/images/genai/traces-tab.png",alt:"Traces tab in sidebar"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Use the checkboxes to the left of the trace list to select traces to export to your dataset. To select all traces, click the box next to ",(0,s.jsx)(t.strong,{children:"Trace ID"}),"."]}),"\n",(0,s.jsx)("video",{src:(0,f.default)("/images/genai/select-traces.mp4"),controls:!0,loop:!0,autoPlay:!0,muted:!0,"aria-label":"Selecting traces for eval dataset"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Click ",(0,s.jsx)(t.strong,{children:"Actions"}),". From the drop-down menu, select ",(0,s.jsx)(t.strong,{children:"Add to evaluation dataset"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.strong,{children:"Export traces to evaluation dataset"})," dialog appears."]}),"\n",(0,s.jsx)(i.A,{src:"/images/genai/add-to-dataset-dialog.png",alt:"Add to dataset dialog",width:"50%"}),"\n",(0,s.jsx)(t.p,{children:"If evaluation datasets exist for this experiment, they appear in the dialog."}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["Click ",(0,s.jsx)(t.strong,{children:"Export"})," next to the dataset you want to add these traces to."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"If no evaluation dataset exists for the experiment:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["Click ",(0,s.jsx)(t.strong,{children:"Create new dataset"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["In the ",(0,s.jsx)(t.strong,{children:"Create Dataset"})," dialog, enter a name for the evaluation dataset and click ",(0,s.jsx)(t.strong,{children:"Create"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["Click ",(0,s.jsx)(t.strong,{children:"Export"})," next to the dataset you just created."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"create-a-dataset-using-the-sdk-and-add-records",children:"Create a dataset using the SDK and add records"}),"\n",(0,s.jsx)(t.p,{children:"This section describes several options for adding records to the evaluation dataset."}),"\n",(0,s.jsx)(d.A,{children:(0,s.jsxs)(o.A,{children:[(0,s.jsxs)(l.A,{value:"from-existing-traces",label:"From existing traces",default:!0,children:[(0,s.jsx)(t.p,{children:"One of the most effective ways to build a relevant evaluation dataset is by curating examples directly from your application's historical interactions captured by MLflow Tracing."}),(0,s.jsxs)(t.p,{children:["Programmatically search for traces and then add them to the dataset using ",(0,s.jsx)(t.code,{children:"search_traces()"}),". Use filters to identify traces by success, failure, use in production, or other properties. See ",(0,s.jsx)(t.a,{href:"/genai/tracing/search-traces#search-traces-overview",children:"Search traces"}),". You can also add ground-truth expectations to your traces before or after adding them to an evaluation dataset using ",(0,s.jsx)(t.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html?highlight=log_expectation#mlflow.log_expectation",children:(0,s.jsx)(t.code,{children:"log_expectation()"})}),"."]}),(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.datasets import create_dataset, set_dataset_tags\n\n# Create your evaluation dataset\ndataset = create_dataset(\n    name="production_validation_set",\n    experiment_id=["0"],  # "0" is the default experiment\n    tags={"team": "ml-platform", "stage": "validation"},\n)\n\n# Optionally, add additional tags to your dataset.\n# Tags can be used to search for datasets with search_datasets API\nset_dataset_tags(\n    dataset_id=dataset.dataset_id,\n    tags={"environment": "dev", "validation_version": "1.3"},\n)\n\n# 2. Search for traces\ntraces = mlflow.search_traces(\n    filter_string="attributes.name = \'chat_completion\' AND tags.environment = \'production\'",\n    order_by=["attributes.timestamp_ms DESC"],\n    max_results=10,\n)\n\nprint(f"Found {len(traces)} successful traces")\n\n# 3. Add expectations to the traces\nfor trace in traces:\n    mlflow.log_expectation(\n        trace_id=trace.info.trace_id,\n        name="expected_answer",\n        value=("Correct answer for this input"),\n    )\n\n# 4. Add the traces to the evaluation dataset\neval_dataset = eval_dataset.merge_records(traces)\nprint(f"Added {len(traces)} records to evaluation dataset")\n'})}),(0,s.jsx)(t.h4,{id:"select-traces-for-evaluation-datasets",children:"Select traces for evaluation datasets"}),(0,s.jsx)(t.p,{children:"Before adding traces to your dataset, identify which traces represent important test cases for your evaluation needs. You can use both quantitative and qualitative analysis to select representative traces."}),(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Quantitative trace selection"})}),(0,s.jsx)(t.p,{children:"Use the MLflow UI or SDK to filter and analyze traces based on measurable characteristics:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"In the MLflow UI"}),": Filter by tags (e.g., ",(0,s.jsx)(t.code,{children:"tag.quality_score < 0.7"}),"), search for specific inputs/outputs, sort by latency or token usage"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Programmatically"}),": Query traces to perform advanced analysis"]}),"\n"]}),(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\n\n# Search for traces with potential quality issues\ntraces_df = mlflow.search_traces(\n    filter_string="tag.quality_score < 0.7",\n    max_results=100,\n    extract_fields=[\n        "span.end_time",\n        "span.inputs.messages",\n        "span.outputs.choices",\n        "span.attributes.usage.total_tokens",\n    ],\n)\n\n# Analyze patterns\n# For example, check if quality issues correlate with token usage\ncorrelation = traces_df["span.attributes.usage.total_tokens"].corr(\n    traces_df["tag.quality_score"]\n)\nprint(f"Correlation between token usage and quality: {correlation}")\n'})}),(0,s.jsxs)(t.p,{children:["For complete trace query syntax and examples, see ",(0,s.jsx)(t.a,{href:"/genai/tracing/search-traces#search-query-syntax",children:"Search Query Syntax"}),"."]}),(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Qualitative trace selection"})}),(0,s.jsx)(t.p,{children:"Review individual traces to identify patterns requiring human judgment:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Examine inputs that led to low-quality outputs"}),"\n",(0,s.jsx)(t.li,{children:"Look for patterns in how your application handled edge cases"}),"\n",(0,s.jsx)(t.li,{children:"Identify missing context or faulty reasoning"}),"\n",(0,s.jsx)(t.li,{children:"Compare high-quality vs. low-quality traces to understand differentiating factors"}),"\n"]}),(0,s.jsx)(t.p,{children:"Once you've identified representative traces, add them to your dataset using the search and merge methods described above."}),(0,s.jsxs)(t.p,{children:["You can also use AI Insights features like ",(0,s.jsx)(t.a,{href:"/genai/eval-monitor/ai-insights/ai-issue-discovery/",children:"Analyze Experiment"})," to automatically discover quality and operational issues across your traces."]})]}),(0,s.jsxs)(l.A,{value:"from-dicts",label:"From Dictionaries",children:[(0,s.jsxs)(t.p,{children:["You can curate examples from scratch. Your data must match (or be transformed to match) the ",(0,s.jsx)(t.a,{href:"/genai/concepts/evaluation-datasets#record-structure",children:"evaluation dataset schema"}),"."]}),(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.datasets import create_dataset\n\n# Create dataset with manual test cases\ndataset = create_dataset(\n    name="regression_test_suite",\n    experiment_id=["0", "1"],  # Multiple experiments\n    tags={"type": "regression", "priority": "critical"},\n)\n\n# Define test cases with expected outputs (ground truth)\ntest_cases = [\n    {\n        "inputs": {\n            "question": "How do I reset my password?",\n            "context": "user_support",\n        },\n        "expectations": {\n            "expected_answer": (\n                "To reset your password, click \'Forgot Password\' on the login page, "\n                "enter your email, and follow the link sent to your inbox"\n            ),\n            "must_contain_steps": True,\n            "expected_tone": "helpful",\n        },\n    },\n    {\n        "inputs": {\n            "question": "What are your refund policies?",\n            "context": "customer_service",\n        },\n        "expectations": {\n            "expected_answer": (\n                "We offer full refunds within 30 days of purchase. "\n                "Refunds after 30 days are subject to approval."\n            ),\n            "must_include_timeframe": True,\n            "must_mention_exceptions": True,\n        },\n    },\n]\n\ndataset.merge_records(test_cases)\n'})})]}),(0,s.jsx)(l.A,{value:"from-pandas",label:"From DataFrame",children:(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import pandas as pd\nfrom mlflow.genai.datasets import create_dataset\n\n# Create dataset\ndataset = create_dataset(\n    name="benchmark_dataset",\n    experiment_id=["0"],\n    tags={"source": "benchmark", "version": "2024.1"},\n)\n\n# Create DataFrame with inputs and expectations (ground truth)\ndf = pd.DataFrame(\n    [\n        {\n            "inputs": {\n                "question": "What is MLflow?",\n                "domain": "general",\n            },\n            "expectations": {\n                "expected_answer": "MLflow is an open-source platform for ML",\n                "must_mention": ["tracking", "experiments", "models"],\n            },\n        },\n        {\n            "inputs": {\n                "question": "How do I track experiments?",\n                "domain": "technical",\n            },\n            "expectations": {\n                "expected_answer": "Use mlflow.start_run() and mlflow.log_params()",\n                "must_mention": ["log_params", "log_metrics"],\n            },\n        },\n        {\n            "inputs": {\n                "question": "Explain model versioning",\n                "domain": "technical",\n            },\n            "expectations": {\n                "expected_answer": "Model Registry provides versioning",\n                "must_mention": ["Model Registry", "versions"],\n            },\n        },\n    ]\n)\n\n# Add records from DataFrame\ndataset.merge_records(df)\n'})})})]})}),"\n",(0,s.jsx)(t.h3,{id:"preview-the-dataset",children:"Preview the dataset"}),"\n",(0,s.jsx)(t.p,{children:"Optionally, you can examine the dataset by converting it to a dataframe."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'df = eval_dataset.to_df()\nprint(f"\\nDataset preview:")\nprint(f"Total records: {len(df)}")\nprint("\\nSample record:")\nsample = df.iloc[0]\nprint(f"Inputs: {sample[\'inputs\']}")\n'})}),"\n",(0,s.jsx)(t.h3,{id:"understanding-source-types",children:"Understanding Source Types"}),"\n",(0,s.jsxs)(t.p,{children:["Every record in an evaluation dataset has a ",(0,s.jsx)(t.strong,{children:"source type"})," that tracks its provenance. This enables you to analyze model performance by data origin and understand which types of test data are most valuable."]}),"\n",(0,s.jsx)(t.h2,{id:"update-existing-datasets",children:"Update existing datasets"}),"\n",(0,s.jsxs)(t.p,{children:["You can use the UI or the SDK to update an evaluation dataset. For UI instructions, see ",(0,s.jsx)(t.a,{href:"#create-or-update-a-dataset-using-the-ui",children:"Create or update a dataset using the UI"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"To use the MLflow SDK to update and existing evaluation dataset:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow.genai.datasets\nimport pandas as pd\n\n# Load existing dataset\ndataset = mlflow.genai.datasets.get_dataset(name="eval_dataset")\n\n# Add new test cases\nnew_cases = [\n    {\n        "inputs": {"question": "What are MLflow models?"},\n        "expectations": {\n            "expected_facts": ["model packaging", "deployment", "registry"],\n            "min_response_length": 100,\n        },\n    }\n]\n\n# Merge new cases\ndataset = dataset.merge_records(new_cases)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(t.p,{children:"Ready to improve your GenAI testing? Start with these resources:"}),"\n",(0,s.jsxs)(c.A,{children:[(0,s.jsx)(u.A,{icon:h,iconSize:48,title:"Dataset Structure",description:"Understand how evaluation datasets organize test inputs, expectations, and metadata",href:"/genai/concepts/evaluation-datasets",linkText:"Learn the concepts \u2192",containerHeight:64}),(0,s.jsx)(u.A,{icon:p.A,iconSize:48,title:"Setting Expectations",description:"Learn how to define ground truth and expected outputs for your AI system",href:"/genai/assessments/expectations",linkText:"Define expectations \u2192",containerHeight:64}),(0,s.jsx)(u.A,{icon:m.A,iconSize:48,title:"Evaluation Framework",description:"Run systematic evaluations using your datasets with automated scorers",href:"/genai/eval-monitor",linkText:"Learn evaluation \u2192",containerHeight:64})]})]})}function w(e={}){let{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(j,{...e})}):j(e)}},75689(e,t,a){a.d(t,{A:()=>l});var n=a(96540);let s=e=>{let t=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,a)=>a?a.toUpperCase():t.toLowerCase());return t.charAt(0).toUpperCase()+t.slice(1)},r=(...e)=>e.filter((e,t,a)=>!!e&&""!==e.trim()&&a.indexOf(e)===t).join(" ").trim();var i={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let o=(0,n.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:a=2,absoluteStrokeWidth:s,className:o="",children:l,iconNode:d,...c},u)=>(0,n.createElement)("svg",{ref:u,...i,width:t,height:t,stroke:e,strokeWidth:s?24*Number(a)/Number(t):a,className:r("lucide",o),...!l&&!(e=>{for(let t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0})(c)&&{"aria-hidden":"true"},...c},[...d.map(([e,t])=>(0,n.createElement)(e,t)),...Array.isArray(l)?l:[l]])),l=(e,t)=>{let a=(0,n.forwardRef)(({className:a,...i},l)=>(0,n.createElement)(o,{ref:l,iconNode:t,className:r(`lucide-${s(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,a),...i}));return a.displayName=s(e),a}},96393(e,t,a){a.d(t,{A:()=>n});let n=(0,a(75689).A)("chart-bar",[["path",{d:"M3 3v16a2 2 0 0 0 2 2h16",key:"c24i48"}],["path",{d:"M7 16h8",key:"srdodz"}],["path",{d:"M7 11h12",key:"127s9w"}],["path",{d:"M7 6h3",key:"w9rmul"}]])},47792(e,t,a){a.d(t,{A:()=>n});let n=(0,a(75689).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},57250(e,t,a){a.d(t,{A:()=>r});var n=a(74848);a(96540);var s=a(34164);function r({children:e,hidden:t,className:a}){return(0,n.jsx)("div",{role:"tabpanel",className:(0,s.A)("tabItem_Ymn6",a),hidden:t,children:e})}},78010(e,t,a){a.d(t,{A:()=>y});var n=a(74848),s=a(96540),r=a(34164),i=a(88287),o=a(28584),l=a(56347),d=a(99989),c=a(96629),u=a(80618),h=a(41367);function p(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){let{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m({value:e,tabValues:t}){return t.some(t=>t.value===e)}var f=a(19863);function x({className:e,block:t,selectedValue:a,selectValue:s,tabValues:i}){let l=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.a_)(),c=e=>{let t=e.currentTarget,n=i[l.indexOf(t)].value;n!==a&&(d(t),s(n))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{let a=l.indexOf(e.currentTarget)+1;t=l[a]??l[0];break}case"ArrowLeft":{let a=l.indexOf(e.currentTarget)-1;t=l[a]??l[l.length-1]}}t?.focus()};return(0,n.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},e),children:i.map(({value:e,label:t,attributes:s})=>(0,n.jsx)("li",{role:"tab",tabIndex:a===e?0:-1,"aria-selected":a===e,ref:e=>{l.push(e)},onKeyDown:u,onClick:c,...s,className:(0,r.A)("tabs__item","tabItem_LNqP",s?.className,{"tabs__item--active":a===e}),children:t??e},e))})}function g({lazy:e,children:t,selectedValue:a}){let i=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){let e=i.find(e=>e.props.value===a);return e?(0,s.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,n.jsx)("div",{className:"margin-top--md",children:i.map((e,t)=>(0,s.cloneElement)(e,{key:t,hidden:e.props.value!==a}))})}function v(e){let t=function(e){let t,{defaultValue:a,queryString:n=!1,groupId:r}=e,i=function(e){let{values:t,children:a}=e;return(0,s.useMemo)(()=>{let e=t??p(a).map(({props:{value:e,label:t,attributes:a,default:n}})=>({value:e,label:t,attributes:a,default:n})),n=(0,u.XI)(e,(e,t)=>e.value===t.value);if(n.length>0)throw Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[t,a])}(e),[o,f]=(0,s.useState)(()=>(function({defaultValue:e,tabValues:t}){if(0===t.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:t}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let a=t.find(e=>e.default)??t[0];if(!a)throw Error("Unexpected error: 0 tabValues");return a.value})({defaultValue:a,tabValues:i})),[x,g]=function({queryString:e=!1,groupId:t}){let a=(0,l.W6)(),n=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c.aZ)(n),(0,s.useCallback)(e=>{if(!n)return;let t=new URLSearchParams(a.location.search);t.set(n,e),a.replace({...a.location,search:t.toString()})},[n,a])]}({queryString:n,groupId:r}),[v,y]=function({groupId:e}){let t=e?`docusaurus.tab.${e}`:null,[a,n]=(0,h.Dv)(t);return[a,(0,s.useCallback)(e=>{t&&n.set(e)},[t,n])]}({groupId:r}),j=m({value:t=x??v,tabValues:i})?t:null;return(0,d.A)(()=>{j&&f(j)},[j]),{selectedValue:o,selectValue:(0,s.useCallback)(e=>{if(!m({value:e,tabValues:i}))throw Error(`Can't select invalid tab value=${e}`);f(e),g(e),y(e)},[g,y,i]),tabValues:i}}(e);return(0,n.jsxs)("div",{className:(0,r.A)(i.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,n.jsx)(x,{...t,...e}),(0,n.jsx)(g,{...t,...e})]})}function y(e){let t=(0,f.A)();return(0,n.jsx)(v,{...e,children:p(e.children)},String(t))}},46077(e,t,a){a.d(t,{A:()=>r});var n=a(74848);a(96540);var s=a(66497);function r({src:e,alt:t,width:a,caption:r,className:i}){return(0,n.jsxs)("div",{className:`container_JwLF ${i||""}`,children:[(0,n.jsx)("div",{className:"imageWrapper_RfGN",style:a?{width:a}:{},children:(0,n.jsx)("img",{src:(0,s.default)(e),alt:t,className:"image_bwOA"})}),r&&(0,n.jsx)("p",{className:"caption_jo2G",children:r})]})}},95986(e,t,a){a.d(t,{A:()=>s});var n=a(74848);a(96540);function s({children:e}){return(0,n.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,t,a){a.d(t,{A:()=>d});var n=a(74848);a(96540);var s=a(95310),r=a(34164);let i="tileImage_O4So";var o=a(66497),l=a(92802);function d({icon:e,image:t,imageDark:a,imageWidth:d,imageHeight:c,iconSize:u=32,containerHeight:h,title:p,description:m,href:f,linkText:x="Learn more \u2192",className:g}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let v=h?{height:`${h}px`}:{},y={};return d&&(y.width=`${d}px`),c&&(y.height=`${c}px`),(0,n.jsxs)(s.A,{href:f,className:(0,r.A)("tileCard_NHsj",g),children:[(0,n.jsx)("div",{className:"tileIcon_pyoR",style:v,children:e?(0,n.jsx)(e,{size:u}):a?(0,n.jsx)(l.A,{sources:{light:(0,o.default)(t),dark:(0,o.default)(a)},alt:p,className:i,style:y}):(0,n.jsx)("img",{src:(0,o.default)(t),alt:p,className:i,style:y})}),(0,n.jsx)("h3",{children:p}),(0,n.jsx)("p",{children:m}),(0,n.jsx)("div",{className:"tileLink_iUbu",children:x})]})}},10440(e,t,a){a.d(t,{A:()=>r});var n=a(74848);a(96540);var s=a(34164);function r({children:e,className:t}){return(0,n.jsx)("div",{className:(0,s.A)("tilesGrid_hB9N",t),children:e})}},28453(e,t,a){a.d(t,{R:()=>i,x:()=>o});var n=a(96540);let s={},r=n.createContext(s);function i(e){let t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);