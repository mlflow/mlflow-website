"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[563],{10493:(e,n,t)=>{t.d(n,{Zp:()=>s,AC:()=>i,WO:()=>m,_C:()=>p,$3:()=>c,jK:()=>h});var l=t(34164);const a={CardGroup:"CardGroup_P84T",NoGap:"NoGap_O9Dj",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardRounded:"SmallLogoCardRounded_X50_",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var o=t(28774),r=t(74848);const i=({children:e,isSmall:n,cols:t,noGap:o})=>(0,r.jsx)("div",{className:(0,l.A)(a.CardGroup,n?a.AutofillColumns:t?a[`Cols${t}`]:a.MaxThreeColumns,o&&a.NoGap),children:e}),s=({children:e,link:n=""})=>n?(0,r.jsx)(o.A,{className:(0,l.A)(a.Link,a.Card,a.CardBordered),to:n,children:e}):(0,r.jsx)("div",{className:(0,l.A)(a.Card,a.CardBordered),children:e}),p=({headerText:e,link:n,text:t})=>(0,r.jsx)(s,{link:n,children:(0,r.jsxs)("span",{children:[(0,r.jsx)("div",{className:(0,l.A)(a.CardTitle,a.BoxRoot,a.PaddingBottom4),style:{pointerEvents:"none"},children:(0,r.jsx)("div",{className:(0,l.A)(a.BoxRoot,a.FlexFlex,a.FlexAlignItemsCenter,a.FlexDirectionRow,a.FlexJustifyContentFlexStart,a.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,r.jsx)("div",{className:(0,l.A)(a.BoxRoot,a.BoxHideIfEmpty,a.MarginTop4,a.MarginLeft4),style:{pointerEvents:"auto"},children:(0,r.jsx)("span",{className:"",children:e})})})}),(0,r.jsx)("span",{className:(0,l.A)(a.TextColor,a.CardBody),children:(0,r.jsx)("p",{children:t})})]})}),m=({description:e,children:n,link:t})=>(0,r.jsx)(s,{link:t,children:(0,r.jsxs)("div",{className:a.LogoCardContent,children:[(0,r.jsx)("div",{className:a.LogoCardImage,children:n}),(0,r.jsx)("p",{className:a.TextColor,children:e})]})}),c=({children:e,link:n})=>(0,r.jsx)("div",{className:(0,l.A)(a.Card,a.CardBordered,a.SmallLogoCardRounded),children:n?(0,r.jsx)(o.A,{className:(0,l.A)(a.Link),to:n,children:(0,r.jsx)("div",{className:a.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,l.A)("max-height-img-container",a.SmallLogoCardImage),children:e})})}):(0,r.jsx)("div",{className:a.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,l.A)("max-height-img-container",a.SmallLogoCardImage),children:e})})}),h=({title:e,description:n,link:t=""})=>(0,r.jsx)(s,{link:t,children:(0,r.jsxs)("div",{className:a.TitleCardContent,children:[(0,r.jsx)("div",{className:(0,l.A)(a.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,r.jsx)("hr",{className:(0,l.A)(a.TitleCardSeparator),style:{margin:"12px 0"}}),(0,r.jsx)("p",{className:(0,l.A)(a.TextColor),children:n})]})})},49374:(e,n,t)=>{t.d(n,{B:()=>i});t(96540);const l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var a=t(86025),o=t(74848);const r=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(l[e])return e}return null};function i({fn:e,children:n,hash:t}){const i=r(e);if(!i)return(0,o.jsx)(o.Fragment,{children:n});const s=(0,a.Ay)(`/${l[i]}#${t??e}`);return(0,o.jsx)("a",{href:s,target:"_blank",children:n??(0,o.jsxs)("code",{children:[e,"()"]})})}},59571:(e,n,t)=>{t.d(n,{A:()=>l});const l=t.p+"assets/images/ag2-trace-729f69365b00a8a7d17f6d0722658818.png"},62386:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>m});const l=JSON.parse('{"id":"tracing/integrations/listing/ag2","title":"Tracing AG2\ud83e\udd16","description":"AG2 Tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/ag2.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/ag2","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/ag2","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"sidebar_label":"AG2"},"sidebar":"genAISidebar","previous":{"title":"DSPy","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/dspy"},"next":{"title":"AutoGen","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/autogen"}}');var a=t(74848),o=t(28453),r=t(49374);t(10493),t(14252),t(11470),t(19365);const i={sidebar_position:6,sidebar_label:"AG2"},s="Tracing AG2\ud83e\udd16",p={},m=[{value:"Basic Example",id:"basic-example",level:3},{value:"Token usage",id:"token-usage",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"tracing-ag2",children:"Tracing AG2\ud83e\udd16"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"AG2 Tracing via autolog",src:t(59571).A+"",width:"1740",height:"1004"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://ag2.ai/",children:"AG2"})," is an open-source framework for building and orchestrating AI agent interactions."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"../../",children:"MLflow Tracing"})," provides automatic tracing capability for AG2, an open-source multi-agent framework. By enabling auto tracing\nfor AG2 by calling the ",(0,a.jsx)(r.B,{fn:"mlflow.ag2.autolog"})," function, MLflow will capture nested traces and logged them to the active MLflow Experiment upon agents execution.\nNote that since AG2 is built based on ",(0,a.jsx)(n.a,{href:"https://microsoft.github.io/autogen/0.2/",children:"AutoGen 0.2"}),", this integration can be used when you use AutoGen 0.2."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.ag2.autolog()\n"})}),"\n",(0,a.jsx)(n.p,{children:"MLflow captures the following information about the multi-agent execution:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Which agent is called at different turns"}),"\n",(0,a.jsx)(n.li,{children:"Messages passed between agents"}),"\n",(0,a.jsx)(n.li,{children:"LLM and tool calls made by each agent, organized per an agent and a turn"}),"\n",(0,a.jsx)(n.li,{children:"Latencies"}),"\n",(0,a.jsx)(n.li,{children:"Any exception if raised"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"basic-example",children:"Basic Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nfrom typing import Annotated, Literal\n\nfrom autogen import ConversableAgent\n\nimport mlflow\n\n# Turn on auto tracing for AG2\nmlflow.ag2.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("AG2")\n\n\n# Define a simple multi-agent workflow using AG2\nconfig_list = [\n    {\n        "model": "gpt-4o-mini",\n        # Please set your OpenAI API Key to the OPENAI_API_KEY env var before running this example\n        "api_key": os.environ.get("OPENAI_API_KEY"),\n    }\n]\n\nOperator = Literal["+", "-", "*", "/"]\n\n\ndef calculator(a: int, b: int, operator: Annotated[Operator, "operator"]) -> int:\n    if operator == "+":\n        return a + b\n    elif operator == "-":\n        return a - b\n    elif operator == "*":\n        return a * b\n    elif operator == "/":\n        return int(a / b)\n    else:\n        raise ValueError("Invalid operator")\n\n\n# First define the assistant agent that suggests tool calls.\nassistant = ConversableAgent(\n    name="Assistant",\n    system_message="You are a helpful AI assistant. "\n    "You can help with simple calculations. "\n    "Return \'TERMINATE\' when the task is done.",\n    llm_config={"config_list": config_list},\n)\n\n# The user proxy agent is used for interacting with the assistant agent\n# and executes tool calls.\nuser_proxy = ConversableAgent(\n    name="Tool Agent",\n    llm_config=False,\n    is_termination_msg=lambda msg: msg.get("content") is not None\n    and "TERMINATE" in msg["content"],\n    human_input_mode="NEVER",\n)\n\n# Register the tool signature with the assistant agent.\nassistant.register_for_llm(name="calculator", description="A simple calculator")(\n    calculator\n)\nuser_proxy.register_for_execution(name="calculator")(calculator)\nresponse = user_proxy.initiate_chat(\n    assistant, message="What is (44231 + 13312 / (230 - 20)) * 4?"\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"token-usage",children:"Token usage"}),"\n",(0,a.jsxs)(n.p,{children:["MLflow >= 3.2.0 supports token usage tracking for AG2. The token usage for each LLM call will be logged in the ",(0,a.jsx)(n.code,{children:"mlflow.chat.tokenUsage"})," attribute. The total token usage throughout the trace will be\navailable in the ",(0,a.jsx)(n.code,{children:"token_usage"})," field of the trace info object."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\nimport mlflow\n\nmlflow.ag2.autolog()\n\n# Register and run the tool signature with the assistant agent which is defined in above section.\nassistant.register_for_llm(name="calculator", description="A simple calculator")(\n    calculator\n)\nuser_proxy.register_for_execution(name="calculator")(calculator)\nresponse = user_proxy.initiate_chat(\n    assistant, message="What is (44231 + 13312 / (230 - 20)) * 4?"\n)\n\n# Get the trace object just created\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Detailed usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 1569\n  Output tokens: 229\n  Total tokens: 1798\n\n== Detailed usage for each LLM call: ==\nchat_completion_1:\n  Input tokens: 110\n  Output tokens: 61\n  Total tokens: 171\nchat_completion_2:\n  Input tokens: 191\n  Output tokens: 61\n  Total tokens: 252\nchat_completion_3:\n  Input tokens: 269\n  Output tokens: 24\n  Total tokens: 293\nchat_completion_4:\n  Input tokens: 302\n  Output tokens: 23\n  Total tokens: 325\nchat_completion_5:\n  Input tokens: 333\n  Output tokens: 22\n  Total tokens: 355\nchat_completion_6:\n  Input tokens: 364\n  Output tokens: 38\n  Total tokens: 402\n"})}),"\n",(0,a.jsx)(n.h3,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,a.jsxs)(n.p,{children:["Auto tracing for AG2 can be disabled globally by calling ",(0,a.jsx)(n.code,{children:"mlflow.ag2.autolog(disable=True)"})," or ",(0,a.jsx)(n.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);