"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["9000"],{78284(e,n,l){l.r(n),l.d(n,{metadata:()=>t,default:()=>d,frontMatter:()=>s,contentTitle:()=>c,toc:()=>m,assets:()=>p});var t=JSON.parse('{"id":"eval-monitor/faq","title":"Evaluate & Monitor FAQ","description":"This page addresses frequently asked questions about MLflow\'s GenAI evaluation.","source":"@site/docs/genai/eval-monitor/faq.mdx","sourceDirName":"eval-monitor","slug":"/eval-monitor/faq","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/faq","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Migrating from MLflow 2 LLM Evaluation","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/legacy-llm-evaluation"},"next":{"title":"Prompt Registry","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/"}}'),a=l(74848),r=l(28453),o=l(54725),i=l(46077);let s={},c="Evaluate & Monitor FAQ",p={},m=[{value:"Where can I find the evaluation results in MLflow UI?",id:"where-can-i-find-the-evaluation-results-in-mlflow-ui",level:2},{value:"How to change the concurrency of the evaluation?",id:"how-to-change-the-concurrency-of-the-evaluation",level:2},{value:"Can I use async functions as predict_fn?",id:"can-i-use-async-functions-as-predict_fn",level:2},{value:"Why does MLflow make N+1 predictions during evaluation?",id:"why-does-mlflow-make-n1-predictions-during-evaluation",level:2},{value:"How do I change the name of the evaluation run?",id:"how-do-i-change-the-name-of-the-evaluation-run",level:2},{value:"How do I use Databricks Model Serving endpoints as the predict function?",id:"how-do-i-use-databricks-model-serving-endpoints-as-the-predict-function",level:2},{value:"How to migrate from MLflow 2 LLM Evaluation?",id:"how-to-migrate-from-mlflow-2-llm-evaluation",level:2},{value:"How do I track the cost of LLM judges?",id:"how-do-i-track-the-cost-of-llm-judges",level:2},{value:"How do I pass additional inference parameters to judge LLMs?",id:"how-do-i-pass-additional-inference-parameters-to-judge-llms",level:2},{value:"How do I debug my scorers?",id:"how-do-i-debug-my-scorers",level:2},{value:"How do I programmatically access evaluation results?",id:"how-do-i-programmatically-access-evaluation-results",level:2},{value:"Accessing Aggregated Metrics",id:"accessing-aggregated-metrics",level:3},{value:"Accessing the Results DataFrame",id:"accessing-the-results-dataframe",level:3},{value:"Accessing Traces from Results",id:"accessing-traces-from-results",level:3}];function h(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"evaluate--monitor-faq",children:"Evaluate & Monitor FAQ"})}),"\n",(0,a.jsx)(n.p,{children:"This page addresses frequently asked questions about MLflow's GenAI evaluation."}),"\n",(0,a.jsx)(n.h2,{id:"where-can-i-find-the-evaluation-results-in-mlflow-ui",children:"Where can I find the evaluation results in MLflow UI?"}),"\n",(0,a.jsx)(n.p,{children:"After an evaluation completes, you can find the resulting runs on the experiment page. Click the run name to view aggregated metrics and metadata in the overview pane."}),"\n",(0,a.jsxs)(n.p,{children:["To inspect per-row evaluation results, open the ",(0,a.jsx)(n.strong,{children:"Traces"})," tab on the run overview page."]}),"\n",(0,a.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/quickstart-eval-result.png",alt:"Detailed Evaluation Results",width:"90%"}),"\n",(0,a.jsx)(n.h2,{id:"how-to-change-the-concurrency-of-the-evaluation",children:"How to change the concurrency of the evaluation?"}),"\n",(0,a.jsx)(n.p,{children:"MLflow uses thread pools to run the predict function and scorers in parallel. You can configure concurrency at two levels:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"1. Data-level concurrency:"})," Controls how many data items are evaluated in parallel."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Limit concurrent data items being evaluated (default: 10)\nexport MLFLOW_GENAI_EVAL_MAX_WORKERS=5\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"2. Scorer-level concurrency:"})," Controls how many scorers run in parallel for each data item. The actual number of scorer workers will not exceed the number of scorers being used."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Limit concurrent scorer execution (default: 10)\nexport MLFLOW_GENAI_EVAL_MAX_SCORER_WORKERS=2\n\n# For strict rate limiting, run scorers sequentially\nexport MLFLOW_GENAI_EVAL_MAX_SCORER_WORKERS=1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The total maximum concurrent API calls is approximately ",(0,a.jsx)(n.code,{children:"MLFLOW_GENAI_EVAL_MAX_WORKERS \xd7 min(MLFLOW_GENAI_EVAL_MAX_SCORER_WORKERS, num_scorers)"}),". Adjust these values based on your LLM provider's rate limits, especially when using free tiers."]}),"\n",(0,a.jsx)(n.h2,{id:"can-i-use-async-functions-as-predict_fn",children:"Can I use async functions as predict_fn?"}),"\n",(0,a.jsx)(n.p,{children:"Yes! MLflow automatically detects and wraps async functions. The async function will be executed with a timeout to prevent indefinite hangs."}),"\n",(0,a.jsxs)(n.p,{children:["Configure the timeout (in seconds) using the ",(0,a.jsx)(n.code,{children:"MLFLOW_GENAI_EVAL_ASYNC_TIMEOUT"})," environment variable (default: ",(0,a.jsx)(n.code,{children:"300"})," seconds):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"export MLFLOW_GENAI_EVAL_ASYNC_TIMEOUT=600  # 10 minutes\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"For Jupyter Notebooks:"})," Install ",(0,a.jsx)(n.code,{children:"nest_asyncio"})," to use async functions in notebook environments:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install nest_asyncio\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example with an async predict function:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def async_predict_fn(question: str) -> str:\n    """Async prediction function using OpenAI"""\n    response = await client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[{"role": "user", "content": question}],\n    )\n    return response.choices[0].message.content\n\n\nmlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=async_predict_fn,  # Async function automatically supported\n    scorers=[Correctness()],\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"why-does-mlflow-make-n1-predictions-during-evaluation",children:"Why does MLflow make N+1 predictions during evaluation?"}),"\n",(0,a.jsxs)(n.p,{children:["MLflow requires the predict function passed through the ",(0,a.jsx)(n.code,{children:"predict_fn"})," parameter to emit a single trace per call. To ensure the function produces a trace, MLflow first runs one additional prediction on a single input."]}),"\n",(0,a.jsxs)(n.p,{children:["If you are confident the predict function already generates traces, skip this validation by setting the ",(0,a.jsx)(n.code,{children:"MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION"})," environment variable to ",(0,a.jsx)(n.code,{children:"true"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"export MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION=true\n"})}),"\n",(0,a.jsx)(n.h2,{id:"how-do-i-change-the-name-of-the-evaluation-run",children:"How do I change the name of the evaluation run?"}),"\n",(0,a.jsxs)(n.p,{children:["By default, ",(0,a.jsx)(n.code,{children:"mlflow.genai.evaluate"})," generates a random run name. Set a custom name by wrapping the call with ",(0,a.jsx)(n.code,{children:"mlflow.start_run"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'with mlflow.start_run(run_name="My Evaluation Run") as run:\n    mlflow.genai.evaluate(...)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"how-do-i-use-databricks-model-serving-endpoints-as-the-predict-function",children:"How do I use Databricks Model Serving endpoints as the predict function?"}),"\n",(0,a.jsxs)(n.p,{children:["MLflow provides ",(0,a.jsx)(o.B,{fn:"mlflow.genai.to_predict_fn"}),", which wraps a Databricks Model Serving endpoint so it behaves like a predict function compatible with GenAI evaluation."]}),"\n",(0,a.jsx)(n.p,{children:"The wrapper:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Translates each input sample into the request payload expected by the endpoint."}),"\n",(0,a.jsxs)(n.li,{children:["Injects ",(0,a.jsx)(n.code,{children:'{"databricks_options": {"return_trace": True}}'})," so the endpoint returns a model-generated trace."]}),"\n",(0,a.jsx)(n.li,{children:"Copies the trace into the current experiment so it appears in the MLflow UI."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Correctness\n\nmlflow.genai.evaluate(\n    # The {"messages": ...} part must be compatible with the request schema of the endpoint\n    data=[{"inputs": {"messages": [{"role": "user", "content": "What is MLflow?"}]}}],\n    # Your Databricks Model Serving endpoint URI\n    predict_fn=mlflow.genai.to_predict_fn("endpoints:/chat"),\n    scorers=[Correctness()],\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"how-to-migrate-from-mlflow-2-llm-evaluation",children:"How to migrate from MLflow 2 LLM Evaluation?"}),"\n",(0,a.jsxs)(n.p,{children:["See the ",(0,a.jsx)(n.a,{href:"/genai/eval-monitor/legacy-llm-evaluation",children:"Migrating from MLflow 2 LLM Evaluation"})," guide."]}),"\n",(0,a.jsx)(n.h2,{id:"how-do-i-track-the-cost-of-llm-judges",children:"How do I track the cost of LLM judges?"}),"\n",(0,a.jsxs)(n.p,{children:["MLflow visualizes the cost of LLM judges in the assessment pane of the trace details page.\nWhen you open an assessment logged by an LLM judge, you can see the cost incurred for running the judge model.\nThis feature is available only when you have the ",(0,a.jsx)(n.a,{href:"https://github.com/BerriAI/litellm",children:"LiteLLM"})," library installed."]}),"\n",(0,a.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/tracking-judge-cost.png",alt:"LLM Judge Cost",width:"90%"}),"\n",(0,a.jsxs)(n.p,{children:["Managing the balance between cost and accuracy is important.\nTo use a more cost-effective LLM model while maintaining accuracy, you can leverage the ",(0,a.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/alignment",children:"LLM Judge Alignment"})," feature."]}),"\n",(0,a.jsx)(n.h2,{id:"how-do-i-pass-additional-inference-parameters-to-judge-llms",children:"How do I pass additional inference parameters to judge LLMs?"}),"\n",(0,a.jsxs)(n.p,{children:["LLM-as-a-Judge scorers like ",(0,a.jsx)(n.code,{children:"Correctness"})," and ",(0,a.jsx)(n.code,{children:"Guidelines"})," accept an ",(0,a.jsx)(n.code,{children:"inference_params"})," argument to customize the judge model's behavior. This allows you to control parameters such as ",(0,a.jsx)(n.code,{children:"temperature"}),", ",(0,a.jsx)(n.code,{children:"max_tokens"}),", and other model-specific settings."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import Correctness, Guidelines\n\n# Pass inference parameters to control judge behavior\ncorrectness_scorer = Correctness(\n    inference_params={\n        "temperature": 0.0,  # More deterministic responses\n        "max_tokens": 500,\n    }\n)\n\nguidelines_scorer = Guidelines(\n    name="tone_check",\n    guidelines="The response should be professional and helpful.",\n    inference_params={\n        "temperature": 0.0,\n    },\n)\n\nmlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=my_predict_fn,\n    scorers=[correctness_scorer, guidelines_scorer],\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"The available inference parameters depend on the model provider being used. Common parameters include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"temperature"}),": Controls randomness (0.0 = deterministic, higher = more random)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_tokens"}),": Maximum tokens in the response"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"top_p"}),": Nucleus sampling parameter"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"how-do-i-debug-my-scorers",children:"How do I debug my scorers?"}),"\n",(0,a.jsxs)(n.p,{children:["To debug your scorers, you can enable tracing for the scorer functions by setting the ",(0,a.jsx)(n.code,{children:"MLFLOW_GENAI_EVAL_ENABLE_SCORER_TRACING"})," environment variable to ",(0,a.jsx)(n.code,{children:"true"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"export MLFLOW_GENAI_EVAL_ENABLE_SCORER_TRACING=true\n"})}),"\n",(0,a.jsxs)(n.p,{children:["When this is set to ",(0,a.jsx)(n.code,{children:"true"}),", MLflow will trace scorer executions during the evaluation and allow\nyou to inspect the input, output, and internal steps during the scorer execution."]}),"\n",(0,a.jsx)(n.p,{children:'To view the scorer trace, you can open the assessment pane of the trace details page and click the\n"View trace" link.'}),"\n",(0,a.jsx)(n.h2,{id:"how-do-i-programmatically-access-evaluation-results",children:"How do I programmatically access evaluation results?"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(o.B,{fn:"mlflow.genai.evaluate"})," function returns an ",(0,a.jsx)(o.B,{fn:"mlflow.genai.evaluation.entities.EvaluationResult"})," object that contains all evaluation results. Here's how to access different parts of the results:"]}),"\n",(0,a.jsx)(n.h3,{id:"accessing-aggregated-metrics",children:"Accessing Aggregated Metrics"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"metrics"})," property returns a dictionary of aggregated metric values across all evaluated rows:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"result = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=my_predict_fn,\n    scorers=[Correctness(), Safety()],\n)\n\n# Access aggregated metrics\nprint(result.metrics)\n# Output: {'correctness/mean': 0.85, 'safety/mean': 0.95}\n\n# Access a specific metric\ncorrectness_score = result.metrics.get(\"correctness/mean\")\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To get metrics for an existing evaluation run, you can use ",(0,a.jsx)(o.B,{fn:"mlflow.get_run"})," API:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"run = mlflow.get_run(run_id=\"<run_id>\")\nprint(run.data.metrics)\n# Output: {'correctness/mean': 0.85, 'safety/mean': 0.95}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"accessing-the-results-dataframe",children:"Accessing the Results DataFrame"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"result_df"})," property returns a pandas DataFrame containing detailed per-row evaluation results:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Export to CSV for further analysis\nresult.result_df.to_csv("evaluation_results.csv", index=False)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"accessing-traces-from-results",children:"Accessing Traces from Results"}),"\n",(0,a.jsxs)(n.p,{children:["You can get the traces for the evaluation run by using ",(0,a.jsx)(o.B,{fn:"mlflow.search_traces"})," API:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'traces = mlflow.search_traces(run_id=result.run_id, return_type="list")\n'})})]})}function d(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},54725(e,n,l){l.d(n,{B:()=>o});var t=l(74848);l(96540);var a=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),r=l(66497);function o({fn:e,children:n,hash:l}){let o=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let l=n.slice(0,e).join(".");if(a[l])return l}return null})(e);if(!o)return(0,t.jsx)(t.Fragment,{children:n});let i=(0,r.default)(`/${a[o]}#${l??e}`);return(0,t.jsx)("a",{href:i,target:"_blank",children:n??(0,t.jsxs)("code",{children:[e,"()"]})})}},46077(e,n,l){l.d(n,{A:()=>r});var t=l(74848);l(96540);var a=l(66497);function r({src:e,alt:n,width:l,caption:r,className:o}){return(0,t.jsxs)("div",{className:`container_JwLF ${o||""}`,children:[(0,t.jsx)("div",{className:"imageWrapper_RfGN",style:l?{width:l}:{},children:(0,t.jsx)("img",{src:(0,a.default)(e),alt:n,className:"image_bwOA"})}),r&&(0,t.jsx)("p",{className:"caption_jo2G",children:r})]})}},28453(e,n,l){l.d(n,{R:()=>o,x:()=>i});var t=l(96540);let a={},r=t.createContext(a);function o(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);