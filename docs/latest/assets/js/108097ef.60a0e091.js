"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["530"],{71319(e,t,o){o.r(t),o.d(t,{metadata:()=>r,default:()=>c,frontMatter:()=>a,contentTitle:()=>m,toc:()=>f,assets:()=>s});var r=JSON.parse('{"id":"prompt-registry/optimize-prompts/langgraph-optimization","title":"Optimizing Prompts for LangGraph","description":"This guide demonstrates how to leverage  alongside LangGraph to enhance your agent\'s prompts automatically. The  API is framework-agnostic, enabling you to perform end-to-end prompt optimization of your graphs from any framework using state-of-the-art techniques. For more information about the API, please visit Optimize Prompts.","source":"@site/docs/genai/prompt-registry/optimize-prompts/langgraph-optimization.mdx","sourceDirName":"prompt-registry/optimize-prompts","slug":"/prompt-registry/optimize-prompts/langgraph-optimization","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/optimize-prompts/langgraph-optimization","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"sidebar_label":"LangGraph Optimization"},"sidebar":"genAISidebar","previous":{"title":"LangChain Optimization","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/optimize-prompts/langchain-optimization"},"next":{"title":"OpenAI Agent Optimization","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/optimize-prompts/openai-agent-optimization"}}'),n=o(74848),l=o(28453),p=o(54725),i=o(66497);let a={sidebar_position:8,sidebar_label:"LangGraph Optimization"},m="Optimizing Prompts for LangGraph",s={},f=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Basic Example",id:"basic-example",level:2}];function h(e){let t={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"optimizing-prompts-for-langgraph",children:"Optimizing Prompts for LangGraph"})}),"\n",(0,n.jsx)("p",{style:{display:"flex",justifyContent:"center",margin:"1em 0"},children:(0,n.jsx)("img",{src:(0,i.default)("/images/logos/langgraph-logo.png"),alt:"LangGraph Logo",style:{width:300,objectFit:"contain"}})}),"\n",(0,n.jsxs)(t.p,{children:["This guide demonstrates how to leverage ",(0,n.jsx)(p.B,{fn:"mlflow.genai.optimize_prompts"})," alongside ",(0,n.jsx)(t.a,{href:"https://langchain-ai.github.io/langgraph/",children:"LangGraph"})," to enhance your agent's prompts automatically. The ",(0,n.jsx)(p.B,{fn:"mlflow.genai.optimize_prompts"})," API is framework-agnostic, enabling you to perform end-to-end prompt optimization of your graphs from any framework using state-of-the-art techniques. For more information about the API, please visit ",(0,n.jsx)(t.a,{href:"/genai/prompt-registry/optimize-prompts",children:"Optimize Prompts"}),"."]}),"\n",(0,n.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"pip install -U langgraph langchain langchain-openai 'mlflow[genai]' gepa litellm\n"})}),"\n",(0,n.jsx)(t.p,{children:"Set your OpenAI API key:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'export OPENAI_API_KEY="your-api-key"\n'})}),"\n",(0,n.jsx)(t.p,{children:"Set tracking server and MLflow experiment:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'import mlflow\n\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("LangGraph Optimization")\n'})}),"\n",(0,n.jsx)(t.h2,{id:"basic-example",children:"Basic Example"}),"\n",(0,n.jsx)(t.p,{children:"Here's a complete example of optimizing a customer support agent built with LangGraph. This example demonstrates how to optimize system and user prompts in a stateful graph workflow, showing the minimal code changes needed to integrate prompt optimization into your LangGraph applications."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Correctness\nfrom mlflow.genai.optimize.optimizers import GepaPromptOptimizer\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\n# Step 1: Register your initial prompts\nsystem_prompt = mlflow.genai.register_prompt(\n    name="customer-support-system",\n    template="You are a helpful customer support agent for an e-commerce platform. "\n    "Assist customers with their questions about orders, returns, and products.",\n)\n\nuser_prompt = mlflow.genai.register_prompt(\n    name="customer-support-query",\n    template="Customer inquiry: {{query}}",\n)\n\n\n# Step 2: Define state schema for LangGraph\nclass AgentState(TypedDict):\n    messages: Annotated[list, operator.add]\n    query: str\n    llm_calls: int\n\n\n# Step 3: Create a prediction function that uses LangGraph\ndef predict_fn(query):\n    # Load prompts from registry\n    system_prompt = mlflow.genai.load_prompt("prompts:/customer-support-system@latest")\n    user_prompt = mlflow.genai.load_prompt("prompts:/customer-support-query@latest")\n\n    # Initialize model\n    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)\n\n    # Define the LLM node\n    def llm_node(state: AgentState):\n        formatted_user_msg = user_prompt.format(query=state["query"])\n        messages = [\n            SystemMessage(content=system_prompt.template),\n            HumanMessage(content=formatted_user_msg),\n        ]\n        response = model.invoke(messages)\n        return {\n            "messages": [response],\n            "llm_calls": state.get("llm_calls", 0) + 1,\n        }\n\n    # Build the graph\n    graph_builder = StateGraph(AgentState)\n    graph_builder.add_node("llm_call", llm_node)\n    graph_builder.add_edge(START, "llm_call")\n    graph_builder.add_edge("llm_call", END)\n\n    # Compile and run\n    agent = graph_builder.compile()\n    result = agent.invoke({"query": query, "messages": [], "llm_calls": 0})\n\n    return result["messages"][-1].content\n\n\n# Step 4: Prepare training data\ndataset = [\n    {\n        "inputs": {"query": "Where is my order #12345?"},\n        "expectations": {\n            "expected_response": "I\'d be happy to help you track your order #12345. "\n            "Please check your email for a tracking link, or I can look it up for you if you provide your email address."\n        },\n    },\n    {\n        "inputs": {"query": "How do I return a defective product?"},\n        "expectations": {\n            "expected_response": "I\'m sorry to hear your product is defective. You can initiate a return "\n            "through your account\'s order history within 30 days of purchase. We\'ll send you a prepaid shipping label."\n        },\n    },\n    {\n        "inputs": {"query": "Do you have this item in blue?"},\n        "expectations": {\n            "expected_response": "I\'d be happy to check product availability for you. "\n            "Could you please provide the product name or SKU so I can verify if it\'s available in blue?"\n        },\n    },\n    # more data...\n]\n\n# Step 5: Optimize the prompts\nresult = mlflow.genai.optimize_prompts(\n    predict_fn=predict_fn,\n    train_data=dataset,\n    prompt_uris=[system_prompt.uri, user_prompt.uri],\n    optimizer=GepaPromptOptimizer(reflection_model="openai:/gpt-4o"),\n    scorers=[Correctness(model="openai:/gpt-4o")],\n)\n\n# Step 6: Use the optimized prompts\noptimized_system_prompt = result.optimized_prompts[0]\noptimized_user_prompt = result.optimized_prompts[1]\n\nprint(f"Optimized system prompt URI: {optimized_system_prompt.uri}")\nprint(f"Optimized system template: {optimized_system_prompt.template}")\nprint(f"Optimized user prompt URI: {optimized_user_prompt.uri}")\nprint(f"Optimized user template: {optimized_user_prompt.template}")\n\n# Since your graph already uses @latest, it will automatically use the optimized prompts\npredict_fn("Can I get a refund for order #67890?")\n'})})]})}function c(e={}){let{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},54725(e,t,o){o.d(t,{B:()=>p});var r=o(74848);o(96540);var n=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),l=o(66497);function p({fn:e,children:t,hash:o}){let p=(e=>{let t=e.split(".");for(let e=t.length;e>0;e--){let o=t.slice(0,e).join(".");if(n[o])return o}return null})(e);if(!p)return(0,r.jsx)(r.Fragment,{children:t});let i=(0,l.default)(`/${n[p]}#${o??e}`);return(0,r.jsx)("a",{href:i,target:"_blank",children:t??(0,r.jsxs)("code",{children:[e,"()"]})})}},28453(e,t,o){o.d(t,{R:()=>p,x:()=>i});var r=o(96540);let n={},l=r.createContext(n);function p(e){let t=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:p(e.components),r.createElement(l.Provider,{value:t},e.children)}}}]);