"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7067],{2952:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/what-to-do-with-hyperparam-runs-f300784edeacd21609ad71852b184bd5.svg"},24629:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/tag-exp-run-relationship-fc898eccc4bb05fe59f41372ab5f6b50.svg"},27594:(e,n,t)=>{t.d(n,{O:()=>a});var r=t(96540),i=t(74848);function a({children:e,href:n}){const t=(0,r.useCallback)((async e=>{if(e.preventDefault(),window.gtag)try{window.gtag("event","notebook-download",{href:n})}catch{}const t=await fetch(n),r=await t.blob(),i=window.URL.createObjectURL(r),a=document.createElement("a");a.style.display="none",a.href=i;const s=n.split("/").pop();a.download=s,document.body.appendChild(a),a.click(),window.URL.revokeObjectURL(i),document.body.removeChild(a)}),[n]);return(0,i.jsx)("a",{className:"button button--primary",style:{marginBottom:"1rem",display:"block",width:"min-content"},href:n,download:!0,onClick:t,children:e})}},28209:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"traditional-ml/tutorials/hyperparameter-tuning/part1-child-runs/index","title":"Understanding Parent and Child Runs in MLflow","description":"Introduction","source":"@site/docs/classic-ml/traditional-ml/tutorials/hyperparameter-tuning/part1-child-runs/index.mdx","sourceDirName":"traditional-ml/tutorials/hyperparameter-tuning/part1-child-runs","slug":"/traditional-ml/tutorials/hyperparameter-tuning/part1-child-runs/","permalink":"/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/part1-child-runs/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"The Parent-Child relationship with runs"},"sidebar":"classicMLSidebar","previous":{"title":"Hyperparameter Tuning with MLflow and Optuna","permalink":"/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/"},"next":{"title":"Logging plots with MLflow","permalink":"/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/part2-logging-plots/"}}');var i=t(74848),a=t(28453),s=t(27594);const o={sidebar_position:2,sidebar_label:"The Parent-Child relationship with runs"},l="Understanding Parent and Child Runs in MLflow",c={},h=[{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts of MLflow: Tags, Experiments, and Runs",id:"core-concepts-of-mlflow-tags-experiments-and-runs",level:2},{value:"Key Aspects",id:"key-aspects",level:3},{value:"The Real-world Challenge: Hyperparameter Tuning",id:"the-real-world-challenge-hyperparameter-tuning",level:2},{value:"Benefits of Hyperparameter Tuning",id:"benefits-of-hyperparameter-tuning",level:3},{value:"What are Parent and Child Runs?",id:"what-are-parent-and-child-runs",level:2},{value:"Benefits",id:"benefits",level:2},{value:"Relationship between Experiments, Parent Runs, and Child Runs",id:"relationship-between-experiments-parent-runs-and-child-runs",level:2},{value:"Practical Example",id:"practical-example",level:2},{value:"Naive Approach with no child runs",id:"naive-approach-with-no-child-runs",level:3},{value:"Adapting for Parent and Child Runs",id:"adapting-for-parent-and-child-runs",level:3},{value:"Challenge",id:"challenge",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"understanding-parent-and-child-runs-in-mlflow",children:"Understanding Parent and Child Runs in MLflow"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Machine learning projects often involve intricate relationships. These connections can emerge at\nvarious stages, be it the project's conception, during data preprocessing, in the model's architecture,\nor even during the model's tuning process. MLflow provides tools to efficiently capture and represent\nthese relationships."}),"\n",(0,i.jsx)(n.h2,{id:"core-concepts-of-mlflow-tags-experiments-and-runs",children:"Core Concepts of MLflow: Tags, Experiments, and Runs"}),"\n",(0,i.jsxs)(n.p,{children:["In our foundational MLflow tutorial, we highlighted a fundamental relationship: the association\nbetween ",(0,i.jsx)(n.strong,{children:"tags"}),", ",(0,i.jsx)(n.strong,{children:"experiments"}),", and ",(0,i.jsx)(n.strong,{children:"runs"}),". This association is crucial when dealing with\ncomplex ML projects, such as forecasting models for individual products in a supermarket, as\npresented in our example. The diagram below offers a visual representation:"]}),"\n",(0,i.jsx)("figure",{className:"center-div",style:{width:1024,maxWidth:"100%",textAlign:"center"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Tags, experiments, and runs relationships",src:t(24629).A+"",width:"1062",height:"801"}),"\n",(0,i.jsx)("figcaption",{children:"A model grouping hierarchy"})]})}),"\n",(0,i.jsx)(n.h3,{id:"key-aspects",children:"Key Aspects"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tags"}),": These are instrumental in defining business-level filtering keys. They aid in retrieving relevant experiments and their runs."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Experiments"}),": They set boundaries, both from a business perspective and data-wise. For instance, sales data for carrots wouldn't be used to predict sales of apples without prior validation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Runs"}),": Each run captures a specific hypothesis or iteration of training, nestled within the context of the experiment."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"the-real-world-challenge-hyperparameter-tuning",children:"The Real-world Challenge: Hyperparameter Tuning"}),"\n",(0,i.jsx)(n.p,{children:"While the above model suffices for introductory purposes, real-world scenarios introduce complexities. One such complexity arises when tuning models."}),"\n",(0,i.jsx)(n.p,{children:"Model tuning is paramount. Methods range from grid search (though typically not recommended due to\ninefficiencies) to random searches, and more advanced approaches like automated hyperparameter tuning.\nThe objective remains the same: to optimally traverse the model's parameter space."}),"\n",(0,i.jsx)(n.h3,{id:"benefits-of-hyperparameter-tuning",children:"Benefits of Hyperparameter Tuning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Loss Metric Relationship"}),": By analyzing the relationship between hyperparameters and optimization loss metrics, we can discern potentially irrelevant parameters."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parameter Space Analysis"}),": Monitoring the range of tested values can indicate if we need to constrict or expand our search space."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Sensitivity Analysis"}),": Estimating how a model reacts to specific parameters can pinpoint potential feature set issues."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"But here lies the challenge: How do we systematically store the extensive data produced during hyperparameter tuning?"}),"\n",(0,i.jsx)("figure",{className:"center-div",style:{width:1024,maxWidth:"100%",textAlign:"center"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Challenges with hyperparameter data storage",src:t(2952).A+"",width:"872",height:"358"}),"\n",(0,i.jsx)("figcaption",{children:"The quandary of storing hyperparameter data"})]})}),"\n",(0,i.jsx)(n.p,{children:"In the upcoming sections, we'll delve deeper, exploring MLflow's capabilities to address this\nchallenge, focusing on the concepts of Parent and Child Runs."}),"\n",(0,i.jsx)(n.h2,{id:"what-are-parent-and-child-runs",children:"What are Parent and Child Runs?"}),"\n",(0,i.jsx)(n.p,{children:'At its core, MLflow allows users to track experiments, which are essentially named groups of runs.\nA "run" in this context refers to a single execution of a model training event, where you can log\nparameters, metrics, tags, and artifacts associated with the training process.\nThe concept of Parent and Child Runs introduces a hierarchical structure to these runs.'}),"\n",(0,i.jsx)(n.p,{children:"Imagine a scenario where you're testing a deep learning model with different architectures. Each\narchitecture can be considered a parent run, and every iteration of hyperparameter tuning for that\narchitecture becomes a child run nested under its respective parent."}),"\n",(0,i.jsx)(n.h2,{id:"benefits",children:"Benefits"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Organizational Clarity"}),": By using Parent and Child Runs, you can easily group related runs together. For instance, if you're running a hyperparameter search using a Bayesian approach on a particular model architecture, every iteration can be logged as a child run, while the overarching Bayesian optimization process can be the parent run."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Enhanced Traceability"}),": When working on large projects with a broad product hierarchy, child runs can represent individual products or variants, making it straightforward to trace back results, metrics, or artifacts to their specific run."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": As your experiments grow in number and complexity, having a nested structure ensures that your tracking remains scalable. It's much easier to navigate through a structured hierarchy than a flat list of hundreds or thousands of runs."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Improved Collaboration"}),": For teams, this approach ensures that members can easily understand the structure and flow of experiments conducted by their peers, promoting collaboration and knowledge sharing."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"relationship-between-experiments-parent-runs-and-child-runs",children:"Relationship between Experiments, Parent Runs, and Child Runs"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Experiments"}),': Consider experiments as the topmost layer. They are named entities under which all related runs reside. For instance, an experiment named "Deep Learning Architectures" might contain runs related to various architectures you\'re testing.']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parent Runs"}),": Within an experiment, a parent run represents a significant segment or phase of your workflow. Taking the earlier example, each specific architecture (like CNN, RNN, or Transformer) can be a parent run."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Child Runs"}),": Nested within parent runs are child runs. These are iterations or variations within the scope of their parent. For a CNN parent run, different sets of hyperparameters or slight architectural tweaks can each be a child run."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"practical-example",children:"Practical Example"}),"\n",(0,i.jsx)(n.p,{children:"For this example, let's image that we're working through a fine-tuning exercise for a particular modeling solution.\nWe're going through the tuning phase of rough adjustments initially, attempting to determine which parameter ranges and\ncategorical selection values that we might want to consider for a full hyperparameter tuning run with a much higher\niteration count."}),"\n",(0,i.jsx)(n.h3,{id:"naive-approach-with-no-child-runs",children:"Naive Approach with no child runs"}),"\n",(0,i.jsx)(n.p,{children:"In this first phase, we will be trying relatively small batches of different combinations of parameters and\nevaluating them within the MLflow UI to determine whether we should include or exempt certain values based on the\nrelatively performance amongst our iterative trials."}),"\n",(0,i.jsx)(n.p,{children:"If we were to use each iteration as its own MLflow run, our code might look something like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import random\nimport mlflow\nfrom functools import partial\nfrom itertools import starmap\nfrom more_itertools import consume\n\n\n# Define a function to log parameters and metrics\ndef log_run(run_name, test_no):\n    with mlflow.start_run(run_name=run_name):\n        mlflow.log_param("param1", random.choice(["a", "b", "c"]))\n        mlflow.log_param("param2", random.choice(["d", "e", "f"]))\n        mlflow.log_metric("metric1", random.uniform(0, 1))\n        mlflow.log_metric("metric2", abs(random.gauss(5, 2.5)))\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f"run_{i}_test_{test_no}" for i in range(num_runs))\n\n\n# Execute tuning function\ndef execute_tuning(test_no):\n    # Partial application of the log_run function\n    log_current_run = partial(log_run, test_no=test_no)\n    # Generate run names and apply log_current_run function to each run name\n    runs = starmap(\n        log_current_run, ((run_name,) for run_name in generate_run_names(test_no))\n    )\n    # Consume the iterator to execute the runs\n    consume(runs)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri("http://localhost:8080")\nmlflow.set_experiment("No Child Runs")\n\n# Execute 5 hyperparameter tuning runs\nconsume(starmap(execute_tuning, ((x,) for x in range(5))))\n'})}),"\n",(0,i.jsx)(n.p,{children:"After executing this, we can navigate to the MLflow UI to see the results of the iterations and compare each run's\nerror metrics to the parameters that were selected."}),"\n",(0,i.jsx)("figure",{className:"center-div",style:{width:1024,maxWidth:"100%",textAlign:"center"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Hyperparameter tuning no child runs",src:t(86705).A+"",width:"2048",height:"1637"}),"\n",(0,i.jsx)("figcaption",{children:"Initial Hyperparameter tuning execution"})]})}),"\n",(0,i.jsx)(n.p,{children:"What happens when we need to run this again with some slight modifications?"}),"\n",(0,i.jsx)(n.p,{children:"Our code might change in-place with the values being tested:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def log_run(run_name, test_no):\n    with mlflow.start_run(run_name=run_name):\n        mlflow.log_param("param1", random.choice(["a", "c"]))  # remove \'b\'\n        # remainder of code ...\n'})}),"\n",(0,i.jsx)(n.p,{children:"When we execute this and navigate back to the UI, it is now significantly more difficult to determine\nwhich run results are associated with a particular parameter grouping. For this example, it isn't\nparticularly problematic since the features are identical and the parameter search space is a subset of the\noriginal hyperparameter test."}),"\n",(0,i.jsx)(n.p,{children:"This may become a serious problem for analysis if we:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Add terms to the original hyperparameter search space"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Modify the feature data (add or remove features)"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Change the underlying model architecture (test 1 is a Random Forest model, while test 2 is a Gradient Boosted Trees model)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Let's take a look at the UI and see if it is clear which iteration a particular run is a member of."}),"\n",(0,i.jsx)("figure",{className:"center-div",style:{width:1024,maxWidth:"100%",textAlign:"center"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Adding more runs",src:t(67602).A+"",width:"2048",height:"1637"}),"\n",(0,i.jsx)("figcaption",{children:"Challenges with iterative tuning without child run encapsulation"})]})}),"\n",(0,i.jsx)(n.p,{children:"It's not too hard to imagine how complicated this can become if there are thousands of runs in this experiment."}),"\n",(0,i.jsx)(n.p,{children:"There is a solution for this, though. We can setup the exact same testing scenario with few small modifications to make it easy to find\nrelated runs, declutter the UI, and greatly simplify the overall process of evaluating hyperparameter ranges and parameter inclusions\nduring the process of tuning. Only a few modification are needed:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Use child runs by adding a nested ",(0,i.jsx)(n.code,{children:"start_run()"})," context within a parent run's context."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Add disambiguation information to the runs in the form of modifying the ",(0,i.jsx)(n.code,{children:"run_name"})," of the parent run"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Add tag information to the parent and child runs to enable searching on keys that identify a family of runs"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"adapting-for-parent-and-child-runs",children:"Adapting for Parent and Child Runs"}),"\n",(0,i.jsx)(n.p,{children:"The code below demonstrates these modifications to our original hyperparameter tuning example."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import random\nimport mlflow\nfrom functools import partial\nfrom itertools import starmap\nfrom more_itertools import consume\n\n\n# Define a function to log parameters and metrics and add tag\n# logging for search_runs functionality\ndef log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):\n    with mlflow.start_run(run_name=run_name, nested=True):\n        mlflow.log_param("param1", random.choice(param1_choices))\n        mlflow.log_param("param2", random.choice(param2_choices))\n        mlflow.log_metric("metric1", random.uniform(0, 1))\n        mlflow.log_metric("metric2", abs(random.gauss(5, 2.5)))\n        mlflow.set_tag("test_identifier", tag_ident)\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f"run_{i}_test_{test_no}" for i in range(num_runs))\n\n\n# Execute tuning function, allowing for param overrides,\n# run_name disambiguation, and tagging support\ndef execute_tuning(\n    test_no,\n    param1_choices=["a", "b", "c"],\n    param2_choices=["d", "e", "f"],\n    test_identifier="",\n):\n    ident = "default" if not test_identifier else test_identifier\n    # Use a parent run to encapsulate the child runs\n    with mlflow.start_run(run_name=f"parent_run_test_{ident}_{test_no}"):\n        # Partial application of the log_run function\n        log_current_run = partial(\n            log_run,\n            test_no=test_no,\n            param1_choices=param1_choices,\n            param2_choices=param2_choices,\n            tag_ident=ident,\n        )\n        mlflow.set_tag("test_identifier", ident)\n        # Generate run names and apply log_current_run function to each run name\n        runs = starmap(\n            log_current_run, ((run_name,) for run_name in generate_run_names(test_no))\n        )\n        # Consume the iterator to execute the runs\n        consume(runs)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri("http://localhost:8080")\nmlflow.set_experiment("Nested Child Association")\n\n# Define custom parameters\nparam_1_values = ["x", "y", "z"]\nparam_2_values = ["u", "v", "w"]\n\n# Execute hyperparameter tuning runs with custom parameter choices\nconsume(\n    starmap(execute_tuning, ((x, param_1_values, param_2_values) for x in range(5)))\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"We can view the results of executing this in the UI:"}),"\n",(0,i.jsx)(n.p,{children:"The real benefit of this nested architecture becomes much more apparent when we add additional runs\nwith different conditions of hyperparameter selection criteria."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Execute modified hyperparameter tuning runs with custom parameter choices\nparam_1_values = ["a", "b"]\nparam_2_values = ["u", "v", "w"]\nident = "params_test_2"\nconsume(\n    starmap(\n        execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))\n    )\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"... and even more runs ..."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'param_1_values = ["b", "c"]\nparam_2_values = ["d", "f"]\nident = "params_test_3"\nconsume(\n    starmap(\n        execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))\n    )\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Once we execute these three tuning run tests, we can view the results in the UI:"}),"\n",(0,i.jsx)("figure",{className:"center-div",style:{width:1024,maxWidth:"100%",textAlign:"center"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Using child runs",src:t(93945).A+"",width:"2048",height:"1637"}),"\n",(0,i.jsx)("figcaption",{children:"Encapsulating tests with child runs"})]})}),"\n",(0,i.jsx)(n.p,{children:"In the above video, you can see that we purposefully avoided including the parent run in the run comparison.\nThis is due to the fact that no metrics or parameters were actually written to these parent runs; rather, they\nwere used purely for organizational purposes to limit the volume of runs visible within the UI."}),"\n",(0,i.jsx)(n.p,{children:"In practice, it is best to store the best conditions found with a hyperparamter execution of child runs within\nthe parent's run data."}),"\n",(0,i.jsx)(n.h2,{id:"challenge",children:"Challenge"}),"\n",(0,i.jsx)(n.p,{children:"As an exercise, if you are interested, you may download the notebook with these two examples and modify the\ncode within in order to achieve this."}),"\n",(0,i.jsx)("p",{children:(0,i.jsx)(s.O,{href:"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb",children:"Download the notebook"})}),"\n",(0,i.jsx)(n.p,{children:"The notebook contains an example implementation of this, but it is\nrecommended to develop your own implementation that fulfills the following requirements:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Record the lowest metric1 value amongst the children and the associated parameters with that child run in the parent run's information."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Add the ability to specify an iteration count to the number of children created from the calling entry point."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The results in the UI for this challenge are shown below."}),"\n",(0,i.jsx)("figure",{className:"center-div",style:{width:1024,maxWidth:"100%",textAlign:"center"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Challenge",src:t(69705).A+"",width:"2048",height:"1637"}),"\n",(0,i.jsx)("figcaption",{children:"Adding best child run data to parent run"})]})}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"The usage of parent and child runs associations can greatly simplify iterative model development.\nWith repetitive and high-data-volume tasks such as hyperparameter tuning, encapsulating a training run's\nparameter search space or feature engineering evaluation runs can help to ensure that you're comparing\nexactly what you intend to compare, all with minimal effort."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(96540);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}},67602:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/no-child-more-be952d9e9b8c849bf5b05290d49ce24a.gif"},69705:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/parent-child-challenge-b7198bdf22d9b91c5bff5de671e5f629.gif"},86705:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/no-child-first-ac9b59845a4d7aa095ec913220cc5913.gif"},93945:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/child-runs-3a77b805cd47d1e997c3268c47b6d22d.gif"}}]);