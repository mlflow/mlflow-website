"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1452],{65:(e,n,o)=>{o.d(n,{A:()=>t});const t=o.p+"assets/images/models_from_code-3299c0ebefae1e6e936610a593c27c31.png"},14252:(e,n,o)=>{o.d(n,{A:()=>a});o(96540);var t=o(65195);const l={tableOfContentsInline:"tableOfContentsInline_prmo"};var i=o(74848);function a({toc:e,minHeadingLevel:n,maxHeadingLevel:o}){return(0,i.jsx)("div",{className:l.tableOfContentsInline,children:(0,i.jsx)(t.A,{toc:e,minHeadingLevel:n,maxHeadingLevel:o,className:"table-of-contents",linkClassName:null})})}},28453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>s});var t=o(96540);const l={},i=t.createContext(l);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},49374:(e,n,o)=>{o.d(n,{B:()=>r});o(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=o(86025),i=o(28774),a=o(74848);const s=e=>{const n=e.split(".");for(let o=n.length;o>0;o--){const e=n.slice(0,o).join(".");if(t[e])return e}return null};function r({fn:e,children:n}){const o=s(e);if(!o)return(0,a.jsx)(a.Fragment,{children:n});const r=(0,l.Ay)(`/${t[o]}#${e}`);return(0,a.jsx)(i.A,{to:r,target:"_blank",children:n??(0,a.jsxs)("code",{children:[e,"()"]})})}},65195:(e,n,o)=>{o.d(n,{A:()=>u});var t=o(96540),l=o(6342);function i(e){const n=e.map((e=>({...e,parentIndex:-1,children:[]}))),o=Array(7).fill(-1);n.forEach(((e,n)=>{const t=o.slice(2,e.level);e.parentIndex=Math.max(...t),o[e.level]=n}));const t=[];return n.forEach((e=>{const{parentIndex:o,...l}=e;o>=0?n[o].children.push(l):t.push(l)})),t}function a({toc:e,minHeadingLevel:n,maxHeadingLevel:o}){return e.flatMap((e=>{const t=a({toc:e.children,minHeadingLevel:n,maxHeadingLevel:o});return function(e){return e.level>=n&&e.level<=o}(e)?[{...e,children:t}]:t}))}function s(e){const n=e.getBoundingClientRect();return n.top===n.bottom?s(e.parentNode):n}function r(e,{anchorTopOffset:n}){const o=e.find((e=>s(e).top>=n));if(o){return function(e){return e.top>0&&e.bottom<window.innerHeight/2}(s(o))?o:e[e.indexOf(o)-1]??null}return e[e.length-1]??null}function d(){const e=(0,t.useRef)(0),{navbar:{hideOnScroll:n}}=(0,l.p)();return(0,t.useEffect)((()=>{e.current=n?0:document.querySelector(".navbar").clientHeight}),[n]),e}function c(e){const n=(0,t.useRef)(void 0),o=d();(0,t.useEffect)((()=>{if(!e)return()=>{};const{linkClassName:t,linkActiveClassName:l,minHeadingLevel:i,maxHeadingLevel:a}=e;function s(){const e=function(e){return Array.from(document.getElementsByClassName(e))}(t),s=function({minHeadingLevel:e,maxHeadingLevel:n}){const o=[];for(let t=e;t<=n;t+=1)o.push(`h${t}.anchor`);return Array.from(document.querySelectorAll(o.join()))}({minHeadingLevel:i,maxHeadingLevel:a}),d=r(s,{anchorTopOffset:o.current}),c=e.find((e=>d&&d.id===function(e){return decodeURIComponent(e.href.substring(e.href.indexOf("#")+1))}(e)));e.forEach((e=>{!function(e,o){o?(n.current&&n.current!==e&&n.current.classList.remove(l),e.classList.add(l),n.current=e):e.classList.remove(l)}(e,e===c)}))}return document.addEventListener("scroll",s),document.addEventListener("resize",s),s(),()=>{document.removeEventListener("scroll",s),document.removeEventListener("resize",s)}}),[e,o])}var m=o(28774),h=o(74848);function p({toc:e,className:n,linkClassName:o,isChild:t}){return e.length?(0,h.jsx)("ul",{className:t?void 0:n,children:e.map((e=>(0,h.jsxs)("li",{children:[(0,h.jsx)(m.A,{to:`#${e.id}`,className:o??void 0,dangerouslySetInnerHTML:{__html:e.value}}),(0,h.jsx)(p,{isChild:!0,toc:e.children,className:n,linkClassName:o})]},e.id)))}):null}const f=t.memo(p);function u({toc:e,className:n="table-of-contents table-of-contents__left-border",linkClassName:o="table-of-contents__link",linkActiveClassName:s,minHeadingLevel:r,maxHeadingLevel:d,...m}){const p=(0,l.p)(),u=r??p.tableOfContents.minHeadingLevel,x=d??p.tableOfContents.maxHeadingLevel,g=function({toc:e,minHeadingLevel:n,maxHeadingLevel:o}){return(0,t.useMemo)((()=>a({toc:i(e),minHeadingLevel:n,maxHeadingLevel:o})),[e,n,o])}({toc:e,minHeadingLevel:u,maxHeadingLevel:x});return c((0,t.useMemo)((()=>{if(o&&s)return{linkClassName:o,linkActiveClassName:s,minHeadingLevel:u,maxHeadingLevel:x}}),[o,s,u,x])),(0,h.jsx)(f,{toc:g,className:n,linkClassName:o,...m})}},72839:(e,n,o)=>{o.d(n,{X:()=>l});var t=o(74848);function l({children:e}){return(0,t.jsx)("div",{className:"w-full overflow-x-auto",children:(0,t.jsx)("table",{children:e})})}},88353:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>p,contentTitle:()=>h,default:()=>x,frontMatter:()=>m,metadata:()=>t,toc:()=>f});const t=JSON.parse('{"id":"model/index","title":"MLflow Models","description":"An MLflow Model is a standard format for packaging machine learning models that can be used in a","source":"@site/docs/classic-ml/model/index.mdx","sourceDirName":"model","slug":"/model/","permalink":"/docs/latest/ml/model/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_label":"Overview","sidebar_position":0,"toc_max_heading_level":2},"sidebar":"classicMLSidebar","previous":{"title":"Tracking APIs \ud83d\udee0\ufe0f","permalink":"/docs/latest/ml/tracking/tracking-api/"},"next":{"title":"Model Signature","permalink":"/docs/latest/ml/model/signatures/"}}');var l=o(74848),i=o(28453),a=o(14252),s=o(49374),r=o(28774),d=o(86025),c=o(72839);const m={sidebar_label:"Overview",sidebar_position:0,toc_max_heading_level:2},h="MLflow Models",p={},f=[{value:"Storage Format",id:"storage-format",level:2},{value:"MLmodel file",id:"mlmodel-file",level:3},{value:"Additional Logged Files",id:"additional-logged-files",level:3},{value:"Environment variables file",id:"environment-variables-file",level:3},{value:"Managing Model Dependencies",id:"managing-model-dependencies",level:2},{value:"Model Signatures And Input Examples",id:"model-signatures-and-input-examples",level:2},{value:"Model API",id:"model-api",level:2},{value:"Models From Code",id:"models-from-code",level:2},{value:"Built-In Model Flavors",id:"models_built-in-model-flavors",level:2},{value:"Python Function (<code>python_function</code>)",id:"pyfunc-model-flavor",level:3},{value:"How To Save Model As Python Function",id:"how-to-save-model-as-python-function",level:4},{value:"How To Load And Score Python Function Models",id:"how-to-load-and-score-python-function-models",level:4},{value:"Loading Models",id:"loading-models",level:5},{value:"Scoring Models",id:"scoring-models",level:5},{value:"Demonstrating <code>predict_stream()</code>",id:"demonstrating-predict_stream",level:5},{value:"Python Function Model Interfaces",id:"python-function-model-interfaces",level:4},{value:"R Function (<code>crate</code>)",id:"r-function-crate",level:3},{value:"<code>crate</code> usage",id:"crate-usage",level:4},{value:"H<sub>2</sub>O (<code>h2o</code>)",id:"h2o-h2o",level:3},{value:"h2o pyfunc usage",id:"h2o-pyfunc-usage",level:4},{value:"Keras (<code>keras</code>)",id:"tf-keras-example",level:3},{value:"PyTorch (<code>pytorch</code>)",id:"pytorch-pytorch",level:3},{value:"Scikit-learn (<code>sklearn</code>)",id:"scikit-learn-sklearn",level:3},{value:"Spark MLlib (<code>spark</code>)",id:"spark-mllib-spark",level:3},{value:"TensorFlow (<code>tensorflow</code>)",id:"tensorflow-tensorflow",level:3},{value:"ONNX (<code>onnx</code>)",id:"onnx-onnx",level:3},{value:"ONNX pyfunc usage example",id:"onnx-pyfunc-usage-example",level:4},{value:"XGBoost (<code>xgboost</code>)",id:"xgboost-xgboost",level:3},{value:"LightGBM (<code>lightgbm</code>)",id:"lightgbm-lightgbm",level:3},{value:"<code>LightGBM</code> pyfunc usage",id:"lightgbm-pyfunc-usage",level:4},{value:"CatBoost (<code>catboost</code>)",id:"catboost-catboost",level:3},{value:"<code>CatBoost</code> pyfunc usage",id:"catboost-pyfunc-usage",level:4},{value:"Spacy(<code>spaCy</code>)",id:"spacyspacy",level:3},{value:"Statsmodels (<code>statsmodels</code>)",id:"statsmodels-statsmodels",level:3},{value:"Statsmodels pyfunc usage",id:"statsmodels-pyfunc-usage",level:4},{value:"Prophet (<code>prophet</code>)",id:"prophet-prophet",level:3},{value:"Pmdarima (<code>pmdarima</code>)",id:"pmdarima-flavor",level:3},{value:"John Snow Labs (<code>johnsnowlabs</code>)",id:"john-snow-labs-johnsnowlabs",level:3},{value:"To deploy the John Snow Labs model as a container",id:"to-deploy-the-john-snow-labs-model-as-a-container",level:4},{value:"To deploy the John Snow Labs model without a container",id:"to-deploy-the-john-snow-labs-model-without-a-container",level:4},{value:"Diviner (<code>diviner</code>)",id:"diviner-diviner",level:3},{value:"Diviner Types",id:"diviner-types",level:4},{value:"Metrics and Parameters logging for Diviner",id:"metrics-and-parameters-logging-for-diviner",level:4},{value:"Diviner pyfunc usage",id:"diviner-pyfunc-usage",level:4},{value:"Transformers (<code>transformers</code>)",id:"transformers-transformers",level:3},{value:"SentenceTransformers (<code>sentence_transformers</code>)",id:"sentencetransformers-sentence_transformers",level:3},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Model Customization",id:"model-customization",level:2},{value:"Custom Python Models",id:"custom-python-models",level:3},{value:"Example: Creating a model with type hints",id:"example-creating-a-model-with-type-hints",level:4},{value:"Example: Creating a custom \u201cadd n\u201d model",id:"example-creating-a-custom-add-n-model",level:4},{value:"Example: Saving an XGBoost model in MLflow format",id:"example-saving-an-xgboost-model-in-mlflow-format",level:4},{value:"Example: Logging a transformers model with hf:/ schema to avoid copying large files",id:"example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files",level:4},{value:"Custom Flavors",id:"custom-flavors",level:3},{value:"Validate Models before Deployment",id:"validate-models-before-deployment",level:2},{value:"Environment managers",id:"environment-managers",level:3},{value:"Built-In Deployment Tools",id:"built-in-deployment",level:2},{value:"Export a <code>python_function</code> model as an Apache Spark UDF",id:"export-a-python_function-model-as-an-apache-spark-udf",level:2},{value:"Deployment to Custom Targets",id:"deployment_plugin",level:2},{value:"Commands",id:"commands",level:3},{value:"Community Model Flavors",id:"community-model-flavors",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"mlflow-models",children:"MLflow Models"})}),"\n",(0,l.jsx)(n.p,{children:'An MLflow Model is a standard format for packaging machine learning models that can be used in a\nvariety of downstream tools---for example, real-time serving through a REST API or batch inference\non Apache Spark. The format defines a convention that lets you save a model in different "flavors"\nthat can be understood by different downstream tools.'}),"\n",(0,l.jsx)(n.h2,{id:"storage-format",children:"Storage Format"}),"\n",(0,l.jsxs)(n.p,{children:["Each MLflow Model is a directory containing arbitrary files, together with an ",(0,l.jsx)(n.code,{children:"MLmodel"}),"\nfile in the root of the directory that can define multiple ",(0,l.jsx)(n.em,{children:"flavors"})," that the model can be viewed\nin."]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.strong,{children:"model"})," aspect of the MLflow Model can either be a serialized object (e.g., a pickled ",(0,l.jsx)(n.code,{children:"scikit-learn"})," model)\nor a Python script (or notebook, if running in Databricks) that contains the model instance that has been defined\nwith the ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API."]}),"\n",(0,l.jsxs)(n.p,{children:['Flavors are the key concept that makes MLflow Models powerful: they are a convention that deployment\ntools can use to understand the model, which makes it possible to write tools that work with models\nfrom any ML library without having to integrate each tool with each library. MLflow defines\nseveral "standard" flavors that all of its built-in deployment tools support, such as a "Python\nfunction" flavor that describes how to run the model as a Python function. However, libraries can\nalso define and use other flavors. For example, MLflow\'s ',(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:(0,l.jsx)(n.code,{children:"mlflow.sklearn"})}),"\nlibrary allows loading models back as a scikit-learn ",(0,l.jsx)(n.code,{children:"Pipeline"})," object for use in code that is aware of\nscikit-learn, or as a generic Python function for use in tools that just need to apply the model\n(for example, the ",(0,l.jsx)(n.code,{children:"mlflow deployments"})," tool with the option ",(0,l.jsx)(n.code,{children:"-t sagemaker"})," for deploying models\nto Amazon SageMaker)."]}),"\n",(0,l.jsx)(n.h3,{id:"mlmodel-file",children:"MLmodel file"}),"\n",(0,l.jsxs)(n.p,{children:["All of the flavors that a particular model supports are defined in its ",(0,l.jsx)(n.code,{children:"MLmodel"})," file in YAML\nformat. For example, running ",(0,l.jsx)(n.code,{children:"python examples/sklearn_logistic_regression/train.py"})," from within the\n",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/sklearn_logistic_regression/train.py",children:"MLflow repo"}),"\nwill create the following files under the ",(0,l.jsx)(n.code,{children:"model"})," directory:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'# Directory written by mlflow.sklearn.save_model(model, "model", input_example=...)\nmodel/\n\u251c\u2500\u2500 MLmodel\n\u251c\u2500\u2500 model.pkl\n\u251c\u2500\u2500 conda.yaml\n\u251c\u2500\u2500 python_env.yaml\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 input_example.json (optional, only logged when input example is provided and valid during model logging)\n\u251c\u2500\u2500 serving_input_example.json (optional, only logged when input example is provided and valid during model logging)\n\u2514\u2500\u2500 environment_variables.txt (optional, only logged when environment variables are used during model inference)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["And its ",(0,l.jsx)(n.code,{children:"MLmodel"})," file describes two flavors:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:"time_created: 2018-05-25T17:28:53.35\n\nflavors:\n  sklearn:\n    sklearn_version: 0.19.1\n    pickled_model: model.pkl\n  python_function:\n    loader_module: mlflow.sklearn\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Apart from a ",(0,l.jsx)(n.strong,{children:"flavors"})," field listing the model flavors, the MLmodel YAML format can contain\nthe following fields:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"time_created"}),": Date and time when the model was created, in UTC ISO 8601 format."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"run_id"}),": ID of the run that created the model, if the model was saved using ",(0,l.jsx)(n.a,{href:"/ml/tracking",children:"tracking"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"signature"}),": ",(0,l.jsx)(n.a,{href:"/ml/model/signatures",children:"model signature"})," in JSON format."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"input_example"}),": reference to an artifact with ",(0,l.jsx)(n.a,{href:"/ml/model/signatures",children:"input example"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"databricks_runtime"}),": Databricks runtime version and type, if the model was trained in a Databricks notebook or job."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"mlflow_version"}),": The version of MLflow that was used to log the model."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"additional-logged-files",children:"Additional Logged Files"}),"\n",(0,l.jsxs)(n.p,{children:["For environment recreation, we automatically log ",(0,l.jsx)(n.code,{children:"conda.yaml"}),", ",(0,l.jsx)(n.code,{children:"python_env.yaml"}),", and ",(0,l.jsx)(n.code,{children:"requirements.txt"})," files whenever a model is logged.\nThese files can then be used to reinstall dependencies using ",(0,l.jsx)(n.code,{children:"conda"})," or ",(0,l.jsx)(n.code,{children:"virtualenv"})," with ",(0,l.jsx)(n.code,{children:"pip"}),". Please see\n",(0,l.jsx)(n.a,{href:"/ml/model/dependencies#how-mlflow-records-dependencies",children:"How MLflow Model Records Dependencies"})," for more details about these files."]}),"\n",(0,l.jsxs)(n.p,{children:["If a model input example is provided when logging the model, two additional files ",(0,l.jsx)(n.code,{children:"input_example.json"})," and ",(0,l.jsx)(n.code,{children:"serving_input_example.json"})," are logged.\nSee ",(0,l.jsx)(n.a,{href:"/ml/model/signatures",children:"Model Input Example"})," for more details."]}),"\n",(0,l.jsxs)(n.p,{children:["When logging a model, model metadata files (",(0,l.jsx)(n.code,{children:"MLmodel"}),", ",(0,l.jsx)(n.code,{children:"conda.yaml"}),", ",(0,l.jsx)(n.code,{children:"python_env.yaml"}),", ",(0,l.jsx)(n.code,{children:"requirements.txt"}),") are copied to a subdirectory named ",(0,l.jsx)(n.code,{children:"metadata"}),".\nFor wheeled models, ",(0,l.jsx)(n.code,{children:"original_requirements.txt"})," file is also copied."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When a model registered in the MLflow Model Registry is downloaded, a YAML file named\n",(0,l.jsx)(n.code,{children:"registered_model_meta"})," is added to the model directory on the downloader's side.\nThis file contains the name and version of the model referenced in the MLflow Model Registry,\nand will be used for deployment and other purposes."]})}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:["If you log a model within Databricks, MLflow also creates a ",(0,l.jsx)(n.code,{children:"metadata"})," subdirectory within\nthe model directory. This subdirectory contains the lightweight copy of aforementioned\nmetadata files for internal use."]})}),"\n",(0,l.jsx)(n.h3,{id:"environment-variables-file",children:"Environment variables file"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow records the environment variables that are used during model inference in ",(0,l.jsx)(n.code,{children:"environment_variables.txt"}),"\nfile when logging a model."]}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"environment_variables.txt"})," file ",(0,l.jsx)(n.strong,{children:"only contains names"})," of the environment variables that are used during\nmodel inference, ",(0,l.jsx)(n.strong,{children:"values are not stored"}),"."]})}),"\n",(0,l.jsx)(n.p,{children:"Currently MLflow only logs the environment variables whose name contains any of the following keywords:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'RECORD_ENV_VAR_ALLOWLIST = {\n    # api key related\n    "API_KEY",  # e.g. OPENAI_API_KEY\n    "API_TOKEN",\n    # databricks auth related\n    "DATABRICKS_HOST",\n    "DATABRICKS_USERNAME",\n    "DATABRICKS_PASSWORD",\n    "DATABRICKS_TOKEN",\n    "DATABRICKS_INSECURE",\n    "DATABRICKS_CLIENT_ID",\n    "DATABRICKS_CLIENT_SECRET",\n    "_DATABRICKS_WORKSPACE_HOST",\n    "_DATABRICKS_WORKSPACE_ID",\n}\n'})}),"\n",(0,l.jsx)(n.p,{children:"Example of a pyfunc model that uses environment variables:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["TEST_API_KEY"] = "test_api_key"\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        if os.environ.get("TEST_API_KEY"):\n            return model_input\n        raise Exception("API key not found")\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="model", python_model=MyModel(), input_example="data"\n    )\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Environment variable ",(0,l.jsx)(n.code,{children:"TEST_API_KEY"})," is logged in the environment_variables.txt file like below"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"# This file records environment variable names that are used during model inference.\n# They might need to be set when creating a serving endpoint from this model.\n# Note: it is not guaranteed that all environment variables listed here are required\nTEST_API_KEY\n"})}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:["Before you deploy a model to a serving endpoint, ",(0,l.jsx)(n.strong,{children:"review the environment_variables.txt file"})," to ensure\nall necessary environment variables for model inference are set. Note that ",(0,l.jsx)(n.strong,{children:"not all environment variables\nlisted in the file are always required for model inference."})," For detailed instructions on setting\nenvironment variables on a databricks serving endpoint, refer to\n",(0,l.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/store-env-variable-model-serving.html#add-plain-text-environment-variables",children:"this guidance"}),"."]})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["To disable this feature, set the environment variable ",(0,l.jsx)(n.code,{children:"MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING"})," to ",(0,l.jsx)(n.code,{children:"false"}),"."]})}),"\n",(0,l.jsx)(n.h2,{id:"managing-model-dependencies",children:"Managing Model Dependencies"}),"\n",(0,l.jsxs)(n.p,{children:["An MLflow Model infers dependencies required for the model flavor and automatically logs them. However, it also allows\nyou to define extra dependencies or custom Python code, and offer a tool to validate them in a sandbox environment.\nPlease refer to ",(0,l.jsx)(n.a,{href:"/ml/model/dependencies",children:"Managing Dependencies in MLflow Models"})," for more details."]}),"\n",(0,l.jsx)(n.h2,{id:"model-signatures-and-input-examples",children:"Model Signatures And Input Examples"}),"\n",(0,l.jsx)(n.p,{children:"In MLflow, understanding the intricacies of model signatures and input examples is crucial for\neffective model management and deployment."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Signature"}),": Defines the schema for model inputs, outputs, and additional inference parameters,\npromoting a standardized interface for model interaction."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Input Example"}),": Provides a concrete instance of valid model input, aiding in understanding and\ntesting model requirements. Additionally, if an input example is provided when logging a model, a model\nsignature will be automatically inferred and stored if not explicitly provided."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Serving Payload Example"}),": Provides a json payload example for querying a deployed model endpoint.\nIf an input example is provided when logging a model, a serving payload example is automatically generated\nfrom the input example and saved as ",(0,l.jsx)(n.code,{children:"serving_input_example.json"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Our documentation delves into several key areas:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Supported Signature Types"}),": We cover the different data types that are supported, such as tabular data\nfor traditional machine learning models and tensors for deep learning models."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Signature Enforcement"}),": Discusses how MLflow enforces schema compliance, ensuring that the provided\ninputs match the model's expectations."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Logging Models with Signatures"}),": Guides on how to incorporate signatures when logging models, enhancing\nclarity and reliability in model operations."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["For a detailed exploration of these concepts, including examples and best practices, visit the\n",(0,l.jsx)(n.a,{href:"/ml/model/signatures",children:"Model Signatures and Examples Guide"}),". If you would like to see signature enforcement in action,\nsee the ",(0,l.jsx)(n.a,{href:"/ml/model/notebooks/signature_examples",children:"notebook tutorial on Model Signatures"})," to learn more."]}),"\n",(0,l.jsx)(n.h2,{id:"model-api",children:"Model API"}),"\n",(0,l.jsxs)(n.p,{children:["You can save and load MLflow Models in multiple ways. First, MLflow includes integrations with\nseveral common libraries. For example, ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:"mlflow.sklearn"}),"\ncontains ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.save_model",children:"save_model"}),", ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.log_model",children:"log_model"}),",\nand ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.load_model",children:"load_model"})," functions for scikit-learn models. Second,\nyou can use the ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model",children:"mlflow.models.Model"})," class to create and write models. This\nclass has four key functions:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.add_flavor",children:"add_flavor"})," to add a flavor to the model. Each flavor\nhas a string name and a dictionary of key-value attributes, where the values can be any object\nthat can be serialized to YAML."]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.save",children:"save"})," to save the model to a local directory."]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.log",children:"log"})," to log the model as an artifact in the\ncurrent run using MLflow Tracking."]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.load",children:"load"})," to load a model from a local directory or\nfrom an artifact in a previous run."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"models-from-code",children:"Models From Code"}),"\n",(0,l.jsxs)(n.p,{children:["To ",(0,l.jsx)(n.strong,{children:"learn more about the Models From Code feature"}),", please visit ",(0,l.jsx)(n.a,{href:"/ml/model/models-from-code",children:"the deep dive guide"}),"\nfor more in-depth explanation and to see additional examples."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"The Models from Code feature is available in MLflow versions 2.12.2 and later. This feature is experimental and may change in future releases."})}),"\n",(0,l.jsxs)(n.p,{children:["The Models from Code feature allows you to define and log models directly from a stand-alone python script. This feature is particularly useful when you want to\nlog models that can be effectively stored as a code representation (models that do not need optimized weights through training) or applications\nthat rely on external services (e.g., LangChain chains). Another benefit is that this approach entirely bypasses the use of the ",(0,l.jsx)(n.code,{children:"pickle"})," or\n",(0,l.jsx)(n.code,{children:"cloudpickle"})," modules within Python, which can carry security risks when loading untrusted models."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["This feature is only supported for ",(0,l.jsx)(n.strong,{children:"LangChain"}),", ",(0,l.jsx)(n.strong,{children:"LlamaIndex"}),", and ",(0,l.jsx)(n.strong,{children:"PythonModel"})," models."]})}),"\n",(0,l.jsxs)(n.p,{children:["In order to log a model from code, you can leverage the ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API. This API allows you to define a model by specifying\nan instance of the model class directly within the file where the model is defined. When logging such a model, a\nfile path is specified (instead of an object) that points to the Python file containing both the model class definition and the usage of the\n",(0,l.jsx)(n.code,{children:"set_model"})," API applied on an instance of your custom model."]}),"\n",(0,l.jsx)(n.p,{children:"The figure below provides a comparison of the standard model logging process and the Models from Code feature for models that are eligible to be\nsaved using the Models from Code feature:"}),"\n",(0,l.jsx)("div",{class:"center-div",style:{width:"60%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"Models from Code",src:o(65).A+"",width:"1821",height:"1299"})})}),"\n",(0,l.jsxs)(n.p,{children:["For example, defining a model in a separate file named ",(0,l.jsx)(n.code,{children:"my_model.py"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\nfrom mlflow.models import set_model\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\n# Define the custom PythonModel instance that will be used for inference\nset_model(MyModel())\n"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["The Models from code feature does not support capturing import statements that are from external file references. If you have dependencies that\nare not captured via a ",(0,l.jsx)(n.code,{children:"pip"})," install, dependencies will need to be included and resolved via appropriate absolute path import references from\nusing the ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/model/dependencies.html#saving-extra-code-with-an-mlflow-model-manual-declaration",children:"code_paths feature"}),".\nFor simplicity's sake, it is recommended to encapsulate all of your required local dependencies for a model defined from code within the same\npython script file due to limitations around ",(0,l.jsx)(n.code,{children:"code_paths"})," dependency pathing resolution."]})}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:["When defining a model from code and using the ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API, the code that is defined in the script that is being logged\nwill be executed internally to ensure that it is valid code. If you have connections to external services within your script (e.g. you are connecting\nto a GenAI service within LangChain), be aware that you will incur a connection request to that service when the model is being logged."]})}),"\n",(0,l.jsx)(n.p,{children:"Then, logging the model from the file path in a different python script:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmodel_path = "my_model.py"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=model_path,  # Define the model as the path to the Python file\n        name="my_model",\n    )\n\n# Loading the model behaves exactly as if an instance of MyModel had been logged\nmy_model = mlflow.pyfunc.load_model(model_info.model_uri)\n'})}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API is ",(0,l.jsx)(n.strong,{children:"not threadsafe"}),". Do not attempt to use this feature if you are logging models concurrently\nfrom multiple threads. This fluent API utilizes a global active model state that has no consistency guarantees. If you are interested in threadsafe\nlogging APIs, please use the ",(0,l.jsx)(s.B,{fn:"mlflow.client.MlflowClient",children:"mlflow.client.MlflowClient"})," APIs for logging models."]})}),"\n",(0,l.jsx)(n.h2,{id:"models_built-in-model-flavors",children:"Built-In Model Flavors"}),"\n",(0,l.jsx)(n.p,{children:"MLflow provides several standard flavors that might be useful in your applications. Specifically,\nmany of its deployment tools support these flavors, so you can export your own model in one of these\nflavors to benefit from all these tools:"}),"\n",(0,l.jsx)(a.A,{toc:f.slice(f.findIndex((e=>"models_built-in-model-flavors"===e.id))+1,f.findIndex((e=>"model-evaluation"===e.id)))}),"\n",(0,l.jsxs)(n.h3,{id:"pyfunc-model-flavor",children:["Python Function (",(0,l.jsx)(n.code,{children:"python_function"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"python_function"})," model flavor serves as a default model interface for MLflow Python models.\nAny MLflow Python model is expected to be loadable as a ",(0,l.jsx)(n.code,{children:"python_function"})," model. This enables\nother MLflow tools to work with any python model regardless of which persistence module or\nframework was used to produce the model. This interoperability is very powerful because it allows\nany Python model to be productionized in a variety of environments."]}),"\n",(0,l.jsxs)(n.p,{children:["In addition, the ",(0,l.jsx)(n.code,{children:"python_function"})," model flavor defines a generic\nfilesystem ",(0,l.jsx)(r.A,{to:"/api_reference/python_api/mlflow.pyfunc.html#pyfunc-filesystem-format",target:"_blank",children:"model format"}),"\nfor Python models and provides utilities for saving and loading models\nto and from this format. The format is self-contained in the sense that it includes all the\ninformation necessary to load and use a model. Dependencies are stored either directly with the\nmodel or referenced via conda environment. This model format allows other tools to integrate\ntheir models with MLflow."]}),"\n",(0,l.jsx)(n.h4,{id:"how-to-save-model-as-python-function",children:"How To Save Model As Python Function"}),"\n",(0,l.jsxs)(n.p,{children:["Most ",(0,l.jsx)(n.code,{children:"python_function"})," models are saved as part of other model flavors - for example, all mlflow\nbuilt-in flavors include the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor in the exported models. In addition,\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:"mlflow.pyfunc"})," module defines functions for creating ",(0,l.jsx)(n.code,{children:"python_function"})," models explicitly.\nThis module also includes utilities for creating custom Python models, which is a convenient way of\nadding custom python code to ML models. For more information, see the ",(0,l.jsx)(n.a,{href:"#custom-python-models",children:"custom Python models\ndocumentation"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For information on how to store a custom model from a python script (models from code functionality),\nsee the ",(0,l.jsx)(n.a,{href:"/ml/model/models-from-code",children:"guide to models from code"})," for the recommended approaches."]}),"\n",(0,l.jsx)(n.h4,{id:"how-to-load-and-score-python-function-models",children:"How To Load And Score Python Function Models"}),"\n",(0,l.jsx)(n.h5,{id:"loading-models",children:"Loading Models"}),"\n",(0,l.jsxs)(n.p,{children:["You can load ",(0,l.jsx)(n.code,{children:"python_function"})," models in Python by using the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"})," function. It is important\nto note that ",(0,l.jsx)(n.code,{children:"load_model"})," assumes all dependencies are already available and ",(0,l.jsx)(n.em,{children:"will not"})," perform any checks or installations\nof dependencies. For deployment options that handle dependencies, refer to the ",(0,l.jsx)(n.a,{href:"#built-in-deployment",children:"model deployment section"}),"."]}),"\n",(0,l.jsx)(n.h5,{id:"scoring-models",children:"Scoring Models"}),"\n",(0,l.jsx)(n.p,{children:"Once a model is loaded, it can be scored in two primary ways:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Synchronous Scoring"}),"\nThe standard method for scoring is using the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.PyFuncModel.predict",children:(0,l.jsx)(n.code,{children:"predict"})})," method, which supports various\ninput types and returns a scalar or collection based on the input data. The method signature is:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"predict(data: Union[pandas.Series, pandas.DataFrame, numpy.ndarray, csc_matrix, csr_matrix, List[Any], Dict[str, Any], str],\n        params: Optional[Dict[str, Any]] = None) \u2192 Union[pandas.Series, pandas.DataFrame, numpy.ndarray, list, str]\n"})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Synchronous Streaming Scoring"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"predict_stream"})," is a new interface that was added to MLflow in the 2.12.2 release. Previous versions of MLflow will not support this interface.\nIn order to utilize ",(0,l.jsx)(n.code,{children:"predict_stream"})," in a custom Python Function Model, you must implement the ",(0,l.jsx)(n.code,{children:"predict_stream"})," method in your model class and\nreturn a generator type."]})}),"\n",(0,l.jsxs)(n.p,{children:["For models that support streaming data processing, ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.PyFuncModel.predict_stream",children:"predict_stream"}),"\nmethod is available. This method returns a",(0,l.jsx)(n.code,{children:"generator"}),", which yields a stream of responses, allowing for efficient processing of\nlarge datasets or continuous data streams. Note that the ",(0,l.jsx)(n.code,{children:"predict_stream"})," method is not available for all model types.\nThe usage involves iterating over the generator to consume the responses:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"predict_stream(data: Any, params: Optional[Dict[str, Any]] = None) \u2192 GeneratorType\n"})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h5,{id:"demonstrating-predict_stream",children:["Demonstrating ",(0,l.jsx)(n.code,{children:"predict_stream()"})]}),"\n",(0,l.jsxs)(n.p,{children:["Below is an example demonstrating how to define, save, load, and use a streamable model with the ",(0,l.jsx)(n.code,{children:"predict_stream()"})," method:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\n\n# Define a custom model that supports streaming\nclass StreamableModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        # Regular predict method implementation (optional for this demo)\n        return "regular-predict-output"\n\n    def predict_stream(self, context, model_input, params=None):\n        # Yielding elements one at a time\n        for element in ["a", "b", "c", "d", "e"]:\n            yield element\n\n\n# Save the model to a directory\ntmp_path = "/tmp/test_model"\npyfunc_model_path = os.path.join(tmp_path, "pyfunc_model")\npython_model = StreamableModel()\nmlflow.pyfunc.save_model(path=pyfunc_model_path, python_model=python_model)\n\n# Load the model\nloaded_pyfunc_model = mlflow.pyfunc.load_model(model_uri=pyfunc_model_path)\n\n# Use predict_stream to get a generator\nstream_output = loaded_pyfunc_model.predict_stream("single-input")\n\n# Consuming the generator using next\nprint(next(stream_output))  # Output: \'a\'\nprint(next(stream_output))  # Output: \'b\'\n\n# Alternatively, consuming the generator using a for-loop\nfor response in stream_output:\n    print(response)  # This will print \'c\', \'d\', \'e\'\n'})}),"\n",(0,l.jsx)(n.h4,{id:"python-function-model-interfaces",children:"Python Function Model Interfaces"}),"\n",(0,l.jsxs)(n.p,{children:["All PyFunc models will support ",(0,l.jsx)(n.code,{children:"pandas.DataFrame"})," as an input. In addition to ",(0,l.jsx)(n.code,{children:"pandas.DataFrame"}),",\nDL PyFunc models will also support tensor inputs in the form of ",(0,l.jsx)(n.code,{children:"numpy.ndarrays"}),". To verify\nwhether a model flavor supports tensor inputs, please check the flavor's documentation."]}),"\n",(0,l.jsxs)(n.p,{children:["For models with a column-based schema, inputs are typically provided in the form of a ",(0,l.jsx)(n.code,{children:"pandas.DataFrame"}),".\nIf a dictionary mapping column name to values is provided as input for schemas with named columns or if a\npython ",(0,l.jsx)(n.code,{children:"List"})," or a ",(0,l.jsx)(n.code,{children:"numpy.ndarray"})," is provided as input for schemas with unnamed columns, MLflow will cast the\ninput to a DataFrame. Schema enforcement and casting with respect to the expected data types is performed against\nthe DataFrame."]}),"\n",(0,l.jsxs)(n.p,{children:["For models with a tensor-based schema, inputs are typically provided in the form of a ",(0,l.jsx)(n.code,{children:"numpy.ndarray"})," or a\ndictionary mapping the tensor name to its np.ndarray value. Schema enforcement will check the provided input's\nshape and type against the shape and type specified in the model's schema and throw an error if they do not match."]}),"\n",(0,l.jsx)(n.p,{children:"For models where no schema is defined, no changes to the model inputs and outputs are made. MLflow will\npropagate any errors raised by the model if the model does not accept the provided input type."}),"\n",(0,l.jsxs)(n.p,{children:["The python environment that a PyFunc model is loaded into for prediction or inference may differ from the environment\nin which it was trained. In the case of an environment mismatch, a warning message will be printed when\ncalling ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),". This warning statement will identify the packages that have a version mismatch\nbetween those used during training and the current environment. In order to get the full dependencies of the\nenvironment in which the model was trained, you can call ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.get_model_dependencies"}),".\nFurthermore, if you want to run model inference in the same environment used in model training, you can\ncall ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"})," with the ",(0,l.jsx)(n.code,{children:"env_manager"}),' argument set as "conda". This will generate the environment\nfrom the ',(0,l.jsx)(n.code,{children:"conda.yaml"})," file, ensuring that the python UDF will execute with the exact package versions that were used\nduring training."]}),"\n",(0,l.jsx)(n.p,{children:"Some PyFunc models may accept model load configuration, which controls how the model is loaded and predictions\ncomputed. You can learn which configuration the model supports by inspecting the model's flavor metadata:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"model_info = mlflow.models.get_model_info(model_uri)\nmodel_info.flavors[mlflow.pyfunc.FLAVOR_NAME][mlflow.pyfunc.MODEL_CONFIG]\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Alternatively, you can load the PyFunc model and inspect the ",(0,l.jsx)(n.code,{children:"model_config"})," property:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"pyfunc_model = mlflow.pyfunc.load_model(model_uri)\npyfunc_model.model_config\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Model configuration can be changed at loading time by indicating ",(0,l.jsx)(n.code,{children:"model_config"})," parameter in\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"})," method:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"pyfunc_model = mlflow.pyfunc.load_model(model_uri, model_config=dict(temperature=0.93))\n"})}),"\n",(0,l.jsx)(n.p,{children:"When a model configuration value is changed, those values the configuration the model was saved with. Indicating an\ninvalid model configuration key for a model results in that configuration being ignored. A warning is displayed mentioning\nthe ignored entries."}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Model configuration vs parameters with default values in signatures:"})," Use model configuration when you need to provide\nmodel publishers for a way to change how the model is loaded into memory and how predictions are computed for all the\nsamples. For instance, a key like ",(0,l.jsx)(n.code,{children:"user_gpu"}),". Model consumers are not able to change those values at predict time. Use\nparameters with default values in the signature to provide a users the ability to change how predictions are computed on\neach data sample."]})}),"\n",(0,l.jsxs)(n.h3,{id:"r-function-crate",children:["R Function (",(0,l.jsx)(n.code,{children:"crate"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"crate"})," model flavor defines a generic model format for representing an arbitrary R prediction\nfunction as an MLflow model using the ",(0,l.jsx)(n.code,{children:"crate"})," function from the\n",(0,l.jsx)(n.a,{href:"https://github.com/r-lib/carrier",children:"carrier"})," package. The prediction function is expected to take a dataframe as input and\nproduce a dataframe, a vector or a list with the predictions as output."]}),"\n",(0,l.jsx)(n.p,{children:"This flavor requires R to be installed in order to be used."}),"\n",(0,l.jsxs)(n.h4,{id:"crate-usage",children:[(0,l.jsx)(n.code,{children:"crate"})," usage"]}),"\n",(0,l.jsx)(n.p,{children:"For a minimal crate model, an example configuration for the predict function is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-r",children:'library(mlflow)\nlibrary(carrier)\n# Load iris dataset\ndata("iris")\n\n# Learn simple linear regression model\nmodel <- lm(Sepal.Width~Sepal.Length, data = iris)\n\n# Define a crate model\n# call package functions with an explicit :: namespace.\ncrate_model <- crate(\n  function(new_obs)  stats::predict(model, data.frame("Sepal.Length" = new_obs)),\n  model = model\n)\n\n# log the model\nmodel_path <- mlflow_log_model(model = crate_model, artifact_path = "iris_prediction")\n\n# load the logged model and make a prediction\nmodel_uri <- paste0(mlflow_get_run()$artifact_uri, "/iris_prediction")\nmlflow_model <- mlflow_load_model(model_uri = model_uri,\n                                  flavor = NULL,\n                                  client = mlflow_client())\n\nprediction <- mlflow_predict(model = mlflow_model, data = 5)\nprint(prediction)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"h2o-h2o",children:["H",(0,l.jsx)("sub",{children:"2"}),"O (",(0,l.jsx)(n.code,{children:"h2o"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"h2o"})," model flavor enables logging and loading H2O models."]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.h2o",children:"mlflow.h2o"})," module defines ",(0,l.jsx)(s.B,{fn:"mlflow.h2o.save_model",children:"save_model()"}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.h2o.log_model",children:"log_model()"})," methods in python,\nand ",(0,l.jsx)(r.A,{to:(0,d.Ay)("/api_reference/R-api.html#mlflow-save-model-h2o"),target:"_blank",children:"mlflow_save_model"}),"\nand ",(0,l.jsx)(r.A,{to:(0,d.Ay)("/api_reference/R-api.html#mlflow-log-model"),target:"_blank",children:"mlflow_log_model"})," in R for saving H2O\nmodels in MLflow Model format. These methods produce MLflow Models with the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor, allowing you to load them\nas generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can be scored with only DataFrame input. When you load\nMLflow Models with the ",(0,l.jsx)(n.code,{children:"h2o"})," flavor using ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),",\nthe ",(0,l.jsx)(n.a,{href:"http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.init",children:"h2o.init()"})," method is\ncalled. Therefore, the correct version of ",(0,l.jsx)(n.code,{children:"h2o(-py)"})," must be installed in the loader's\nenvironment. You can customize the arguments given to\n",(0,l.jsx)(n.a,{href:"http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.init",children:"h2o.init()"})," by modifying the\n",(0,l.jsx)(n.code,{children:"init"})," entry of the persisted H2O model's YAML configuration file: ",(0,l.jsx)(n.code,{children:"model.h2o/h2o.yaml"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["Finally, you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.h2o.load_model"})," method to load MLflow Models with the\n",(0,l.jsx)(n.code,{children:"h2o"})," flavor as H2O model objects."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.h2o",children:"mlflow.h2o"}),"."]}),"\n",(0,l.jsx)(n.h4,{id:"h2o-pyfunc-usage",children:"h2o pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"For a minimal h2o model, here is an example of the pyfunc predict() method in a classification scenario:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport h2o\n\nh2o.init()\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n# import the prostate data\ndf = h2o.import_file(\n    "http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv.zip"\n)\n\n# convert the columns to factors\ndf["CAPSULE"] = df["CAPSULE"].asfactor()\ndf["RACE"] = df["RACE"].asfactor()\ndf["DCAPS"] = df["DCAPS"].asfactor()\ndf["DPROS"] = df["DPROS"].asfactor()\n\n# split the data\ntrain, test, valid = df.split_frame(ratios=[0.7, 0.15])\n\n# generate a GLM model\nglm_classifier = H2OGeneralizedLinearEstimator(\n    family="binomial", lambda_=0, alpha=0.5, nfolds=5, compute_p_values=True\n)\n\nwith mlflow.start_run():\n    glm_classifier.train(\n        y="CAPSULE", x=["AGE", "RACE", "VOL", "GLEASON"], training_frame=train\n    )\n    metrics = glm_classifier.model_performance()\n    metrics_to_track = ["MSE", "RMSE", "r2", "logloss"]\n    metrics_to_log = {\n        key: value\n        for key, value in metrics._metric_json.items()\n        if key in metrics_to_track\n    }\n    params = glm_classifier.params\n    mlflow.log_params(params)\n    mlflow.log_metrics(metrics_to_log)\n    model_info = mlflow.h2o.log_model(glm_classifier, name="h2o_model_info")\n\n# load h2o model and make a prediction\nh2o_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\ntest_df = test.as_data_frame()\npredictions = h2o_pyfunc.predict(test_df)\nprint(predictions)\n\n# it is also possible to load the model and predict using h2o methods on the h2o frame\n\n# h2o_model = mlflow.h2o.load_model(model_info.model_uri)\n# predictions = h2o_model.predict(test)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"tf-keras-example",children:["Keras (",(0,l.jsx)(n.code,{children:"keras"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for using the ",(0,l.jsx)(n.code,{children:"keras"})," flavor ",(0,l.jsx)(n.a,{href:"/ml/deep-learning/keras/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"pytorch-pytorch",children:["PyTorch (",(0,l.jsx)(n.code,{children:"pytorch"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for using the ",(0,l.jsx)(n.code,{children:"pytorch"})," flavor ",(0,l.jsx)(n.a,{href:"/ml/deep-learning/pytorch/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch",children:"mlflow.pytorch"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"scikit-learn-sklearn",children:["Scikit-learn (",(0,l.jsx)(n.code,{children:"sklearn"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for using the ",(0,l.jsx)(n.code,{children:"sklearn"})," flavor ",(0,l.jsx)(n.a,{href:"/ml/traditional-ml/sklearn/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For API information, see ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:"mlflow.sklearn"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"spark-mllib-spark",children:["Spark MLlib (",(0,l.jsx)(n.code,{children:"spark"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for using the ",(0,l.jsx)(n.code,{children:"spark"})," flavor ",(0,l.jsx)(n.a,{href:"/ml/traditional-ml/sparkml/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.spark",children:"mlflow.spark"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"tensorflow-tensorflow",children:["TensorFlow (",(0,l.jsx)(n.code,{children:"tensorflow"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for the ",(0,l.jsx)(n.code,{children:"tensorflow"})," integration ",(0,l.jsx)(n.a,{href:"/ml/deep-learning/tensorflow/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"onnx-onnx",children:["ONNX (",(0,l.jsx)(n.code,{children:"onnx"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"onnx"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"http://onnx.ai/",children:"ONNX models"})," in MLflow format via\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.log_model"})," methods. These\nmethods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can be scored with both DataFrame input and numpy array input. The ",(0,l.jsx)(n.code,{children:"python_function"}),"\nrepresentation of an MLflow ONNX model uses the ",(0,l.jsx)(n.a,{href:"https://github.com/microsoft/onnxruntime",children:"ONNX Runtime execution engine"}),"\nfor evaluation. Finally, you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.load_model"})," method to load MLflow\nModels with the ",(0,l.jsx)(n.code,{children:"onnx"})," flavor in native ONNX format."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.onnx",children:(0,l.jsx)(n.code,{children:"mlflow.onnx"})})," and ",(0,l.jsx)(n.a,{href:"http://onnx.ai/",children:"http://onnx.ai/"}),"."]}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["The default behavior for saving ONNX files is to use the ONNX save option ",(0,l.jsx)(n.code,{children:"save_as_external_data=True"}),"\nin order to support model files that are ",(0,l.jsx)(n.strong,{children:"in excess of 2GB"}),". For edge deployments of small model files, this\nmay create issues. If you need to save a small model as a single file for such deployment considerations,\nyou can set the parameter ",(0,l.jsx)(n.code,{children:"save_as_external_data=False"})," in either ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.save_model"}),"\nor ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.log_model"})," to force the serialization of the model as a small file. Note that if the\nmodel is in excess of 2GB, ",(0,l.jsx)(n.strong,{children:"saving as a single file will not work"}),"."]})}),"\n",(0,l.jsx)(n.h4,{id:"onnx-pyfunc-usage-example",children:"ONNX pyfunc usage example"}),"\n",(0,l.jsx)(n.p,{children:"For an ONNX model, an example configuration that uses pytorch to train a dummy model,\nconverts it to ONNX, logs to mlflow and makes a prediction using pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport mlflow\nfrom mlflow.models import infer_signature\nimport onnx\nimport torch\nfrom torch import nn\n\n# define a torch model\nnet = nn.Linear(6, 1)\nloss_function = nn.L1Loss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n\nX = torch.randn(6)\ny = torch.randn(1)\n\n# run model training\nepochs = 5\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(X)\n\n    loss = loss_function(outputs, y)\n    loss.backward()\n\n    optimizer.step()\n\n# convert model to ONNX and load it\ntorch.onnx.export(net, X, "model.onnx")\nonnx_model = onnx.load_model("model.onnx")\n\n# log the model into a mlflow run\nwith mlflow.start_run():\n    signature = infer_signature(X.numpy(), net(X).detach().numpy())\n    model_info = mlflow.onnx.log_model(onnx_model, name="model", signature=signature)\n\n# load the logged model and make a prediction\nonnx_pyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\n\npredictions = onnx_pyfunc.predict(X.numpy())\nprint(predictions)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"xgboost-xgboost",children:["XGBoost (",(0,l.jsx)(n.code,{children:"xgboost"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for the ",(0,l.jsx)(n.code,{children:"xgboost"})," integration ",(0,l.jsx)(n.a,{href:"/ml/traditional-ml/xgboost/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.xgboost",children:"mlflow.xgboost"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"lightgbm-lightgbm",children:["LightGBM (",(0,l.jsx)(n.code,{children:"lightgbm"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"lightgbm"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm-booster",children:"LightGBM models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"lightgbm"}),"\nmodel flavor in native LightGBM format."]}),"\n",(0,l.jsxs)(n.p,{children:["Note that the scikit-learn API for LightGBM is now supported. For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm",children:(0,l.jsx)(n.code,{children:"mlflow.lightgbm"})}),"."]}),"\n",(0,l.jsxs)(n.h4,{id:"lightgbm-pyfunc-usage",children:[(0,l.jsx)(n.code,{children:"LightGBM"})," pyfunc usage"]}),"\n",(0,l.jsx)(n.p,{children:"The example below"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Loads the IRIS dataset from ",(0,l.jsx)(n.code,{children:"scikit-learn"})]}),"\n",(0,l.jsxs)(n.li,{children:["Trains a LightGBM ",(0,l.jsx)(n.code,{children:"LGBMClassifier"})]}),"\n",(0,l.jsxs)(n.li,{children:["Logs the model and feature importance's using ",(0,l.jsx)(n.code,{children:"mlflow"})]}),"\n",(0,l.jsx)(n.li,{children:"Loads the logged model and makes predictions"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from lightgbm import LGBMClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport mlflow\nfrom mlflow.models import infer_signature\n\ndata = load_iris()\n\n# Remove special characters from feature names to be able to use them as keys for mlflow metrics\nfeature_names = [\n    name.replace(" ", "_").replace("(", "").replace(")", "")\n    for name in data["feature_names"]\n]\nX_train, X_test, y_train, y_test = train_test_split(\n    data["data"], data["target"], test_size=0.2\n)\n# create model instance\nlgb_classifier = LGBMClassifier(\n    n_estimators=10,\n    max_depth=3,\n    learning_rate=1,\n    objective="binary:logistic",\n    random_state=123,\n)\n\n# Fit and save model and LGBMClassifier feature importances as mlflow metrics\nwith mlflow.start_run():\n    lgb_classifier.fit(X_train, y_train)\n    feature_importances = dict(zip(feature_names, lgb_classifier.feature_importances_))\n    feature_importance_metrics = {\n        f"feature_importance_{feature_name}": imp_value\n        for feature_name, imp_value in feature_importances.items()\n    }\n    mlflow.log_metrics(feature_importance_metrics)\n    signature = infer_signature(X_train, lgb_classifier.predict(X_train))\n    model_info = mlflow.lightgbm.log_model(\n        lgb_classifier, name="iris-classifier", signature=signature\n    )\n\n# Load saved model and make predictions\nlgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri)\ny_pred = lgb_classifier_saved.predict(X_test)\nprint(y_pred)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"catboost-catboost",children:["CatBoost (",(0,l.jsx)(n.code,{children:"catboost"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"catboost"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://catboost.ai/docs/concepts/python-reference_catboost.html",children:"CatBoost models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.catboost.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.catboost.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.catboost.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"catboost"}),"\nmodel flavor in native CatBoost format."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.catboost",children:(0,l.jsx)(n.code,{children:"mlflow.catboost"})}),"."]}),"\n",(0,l.jsxs)(n.h4,{id:"catboost-pyfunc-usage",children:[(0,l.jsx)(n.code,{children:"CatBoost"})," pyfunc usage"]}),"\n",(0,l.jsx)(n.p,{children:"For a CatBoost Classifier model, an example configuration for the pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import infer_signature\nfrom catboost import CatBoostClassifier\nfrom sklearn import datasets\n\n# prepare data\nX, y = datasets.load_wine(as_frame=False, return_X_y=True)\n\n# train the model\nmodel = CatBoostClassifier(\n    iterations=5,\n    loss_function="MultiClass",\n    allow_writing_files=False,\n)\nmodel.fit(X, y)\n\n# create model signature\npredictions = model.predict(X)\nsignature = infer_signature(X, predictions)\n\n# log the model into a mlflow run\nwith mlflow.start_run():\n    model_info = mlflow.catboost.log_model(model, name="model", signature=signature)\n\n# load the logged model and make a prediction\ncatboost_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\nprint(catboost_pyfunc.predict(X[:5]))\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"spacyspacy",children:["Spacy(",(0,l.jsx)(n.code,{children:"spaCy"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for the ",(0,l.jsx)(n.code,{children:"spaCy"})," integration ",(0,l.jsx)(n.a,{href:"/ml/deep-learning/spacy/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"statsmodels-statsmodels",children:["Statsmodels (",(0,l.jsx)(n.code,{children:"statsmodels"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"statsmodels"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://www.statsmodels.org/stable/api.html",children:"Statsmodels models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels.save_model"}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels.log_model"})," methods. These methods also add the ",(0,l.jsx)(n.code,{children:"python_function"}),"\nflavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python\nfunctions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),". This loaded PyFunc model can only\nbe scored with DataFrame input. You can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels.load_model"}),"\nmethod to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"statsmodels"})," model flavor in native statsmodels format."]}),"\n",(0,l.jsxs)(n.p,{children:["As for now, automatic logging is restricted to parameters, metrics and models generated by a call to ",(0,l.jsx)(n.code,{children:"fit"}),"\non a ",(0,l.jsx)(n.code,{children:"statsmodels"})," model."]}),"\n",(0,l.jsx)(n.h4,{id:"statsmodels-pyfunc-usage",children:"Statsmodels pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"The following 2 examples illustrate usage of a basic regression model (OLS) and an ARIMA time series model\nfrom the following statsmodels apis : statsmodels.formula.api and statsmodels.tsa.api"}),"\n",(0,l.jsx)(n.p,{children:"For a minimal statsmodels regression model, here is an example of the pyfunc predict() method:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nimport statsmodels.formula.api as smf\n\n# load the diabetes dataset from sklearn\ndiabetes = load_diabetes()\n\n# create X and y dataframes for the features and target\nX = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(data=diabetes.target, columns=["target"])\n\n# concatenate X and y dataframes\ndf = pd.concat([X, y], axis=1)\n\n# create the linear regression model (ordinary least squares)\nmodel = smf.ols(\n    formula="target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6", data=df\n)\n\nmlflow.statsmodels.autolog(\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=False,\n    silent=False,\n    registered_model_name=None,\n)\n\nwith mlflow.start_run():\n    res = model.fit(method="pinv", use_t=True)\n    model_info = mlflow.statsmodels.log_model(res, name="OLS_model")\n\n# load the pyfunc model\nstatsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# generate predictions\npredictions = statsmodels_pyfunc.predict(X)\nprint(predictions)\n'})}),"\n",(0,l.jsx)(n.p,{children:"For a minimal time series ARIMA model, here is an example of the pyfunc predict() method :"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# create a time series dataset with seasonality\nnp.random.seed(0)\n\n# generate a time index with a daily frequency\ndates = pd.date_range(start="2022-12-01", end="2023-12-01", freq="D")\n\n# generate the seasonal component (weekly)\nseasonality = np.sin(np.arange(len(dates)) * (2 * np.pi / 365.25) * 7)\n\n# generate the trend component\ntrend = np.linspace(-5, 5, len(dates)) + 2 * np.sin(\n    np.arange(len(dates)) * (2 * np.pi / 365.25) * 0.1\n)\n\n# generate the residual component\nresiduals = np.random.normal(0, 1, len(dates))\n\n# generate the final time series by adding the components\ntime_series = seasonality + trend + residuals\n\n# create a dataframe from the time series\ndata = pd.DataFrame({"date": dates, "value": time_series})\ndata.set_index("date", inplace=True)\n\norder = (1, 0, 0)\n# create the ARIMA model\nmodel = ARIMA(data, order=order)\n\nmlflow.statsmodels.autolog(\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=False,\n    silent=False,\n    registered_model_name=None,\n)\n\nwith mlflow.start_run():\n    res = model.fit()\n    mlflow.log_params(\n        {\n            "order": order,\n            "trend": model.trend,\n            "seasonal_order": model.seasonal_order,\n        }\n    )\n    mlflow.log_params(res.params)\n    mlflow.log_metric("aic", res.aic)\n    mlflow.log_metric("bic", res.bic)\n    model_info = mlflow.statsmodels.log_model(res, name="ARIMA_model")\n\n# load the pyfunc model\nstatsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# prediction dataframes for a TimeSeriesModel must have exactly one row and include columns called start and end\nstart = pd.to_datetime("2024-01-01")\nend = pd.to_datetime("2024-01-07")\n\n# generate predictions\nprediction_data = pd.DataFrame({"start": start, "end": end}, index=[0])\npredictions = statsmodels_pyfunc.predict(prediction_data)\nprint(predictions)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels",children:(0,l.jsx)(n.code,{children:"mlflow.statsmodels"})}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"prophet-prophet",children:["Prophet (",(0,l.jsx)(n.code,{children:"prophet"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for the ",(0,l.jsx)(n.code,{children:"prophet"})," integration ",(0,l.jsx)(n.a,{href:"/ml/traditional-ml/prophet/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.prophet",children:(0,l.jsx)(n.code,{children:"mlflow.prophet"})}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"pmdarima-flavor",children:["Pmdarima (",(0,l.jsx)(n.code,{children:"pmdarima"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"pmdarima"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"http://alkaline-ml.com/pmdarima/",children:"pmdarima models"})," in MLflow\nformat via the ",(0,l.jsx)(s.B,{fn:"mlflow.pmdarima.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.pmdarima.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodel to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with a DataFrame input.\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.pmdarima.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"pmdarima"}),"\nmodel flavor in native pmdarima formats."]}),"\n",(0,l.jsxs)(n.p,{children:["The interface for utilizing a ",(0,l.jsx)(n.code,{children:"pmdarima"})," model loaded as a ",(0,l.jsx)(n.code,{children:"pyfunc"})," type for generating forecast predictions uses\na ",(0,l.jsx)(n.em,{children:"single-row"})," ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," configuration argument. The following columns in this configuration\n",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," are supported:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"n_periods"})," (required) - specifies the number of future periods to generate starting from the last datetime value\nof the training dataset, utilizing the frequency of the input training series when the model was trained\n(for example, if the training data series elements represent one value per hour, in order to forecast 3 days of\nfuture data, set the column ",(0,l.jsx)(n.code,{children:"n_periods"})," to ",(0,l.jsx)(n.code,{children:"72"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"X"})," (optional) - exogenous regressor values (",(0,l.jsx)(n.em,{children:"only supported in pmdarima version >= 1.8.0"}),") a 2D array of values for\nfuture time period events. For more information, read the underlying library\n",(0,l.jsx)(n.a,{href:"https://www.statsmodels.org/stable/endog_exog.html",children:"explanation"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"return_conf_int"})," (optional) - a boolean (Default: ",(0,l.jsx)(n.code,{children:"False"}),") for whether to return confidence interval values.\nSee above note."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"alpha"})," (optional) - the significance value for calculating confidence intervals. (Default: ",(0,l.jsx)(n.code,{children:"0.05"}),")"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["An example configuration for the ",(0,l.jsx)(n.code,{children:"pyfunc"})," predict of a ",(0,l.jsx)(n.code,{children:"pmdarima"})," model is shown below, with a future period\nprediction count of 100, a confidence interval calculation generation, no exogenous regressor elements, and a default\nalpha of ",(0,l.jsx)(n.code,{children:"0.05"}),":"]}),"\n",(0,l.jsxs)(c.X,{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Index"}),(0,l.jsx)("th",{children:"n-periods"}),(0,l.jsx)("th",{children:"return_conf_int"})]})}),(0,l.jsx)("tbody",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"0"}),(0,l.jsx)("td",{children:"100"}),(0,l.jsx)("td",{children:"True"})]})})]}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," passed to a ",(0,l.jsx)(n.code,{children:"pmdarima"})," ",(0,l.jsx)(n.code,{children:"pyfunc"})," flavor must only contain 1 row."]})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When predicting a ",(0,l.jsx)(n.code,{children:"pmdarima"})," flavor, the ",(0,l.jsx)(n.code,{children:"predict"})," method's ",(0,l.jsx)(n.code,{children:"DataFrame"})," configuration column\n",(0,l.jsx)(n.code,{children:"return_conf_int"}),"'s value controls the output format. When the column's value is set to ",(0,l.jsx)(n.code,{children:"False"})," or ",(0,l.jsx)(n.code,{children:"None"}),"\n(which is the default if this column is not supplied in the configuration ",(0,l.jsx)(n.code,{children:"DataFrame"}),"), the schema of the\nreturned ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," is a single column: ",(0,l.jsx)(n.code,{children:'["yhat"]'}),". When set to ",(0,l.jsx)(n.code,{children:"True"}),", the schema of the returned\n",(0,l.jsx)(n.code,{children:"DataFrame"})," is: ",(0,l.jsx)(n.code,{children:'["yhat", "yhat_lower", "yhat_upper"]'})," with the respective lower (",(0,l.jsx)(n.code,{children:"yhat_lower"}),") and\nupper (",(0,l.jsx)(n.code,{children:"yhat_upper"}),") confidence intervals added to the forecast predictions (",(0,l.jsx)(n.code,{children:"yhat"}),")."]})}),"\n",(0,l.jsx)(n.p,{children:"Example usage of pmdarima artifact loaded as a pyfunc with confidence intervals calculated:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import pmdarima\nimport mlflow\nimport pandas as pd\n\ndata = pmdarima.datasets.load_airpassengers()\n\nwith mlflow.start_run():\n    model = pmdarima.auto_arima(data, seasonal=True)\n    mlflow.pmdarima.save_model(model, "/tmp/model.pmd")\n\nloaded_pyfunc = mlflow.pyfunc.load_model("/tmp/model.pmd")\n\nprediction_conf = pd.DataFrame(\n    [{"n_periods": 4, "return_conf_int": True, "alpha": 0.1}]\n)\n\npredictions = loaded_pyfunc.predict(prediction_conf)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Output (",(0,l.jsx)(n.code,{children:"Pandas DataFrame"}),"):"]}),"\n",(0,l.jsxs)(c.X,{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Index"}),(0,l.jsx)("th",{children:"yhat"}),(0,l.jsx)("th",{children:"yhat_lower"}),(0,l.jsx)("th",{children:"yhat_upper"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"0"}),(0,l.jsx)("td",{children:"467.573731"}),(0,l.jsx)("td",{children:"423.30995"}),(0,l.jsx)("td",{children:"511.83751"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"1"}),(0,l.jsx)("td",{children:"490.494467"}),(0,l.jsx)("td",{children:"416.17449"}),(0,l.jsx)("td",{children:"564.81444"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2"}),(0,l.jsx)("td",{children:"509.138684"}),(0,l.jsx)("td",{children:"420.56255"}),(0,l.jsx)("td",{children:"597.71117"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"3"}),(0,l.jsx)("td",{children:"492.554714"}),(0,l.jsx)("td",{children:"397.30634"}),(0,l.jsx)("td",{children:"587.80309"})]})]})]}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["Signature logging for ",(0,l.jsx)(n.code,{children:"pmdarima"})," will not function correctly if ",(0,l.jsx)(n.code,{children:"return_conf_int"})," is set to ",(0,l.jsx)(n.code,{children:"True"})," from\na non-pyfunc artifact. The output of the native ",(0,l.jsx)(n.code,{children:"ARIMA.predict()"})," when returning confidence intervals is not\na recognized signature type."]})}),"\n",(0,l.jsxs)(n.h3,{id:"john-snow-labs-johnsnowlabs",children:["John Snow Labs (",(0,l.jsx)(n.code,{children:"johnsnowlabs"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"johnsnowlabs"})," model flavor gives you access to\n",(0,l.jsx)(n.a,{href:"https://nlp.johnsnowlabs.com/models",children:"20.000+ state-of-the-art enterprise NLP models in 200+ languages"}),"\nfor medical, finance, legal and many more domains."]}),"\n",(0,l.jsxs)(n.p,{children:["You can use ",(0,l.jsx)(s.B,{fn:"mlflow.johnsnowlabs.log_model"})," to log and export your model as"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.PyFuncModel",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc.PyFuncModel"})}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["This enables you to integrate ",(0,l.jsx)(n.a,{href:"https://nlp.johnsnowlabs.com/models",children:"any John Snow Labs model"}),"\ninto the MLflow framework. You can easily deploy your models for inference with MLflows serve functionalities.\nModels are interpreted as a generic Python function for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.johnsnowlabs.load_model"})," function to load a saved or logged MLflow\nModel with the ",(0,l.jsx)(n.code,{children:"johnsnowlabs"})," flavor from an stored artifact."]}),"\n",(0,l.jsxs)(n.p,{children:["Features include: LLM's, Text Summarization, Question Answering, Named Entity Recognition, Relation\nExtraction, Sentiment Analysis, Spell Checking, Image Classification, Automatic Speech Recognition and much more,\npowered by the latest Transformer Architectures. The models are provided by ",(0,l.jsx)(n.a,{href:"https://www.johnsnowlabs.com/",children:"John Snow Labs"}),"\nand requires a ",(0,l.jsx)(n.a,{href:"https://www.johnsnowlabs.com/",children:"John Snow Labs"})," Enterprise NLP License.\n",(0,l.jsx)(n.a,{href:"https://www.johnsnowlabs.com/schedule-a-demo/",children:"You can reach out to us"})," for a research or industry license."]}),"\n",(0,l.jsx)(n.p,{children:"Example: Export a John Snow Labs to MLflow format"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import json\nimport os\n\nimport pandas as pd\nfrom johnsnowlabs import nlp\n\nimport mlflow\nfrom mlflow.pyfunc import spark_udf\n\n# 1) Write your raw license.json string into the \'JOHNSNOWLABS_LICENSE_JSON\' env variable for MLflow\ncreds = {\n    "AWS_ACCESS_KEY_ID": "...",\n    "AWS_SECRET_ACCESS_KEY": "...",\n    "SPARK_NLP_LICENSE": "...",\n    "SECRET": "...",\n}\nos.environ["JOHNSNOWLABS_LICENSE_JSON"] = json.dumps(creds)\n\n# 2) Install enterprise libraries\nnlp.install()\n# 3) Start a Spark session with enterprise libraries\nspark = nlp.start()\n\n# 4) Load a model and test it\nnlu_model = "en.classify.bert_sequence.covid_sentiment"\nmodel_save_path = "my_model"\njohnsnowlabs_model = nlp.load(nlu_model)\njohnsnowlabs_model.predict(["I hate COVID,", "I love COVID"])\n\n# 5) Export model with pyfunc and johnsnowlabs flavors\nwith mlflow.start_run():\n    model_info = mlflow.johnsnowlabs.log_model(johnsnowlabs_model, name=model_save_path)\n\n# 6) Load model with johnsnowlabs flavor\nmlflow.johnsnowlabs.load_model(model_info.model_uri)\n\n# 7) Load model with pyfunc flavor\nmlflow.pyfunc.load_model(model_save_path)\n\npandas_df = pd.DataFrame({"text": ["Hello World"]})\nspark_df = spark.createDataFrame(pandas_df).coalesce(1)\npyfunc_udf = spark_udf(\n    spark=spark,\n    model_uri=model_save_path,\n    env_manager="virtualenv",\n    result_type="string",\n)\nnew_df = spark_df.withColumn("prediction", pyfunc_udf(*pandas_df.columns))\n\n# 9) You can now use the mlflow models serve command to serve the model see next section\n\n# 10)  You can also use x command to deploy model inside of a container see next section\n'})}),"\n",(0,l.jsx)(n.h4,{id:"to-deploy-the-john-snow-labs-model-as-a-container",children:"To deploy the John Snow Labs model as a container"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Start the Docker Container"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'docker run -p 5001:8080 -e JOHNSNOWLABS_LICENSE_JSON=your_json_string "mlflow-pyfunc"\n'})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Query server"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:5001/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_split": {\n      "columns": ["text"],\n      "data": [["I hate covid"], ["I love covid"]]\n  }\n}\'\n'})}),"\n",(0,l.jsx)(n.h4,{id:"to-deploy-the-john-snow-labs-model-without-a-container",children:"To deploy the John Snow Labs model without a container"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Export env variable and start server"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"export JOHNSNOWLABS_LICENSE_JSON=your_json_string\nmlflow models serve -m <model_uri>\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Query server"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:5000/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_split": {\n      "columns": ["text"],\n      "data": [["I hate covid"], ["I love covid"]]\n  }\n}\'\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"diviner-diviner",children:["Diviner (",(0,l.jsx)(n.code,{children:"diviner"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"diviner"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://databricks-diviner.readthedocs.io/en/latest/index.html",children:"diviner models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.diviner.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.diviner.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the model to be\ninterpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with a DataFrame input.\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.diviner.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"diviner"}),"\nmodel flavor in native diviner formats."]}),"\n",(0,l.jsx)(n.h4,{id:"diviner-types",children:"Diviner Types"}),"\n",(0,l.jsxs)(n.p,{children:["Diviner is a library that provides an orchestration framework for performing time series forecasting on groups of\nrelated series. Forecasting in ",(0,l.jsx)(n.code,{children:"diviner"})," is accomplished through wrapping popular open source libraries such as\n",(0,l.jsx)(n.a,{href:"https://facebook.github.io/prophet/",children:"prophet"})," and ",(0,l.jsx)(n.a,{href:"http://alkaline-ml.com/pmdarima/",children:"pmdarima"}),". The ",(0,l.jsx)(n.code,{children:"diviner"}),"\nlibrary offers a simplified set of APIs to simultaneously generate distinct time series forecasts for multiple data\ngroupings using a single input DataFrame and a unified high-level API."]}),"\n",(0,l.jsx)(n.h4,{id:"metrics-and-parameters-logging-for-diviner",children:"Metrics and Parameters logging for Diviner"}),"\n",(0,l.jsxs)(n.p,{children:["Unlike other flavors that are supported in MLflow, Diviner has the concept of grouped models. As a collection of many\n(perhaps thousands) of individual forecasting models, the burden to the tracking server to log individual metrics\nand parameters for each of these models is significant. For this reason, metrics and parameters are exposed for\nretrieval from Diviner's APIs as ",(0,l.jsx)(n.code,{children:"Pandas"})," ",(0,l.jsx)(n.code,{children:"DataFrames"}),", rather than discrete primitive values."]}),"\n",(0,l.jsx)(n.p,{children:"To illustrate, let us assume we are forecasting hourly electricity consumption from major cities around the world.\nA sample of our input data looks like this:"}),"\n",(0,l.jsxs)(c.X,{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"country"}),(0,l.jsx)("th",{children:"city"}),(0,l.jsx)("th",{children:"datetime"}),(0,l.jsx)("th",{children:"watts"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"NewYork"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"23568.9"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"NewYork"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"22331.7"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"Boston"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"14220.1"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"Boston"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"14183.4"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"CA"}),(0,l.jsx)("td",{children:"Toronto"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"18562.2"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"CA"}),(0,l.jsx)("td",{children:"Toronto"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"17681.6"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"MX"}),(0,l.jsx)("td",{children:"MexicoCity"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"19946.8"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"MX"}),(0,l.jsx)("td",{children:"MexicoCity"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"19444.0"})]})]})]}),"\n",(0,l.jsxs)(n.p,{children:["If we were to ",(0,l.jsx)(n.code,{children:"fit"})," a model on this data, supplying the grouping keys as:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'grouping_keys = ["country", "city"]\n'})}),"\n",(0,l.jsx)(n.p,{children:"We will have a model generated for each of the grouping keys that have been supplied:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'[("US", "NewYork"), ("US", "Boston"), ("CA", "Toronto"), ("MX", "MexicoCity")]\n'})}),"\n",(0,l.jsx)(n.p,{children:"With a model constructed for each of these, entering each of their metrics and parameters wouldn't be an issue for the\nMLflow tracking server. What would become a problem, however, is if we modeled each major city on the planet and ran\nthis forecasting scenario every day. If we were to adhere to the conditions of the World Bank, that would mean just\nover 10,000 models as of 2022. After a mere few weeks of running this forecasting every day we would have a very large\nmetrics table."}),"\n",(0,l.jsxs)(n.p,{children:["To eliminate this issue for large-scale forecasting, the metrics and parameters for ",(0,l.jsx)(n.code,{children:"diviner"})," are extracted as a\ngrouping key indexed ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"}),", as shown below for example (float values truncated for visibility):"]}),"\n",(0,l.jsxs)(c.X,{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"grouping_key_columns"}),(0,l.jsx)("th",{children:"country"}),(0,l.jsx)("th",{children:"city"}),(0,l.jsx)("th",{children:"mse"}),(0,l.jsx)("th",{children:"rmse"}),(0,l.jsx)("th",{children:"mae"}),(0,l.jsx)("th",{children:"mape"}),(0,l.jsx)("th",{children:"mdape"}),(0,l.jsx)("th",{children:"smape"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"CA"}),(0,l.jsx)("td",{children:"Toronto"}),(0,l.jsx)("td",{children:"8276851.6"}),(0,l.jsx)("td",{children:"2801.7"}),(0,l.jsx)("td",{children:"2417.7"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.159"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"MX"}),(0,l.jsx)("td",{children:"MexicoCity"}),(0,l.jsx)("td",{children:"3548872.4"}),(0,l.jsx)("td",{children:"1833.8"}),(0,l.jsx)("td",{children:"1584.5"}),(0,l.jsx)("td",{children:"0.15"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.159"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"NewYork"}),(0,l.jsx)("td",{children:"3167846.4"}),(0,l.jsx)("td",{children:"1732.4"}),(0,l.jsx)("td",{children:"1498.2"}),(0,l.jsx)("td",{children:"0.15"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.158"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"Boston"}),(0,l.jsx)("td",{children:"14082666.4"}),(0,l.jsx)("td",{children:"3653.2"}),(0,l.jsx)("td",{children:"3156.2"}),(0,l.jsx)("td",{children:"0.15"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.159"})]})]})]}),"\n",(0,l.jsxs)(n.p,{children:["There are two recommended means of logging the metrics and parameters from a ",(0,l.jsx)(n.code,{children:"diviner"})," model :"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Writing the DataFrames to local storage and using ",(0,l.jsx)(s.B,{fn:"mlflow.log_artifacts"})]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\nimport mlflow\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    params = model.extract_model_params()\n    metrics = model.cross_validate_and_score(\n        horizon="72 hours",\n        period="240 hours",\n        initial="480 hours",\n        parallel="threads",\n        rolling_window=0.1,\n        monthly=False,\n    )\n    params.to_csv(f"{tmpdir}/params.csv", index=False, header=True)\n    metrics.to_csv(f"{tmpdir}/metrics.csv", index=False, header=True)\n\n    mlflow.log_artifacts(tmpdir, artifact_path="data")\n'})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Writing directly as a JSON artifact using ",(0,l.jsx)(s.B,{fn:"mlflow.log_dict"})]}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["The parameters extract from ",(0,l.jsx)(n.code,{children:"diviner"})," models ",(0,l.jsx)(n.em,{children:"may require"})," casting (or dropping of columns) if using the\n",(0,l.jsx)(n.code,{children:"pd.DataFrame.to_dict()"})," approach due to the inability of this method to serialize objects."]})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nparams = model.extract_model_params()\nmetrics = model.cross_validate_and_score(\n    horizon="72 hours",\n    period="240 hours",\n    initial="480 hours",\n    parallel="threads",\n    rolling_window=0.1,\n    monthly=False,\n)\nparams["t_scale"] = params["t_scale"].astype(str)\nparams["start"] = params["start"].astype(str)\nparams = params.drop("stan_backend", axis=1)\n\nmlflow.log_dict(params.to_dict(), "params.json")\nmlflow.log_dict(metrics.to_dict(), "metrics.json")\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Logging of the model artifact is shown in the ",(0,l.jsx)(n.code,{children:"pyfunc"})," example below."]}),"\n",(0,l.jsx)(n.h4,{id:"diviner-pyfunc-usage",children:"Diviner pyfunc usage"}),"\n",(0,l.jsxs)(n.p,{children:["The MLflow Diviner flavor includes an implementation of the ",(0,l.jsx)(n.code,{children:"pyfunc"})," interface for Diviner models. To control\nprediction behavior, you can specify configuration arguments in the first row of a Pandas DataFrame input."]}),"\n",(0,l.jsxs)(n.p,{children:["As this configuration is dependent upon the underlying model type (i.e., the ",(0,l.jsx)(n.code,{children:"diviner.GroupedProphet.forecast()"}),"\nmethod has a different signature than does ",(0,l.jsx)(n.code,{children:"diviner.GroupedPmdarima.predict()"}),"), the Diviner pyfunc implementation\nattempts to coerce arguments to the types expected by the underlying model."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:['Diviner models support both "full group" and "partial group" forecasting. If a column named ',(0,l.jsx)(n.code,{children:'"groups"'})," is present\nin the configuration ",(0,l.jsx)(n.code,{children:"DataFrame"})," submitted to the ",(0,l.jsx)(n.code,{children:"pyfunc"})," flavor, the grouping key values in the first row\nwill be used to generate a subset of forecast predictions. This functionality removes the need to filter a subset\nfrom the full output of all groups forecasts if the results of only a few (or one) groups are needed."]})}),"\n",(0,l.jsxs)(n.p,{children:["For a ",(0,l.jsx)(n.code,{children:"GroupedPmdarima"})," model, an example configuration for the ",(0,l.jsx)(n.code,{children:"pyfunc"})," ",(0,l.jsx)(n.code,{children:"predict()"})," method is:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom pmdarima.arima.auto import AutoARIMA\nfrom diviner import GroupedPmdarima\n\nwith mlflow.start_run():\n    base_model = AutoARIMA(out_of_sample_size=96, maxiter=200)\n    model = GroupedPmdarima(model_template=base_model).fit(\n        df=df,\n        group_key_columns=["country", "city"],\n        y_col="watts",\n        datetime_col="datetime",\n        silence_warnings=True,\n    )\n\n    mlflow.diviner.save_model(diviner_model=model, path="/tmp/diviner_model")\n\ndiviner_pyfunc = mlflow.pyfunc.load_model(model_uri="/tmp/diviner_model")\n\npredict_conf = pd.DataFrame(\n    {\n        "n_periods": 120,\n        "groups": [\n            ("US", "NewYork"),\n            ("CA", "Toronto"),\n            ("MX", "MexicoCity"),\n        ],  # NB: List of tuples required.\n        "predict_col": "wattage_forecast",\n        "alpha": 0.1,\n        "return_conf_int": True,\n        "on_error": "warn",\n    },\n    index=[0],\n)\n\nsubset_forecasts = diviner_pyfunc.predict(predict_conf)\n'})}),"\n",(0,l.jsxs)(n.admonition,{type:"note",children:[(0,l.jsxs)(n.p,{children:["There are several instances in which a configuration ",(0,l.jsx)(n.code,{children:"DataFrame"})," submitted to the ",(0,l.jsx)(n.code,{children:"pyfunc"})," ",(0,l.jsx)(n.code,{children:"predict()"})," method\nwill cause an ",(0,l.jsx)(n.code,{children:"MlflowException"})," to be raised:"]}),(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["If neither ",(0,l.jsx)(n.code,{children:"horizon"})," or ",(0,l.jsx)(n.code,{children:"n_periods"})," are provided."]}),"\n",(0,l.jsxs)(n.li,{children:["The value of ",(0,l.jsx)(n.code,{children:"n_periods"})," or ",(0,l.jsx)(n.code,{children:"horizon"})," is not an integer."]}),"\n",(0,l.jsxs)(n.li,{children:["If the model is of type ",(0,l.jsx)(n.code,{children:"GroupedProphet"}),", ",(0,l.jsx)(n.code,{children:"frequency"})," as a string type must be provided."]}),"\n",(0,l.jsxs)(n.li,{children:["If both ",(0,l.jsx)(n.code,{children:"horizon"})," and ",(0,l.jsx)(n.code,{children:"n_periods"})," are provided with different values."]}),"\n"]})]}),"\n",(0,l.jsxs)(n.h3,{id:"transformers-transformers",children:["Transformers (",(0,l.jsx)(n.code,{children:"transformers"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide, including tutorials and detailed documentation for using the ",(0,l.jsx)(n.code,{children:"transformers"})," integration ",(0,l.jsx)(n.a,{href:"/ml/deep-learning/transformers/",children:"can be found here"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"sentencetransformers-sentence_transformers",children:["SentenceTransformers (",(0,l.jsx)(n.code,{children:"sentence_transformers"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide for the ",(0,l.jsx)(n.code,{children:"sentence-transformers"})," integration ",(0,l.jsx)(n.a,{href:"/ml/deep-learning/sentence-transformers/",children:"can be viewed here"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,l.jsxs)(n.p,{children:["The MLflow evaluation documentation has been relocated and can be ",(0,l.jsx)(n.a,{href:"/ml/evaluation",children:"found here"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"model-customization",children:"Model Customization"}),"\n",(0,l.jsxs)(n.p,{children:["While MLflow's built-in model persistence utilities are convenient for packaging models from various\npopular ML libraries in MLflow Model format, they do not cover every use case. For example, you may\nwant to use a model from an ML library that is not explicitly supported by MLflow's built-in\nflavors. Alternatively, you may want to package custom inference code and data to create an\nMLflow Model. Fortunately, MLflow provides two solutions that can be used to accomplish these\ntasks: ",(0,l.jsx)(n.a,{href:"#custom-python-models",children:"Custom Python Models"})," and ",(0,l.jsx)(n.a,{href:"#custom-flavors",children:"Custom Flavors"}),"."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"In this section:"})}),"\n",(0,l.jsx)(a.A,{toc:f.slice(f.findIndex((e=>"custom-python-models"===e.id)),f.findIndex((e=>"built-in-deployment"===e.id))),minHeadingLevel:3,maxHeadingLevel:4}),"\n",(0,l.jsx)(n.h3,{id:"custom-python-models",children:"Custom Python Models"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc"})})," module provides ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.save_model",children:(0,l.jsx)(n.code,{children:"save_model()"})}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.log_model",children:(0,l.jsx)(n.code,{children:"log_model()"})})," utilities for creating MLflow Models with the\n",(0,l.jsx)(n.code,{children:"python_function"})," flavor that contain user-specified code and ",(0,l.jsx)(n.em,{children:"artifact"})," (file) dependencies.\nThese artifact dependencies may include serialized models produced by any Python ML library."]}),"\n",(0,l.jsxs)(n.p,{children:["Because these custom models contain the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor, they can be deployed\nto any of MLflow's supported production environments, such as SageMaker, AzureML, or local\nREST endpoints."]}),"\n",(0,l.jsxs)(n.p,{children:["The following examples demonstrate how you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc"})}),"\nmodule to create custom Python models. For additional information about model customization with MLflow's\n",(0,l.jsx)(n.code,{children:"python_function"})," utilities, see the ",(0,l.jsx)(r.A,{target:"_blank",to:"/api_reference/python_api/mlflow.pyfunc.html#pyfunc-create-custom",children:"python_function custom models documentation"}),"."]}),"\n",(0,l.jsx)(n.h4,{id:"example-creating-a-model-with-type-hints",children:"Example: Creating a model with type hints"}),"\n",(0,l.jsxs)(n.p,{children:["This example demonstrates how to create a custom Python model with type hints, enabling MLflow to perform data\nvalidation based on the type hints specified for the model input. For additional information about\nPythonModel type hints support, see the ",(0,l.jsx)(n.a,{href:"../model/python_model#type-hint-usage-in-pythonmodel",children:"PythonModel Type Hints Guide"}),"."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"PythonModel with type hints supports data validation starting from MLflow version 2.20.0."})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import pydantic\nimport mlflow\nfrom mlflow.pyfunc import PythonModel\n\n\n# Define the pydantic model input\nclass Message(pydantic.BaseModel):\n    role: str\n    content: str\n\n\nclass CustomModel(PythonModel):\n    # Define the model_input type hint\n    # NB: it must be list[...], check the python model type hints guide for more information\n    def predict(self, model_input: list[Message], params=None) -> list[str]:\n        return [m.content for m in model_input]\n\n\n# Construct the model and test\nmodel = CustomModel()\n\n# The input_example can be a list of Message objects as defined in the type hint\ninput_example = [\n    Message(role="system", content="Hello"),\n    Message(role="user", content="Hi"),\n]\nassert model.predict(input_example) == ["Hello", "Hi"]\n\n# The input example can also be a list of dictionaries that match the Message schema\ninput_example = [\n    {"role": "system", "content": "Hello"},\n    {"role": "user", "content": "Hi"},\n]\nassert model.predict(input_example) == ["Hello", "Hi"]\n\n# Log the model\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="model",\n        python_model=model,\n        input_example=input_example,\n    )\n\n# Load the model as pyfunc\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nassert pyfunc_model.predict(input_example) == ["Hello", "Hi"]\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-creating-a-custom-add-n-model",children:"Example: Creating a custom \u201cadd n\u201d model"}),"\n",(0,l.jsxs)(n.p,{children:["This example defines a class for a custom model that adds a specified numeric value, ",(0,l.jsx)(n.code,{children:"n"}),", to all\ncolumns of a Pandas DataFrame input. Then, it uses the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc"})}),"\nAPIs to save an instance of this model with ",(0,l.jsx)(n.code,{children:"n = 5"})," in MLflow Model format. Finally, it loads the model in\n",(0,l.jsx)(n.code,{children:"python_function"})," format and uses it to evaluate a sample input."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow.pyfunc\n\n\n# Define the model class\nclass AddN(mlflow.pyfunc.PythonModel):\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input, params=None):\n        return model_input.apply(lambda column: column + self.n)\n\n\n# Construct and save the model\nmodel_path = "add_n_model"\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n\n# Evaluate the model\nimport pandas as pd\n\nmodel_input = pd.DataFrame([range(10)])\nmodel_output = loaded_model.predict(model_input)\nassert model_output.equals(pd.DataFrame([range(5, 15)]))\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-saving-an-xgboost-model-in-mlflow-format",children:"Example: Saving an XGBoost model in MLflow format"}),"\n",(0,l.jsxs)(n.p,{children:["This example begins by training and saving a gradient boosted tree model using the XGBoost\nlibrary. Next, it defines a wrapper class around the XGBoost model that conforms to MLflow's\n",(0,l.jsx)(n.code,{children:"python_function"})," ",(0,l.jsx)(r.A,{target:"_blank",to:"/api_reference/python_api/mlflow.pyfunc.html#pyfunc-inference-api",children:"inference API"}),".\nThen, it uses the wrapper class and the saved XGBoost model to construct an MLflow Model that performs inference using the gradient\nboosted tree. Finally, it loads the MLflow Model in ",(0,l.jsx)(n.code,{children:"python_function"})," format and uses it to\nevaluate test data."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Load training and test datasets\nfrom sys import version_info\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nPYTHON_VERSION = f"{version_info.major}.{version_info.minor}.{version_info.micro}"\niris = datasets.load_iris()\nx = iris.data[:, 2:]\ny = iris.target\nx_train, x_test, y_train, _ = train_test_split(x, y, test_size=0.2, random_state=42)\ndtrain = xgb.DMatrix(x_train, label=y_train)\n\n# Train and save an XGBoost model\nxgb_model = xgb.train(params={"max_depth": 10}, dtrain=dtrain, num_boost_round=10)\nxgb_model_path = "xgb_model.pth"\nxgb_model.save_model(xgb_model_path)\n\n# Create an `artifacts` dictionary that assigns a unique name to the saved XGBoost model file.\n# This dictionary will be passed to `mlflow.pyfunc.save_model`, which will copy the model file\n# into the new MLflow Model\'s directory.\nartifacts = {"xgb_model": xgb_model_path}\n\n# Define the model class\nimport mlflow.pyfunc\n\n\nclass XGBWrapper(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        import xgboost as xgb\n\n        self.xgb_model = xgb.Booster()\n        self.xgb_model.load_model(context.artifacts["xgb_model"])\n\n    def predict(self, context, model_input, params=None):\n        input_matrix = xgb.DMatrix(model_input.values)\n        return self.xgb_model.predict(input_matrix)\n\n\n# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.\nimport cloudpickle\n\nconda_env = {\n    "channels": ["defaults"],\n    "dependencies": [\n        f"python={PYTHON_VERSION}",\n        "pip",\n        {\n            "pip": [\n                f"mlflow=={mlflow.__version__}",\n                f"xgboost=={xgb.__version__}",\n                f"cloudpickle=={cloudpickle.__version__}",\n            ],\n        },\n    ],\n    "name": "xgb_env",\n}\n\n# Save the MLflow Model\nmlflow_pyfunc_model_path = "xgb_mlflow_pyfunc"\nmlflow.pyfunc.save_model(\n    path=mlflow_pyfunc_model_path,\n    python_model=XGBWrapper(),\n    artifacts=artifacts,\n    conda_env=conda_env,\n)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n\n# Evaluate the model\nimport pandas as pd\n\ntest_predictions = loaded_model.predict(pd.DataFrame(x_test))\nprint(test_predictions)\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files",children:"Example: Logging a transformers model with hf:/ schema to avoid copying large files"}),"\n",(0,l.jsxs)(n.p,{children:["This example shows how to use a special schema ",(0,l.jsx)(n.code,{children:"hf:/"})," to log a transformers model from huggingface\nhub directly. This is useful when the model is too large and especially when you want to serve the\nmodel directly, but it doesn't save extra space if you want to download and test the model locally."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import infer_signature\nimport numpy as np\nimport transformers\n\n\n# Define a custom PythonModel\nclass QAModel(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n        This method initializes the tokenizer and language model\n        using the specified snapshot location from model context.\n        """\n        snapshot_location = context.artifacts["bert-tiny-model"]\n        # Initialize tokenizer and language model\n        tokenizer = transformers.AutoTokenizer.from_pretrained(snapshot_location)\n        model = transformers.BertForQuestionAnswering.from_pretrained(snapshot_location)\n        self.pipeline = transformers.pipeline(\n            task="question-answering", model=model, tokenizer=tokenizer\n        )\n\n    def predict(self, context, model_input, params=None):\n        question = model_input["question"][0]\n        if isinstance(question, np.ndarray):\n            question = question.item()\n        ctx = model_input["context"][0]\n        if isinstance(ctx, np.ndarray):\n            ctx = ctx.item()\n        return self.pipeline(question=question, context=ctx)\n\n\n# Log the model\ndata = {"question": "Who\'s house?", "context": "The house is owned by Run."}\npyfunc_artifact_path = "question_answering_model"\nwith mlflow.start_run() as run:\n    model_info = mlflow.pyfunc.log_model(\n        name=pyfunc_artifact_path,\n        python_model=QAModel(),\n        artifacts={"bert-tiny-model": "hf:/prajjwal1/bert-tiny"},\n        input_example=data,\n        signature=infer_signature(data, ["Run"]),\n        extra_pip_requirements=["torch", "accelerate", "transformers", "numpy"],\n    )\n'})}),"\n",(0,l.jsx)(n.h3,{id:"custom-flavors",children:"Custom Flavors"}),"\n",(0,l.jsxs)(n.p,{children:["To read about how to build custom integrations and to see examples of community-developed extended library support, check out the ",(0,l.jsx)(n.a,{href:"/ml/community-model-flavors",children:"Community Model Flavors"})," page."]}),"\n",(0,l.jsx)(n.h2,{id:"validate-models-before-deployment",children:"Validate Models before Deployment"}),"\n",(0,l.jsxs)(n.p,{children:["After logging your model with MLflow Tracking, it is highly recommended to validate the model locally before deploying it to production.\nThe ",(0,l.jsx)(s.B,{fn:"mlflow.models.predict"})," API provides a convenient way to test your model in a virtual environment, offering isolated execution and several advantages:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Model dependencies validation: The API helps ensure that the dependencies logged with the model are correct and sufficient by executing the model with an input example in a virtual environment.\nFor more details, refer to ",(0,l.jsx)(n.a,{href:"/ml/model/dependencies/#validating-environment-for-prediction",children:"Validating Environment for Prediction"}),"."]}),"\n",(0,l.jsx)(n.li,{children:"Input data validation: The API can be used to validate the input data interacts with the model as expected by simulating the same data processing during model serving.\nEnsure that the input data is a valid example that aligns with the pyfunc model\u2019s predict function requirements."}),"\n",(0,l.jsxs)(n.li,{children:["Extra environment variables validation: By specifying the ",(0,l.jsx)(n.code,{children:"extra_envs"})," parameter, you can test whether additional environment variables are required for the model to run successfully.\nNote that all existing environment variables in ",(0,l.jsx)(n.code,{children:"os.environ"})," are automatically passed into the virtual environment."]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return model_input\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="model",\n        python_model=MyModel(),\n        input_example=["a", "b", "c"],\n    )\n\nmlflow.models.predict(\n    model_uri=model_info.model_uri,\n    input_data=["a", "b", "c"],\n    pip_requirements_override=["..."],\n    extra_envs={"MY_ENV_VAR": "my_value"},\n)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"environment-managers",children:"Environment managers"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.models.predict"})," API supports the following environment managers to create the virtual environment for prediction:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://virtualenv.pypa.io/en/latest/",children:"virtualenv"}),": The default environment manager."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/",children:"uv"}),": An ",(0,l.jsx)(n.strong,{children:"extremely fast"})," environment manager written in Rust. ",(0,l.jsx)(n.strong,{children:"This is an experimental feature since MLflow 2.20.0."})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://docs.conda.io/projects/conda/",children:"conda"}),": uses conda to create environment."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"local"}),": uses the current environment to run the model. Note that ",(0,l.jsx)(n.code,{children:"pip_requirements_override"})," is not supported in this mode."]}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:["Starting from MLflow 2.20.0, ",(0,l.jsx)(n.code,{children:"uv"})," is available, and ",(0,l.jsx)(n.strong,{children:"it is extremely fast"}),".\nRun ",(0,l.jsx)(n.code,{children:"pip install uv"})," to install uv, or refer to ",(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/getting-started/installation",children:"uv installation guidance"})," for other installation methods."]})}),"\n",(0,l.jsxs)(n.p,{children:["Example of using ",(0,l.jsx)(n.code,{children:"uv"})," to create a virtual environment for prediction:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.models.predict(\n    model_uri="models:/<model_id>",\n    input_data="your_data",\n    env_manager="uv",\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"built-in-deployment",children:"Built-In Deployment Tools"}),"\n",(0,l.jsxs)(n.p,{children:["This information has been moved to the ",(0,l.jsx)(n.a,{href:"/ml/deployment",children:"MLflow Deployment"})," page."]}),"\n",(0,l.jsxs)(n.h2,{id:"export-a-python_function-model-as-an-apache-spark-udf",children:["Export a ",(0,l.jsx)(n.code,{children:"python_function"})," model as an Apache Spark UDF"]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["If you are using a model that has a very long running inference latency (i.e., a\n",(0,l.jsx)(n.code,{children:"transformers"})," model) that could take longer than the default timeout of 60 seconds,\nyou can utilize the ",(0,l.jsx)(n.code,{children:"extra_env"})," argument when defining the ",(0,l.jsx)(n.code,{children:"spark_udf"})," instance for your\nMLflow model, specifying an override to the environment variable ",(0,l.jsx)(n.code,{children:"MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT"}),".\nFor further guidance, please see :py:func:",(0,l.jsx)(n.code,{children:"mlflow.pyfunc.spark_udf"}),"."]})}),"\n",(0,l.jsxs)(n.p,{children:["You can output a ",(0,l.jsx)(n.code,{children:"python_function"})," model as an Apache Spark UDF, which can be uploaded to a\nSpark cluster and used to score the model."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, "<path-to-model>")\ndf = spark_df.withColumn("prediction", pyfunc_udf(struct([...])))\n'})}),"\n",(0,l.jsx)(n.p,{children:"If a model contains a signature, the UDF can be called without specifying column name arguments.\nIn this case, the UDF will be called with column names from signature, so the evaluation\ndataframe's column names must match the model signature's column names."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, "<path-to-model-with-signature>")\ndf = spark_df.withColumn("prediction", pyfunc_udf())\n'})}),"\n",(0,l.jsx)(n.p,{children:"If a model contains a signature with tensor spec inputs,\nyou will need to pass a column of array type as a corresponding UDF argument.\nThe values in this column must be comprised of one-dimensional arrays. The\nUDF will reshape the array values to the required shape with 'C' order\n(i.e. read / write the elements using C-like index order) and cast the values\nas the required tensor spec type. For example, assuming a model\nrequires input 'a' of shape (-1, 2, 3) and input 'b' of shape (-1, 4, 5). In order to\nperform inference on this data, we need to prepare a Spark DataFrame with column 'a'\ncontaining arrays of length 6 and column 'b' containing arrays of length 20. We can then\ninvoke the UDF like following example code:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n# Assuming the model requires input 'a' of shape (-1, 2, 3) and input 'b' of shape (-1, 4, 5)\nmodel_path = \"<path-to-model-requiring-multidimensional-inputs>\"\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n# The `spark_df` has column 'a' containing arrays of length 6 and\n# column 'b' containing arrays of length 20\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf(struct(\"a\", \"b\")))\n"})}),"\n",(0,l.jsxs)(n.p,{children:["The resulting UDF is based on Spark's Pandas UDF and is currently limited to producing either a single\nvalue, an array of values, or a struct containing multiple field values\nof the same type per observation. By default, we return the first\nnumeric column as a double. You can control what result is returned by supplying ",(0,l.jsx)(n.code,{children:"result_type"}),"\nargument. The following values are supported:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'int'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.IntegerType.html#pyspark.sql.types.IntegerType",children:"IntegerType"}),":\nThe leftmost integer that can fit in ",(0,l.jsx)(n.code,{children:"int32"})," result is returned or an exception is raised if there are none."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'long'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html#pyspark.sql.types.LongType",children:"LongType"}),":\nThe leftmost long integer that can fit in ",(0,l.jsx)(n.code,{children:"int64"})," result is returned or an exception is raised if there are none."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType",children:"ArrayType"}),"\n(",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.IntegerType.html#pyspark.sql.types.IntegerType",children:"IntegerType"}),"\n| ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html#pyspark.sql.types.LongType",children:"LongType"}),"):\nReturn all integer columns that can fit into the requested size."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'float'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.FloatType.html#pyspark.sql.types.FloatType",children:"FloatType"}),":\nThe leftmost numeric result cast to ",(0,l.jsx)(n.code,{children:"float32"})," is returned or an exception is raised if there are no numeric columns."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'double'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.DoubleType.html#pyspark.sql.types.DoubleType",children:"DoubleType"}),":\nThe leftmost numeric result cast to ",(0,l.jsx)(n.code,{children:"double"})," is returned or an exception is raised if there are no numeric columns."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType",children:"ArrayType"}),"\n( ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.FloatType.html#pyspark.sql.types.FloatType",children:"FloatType"}),"\n| ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.DoubleType.html#pyspark.sql.types.DoubleType",children:"DoubleType"})," ):\nReturn all numeric columns cast to the requested type. An exception is raised if there are no numeric columns."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'string'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html#pyspark.sql.types.StringType",children:"StringType"}),":\nResult is the leftmost column cast as string."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType",children:"ArrayType"}),"\n( ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html#pyspark.sql.types.StringType",children:"StringType"})," ):\nReturn all columns cast as string."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'bool'"})," or ",(0,l.jsx)(n.code,{children:"'boolean'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.BooleanType.html#pyspark.sql.types.BooleanType",children:"BooleanType"}),":\nThe leftmost column cast to ",(0,l.jsx)(n.code,{children:"bool"}),"\nis returned or an exception is raised if the values cannot be coerced."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'field1 FIELD1_TYPE, field2 FIELD2_TYPE, ...'"}),": A struct type containing\nmultiple fields separated by comma, each field type must be one of types\nlisted above."]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n# Suppose the PyFunc model `predict` method returns a dict like:\n# `{'prediction': 1-dim_array, 'probability': 2-dim_array}`\n# You can supply result_type to be a struct type containing\n# 2 fields 'prediction' and 'probability' like following.\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, \"<path-to-model>\", result_type=\"prediction float, probability: array<float>\"\n)\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf())\n"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql.types import ArrayType, FloatType\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, "path/to/model", result_type=ArrayType(FloatType())\n)\n# The prediction column will contain all the numeric columns returned by the model as floats\ndf = spark_df.withColumn("prediction", pyfunc_udf(struct("name", "age")))\n'})}),"\n",(0,l.jsxs)(n.p,{children:["If you want to use conda to restore the python environment that was used to train the model,\nset the ",(0,l.jsx)(n.code,{children:"env_manager"})," argument when calling ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql.types import ArrayType, FloatType\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark,\n    "path/to/model",\n    result_type=ArrayType(FloatType()),\n    env_manager="conda",  # Use conda to restore the environment used in training\n)\ndf = spark_df.withColumn("prediction", pyfunc_udf(struct("name", "age")))\n'})}),"\n",(0,l.jsxs)(n.p,{children:["If you want to call ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"})," through Databricks connect in remote client,\nyou need to build the model environment in Databricks runtime first."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from mlflow.pyfunc import build_model_env\n\n# Build the model env and save it as an archive file to the provided UC volume directory\n# and print the saved model env archive file path (like \'/Volumes/.../.../XXXXX.tar.gz\')\nprint(build_model_env(model_uri, "/Volumes/..."))\n\n# print the cluster id. Databricks Connect client needs to use the cluster id.\nprint(spark.conf.get("spark.databricks.clusterUsageTags.clusterId"))\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Once you have pre-built the model environment, you can run ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"})," with 'prebuilt_model_env'\nparameter through Databricks connect in remote client,"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from databricks.connect import DatabricksSession\n\nspark = DatabricksSession.builder.remote(\n    host=os.environ["DATABRICKS_HOST"],\n    token=os.environ["DATABRICKS_TOKEN"],\n    cluster_id="<cluster id>",  # get cluster id by spark.conf.get("spark.databricks.clusterUsageTags.clusterId")\n).getOrCreate()\n\n# The path generated by `build_model_env` in Databricks runtime.\nmodel_env_uc_uri = "dbfs:/Volumes/.../.../XXXXX.tar.gz"\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, model_uri, prebuilt_env_uri=model_env_uc_uri\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"deployment_plugin",children:"Deployment to Custom Targets"}),"\n",(0,l.jsxs)(n.p,{children:["In addition to the built-in deployment tools, MLflow provides a pluggable ",(0,l.jsx)(s.B,{fn:"mlflow.deployments"}),"\nand ",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments",target:"_blank",children:"mlflow deployments CLI"})," for deploying models to custom targets and environments.\nTo deploy to a custom target, you must first install an appropriate third-party Python plugin. See the list of\nknown community-maintained plugins ",(0,l.jsx)(n.a,{href:"/ml/plugins",children:"here"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"commands",children:"Commands"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"mlflow deployments"})," CLI contains the following commands, which can also be invoked programmatically\nusing the ",(0,l.jsx)(s.B,{fn:"mlflow.deployments",children:"mlflow.deployments Python API"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-create",target:"_blank",children:"Create"}),": Deploy an MLflow model to a specified custom target"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-delete",target:"_blank",children:"Delete"}),": Delete a deployment"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-update",target:"_blank",children:"Update"}),": Update an existing deployment, for example to\ndeploy a new model version or change the deployment's configuration (e.g. increase replica count)"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-list",target:"_blank",children:"List"}),": List IDs of all deployments"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-get",target:"_blank",children:"Get"}),": Print a detailed description of a particular deployment"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-run-local",target:"_blank",children:"Run Local"}),": Deploy the model locally for testing"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-help",target:"_blank",children:"Help"}),": Show the help string for the specified target"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"For more info, see:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow deployments --help\nmlflow deployments create --help\nmlflow deployments delete --help\nmlflow deployments update --help\nmlflow deployments list --help\nmlflow deployments get --help\nmlflow deployments run-local --help\nmlflow deployments help --help\n"})}),"\n",(0,l.jsx)(n.h2,{id:"community-model-flavors",children:"Community Model Flavors"}),"\n",(0,l.jsxs)(n.p,{children:["Go to the ",(0,l.jsx)(n.a,{href:"/ml/community-model-flavors",children:"Community Model Flavors"}),"\npage to get an overview of other useful MLflow flavors, which are developed and\nmaintained by the MLflow community."]})]})}function x(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(u,{...e})}):u(e)}}}]);