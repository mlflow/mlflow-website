"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9630],{28453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>o});var i=n(96540);const s={},a=i.createContext(s);function l(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(a.Provider,{value:t},e.children)}},62120:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"developer-workflow/phase1-build-improve","title":"Phase 1: Building and Iteratively Improving Your GenAI App","description":"This phase covers the initial development of your GenAI application, from building your first proof-of-concept through iterative improvement based on feedback from test users. You\'ll establish the foundation for systematic development, debugging, and quality improvement that will serve you throughout your application\'s lifecycle.","source":"@site/docs/genai/developer-workflow/phase1-build-improve.mdx","sourceDirName":"developer-workflow","slug":"/developer-workflow/phase1-build-improve","permalink":"/docs/latest/genai/developer-workflow/phase1-build-improve","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"GenAI Developer Workflow with MLflow","permalink":"/docs/latest/genai/developer-workflow/"},"next":{"title":"Evaluate & Test","permalink":"/docs/latest/genai/developer-workflow/phase2-systematically-test"}}');var s=n(74848),a=n(28453);const l={},o="Phase 1: Building and Iteratively Improving Your GenAI App",r={},c=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Overview",id:"overview",level:2},{value:"Challenge 1: Debug Application Logic",id:"challenge-1-debug-application-logic",level:2},{value:"The Problem",id:"the-problem",level:3},{value:"Solution: Implement MLflow Tracing",id:"solution-implement-mlflow-tracing",level:3},{value:"Implementation Benefits",id:"implementation-benefits",level:3},{value:"Challenge 2: Leverage Test User Queries",id:"challenge-2-leverage-test-user-queries",level:2},{value:"The Problem",id:"the-problem-1",level:3},{value:"Solution: Systematic Query Collection",id:"solution-systematic-query-collection",level:3},{value:"Query Collection Workflow",id:"query-collection-workflow",level:3},{value:"Challenge 3: Track Quality Feedback",id:"challenge-3-track-quality-feedback",level:2},{value:"The Problem",id:"the-problem-2",level:3},{value:"Solution: Integrated Feedback Collection",id:"solution-integrated-feedback-collection",level:3},{value:"Feedback Collection Methods",id:"feedback-collection-methods",level:3},{value:"Challenge 4: Systematic Prompt Iteration",id:"challenge-4-systematic-prompt-iteration",level:2},{value:"The Problem",id:"the-problem-3",level:3},{value:"Solution: MLflow Evaluation Harness",id:"solution-mlflow-evaluation-harness",level:3},{value:"Systematic Evaluation Process",id:"systematic-evaluation-process",level:3},{value:"Phase 1 Summary",id:"phase-1-summary",level:2},{value:"Infrastructure Established",id:"infrastructure-established",level:3},{value:"Data Collection Active",id:"data-collection-active",level:3},{value:"Quality Improvement Process",id:"quality-improvement-process",level:3},{value:"Ready for Scale",id:"ready-for-scale",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"phase-1-building-and-iteratively-improving-your-genai-app",children:"Phase 1: Building and Iteratively Improving Your GenAI App"})}),"\n",(0,s.jsx)(t.p,{children:"This phase covers the initial development of your GenAI application, from building your first proof-of-concept through iterative improvement based on feedback from test users. You'll establish the foundation for systematic development, debugging, and quality improvement that will serve you throughout your application's lifecycle."}),"\n",(0,s.jsx)(t.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"#overview",children:"Overview"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"#challenge-1-debug-application-logic",children:"Challenge 1: Debug Application Logic"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"#challenge-2-leverage-test-user-queries",children:"Challenge 2: Leverage Test User Queries"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"#challenge-3-track-quality-feedback",children:"Challenge 3: Track Quality Feedback"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"#challenge-4-systematic-prompt-iteration",children:"Challenge 4: Systematic Prompt Iteration"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"#phase-1-summary",children:"Phase 1 Summary"})}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(t.p,{children:"During Phase 1, you'll face several key challenges as you develop and refine your GenAI application:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Challenge"}),(0,s.jsx)(t.th,{children:"Solution"}),(0,s.jsx)(t.th,{children:"Key Benefit"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Debug Complex Logic"})}),(0,s.jsx)(t.td,{children:"MLflow Tracing"}),(0,s.jsx)(t.td,{children:"Complete observability across development and production"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Use Real User Queries"})}),(0,s.jsx)(t.td,{children:"Trace Collection & Search"}),(0,s.jsx)(t.td,{children:"Test with realistic usage patterns"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Track Quality Feedback"})}),(0,s.jsx)(t.td,{children:"Feedback APIs & UI"}),(0,s.jsx)(t.td,{children:"Systematic quality assessment"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Get Expert Input"})}),(0,s.jsx)(t.td,{children:"Review App Integration"}),(0,s.jsx)(t.td,{children:"Authoritative validation from domain experts"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Iterate on Prompts"})}),(0,s.jsx)(t.td,{children:"Evaluation Harness"}),(0,s.jsx)(t.td,{children:"Side-by-side comparison of variants"})]})]})]}),"\n",(0,s.jsx)(t.h2,{id:"challenge-1-debug-application-logic",children:"Challenge 1: Debug Application Logic"}),"\n",(0,s.jsx)(t.h3,{id:"the-problem",children:"The Problem"}),"\n",(0,s.jsx)(t.p,{children:"Even simple applications with a single prompt can be difficult to debug. As your app grows beyond basic prompts to include external data sources like vector databases and APIs, debugging becomes increasingly complex. You need to understand what input data was used for each variable in your prompt template, determine whether latency issues stem from the LLM or a slow API call, reproduce user issues by understanding exactly how their input was processed through each step, and maintain the same debugging visibility in both local development and production environments."}),"\n",(0,s.jsx)(t.h3,{id:"solution-implement-mlflow-tracing",children:"Solution: Implement MLflow Tracing"}),"\n",(0,s.jsx)(t.p,{children:"MLflow Tracing provides complete observability into your application's logic, latency, and cost. The same tracing works identically in development and production environments."}),"\n",(0,s.jsx)(t.mermaid,{value:"flowchart TB\n    DEV[\ud83d\udc68\u200d\ud83d\udcbb Developer]\n    IMPL[\ud83d\udd27 Implement MLflow Tracing]\n    CODE[\ud83d\udcdd Your Application Code]\n\n    LOCAL[\ud83d\udcbb Local Development]\n    PROD[\ud83c\udf10 Production Deployment]\n\n    TRACES[\ud83d\udcca Trace Logs]\n    UI[\ud83d\udda5\ufe0f MLflow Trace UI]\n\n    USERS[\ud83d\udc65 End Users]\n\n    DEV --\x3e IMPL\n    IMPL --\x3e CODE\n    CODE --\x3e LOCAL\n    CODE --\x3e PROD\n\n    DEV --\x3e LOCAL\n    USERS --\x3e PROD\n\n    LOCAL --\x3e TRACES\n    PROD --\x3e TRACES\n    TRACES --\x3e UI\n\n    classDef devStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    classDef prodStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    classDef dataStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    classDef userStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n\n    class DEV,LOCAL devStyle\n    class PROD prodStyle\n    class IMPL,CODE,TRACES,UI dataStyle\n    class USERS userStyle"}),"\n",(0,s.jsx)(t.h3,{id:"implementation-benefits",children:"Implementation Benefits"}),"\n",(0,s.jsx)(t.p,{children:"Beyond immediate debugging capabilities, implementing tracing enables future capabilities including quality feedback attachment to specific execution traces, performance optimization through detailed timing analysis, cost tracking across different configurations, and production monitoring with the same observability tools."}),"\n",(0,s.jsx)(t.h2,{id:"challenge-2-leverage-test-user-queries",children:"Challenge 2: Leverage Test User Queries"}),"\n",(0,s.jsx)(t.h3,{id:"the-problem-1",children:"The Problem"}),"\n",(0,s.jsx)(t.p,{children:"During development, beta testers provide valuable real-world examples that reveal natural language patterns showing how users actually phrase questions, realistic usage scenarios you hadn't considered, quality gaps where your application underperforms, and edge cases involving unusual but valid user inputs. You need a systematic way to capture and reuse these examples for testing improvements."}),"\n",(0,s.jsx)(t.h3,{id:"solution-systematic-query-collection",children:"Solution: Systematic Query Collection"}),"\n",(0,s.jsx)(t.p,{children:"MLflow Tracing automatically captures every user interaction with complete context. Use the MLflow Trace UI and SDK to browse, select, and reuse valuable examples."}),"\n",(0,s.jsx)(t.mermaid,{value:"flowchart TB\n    USERS[\ud83d\udc65 Beta Test Users]\n    APP[\ud83d\ude80 Your Application]\n    TRACES[\ud83d\udcca Captured Traces]\n\n    DEV[\ud83d\udc68\u200d\ud83d\udcbb Developer]\n    SEARCH[\ud83d\udd0d Search Traces API]\n    SELECT[\ud83d\udccb Select Valuable Examples]\n    TEST[\ud83e\uddea Test Improvements]\n\n    USERS --\x3e APP\n    APP --\x3e TRACES\n\n    DEV --\x3e SEARCH\n    SEARCH --\x3e TRACES\n    TRACES --\x3e SELECT\n    SELECT --\x3e TEST\n    TEST --\x3e APP\n\n    classDef userStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    classDef appStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    classDef dataStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    classDef devStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n\n    class USERS userStyle\n    class APP appStyle\n    class TRACES,SELECT dataStyle\n    class DEV,SEARCH,TEST devStyle"}),"\n",(0,s.jsx)(t.h3,{id:"query-collection-workflow",children:"Query Collection Workflow"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Automatic capture"})," records all user interactions during beta testing with rich context including inputs, outputs, timing, and metadata. ",(0,s.jsx)(t.strong,{children:"Programmatic access"})," via the ",(0,s.jsx)(t.code,{children:"search_traces"})," SDK enables notebook and IDE integration for easy exploration. ",(0,s.jsx)(t.strong,{children:"Direct reuse"})," allows you to leverage ",(0,s.jsx)(t.code,{children:"trace.inputs"})," for validation testing, making it simple to test improvements against real user queries that previously caused issues or represented important use cases."]}),"\n",(0,s.jsx)(t.p,{children:"The systematic approach ensures you're testing with realistic usage patterns rather than synthetic examples, capturing the natural language variations that users actually employ, and building a comprehensive library of test cases that grows with your application."}),"\n",(0,s.jsx)(t.h2,{id:"challenge-3-track-quality-feedback",children:"Challenge 3: Track Quality Feedback"}),"\n",(0,s.jsx)(t.h3,{id:"the-problem-2",children:"The Problem"}),"\n",(0,s.jsx)(t.p,{children:"Even with a small group of test users, you need reliable ways to capture user feedback on response quality, log your own observations during development iteration, identify problem patterns across different types of queries, and guide improvement priorities based on systematic feedback collection. Without structured feedback collection, quality issues can go unnoticed or be difficult to reproduce and fix."}),"\n",(0,s.jsx)(t.h3,{id:"solution-integrated-feedback-collection",children:"Solution: Integrated Feedback Collection"}),"\n",(0,s.jsx)(t.p,{children:"MLflow's feedback system allows you to attach quality assessments directly to traces through multiple channels."}),"\n",(0,s.jsx)(t.mermaid,{value:"flowchart TB\n    USERS[\ud83d\udc65 Test Users]\n    DEV[\ud83d\udc68\u200d\ud83d\udcbb Developer]\n\n    APP[\ud83d\ude80 Your Application]\n    UI_FEEDBACK[\ud83d\udc4d\ud83d\udc4e In-App Feedback]\n    DEV_FEEDBACK[\ud83d\udcdd Developer Annotations]\n\n    LOG_API[\ud83d\udd27 feedback logging]\n    TRACE_UI[\ud83d\udda5\ufe0f MLflow Trace UI]\n\n    TRACES[\ud83d\udcca Trace Logs with Feedback]\n    ANALYSIS[\ud83d\udcc8 Quality Analysis]\n\n    USERS --\x3e APP\n    USERS --\x3e UI_FEEDBACK\n    DEV --\x3e DEV_FEEDBACK\n\n    UI_FEEDBACK --\x3e LOG_API\n    DEV_FEEDBACK --\x3e TRACE_UI\n\n    APP --\x3e TRACES\n    LOG_API --\x3e TRACES\n    TRACE_UI --\x3e TRACES\n\n    TRACES --\x3e ANALYSIS\n\n    classDef userStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    classDef devStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    classDef appStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    classDef dataStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n    class USERS userStyle\n    class DEV devStyle\n    class APP appStyle\n    class UI_FEEDBACK,DEV_FEEDBACK,LOG_API,TRACE_UI,TRACES,ANALYSIS dataStyle"}),"\n",(0,s.jsx)(t.h3,{id:"feedback-collection-methods",children:"Feedback Collection Methods"}),"\n",(0,s.jsxs)(t.p,{children:["The system supports multiple feedback collection approaches to accommodate different workflows and user types. ",(0,s.jsx)(t.strong,{children:"MLflow Trace UI"})," allows direct annotation of traces during development, perfect for developers who want to mark specific interactions for further analysis."]}),"\n",(0,s.jsx)(t.p,{children:"This multi-channel approach ensures that feedback collection doesn't become a bottleneck while maintaining the structure needed for systematic quality analysis."}),"\n",(0,s.jsx)(t.h2,{id:"challenge-4-systematic-prompt-iteration",children:"Challenge 4: Systematic Prompt Iteration"}),"\n",(0,s.jsx)(t.h3,{id:"the-problem-3",children:"The Problem"}),"\n",(0,s.jsx)(t.p,{children:"When iterating on prompts and code changes to improve quality, you need systematic testing of multiple variations against the same inputs, side-by-side comparison to quickly identify the best performing version, version tracking to promote successful changes to production, debugging capability to understand why specific versions succeeded or failed, and expert validation when needed to confirm quality improvements. Manual testing of variations is time-consuming and error-prone."}),"\n",(0,s.jsx)(t.h3,{id:"solution-mlflow-evaluation-harness",children:"Solution: MLflow Evaluation Harness"}),"\n",(0,s.jsx)(t.p,{children:"The Evaluation Harness enables systematic testing and comparison of multiple application variants with minimal setup effort."}),"\n",(0,s.jsx)(t.mermaid,{value:"flowchart TB\n    DEV[\ud83d\udc68\u200d\ud83d\udcbb Developer]\n    VARIANTS[\ud83d\udd04 App Variants to Test]\n    INPUTS[\ud83d\udcdd Test Inputs]\n\n    HARNESS[\ud83e\uddea MLflow Evaluation Harness]\n    RUNS[\ud83d\udcca Evaluation Runs]\n    EVAL_UI[\ud83d\udda5\ufe0f Evaluation UI]\n\n    COMPARE[\u2696\ufe0f Side-by-Side Comparison]\n    BEST[\u2705 Best Performing Version]\n    DEPLOY[\ud83d\ude80 Deploy to Production]\n\n    EXPERTS[\ud83d\udc68\u200d\ud83d\udcbc Domain Experts]\n    REVIEW[\ud83d\udcf1 Review App]\n\n    DEV --\x3e VARIANTS\n    DEV --\x3e INPUTS\n\n    VARIANTS --\x3e HARNESS\n    INPUTS --\x3e HARNESS\n    HARNESS --\x3e RUNS\n    RUNS --\x3e EVAL_UI\n\n    EVAL_UI --\x3e COMPARE\n    COMPARE --\x3e BEST\n    BEST --\x3e DEPLOY\n\n    RUNS --\x3e REVIEW\n    EXPERTS --\x3e REVIEW\n\n    classDef devStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    classDef evalStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    classDef expertStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    classDef prodStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n\n    class DEV devStyle\n    class VARIANTS,INPUTS,HARNESS,RUNS,EVAL_UI,COMPARE evalStyle\n    class EXPERTS,REVIEW expertStyle\n    class BEST,DEPLOY prodStyle"}),"\n",(0,s.jsx)(t.h3,{id:"systematic-evaluation-process",children:"Systematic Evaluation Process"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Minimal setup"})," allows you to run multiple variants against selected inputs with simple configuration, eliminating the manual effort of testing each variation individually. ",(0,s.jsx)(t.strong,{children:"Visual comparison"})," through the Evaluation UI provides side-by-side output analysis, making it easy to spot differences in quality, tone, accuracy, or completeness across different versions."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Version tracking"})," maintains a complete history of code and prompt changes, enabling you to understand which modifications led to improvements and which caused regressions. ",(0,s.jsx)(t.strong,{children:"Expert integration"})," allows you to share evaluation results with domain experts when technical assessment isn't sufficient to determine the best approach."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Historical record"})," keeping tracks which changes led to improvements over time, building institutional knowledge about what works well for your specific use case and domain. This systematic approach transforms prompt iteration from a manual, error-prone process into a reliable, data-driven workflow that builds confidence in your improvements."]}),"\n",(0,s.jsx)(t.h2,{id:"phase-1-summary",children:"Phase 1 Summary"}),"\n",(0,s.jsx)(t.p,{children:"By the end of Phase 1, you will have established a robust foundation for GenAI application development with comprehensive infrastructure, active data collection, systematic quality improvement processes, and readiness for scaling to broader deployment."}),"\n",(0,s.jsx)(t.h3,{id:"infrastructure-established",children:"Infrastructure Established"}),"\n",(0,s.jsxs)(t.p,{children:["Your technical foundation includes ",(0,s.jsx)(t.strong,{children:"MLflow Tracing"})," implemented for complete observability across all application components, ",(0,s.jsx)(t.strong,{children:"feedback collection"})," systems that capture input from both users and developers, ",(0,s.jsx)(t.strong,{children:"expert review"})," workflows that provide authoritative validation without technical barriers, and an ",(0,s.jsx)(t.strong,{children:"evaluation harness"})," for systematic testing of improvements."]}),"\n",(0,s.jsx)(t.h3,{id:"data-collection-active",children:"Data Collection Active"}),"\n",(0,s.jsxs)(t.p,{children:["Your systematic data collection captures ",(0,s.jsx)(t.strong,{children:"real user queries"})," from beta testing that reflect natural usage patterns, ",(0,s.jsx)(t.strong,{children:"quality feedback"})," that's systematically collected and organized for analysis, ",(0,s.jsx)(t.strong,{children:"performance metrics"})," tracked across different scenarios to understand application behavior, and ",(0,s.jsx)(t.strong,{children:"expert assessments"})," integrated directly with trace data for comprehensive quality evaluation."]}),"\n",(0,s.jsx)(t.h3,{id:"quality-improvement-process",children:"Quality Improvement Process"}),"\n",(0,s.jsxs)(t.p,{children:["The systematic approach to quality improvement includes ",(0,s.jsx)(t.strong,{children:"systematic testing"})," of prompt and code variations against realistic inputs, ",(0,s.jsx)(t.strong,{children:"data-driven decisions"})," based on real user interactions rather than assumptions, ",(0,s.jsx)(t.strong,{children:"expert validation"})," of improvements before broader deployment, and ",(0,s.jsx)(t.strong,{children:"version tracking"})," for confident promotion of successful changes to production."]}),"\n",(0,s.jsx)(t.h3,{id:"ready-for-scale",children:"Ready for Scale"}),"\n",(0,s.jsxs)(t.p,{children:["Your Phase 1 implementation creates the foundation for ",(0,s.jsx)(t.strong,{children:"production deployment"})," with comprehensive monitoring capabilities, ",(0,s.jsx)(t.strong,{children:"automated quality assessment"})," using production data patterns, ",(0,s.jsx)(t.strong,{children:"continuous improvement"})," cycles based on systematic evaluation rather than ad-hoc changes, and ",(0,s.jsx)(t.strong,{children:"scalable feedback collection"})," from larger user bases without overwhelming manual processes."]}),"\n",(0,s.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(t.p,{children:["With Phase 1 complete, you're ready to move to ",(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.a,{href:"/genai/developer-workflow/phase2-systematically-test",children:"Phase 2: Systematically Testing Quality, Cost, and Latency"})})," where you'll scale your application to serve real users with confidence through systematic testing and performance optimization."]}),"\n",(0,s.jsx)(t.p,{children:"Phase 1 establishes the systematic development practices that will serve you throughout your GenAI application's lifecycle, ensuring reliable quality improvement and confident scaling."})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);