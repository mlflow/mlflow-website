"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7686],{13484:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>m,contentTitle:()=>s,default:()=>c,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"llms/transformers/large-models/index","title":"Working with Large Models in MLflow Transformers flavor","description":"The features described in this guide are intended for advanced users familiar with Transformers and MLflow. Please understand the limitations and potential risks associated with these features before use.","source":"@site/docs/llms/transformers/large-models/index.mdx","sourceDirName":"llms/transformers/large-models","slug":"/llms/transformers/large-models/","permalink":"/docs/latest/llms/transformers/large-models/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"sidebar_label":"Storage Optimization"},"sidebar":"docsSidebar","previous":{"title":"Task","permalink":"/docs/latest/llms/transformers/task/"},"next":{"title":"Overview","permalink":"/docs/latest/llms/sentence-transformers/"}}');var l=n(74848),r=n(28453),i=n(67756);const a={sidebar_position:4,sidebar_label:"Storage Optimization"},s="Working with Large Models in MLflow Transformers flavor",m={},d=[{value:"Overview",id:"overview",level:2},{value:"Memory-Efficient Model Logging",id:"transformers-memory-efficient-logging",level:2},{value:"Important Notes",id:"important-notes",level:3},{value:"Storage-Efficient Model Logging",id:"transformers-save-pretrained-guide",level:2},{value:"Registering Reference-Only Models for Production",id:"registering-reference-only-models-for-production",level:3},{value:"Databricks Unity Catalog",id:"databricks-unity-catalog",level:4},{value:"OSS Model Registry or Legacy Workspace Model Registry",id:"persist-pretrained-guide",level:3},{value:"Caveats for Skipping Saving of Pretrained Model Weights",id:"caveats-of-save-pretrained",level:3}];function h(e){const o={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(o.header,{children:(0,l.jsx)(o.h1,{id:"working-with-large-models-in-mlflow-transformers-flavor",children:"Working with Large Models in MLflow Transformers flavor"})}),"\n",(0,l.jsx)(o.admonition,{type:"warning",children:(0,l.jsx)(o.p,{children:"The features described in this guide are intended for advanced users familiar with Transformers and MLflow. Please understand the limitations and potential risks associated with these features before use."})}),"\n",(0,l.jsxs)(o.p,{children:["The ",(0,l.jsx)(o.a,{href:"/llms/transformers",children:"MLflow Transformers flavor"})," allows you to track various Transformers models in MLflow. However, logging large models such as Large Language Models (LLMs) can be resource-intensive\ndue to their size and memory requirements. This guide outlines MLflow's features for reducing memory and disk usage when logging models, enabling you to work with large models in resource-constrained environments."]}),"\n",(0,l.jsx)(o.h2,{id:"overview",children:"Overview"}),"\n",(0,l.jsx)(o.p,{children:"The following table summarizes the different methods for logging models with the Transformers flavor. Please be aware that each method has certain limitations and requirements, as described in the following sections."}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Save method"}),(0,l.jsx)("th",{children:"Description"}),(0,l.jsx)("th",{children:"Memory Usage"}),(0,l.jsx)("th",{children:"Disk Usage"}),(0,l.jsx)("th",{children:"Example"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"Normal pipeline-based logging"}),(0,l.jsx)("td",{children:"Log a model using a pipeline instance or a dictionary of pipeline components."}),(0,l.jsx)("td",{children:"High"}),(0,l.jsx)("td",{children:"High"}),(0,l.jsx)("td",{children:(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import mlflow\nimport transformers\n\npipeline = transformers.pipeline(\n    task="text-generation",\n    model="meta-llama/Meta-Llama-3.1-70B",\n)\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        name="model",\n    )\n'})})})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:(0,l.jsx)(o.a,{href:"#transformers-memory-efficient-logging",children:"Memory-Efficient Model Logging"})}),(0,l.jsx)("td",{children:"Log a model by specifying a path to a local checkpoint, avoiding loading the model into memory."}),(0,l.jsx)("td",{children:(0,l.jsx)(o.strong,{children:(0,l.jsx)(o.strong,{children:"Low"})})}),(0,l.jsx)("td",{children:"High"}),(0,l.jsx)("td",{children:(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import mlflow\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        # Pass a path to local checkpoint as a model\n        transformers_model="/path/to/local/checkpoint",\n        # Task argument is required for this saving mode.\n        task="text-generation",\n        name="model",\n    )\n'})})})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:(0,l.jsx)(o.a,{href:"#transformers-save-pretrained-guide",children:"Storage-Efficient Model Logging"})}),(0,l.jsx)("td",{children:"Log a model by saving a reference to the HuggingFace Hub repository instead of the model weights."}),(0,l.jsx)("td",{children:"High"}),(0,l.jsx)("td",{children:(0,l.jsx)(o.strong,{children:"Low"})}),(0,l.jsx)("td",{children:(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import mlflow\nimport transformers\n\npipeline = transformers.pipeline(\n    task="text-generation",\n    model="meta-llama/Meta-Llama-3.1-70B",\n)\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        name="model",\n        # Set save_pretrained to False to save storage space\n        save_pretrained=False,\n    )\n'})})})]})]})]}),"\n",(0,l.jsx)(o.h2,{id:"transformers-memory-efficient-logging",children:"Memory-Efficient Model Logging"}),"\n",(0,l.jsx)(o.p,{children:"Introduced in MLflow 2.16.1, this method allows you to log a model without loading it into memory:"}),"\n",(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import mlflow\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        # Pass a path to local checkpoint as a model to avoid loading the model instance\n        transformers_model="path/to/local/checkpoint",\n        # Task argument is required for this saving mode.\n        task="text-generation",\n        name="model",\n    )\n'})}),"\n",(0,l.jsxs)(o.p,{children:["In the above example, we pass a path to the local model checkpoint/weight as the model argument in\nthe ",(0,l.jsx)(i.B,{fn:"mlflow.transformers.log_model"})," API, instead of a pipeline instance.\nMLflow will inspect the model metadata of the checkpoint and log the model weights without loading them into memory.\nThis way, you can log an enormous multi-billion parameter model to MLflow with minimal computational resources."]}),"\n",(0,l.jsx)(o.h3,{id:"important-notes",children:"Important Notes"}),"\n",(0,l.jsx)(o.p,{children:"Please be aware of the following requirements and limitations when using this feature:"}),"\n",(0,l.jsxs)(o.ol,{children:["\n",(0,l.jsxs)(o.li,{children:["The checkpoint directory ",(0,l.jsx)(o.strong,{children:"must"})," contain a valid config.json file and the model weight files. If a tokenizer is required, its state file must also be present in the checkpoint directory. You can save the tokenizer state in your checkpoint directory by calling ",(0,l.jsx)(o.code,{children:'tokenizer.save_pretrained("path/to/local/checkpoint")'})," method."]}),"\n",(0,l.jsxs)(o.li,{children:["You ",(0,l.jsx)(o.strong,{children:"must"})," specify the ",(0,l.jsx)(o.code,{children:"task"})," argument with the appropriate task name that the model is designed for."]}),"\n",(0,l.jsxs)(o.li,{children:["MLflow may not accurately infer model dependencies in this mode. Please refer to ",(0,l.jsx)(o.a,{href:"/model/dependencies",children:"Managing Dependencies in MLflow Models"})," for more information on managing dependencies for your model."]}),"\n"]}),"\n",(0,l.jsx)(o.admonition,{type:"warning",children:(0,l.jsxs)(o.p,{children:["Ensure you specify the correct task argument, as an incompatible task will cause the model to ",(0,l.jsx)(o.strong,{children:"fail at the load time"}),". You can check the valid task type for your model on the HuggingFace Hub."]})}),"\n",(0,l.jsx)(o.h2,{id:"transformers-save-pretrained-guide",children:"Storage-Efficient Model Logging"}),"\n",(0,l.jsx)(o.p,{children:"Typically, when MLflow logs an ML model, it saves a copy of the model weight to the artifact store.\nHowever, this is not optimal when you use a pretrained model from HuggingFace Hub and have no intention of fine-tuning or otherwise manipulating the model or its weights before logging it.\nFor this very common case, copying the (typically very large) model weights is redundant while developing prompts, testing inference parameters, and otherwise is little more than an unnecessary waste of storage space."}),"\n",(0,l.jsxs)(o.p,{children:["To address this issue, MLflow 2.11.0 introduced a new argument ",(0,l.jsx)(o.code,{children:"save_pretrained"})," in the ",(0,l.jsx)(i.B,{fn:"mlflow.transformers.save_model"})," and ",(0,l.jsx)(i.B,{fn:"mlflow.transformers.log_model"})," APIs.\nWhen with argument is set to ",(0,l.jsx)(o.code,{children:"False"}),", MLflow will forego saving the pretrained model weights, opting instead to store a reference to the underlying repository entry on the HuggingFace Hub;\nspecifically, the repository name and the unique commit hash of the model weights are stored when your components or pipeline are logged. When loading back such a ",(0,l.jsx)(o.em,{children:"reference-only"})," model,\nMLflow will check the repository name and commit hash from the saved metadata, and either download the model weight from the HuggingFace Hub or use the locally cached model from your HuggingFace local cache directory."]}),"\n",(0,l.jsxs)(o.p,{children:["Here is the example of using ",(0,l.jsx)(o.code,{children:"save_pretrained"})," argument for logging a model"]}),"\n",(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import transformers\n\npipeline = transformers.pipeline(\n    task="text-generation",\n    model="meta-llama/Meta-Llama-3.1-70B",\n    torch_dtype="torch.float16",\n)\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        name="model",\n        # Set save_pretrained to False to save storage space\n        save_pretrained=False,\n    )\n'})}),"\n",(0,l.jsxs)(o.p,{children:["In the above example, MLflow will not save a copy of the ",(0,l.jsx)(o.strong,{children:"Llama-3.1-70B"})," model's weights and will instead log the following metadata as a reference to the HuggingFace Hub model.\nThis will save roughly 150GB of storage space and reduce the logging latency significantly as well for each run that you initiate during development."]}),"\n",(0,l.jsx)(o.p,{children:"By navigating to the MLflow UI, you can see the model logged with the repository ID and commit hash:"}),"\n",(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-bash",children:"flavors:\n    ...\n    transformers:\n        source_model_name: meta-llama/Meta-Llama-3.1-70B-Instruct\n        source_model_revision: 33101ce6ccc08fa6249c10a543ebfcac65173393\n        ...\n"})}),"\n",(0,l.jsxs)(o.p,{children:["Before production deployments, you may want to persist the model weight instead of the repository reference. To do so, you can use\nthe ",(0,l.jsx)(i.B,{fn:"mlflow.transformers.persist_pretrained_model"})," API to download the model weight from the HuggingFace Hub and save\nit to the artifact location. Please refer to the ",(0,l.jsx)(o.a,{href:"#persist-pretrained-guide",children:"OSS Model Registry or Legacy Workspace Model Registry"}),"\nsection for more information."]}),"\n",(0,l.jsx)(o.h3,{id:"registering-reference-only-models-for-production",children:"Registering Reference-Only Models for Production"}),"\n",(0,l.jsx)(o.p,{children:'The models logged with either of the above optimized methods are "reference-only", meaning that the model weight is not saved to\nthe artifact store and only the reference to the HuggingFace Hub repository is saved. When you load the model back normally,\nMLflow will download the model weight from the HuggingFace Hub.'}),"\n",(0,l.jsx)(o.p,{children:"However, this may not be suitable for production use cases, as the model weight may be unavailable or the download may fail due to network issues.\nMLflow provides a solution to address this issue when registering reference-models to the Model Registry."}),"\n",(0,l.jsx)(o.h4,{id:"databricks-unity-catalog",children:"Databricks Unity Catalog"}),"\n",(0,l.jsxs)(o.p,{children:["Registering reference-only models to ",(0,l.jsx)(o.a,{href:"https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/index.html",children:"Databricks Unity Catalog Model Registry"}),"\nrequires ",(0,l.jsx)(o.strong,{children:"no additional steps"})," than the normal model registration process. MLflow automatically downloads and registers the model weights to Unity Catalog along\nwith the model metadata."]}),"\n",(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import mlflow\n\nmlflow.set_registry_uri("databricks-uc")\n\n# Log the repository ID as a model. The model weight will not be saved to the artifact store\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model="meta-llama/Meta-Llama-3.1-70B-Instruct",\n        name="model",\n    )\n\n# When registering the model to Unity Catalog Model Registry, MLflow will automatically\n# persist the model weight files. This may take a several minutes for large models.\nmlflow.register_model(model_info.model_uri, "your.model.name")\n'})}),"\n",(0,l.jsx)(o.h3,{id:"persist-pretrained-guide",children:"OSS Model Registry or Legacy Workspace Model Registry"}),"\n",(0,l.jsxs)(o.p,{children:["For OSS Model Registry or the legacy Workspace Model Registry in Databricks, you need to manually persist the\nmodel weight to the artifact store before registering the model. You can use the ",(0,l.jsx)(i.B,{fn:"mlflow.transformers.persist_pretrained_model"}),"\nAPI to download the model weight from the HuggingFace Hub and save it to the artifact location. The process ",(0,l.jsx)(o.strong,{children:"does NOT require re-logging a model"}),"\nbut efficiently update the existing model and metadata in-place."]}),"\n",(0,l.jsx)(o.pre,{children:(0,l.jsx)(o.code,{className:"language-python",children:'import mlflow\n\n# Log the repository ID as a model. The model weight will not be saved to the artifact store\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model="meta-llama/Meta-Llama-3.1-70B-Instruct",\n        name="model",\n    )\n\n# Before registering the model to the non-UC model registry, persist the model weight\n# from the HuggingFace Hub to the artifact location.\nmlflow.transformers.persist_pretrained_model(model_info.model_uri)\n\n# Register the model\nmlflow.register_model(model_info.model_uri, "your.model.name")\n'})}),"\n",(0,l.jsx)(o.h3,{id:"caveats-of-save-pretrained",children:"Caveats for Skipping Saving of Pretrained Model Weights"}),"\n",(0,l.jsx)(o.p,{children:"While these features are useful for saving computational resources and storage space for logging large models, there are some caveats to be aware of:"}),"\n",(0,l.jsxs)(o.ul,{children:["\n",(0,l.jsxs)(o.li,{children:[(0,l.jsx)(o.strong,{children:"Change in Model Availability"}),": If you are using a model from other users' repository, the model may be deleted or become private in the HuggingFace Hub.\nIn such cases, MLflow cannot load the model back. For production use cases, it is recommended to save a copy of the model weights to the artifact store prior\nto moving from development or staging to production for your model."]}),"\n",(0,l.jsxs)(o.li,{children:[(0,l.jsx)(o.strong,{children:"HuggingFace Hub Access"}),": Downloading a model from the HuggingFace Hub might be slow or unstable due to the network latency or the HuggingFace Hub service status.\nMLflow doesn't provide any retry mechanism or robust error handling for model downloading from the HuggingFace Hub. As such, you should not rely on this\nfunctionality for your final production-candidate run."]}),"\n"]}),"\n",(0,l.jsx)(o.p,{children:"By understanding these methods and their limitations, you can effectively work with large Transformers models in MLflow while optimizing resource usage."})]})}function c(e={}){const{wrapper:o}={...(0,r.R)(),...e.components};return o?(0,l.jsx)(o,{...e,children:(0,l.jsx)(h,{...e})}):h(e)}},28453:(e,o,n)=>{n.d(o,{R:()=>i,x:()=>a});var t=n(96540);const l={},r=t.createContext(l);function i(e){const o=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),t.createElement(r.Provider,{value:o},e.children)}},67756:(e,o,n)=>{n.d(o,{B:()=>s});n(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=n(29030),r=n(56289),i=n(74848);const a=e=>{const o=e.split(".");for(let n=o.length;n>0;n--){const e=o.slice(0,n).join(".");if(t[e])return e}return null};function s(e){let{fn:o,children:n}=e;const s=a(o);if(!s)return(0,i.jsx)(i.Fragment,{children:n});const m=(0,l.Ay)(`/${t[s]}#${o}`);return(0,i.jsx)(r.A,{to:m,target:"_blank",children:n??(0,i.jsxs)("code",{children:[o,"()"]})})}}}]);