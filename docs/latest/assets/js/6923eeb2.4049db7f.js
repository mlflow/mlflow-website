"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["2016"],{65060(e,n,t){t.r(n),t.d(n,{metadata:()=>a,default:()=>h,frontMatter:()=>r,contentTitle:()=>s,toc:()=>p,assets:()=>c});var a=JSON.parse('{"id":"tracing/integrations/listing/pydantic_ai","title":"Tracing PydanticAI","description":"PydanticAI Tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/pydantic_ai.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/pydantic_ai","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/pydantic_ai","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"sidebar_label":"PydanticAI"},"sidebar":"genAISidebar","previous":{"title":"Pipecat","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/pipecat"},"next":{"title":"Quarkus LangChain4j","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/quarkus-langchain4j"}}'),i=t(74848),l=t(28453),o=t(54725);let r={sidebar_position:7,sidebar_label:"PydanticAI"},s="Tracing PydanticAI",c={},p=[{value:"Example Usage",id:"example-usage",level:3},{value:"Advanced Example: Utilising MCP Server",id:"advanced-example-utilising-mcp-server",level:2},{value:"Streaming Support",id:"streaming-support",level:2},{value:"Async Streaming with <code>run_stream</code>",id:"async-streaming-with-run_stream",level:3},{value:"Sync Streaming with <code>run_stream_sync</code>",id:"sync-streaming-with-run_stream_sync",level:3},{value:"Token usage",id:"token-usage",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:3}];function m(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"tracing-pydanticai",children:"Tracing PydanticAI"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"PydanticAI Tracing via autolog",src:t(71145).A+"",width:"2878",height:"1500"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://ai.pydantic.dev/",children:"\u200BPydanticAI"})," is a Python framework designed to simplify the development of production-grade generative AI applications. It brings type safety, ergonomic API design, and a developer-friendly experience to GenAI app development.\u200B"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/genai/tracing",children:"MLflow Tracing"})," provides automatic tracing capability for ",(0,i.jsx)(n.a,{href:"https://ai.pydantic.dev/",children:"\u200BPydanticAI"}),", an open source framework for building multi-agent applications. By enabling auto tracing for \u200BPydanticAI by calling the ",(0,i.jsx)(o.B,{fn:"mlflow.pydantic_ai.autolog"})," function, , MLflow will capture nested traces for \u200BPydanticAI workflow execution and logged them to the active MLflow Experiment."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.pydantic_ai.autolog()\n"})}),"\n",(0,i.jsx)(n.p,{children:"MLflow trace automatically captures the following information about \u200BPydanticAI agents:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Agent calls with prompts, kwargs & output responses"}),"\n",(0,i.jsxs)(n.li,{children:["Streaming operations via ",(0,i.jsx)(n.code,{children:"run_stream"})," (async) and ",(0,i.jsx)(n.code,{children:"run_stream_sync"})," (sync)"]}),"\n",(0,i.jsx)(n.li,{children:"LLM requests logging model name, prompt, parameters & response"}),"\n",(0,i.jsx)(n.li,{children:"Tool runs capturing tool name, arguments & usage metrics"}),"\n",(0,i.jsx)(n.li,{children:"MCP server calls & listings for tool-invocation tracing"}),"\n",(0,i.jsx)(n.li,{children:"Span metadata: latency, errors & run-ID linkage"}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["MLflow's PydanticAI integration supports tracing for synchronous, asynchronous, and streaming executions. The ",(0,i.jsx)(n.code,{children:"run_stream_sync"})," method requires PydanticAI version 1.10.0 or later."]})}),"\n",(0,i.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,i.jsx)(n.p,{children:"First, enable auto-tracing for PydanticAI, and optionally create an MLflow experiment to write traces to. This helps organizing your traces better."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Turn on auto tracing by calling mlflow.pydantic_ai.autolog()\nmlflow.pydantic_ai.autolog()\n\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("PydanticAI")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Next, let's define a multi-agent workflow using PydanticAI. The example below sets up a weather agent where users can ask for the weather in multiple locations, and the agent will use the get_lat_lng tool to get the latitude and longitude of the locations, then use the get_weather tool to get the weather for those locations."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\n\nweather_agent = Agent(\n    # Switch to your favorite LLM\n    "google-gla:gemini-2.0-flash",\n    # \'Be concise, reply with one sentence.\' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    system_prompt=(\n        "Be concise, reply with one sentence."\n        "Use the `get_lat_lng` tool to get the latitude and longitude of the locations, "\n        "then use the `get_weather` tool to get the weather."\n    ),\n    deps_type=Deps,\n    retries=2,\n    instrument=True,\n)\n\n\n@weather_agent.tool\nasync def get_lat_lng(\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n    """Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    """\n    if ctx.deps.geo_api_key is None:\n        return {"lat": 51.1, "lng": -0.1}\n\n    params = {\n        "q": location_description,\n        "api_key": ctx.deps.geo_api_key,\n    }\n    r = await ctx.deps.client.get("https://geocode.maps.co/search", params=params)\n    r.raise_for_status()\n    data = r.json()\n\n    if data:\n        return {"lat": data[0]["lat"], "lng": data[0]["lon"]}\n    else:\n        raise ModelRetry("Could not find the location")\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    """Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    """\n\n    if ctx.deps.weather_api_key is None:\n        return {"temperature": "21 \xb0C", "description": "Sunny"}\n\n    params = {\n        "apikey": ctx.deps.weather_api_key,\n        "location": f"{lat},{lng}",\n        "units": "metric",\n    }\n    r = await ctx.deps.client.get(\n        "https://api.tomorrow.io/v4/weather/realtime", params=params\n    )\n    r.raise_for_status()\n    data = r.json()\n\n    values = data["data"]["values"]\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        1000: "Clear, Sunny",\n        1100: "Mostly Clear",\n        1101: "Partly Cloudy",\n        1102: "Mostly Cloudy",\n        1001: "Cloudy",\n        2000: "Fog",\n        2100: "Light Fog",\n        4000: "Drizzle",\n        4001: "Rain",\n        4200: "Light Rain",\n        4201: "Heavy Rain",\n        5000: "Snow",\n        5001: "Flurries",\n        5100: "Light Snow",\n        5101: "Heavy Snow",\n        6000: "Freezing Drizzle",\n        6001: "Freezing Rain",\n        6200: "Light Freezing Rain",\n        6201: "Heavy Freezing Rain",\n        7000: "Ice Pellets",\n        7101: "Heavy Ice Pellets",\n        7102: "Light Ice Pellets",\n        8000: "Thunderstorm",\n    }\n    return {\n        "temperature": f\'{values["temperatureApparent"]:0.0f}\xb0C\',\n        "description": code_lookup.get(values["weatherCode"], "Unknown"),\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        weather_api_key = os.getenv("WEATHER_API_KEY")\n        geo_api_key = os.getenv("GEO_API_KEY")\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            "What is the weather like in London and in Wiltshire?", deps=deps\n        )\n        print("Response:", result.output)\n\n\n# If you are running this on a notebook\nawait main()\n\n# Uncomment this is you are using an IDE or Python script.\n# asyncio.run(main())\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-example-utilising-mcp-server",children:"Advanced Example: Utilising MCP Server"}),"\n",(0,i.jsx)(n.p,{children:"MLflow Tracing automatically captures tool-related interactions from the MCP server in PydanticAI, including call_tool and list_tools operations. These actions are recorded as individual spans in the trace UI."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"PydanticAI MCP Server tracing via autolog",src:t(17974).A+"",width:"2880",height:"1462"})}),"\n",(0,i.jsx)(n.p,{children:"The example below demonstrates how to run an MCP server using PydanticAI with MLflow tracing enabled. All tool invocation and listing operations are automatically captured as trace spans in the UI, along with relevant metadata."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport asyncio\n\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("MCP Server")\nmlflow.pydantic_ai.autolog()\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    "deno",\n    args=[\n        "run",\n        "-N",\n        "-R=node_modules",\n        "-W=node_modules",\n        "--node-modules-dir=auto",\n        "jsr:@pydantic/mcp-run-python",\n        "stdio",\n    ],\n)\n\nagent = Agent("openai:gpt-4o", mcp_servers=[server], instrument=True)\n\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run("How many days between 2000-01-01 and 2025-03-18?")\n    print(result.output)\n    # > There are 9,208 days between January 1, 2000, and March 18, 2025.\n\n\n# If you are running this on a notebook\nawait main()\n\n# Uncomment this is you are using an IDE or Python script.\n# asyncio.run(main())\n'})}),"\n",(0,i.jsx)(n.h2,{id:"streaming-support",children:"Streaming Support"}),"\n",(0,i.jsxs)(n.p,{children:["MLflow supports tracing for PydanticAI's streaming APIs, including both async (",(0,i.jsx)(n.code,{children:"run_stream"}),") and sync (",(0,i.jsx)(n.code,{children:"run_stream_sync"}),") methods. When using streaming, MLflow captures the complete trace including all LLM calls and tool invocations."]}),"\n",(0,i.jsxs)(n.h3,{id:"async-streaming-with-run_stream",children:["Async Streaming with ",(0,i.jsx)(n.code,{children:"run_stream"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"PydanticAI run_stream Tracing",src:t(40938).A+"",width:"3154",height:"1794"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport asyncio\nfrom pydantic_ai import Agent\n\nmlflow.pydantic_ai.autolog()\n\nagent = Agent("openai:gpt-4o", instrument=True)\n\n\nasync def main():\n    async with agent.run_stream("Tell me a joke") as response:\n        # Stream the text as it arrives\n        async for chunk in response.stream_text(delta=True):\n            print(chunk, end="", flush=True)\n        print()\n\n\nasyncio.run(main())\n'})}),"\n",(0,i.jsxs)(n.h3,{id:"sync-streaming-with-run_stream_sync",children:["Sync Streaming with ",(0,i.jsx)(n.code,{children:"run_stream_sync"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"PydanticAI run_stream_sync Tracing",src:t(23450).A+"",width:"3140",height:"1790"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"run_stream_sync"})," method (available in PydanticAI 1.10.0+) provides synchronous streaming:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom pydantic_ai import Agent\n\nmlflow.pydantic_ai.autolog()\n\nagent = Agent("openai:gpt-4o", instrument=True)\n\n# Synchronous streaming\nresult = agent.run_stream_sync("Tell me a joke")\nfor chunk in result.stream_text():\n    print(chunk, end="", flush=True)\nprint()\nprint(f"Final output: {result.get_output()}")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Both streaming methods will create traces that include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The root ",(0,i.jsx)(n.code,{children:"Agent.run_stream"})," or ",(0,i.jsx)(n.code,{children:"Agent.run_stream_sync"})," span"]}),"\n",(0,i.jsxs)(n.li,{children:["Child ",(0,i.jsx)(n.code,{children:"InstrumentedModel.request_stream"})," spans for each LLM call"]}),"\n",(0,i.jsx)(n.li,{children:"Any tool invocation spans if tools are used during the conversation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"token-usage",children:"Token usage"}),"\n",(0,i.jsxs)(n.p,{children:["MLflow >= 3.1.0 supports token usage tracking for PydanticAI. The token usage for each LLM call will be logged in the ",(0,i.jsx)(n.code,{children:"mlflow.chat.tokenUsage"})," attribute. The total token usage throughout the trace will be\navailable in the ",(0,i.jsx)(n.code,{children:"token_usage"})," field of the trace info object."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import json\nimport mlflow\n\nmlflow.pydantic_ai.autolog()\n\n# Run the example given in previous\nawait main()\n\n# Uncomment this is you are using an IDE or Python script.\n# asyncio.run(main())\n\n\n# Get the trace object just created\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Detailed usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 432\n  Output tokens: 53\n  Total tokens: 485\n\n== Detailed usage for each LLM call: ==\nInstrumentedModel.request_1:\n  Input tokens: 108\n  Output tokens: 19\n  Total tokens: 127\nInstrumentedModel.request_2:\n  Input tokens: 145\n  Output tokens: 14\n  Total tokens: 159\nInstrumentedModel.request_3:\n  Input tokens: 179\n  Output tokens: 20\n  Total tokens: 199\n"})}),"\n",(0,i.jsx)(n.h3,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,i.jsxs)(n.p,{children:["Auto tracing for PydanticAI can be disabled globally by calling ",(0,i.jsx)(n.code,{children:"mlflow.pydantic_ai.autolog(disable=True)"})," or ",(0,i.jsx)(n.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function h(e={}){let{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},17974(e,n,t){t.d(n,{A:()=>a});let a=t.p+"assets/images/pydanticai-mcp-tracing-539c1eadbc3bf818d47f3daec180391a.png"},23450(e,n,t){t.d(n,{A:()=>a});let a=t.p+"assets/images/pydanticai-run-stream-sync-tracing-83a16166f581b65ae7e40f97147d48af.png"},40938(e,n,t){t.d(n,{A:()=>a});let a=t.p+"assets/images/pydanticai-run-stream-tracing-5506a5fe5cae34f5d7aad05e0e990b07.png"},71145(e,n,t){t.d(n,{A:()=>a});let a=t.p+"assets/images/pydanticai-tracing-794fbcf11fdb1407e2be31fefaec5536.png"},54725(e,n,t){t.d(n,{B:()=>o});var a=t(74848);t(96540);var i=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),l=t(66497);function o({fn:e,children:n,hash:t}){let o=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(i[t])return t}return null})(e);if(!o)return(0,a.jsx)(a.Fragment,{children:n});let r=(0,l.default)(`/${i[o]}#${t??e}`);return(0,a.jsx)("a",{href:r,target:"_blank",children:n??(0,a.jsxs)("code",{children:[e,"()"]})})}},28453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var a=t(96540);let i={},l=a.createContext(i);function o(e){let n=a.useContext(l);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(l.Provider,{value:n},e.children)}}}]);