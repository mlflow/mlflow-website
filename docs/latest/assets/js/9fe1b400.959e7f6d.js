"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8698],{28453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var a=t(96540);const s={},i=a.createContext(s);function l(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),a.createElement(i.Provider,{value:n},e.children)}},42008:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/llm_evaluate_experiment_view-d77ef6a5e26529af50b5c9071aa2fc1d.png"},61711:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>h,default:()=>f,frontMatter:()=>d,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"llms/llm-evaluate/index","title":"MLflow LLM Evaluation","description":"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics.","source":"@site/docs/llms/llm-evaluate/index.mdx","sourceDirName":"llms/llm-evaluate","slug":"/llms/llm-evaluate/","permalink":"/docs/latest/llms/llm-evaluate/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"description":"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics."},"sidebar":"docsSidebar","previous":{"title":"Detailed Guides","permalink":"/docs/latest/llms/sentence-transformers/guide/"},"next":{"title":"MLflow LLM Evaluation","permalink":"/docs/latest/llms/llm-evaluate/"}}');var s=t(74848),i=t(28453),l=t(65537),r=t(79329),o=t(67756),c=t(56289);const d={description:"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics."},h="MLflow LLM Evaluation",m={},p=[{value:"Full Notebook Guides and Examples",id:"full-notebook-guides-and-examples",level:2},{value:"Quickstart",id:"quickstart",level:2},{value:"LLM Evaluation Metrics",id:"llm-evaluation-metrics",level:2},{value:"Heuristic-based Metrics",id:"heuristic-based-metrics",level:3},{value:"Built-in Heuristic Metrics",id:"built-in-heuristic-metrics",level:4},{value:"Default Metrics with Pre-defined Model Types",id:"llm-eval-default-metrics",level:4},{value:"Use a Custom List of Metrics",id:"llm-eval-custom-metrics",level:4},{value:"Create Custom heuristic-based LLM Evaluation Metrics",id:"create-custom-heuristic-based-llm-evaluation-metrics",level:4},{value:"LLM-as-a-Judge Metrics",id:"llm-as-a-judge-metrics",level:3},{value:"Built-in LLM-as-a-Judge metrics",id:"built-in-llm-as-a-judge-metrics",level:4},{value:"Selecting the Judge Model",id:"selecting-the-judge-model",level:4},{value:"1. SaaS LLM Providers",id:"1-saas-llm-providers",level:5},{value:"2. Self-hosted Proxy Endpoints",id:"2-self-hosted-proxy-endpoints",level:5},{value:"3. MLflow AI Gateway Endpoints",id:"3-mlflow-ai-gateway-endpoints",level:5},{value:"4. Databricks Model Serving",id:"4-databricks-model-serving",level:5},{value:"Overriding Default Judge Parameters",id:"overriding-default-judge-parameters",level:4},{value:"Creating Custom LLM-as-a-Judge Metrics",id:"creating-custom-llm-as-a-judge-metrics",level:4},{value:"Prepare Your Target Models",id:"prepare-your-target-models",level:2},{value:"Evaluating with an MLflow Model",id:"evaluating-with-an-mlflow-model",level:3},{value:"Evaluating with a Custom Function",id:"llm-eval-custom-function",level:3},{value:"Evaluating with an MLflow Deployments Endpoint",id:"evaluating-with-an-mlflow-deployments-endpoint",level:3},{value:"Supported Input Data Formats",id:"supported-input-data-formats",level:4},{value:"Passing Inference Parameters",id:"passing-inference-parameters",level:4},{value:"Examples",id:"examples",level:4},{value:"Evaluating with a Static Dataset",id:"llm-eval-static-dataset",level:3},{value:"Viewing Evaluation Results",id:"viewing-evaluation-results",level:2},{value:"View Evaluation Results via Code",id:"view-evaluation-results-via-code",level:3},{value:"View Evaluation Results via the MLflow UI",id:"view-evaluation-results-via-the-mlflow-ui",level:3}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"mlflow-llm-evaluation",children:"MLflow LLM Evaluation"})}),"\n",(0,s.jsxs)(n.p,{children:["With the emerging of ChatGPT, LLMs have shown its power of text generation in various fields, such as\nquestion answering, translating and text summarization. Evaluating LLMs' performance is slightly different\nfrom traditional ML models, as very often there is no single ground truth to compare against.\nMLflow provides an API ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," to help evaluate your LLMs."]}),"\n",(0,s.jsx)(n.p,{children:"MLflow's LLM evaluation functionality consists of 3 main components:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"A model to evaluate"}),": it can be an MLflow ",(0,s.jsx)(n.code,{children:"pyfunc"})," model, a URI pointing to one registered\nMLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Metrics"}),": the metrics to compute, LLM evaluate will use LLM metrics."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Evaluation data"}),": the data your model is evaluated at, it can be a pandas Dataframe, a python list, a\nnumpy array or an ",(0,s.jsx)(o.B,{fn:"mlflow.data.dataset.Dataset"})," instance."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"full-notebook-guides-and-examples",children:"Full Notebook Guides and Examples"}),"\n",(0,s.jsx)(n.p,{children:"If you're interested in thorough use-case oriented guides that showcase the simplicity and power of MLflow's evaluate\nfunctionality for LLMs, please navigate to the notebook collection below:"}),"\n",(0,s.jsx)(c.A,{to:"/llms/llm-evaluate/notebooks/",children:(0,s.jsx)("button",{className:"button button--primary",children:"View the Notebook Guides"})}),"\n",(0,s.jsx)(n.h2,{id:"quickstart",children:"Quickstart"}),"\n",(0,s.jsxs)(n.p,{children:['Below is a simple example that gives an quick overview of how MLflow LLM evaluation works. The example builds\na simple question-answering model by wrapping "openai/gpt-4" with custom prompt. You can paste it to\nyour IPython or local editor and execute it, and install missing dependencies as prompted. Running the code\nrequires OpenAI API key, if you don\'t have an OpenAI key, you can set it up by following the ',(0,s.jsx)(n.a,{href:"https://platform.openai.com/account/api-keys",children:"OpenAI guide"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"export OPENAI_API_KEY='your-api-key-here'\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\nimport os\nimport pandas as pd\nfrom getpass import getpass\n\neval_data = pd.DataFrame(\n    {\n        "inputs": [\n            "What is MLflow?",\n            "What is Spark?",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for managing the end-to-end machine learning (ML) "\n            "lifecycle. It was developed by Databricks, a company that specializes in big data and "\n            "machine learning solutions. MLflow is designed to address the challenges that data "\n            "scientists and machine learning engineers face when developing, training, and deploying "\n            "machine learning models.",\n            "Apache Spark is an open-source, distributed computing system designed for big data "\n            "processing and analytics. It was developed in response to limitations of the Hadoop "\n            "MapReduce computing model, offering improvements in speed and ease of use. Spark "\n            "provides libraries for various tasks such as data ingestion, processing, and analysis "\n            "through its components like Spark SQL for structured data, Spark Streaming for "\n            "real-time data processing, and MLlib for machine learning tasks",\n        ],\n    }\n)\n\nwith mlflow.start_run() as run:\n    system_prompt = "Answer the following question in two sentences"\n    # Wrap "gpt-4" as an MLflow model.\n    logged_model_info = mlflow.openai.log_model(\n        model="gpt-4",\n        task=openai.chat.completions,\n        name="model",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": "{question}"},\n        ],\n    )\n\n    # Use predefined question-answering metrics to evaluate our model.\n    results = mlflow.evaluate(\n        logged_model_info.model_uri,\n        eval_data,\n        targets="ground_truth",\n        model_type="question-answering",\n    )\n    print(f"See aggregated evaluation results below: \\n{results.metrics}")\n\n    # Evaluation result for each data record is available in `results.tables`.\n    eval_table = results.tables["eval_results_table"]\n    print(f"See evaluation table below: \\n{eval_table}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"llm-evaluation-metrics",children:"LLM Evaluation Metrics"}),"\n",(0,s.jsx)(n.p,{children:"There are two types of LLM evaluation metrics in MLflow:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Heuristic-based metrics"}),": These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe), based on certain functions, such as: Rouge (",(0,s.jsx)(o.B,{fn:"mlflow.metrics.rougeL"}),"), Flesch Kincaid (",(0,s.jsx)(o.B,{fn:"mlflow.metrics.flesch_kincaid_grade_level"}),") or Bilingual Evaluation Understudy (BLEU) (",(0,s.jsx)(o.B,{fn:"mlflow.metrics.bleu"}),"). These metrics are similar to traditional continuous value metrics. For the list of built-in heuristic metrics and how to define a custom metric with your own function definition, see the ",(0,s.jsx)(n.a,{href:"#heuristic-based-metrics",children:"Heuristic-based Metrics"})," section."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LLM-as-a-Judge metrics"}),": LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs. It overcomes the limitations of heuristic-based metrics, which often miss nuances like context and semantic accuracy. LLM-as-a-Judge metrics provides a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation. MLflow provides various built-in LLM-as-a-Judge metrics and supports creating custom metrics with your own prompt, grading criteria, and reference examples. See the ",(0,s.jsx)(n.a,{href:"#llm-as-a-judge-metrics",children:"LLM-as-a-Judge Metrics"})," section for more details."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"heuristic-based-metrics",children:"Heuristic-based Metrics"}),"\n",(0,s.jsx)(n.h4,{id:"built-in-heuristic-metrics",children:"Built-in Heuristic Metrics"}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.metrics.html",children:"this page"})," for the full list of the built-in heuristic metrics."]}),"\n",(0,s.jsx)(n.h4,{id:"llm-eval-default-metrics",children:"Default Metrics with Pre-defined Model Types"}),"\n",(0,s.jsxs)(n.p,{children:['MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, "question-answering". Depending on the\nLLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluations. To use\ndefaults metrics for pre-selected tasks, specify the ',(0,s.jsx)(n.code,{children:"model_type"})," argument in ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"}),", as shown by the example\nbelow:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = mlflow.evaluate(\n    model,\n    eval_data,\n    targets="ground_truth",\n    model_type="question-answering",\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:"The supported LLM model types and associated metrics are listed below:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"question-answering"}),": ",(0,s.jsx)(n.code,{children:'model_type="question-answering"'}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"exact-match"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/evaluate-measurement/toxicity",children:"toxicity"})," [1]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Automated_readability_index",children:"ari_grade_level"})," [2]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level",children:"flesch_kincaid_grade_level"})," [2]"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"text-summarization"}),": ",(0,s.jsx)(n.code,{children:'model_type="text-summarization"'}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/evaluate-metric/rouge",children:"ROUGE"})," [3]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/evaluate-measurement/toxicity",children:"toxicity"})," [1]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Automated_readability_index",children:"ari_grade_level"})," [2]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level",children:"flesch_kincaid_grade_level"})," [2]"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"text models"}),": ",(0,s.jsx)(n.code,{children:'model_type="text"'}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/evaluate-measurement/toxicity",children:"toxicity"})," [1]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Automated_readability_index",children:"ari_grade_level"})," [2]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level",children:"flesch_kincaid_grade_level"})," [2]"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"retrievers"}),": ",(0,s.jsx)(n.code,{children:'model_type="retriever"'}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k",children:"precision_at_k"})," [4]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Recall",children:"recall_at_k"})," [4]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG",children:"ndcg_at_k"})," [4]"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["[1] Requires packages ",(0,s.jsx)(n.a,{href:"https://pypi.org/project/evaluate",children:"evaluate"}),", ",(0,s.jsx)(n.a,{href:"https://pytorch.org/get-started/locally/",children:"torch"}),", and\n",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/installation",children:"transformers"})]}),"\n",(0,s.jsxs)(n.p,{children:["[2] Requires package ",(0,s.jsx)(n.a,{href:"https://pypi.org/project/textstat",children:"textstat"})]}),"\n",(0,s.jsxs)(n.p,{children:["[3] Requires packages ",(0,s.jsx)(n.a,{href:"https://pypi.org/project/evaluate",children:"evaluate"}),", ",(0,s.jsx)(n.a,{href:"https://pypi.org/project/nltk",children:"nltk"}),", and\n",(0,s.jsx)(n.code,{children:"rouge-score <https://pypi.org/project/rouge-score>"}),"_"]}),"\n",(0,s.jsxs)(n.p,{children:["[4] All retriever metrics have a default ",(0,s.jsx)(n.code,{children:"retriever_k"})," value of ",(0,s.jsx)(n.code,{children:"3"})," that can be overridden by specifying ",(0,s.jsx)(n.code,{children:"retriever_k"})," in the ",(0,s.jsx)(n.code,{children:"evaluator_config"})," argument."]}),"\n",(0,s.jsx)(n.h4,{id:"llm-eval-custom-metrics",children:"Use a Custom List of Metrics"}),"\n",(0,s.jsxs)(n.p,{children:["Using the pre-defined metrics associated with a given model type is not the only way to generate scoring metrics\nfor LLM evaluation in MLflow. You can specify a custom list of metrics in the ",(0,s.jsx)(n.code,{children:"extra_metrics"})," argument in ",(0,s.jsx)(n.code,{children:"mlflow.evaluate"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["To add additional metrics to the default metrics list of pre-defined model type, keep the ",(0,s.jsx)(n.code,{children:"model_type"})," and add your metrics to ",(0,s.jsx)(n.code,{children:"extra_metrics"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = mlflow.evaluate(\n    model,\n    eval_data,\n    targets="ground_truth",\n    model_type="question-answering",\n    extra_metrics=[mlflow.metrics.latency()],\n)\n'})}),"\n",(0,s.jsxs)(n.p,{children:['The above code will evaluate your model using all metrics for "question-answering" model plus ',(0,s.jsx)(o.B,{fn:"mlflow.metrics.latency"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["To disable default metric calculation and only calculate your selected metrics, remove the ",(0,s.jsx)(n.code,{children:"model_type"})," argument and define the desired metrics."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = mlflow.evaluate(\n    model,\n    eval_data,\n    targets="ground_truth",\n    extra_metrics=[mlflow.metrics.toxicity(), mlflow.metrics.latency()],\n)\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The full reference for supported evaluation metrics can be found ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate",children:"here"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"create-custom-heuristic-based-llm-evaluation-metrics",children:"Create Custom heuristic-based LLM Evaluation Metrics"}),"\n",(0,s.jsxs)(n.p,{children:["This is very similar to creating custom traditional metrics, with the exception of returning a ",(0,s.jsx)(o.B,{fn:"mlflow.metrics.MetricValue"})," instance.\nBasically you need to:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Implement An ",(0,s.jsx)(n.code,{children:"eval_fn"})," to define your scoring logic. This function must take in 2 args: ",(0,s.jsx)(n.code,{children:"predictions"})," and ",(0,s.jsx)(n.code,{children:"target``. "}),"eval_fn` must return a ",(0,s.jsx)(o.B,{fn:"mlflow.metrics.MetricValue"})," instance."]}),"\n",(0,s.jsxs)(n.li,{children:["Pass ",(0,s.jsx)(n.code,{children:"eval_fn"})," and other arguments to the ",(0,s.jsx)(n.code,{children:"mlflow.metrics.make_metric"})," API to create the metric."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The following code creates a dummy per-row metric called ",(0,s.jsx)(n.code,{children:'"over_10_chars"'}),'; if the model output is greater than 10,\nthe score is "yes", otherwise it is "no".']}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def eval_fn(predictions, targets):\n    scores = ["yes" if len(pred) > 10 else "no" for pred in predictions]\n    return MetricValue(\n        scores=scores,\n        aggregate_results=standard_aggregations(scores),\n    )\n\n\n# Create an EvaluationMetric object.\npassing_code_metric = make_metric(\n    eval_fn=eval_fn, greater_is_better=False, name="over_10_chars"\n)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["To create a custom metric that is dependent on other metrics, include those other metrics' names as an argument after ",(0,s.jsx)(n.code,{children:"predictions"})," and ",(0,s.jsx)(n.code,{children:"targets"}),". This can be the name of a builtin metric or another custom metric.\nEnsure that you do not accidentally have any circular dependencies in your metrics, or the evaluation will fail."]}),"\n",(0,s.jsxs)(n.p,{children:["The following code creates a dummy per-row metric called ",(0,s.jsx)(n.code,{children:'"toxic_or_over_10_chars"'}),': if the model output is greater than 10 or the toxicity score is greater than 0.5, the score is "yes", otherwise it is "no".']}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def eval_fn(predictions, targets, toxicity, over_10_chars):\n    scores = [\n        "yes" if toxicity.scores[i] > 0.5 or over_10_chars.scores[i] else "no"\n        for i in len(toxicity.scores)\n    ]\n    return MetricValue(scores=scores)\n\n\n# Create an EvaluationMetric object.\ntoxic_and_over_10_chars_metric = make_metric(\n    eval_fn=eval_fn, greater_is_better=False, name="toxic_or_over_10_chars"\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"llm-as-a-judge-metrics",children:"LLM-as-a-Judge Metrics"}),"\n",(0,s.jsx)(n.p,{children:"LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs, providing a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation."}),"\n",(0,s.jsx)(n.p,{children:"MLflow supports several builtin LLM-as-a-judge metrics, as well as allowing you to create your own LLM-as-a-judge metrics with custom configurations and prompts."}),"\n",(0,s.jsx)(n.h4,{id:"built-in-llm-as-a-judge-metrics",children:"Built-in LLM-as-a-Judge metrics"}),"\n",(0,s.jsxs)(n.p,{children:["To use built-in LLM-as-a-Judge metrics in MLflow, pass the list of metrics definitions to the ",(0,s.jsx)(n.code,{children:"extra_metrics"})," argument in the ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," function."]}),"\n",(0,s.jsx)(n.p,{children:"The following example uses the built-in answer correctness metric for evaluation, in addition to the latency metric (heuristic):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from mlflow.metrics import latency\nfrom mlflow.metrics.genai import answer_correctness\n\nresults = mlflow.evaluate(\n    eval_data,\n    targets="ground_truth",\n    extra_metrics=[\n        answer_correctness(),\n        latency(),\n    ],\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here is the list of built-in LLM-as-a-Judge metrics. Click on the link to see the full documentation for each metric:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.answer_similarity"}),": Evaluate how similar\na model's generated output is compared to the information in the ground truth\ndata."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.answer_correctness"}),": Evaluate how\nfactually correct a model's generated output is based on the information\nwithin the ground truth data."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.answer_relevance"}),": Evaluate how relevant\nthe model generated output is to the input (context is ignored)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.relevance"}),": Evaluate how relevant the\nmodel generated output is with respect to both the input and the context."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.faithfulness"}),": Evaluate how faithful the\nmodel generated output is based on the context provided."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"selecting-the-judge-model",children:"Selecting the Judge Model"}),"\n",(0,s.jsxs)(n.p,{children:["By default, MLflow will use OpenAI's GPT-4 model as the judge model that scores metrics. You can change the judge model by passing an override to the ",(0,s.jsx)(n.code,{children:"model"})," argument within the metric definition."]}),"\n",(0,s.jsx)(n.h5,{id:"1-saas-llm-providers",children:"1. SaaS LLM Providers"}),"\n",(0,s.jsxs)(n.p,{children:["To use SaaS LLM providers, such as OpenAI or Anthropic, set the ",(0,s.jsx)(n.code,{children:"model"})," parameter in the metrics definition, in the format of ",(0,s.jsx)(n.code,{children:"<provider>:/<model-name>"}),". Currently, MLflow supports ",(0,s.jsx)(n.code,{children:'["openai", "anthropic", "bedrock", "mistral", "togetherai"]'})," as viable LLM providers for any judge model."]}),"\n",(0,s.jsxs)(l.A,{children:[(0,s.jsxs)(r.A,{label:"OpenAI / Azure OpenAI",value:"openai",default:!0,children:[(0,s.jsxs)(n.p,{children:["OpenAI models can be accessed via the ",(0,s.jsx)(n.code,{children:"openai:/<model-name>"})," URI."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"\n\nanswer_correctness = mlflow.metrics.genai.answer_correctness(model="openai:/gpt-4o")\n\n# Test the metric definition\nanswer_correctness(\n    inputs="What is MLflow?",\n    predictions="MLflow is an innovative full self-driving airship.",\n    targets="MLflow is an open-source platform for managing the end-to-end ML lifecycle.",\n)\n'})}),(0,s.jsxs)(n.p,{children:["Azure OpenAI endpoints can be accessed via the same ",(0,s.jsx)(n.code,{children:"openai:/<model-name>"})," URI, by setting the environment variables, such as ",(0,s.jsx)(n.code,{children:"OPENAI_API_BASE"}),", ",(0,s.jsx)(n.code,{children:"OPENAI_API_TYPE"}),", etc."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'os.environ["OPENAI_API_TYPE"] = "azure"\nos.environ["OPENAI_API_BASE"] = "https:/my-azure-openai-endpoint.azure.com/"\nos.environ["OPENAI_DEPLOYMENT_NAME"] = "gpt-4o-mini"\nos.environ["OPENAI_API_VERSION"] = "2024-08-01-preview"\nos.environ["OPENAI_API_KEY"] = "<your-api-key-for-azure-openai-endpoint>"\n'})})]}),(0,s.jsxs)(r.A,{label:"Anthropic",value:"anthropic",children:[(0,s.jsxs)(n.p,{children:["Anthropic models can be accessed via the ",(0,s.jsx)(n.code,{children:"anthropic:/<model-name>"})," URI. Note that the ",(0,s.jsx)(n.code,{children:"default judge parameters <#overriding-default-judge-parameters>"})," need to be overridden by passing the ",(0,s.jsx)(n.code,{children:"parameters"})," argument to the metrics definition, since the default parameters violates the Anthropic endpoint requirement (",(0,s.jsx)(n.code,{children:"temperature"})," and ",(0,s.jsx)(n.code,{children:"top_p"})," cannot be specified together)."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["ANTHROPIC_API_KEY"] = "<your-anthropic-api-key>"\n\nanswer_correctness = mlflow.metrics.genai.answer_correctness(\n    model="anthropic:/claude-3-5-sonnet-20241022",\n    # Override default judge parameters to meet Claude endpoint requirements.\n    parameters={"temperature": 0, "max_tokens": 256},\n)\n\n# Test the metric definition\nanswer_correctness(\n    inputs="What is MLflow?",\n    predictions="MLflow is an innovative full self-driving airship.",\n    targets="MLflow is an open-source platform for managing the end-to-end ML lifecycle.",\n)\n'})})]}),(0,s.jsxs)(r.A,{label:"Bedrock",value:"bedrock",children:[(0,s.jsxs)(n.p,{children:["Bedrock models can be accessed via the ",(0,s.jsx)(n.code,{children:"bedrock:/<model-name>"})," URI. Make sure you have set the authentication information via the environment variables. You can use both role-based or API key-based authentication for accessing Bedrock models."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["AWS_REGION"] = "<your-aws-region>"\n\n# Option 1. Role-based authentication\nos.environ["AWS_ROLE_ARN"] = "<your-aws-role-arn>"\n\n# Option 2. API key-based authentication\nos.environ["AWS_ACCESS_KEY_ID"] = "<your-aws-access-key-id>"\nos.environ["AWS_SECRET_ACCESS_KEY"] = "<your-aws-secret-access-key>"\n# You can also use session token for temporary credentials.\n# os.environ["AWS_SESSION_TOKEN"] = "<your-aws-session-token>"\n\nanswer_correctness = mlflow.metrics.genai.answer_correctness(\n    model="bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0",\n    parameters={\n        "temperature": 0,\n        "max_tokens": 256,\n        "anthropic_version": "bedrock-2023-05-31",\n    },\n)\n\n# Test the metric definition\nanswer_correctness(\n    inputs="What is MLflow?",\n    predictions="MLflow is an innovative full self-driving airship.",\n    targets="MLflow is an open-source platform for managing the end-to-end ML lifecycle.",\n)\n'})})]}),(0,s.jsxs)(r.A,{label:"Mistral",value:"mistral",children:[(0,s.jsxs)(n.p,{children:["Mistral models can be accessed via the ",(0,s.jsx)(n.code,{children:"mistral:/<model-name>"})," URI."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["MISTRAL_API_KEY"] = "<your-mistral-api-key>"\n\nanswer_correctness = mlflow.metrics.genai.answer_correctness(\n    model="mistral:/mistral-small-latest",\n)\n\n# Test the metric definition\nanswer_correctness(\n    inputs="What is MLflow?",\n    predictions="MLflow is an innovative full self-driving airship.",\n    targets="MLflow is an open-source platform for managing the end-to-end ML lifecycle.",\n)\n'})})]}),(0,s.jsxs)(r.A,{label:"TogetherAI",value:"togetherai",children:[(0,s.jsxs)(n.p,{children:["TogetherAI models can be accessed via the ",(0,s.jsx)(n.code,{children:"togetherai:/<model-name>"})," URI."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["TOGETHERAI_API_KEY"] = "<your-togetherai-api-key>"\n\nanswer_correctness = mlflow.metrics.genai.answer_correctness(\n    model="togetherai:/togetherai-small-latest",\n)\n\n# Test the metric definition\nanswer_correctness(\n    inputs="What is MLflow?",\n    predictions="MLflow is an innovative full self-driving airship.",\n    targets="MLflow is an open-source platform for managing the end-to-end ML lifecycle.",\n)\n'})})]})]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Your use of a third party LLM service (e.g., OpenAI) for evaluation may be subject to and governed by the LLM service's terms of use."})}),"\n",(0,s.jsx)(n.h5,{id:"2-self-hosted-proxy-endpoints",children:"2. Self-hosted Proxy Endpoints"}),"\n",(0,s.jsxs)(n.p,{children:["If you are accessing SaaS LLM providers via a proxy endpoint (e.g., for security compliance), you can set the ",(0,s.jsx)(n.code,{children:"proxy_url"})," parameter in the metrics definition. Additionally, use the ",(0,s.jsx)(n.code,{children:"extra_headers"})," parameters to pass extra headers for the endpoint for authentication."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'answer_similarity = mlflow.metrics.genai.answer_similarity(\n    model="openai:/gpt-4o",\n    proxy_url="https://my-proxy-endpoint/chat",\n    extra_headers={"Group-ID": "my-group-id"},\n)\n'})}),"\n",(0,s.jsx)(n.h5,{id:"3-mlflow-ai-gateway-endpoints",children:"3. MLflow AI Gateway Endpoints"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"/llms/deployments/",children:"MLflow AI Gateway"})," is a self-hosted solution that allows you to query various LLM providers in a unified interface. To use an endpoint hosted by MLflow AI Gateway:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Start the MLflow AI Gateway server with your LLM setting by following ",(0,s.jsx)(n.a,{href:"/llms/deployments/#deployments-quickstart",children:"these steps"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Set the MLflow deployment client to target the server address by using :py:func:",(0,s.jsx)(n.code,{children:"~mlflow.deployments.set_deployments_target()"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Set ",(0,s.jsx)(n.code,{children:"endpoints:/<endpoint-name>"})," to the ",(0,s.jsx)(n.code,{children:"model"})," parameter in the metrics definition."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import set_deployments_target\n\n# When the MLflow AI Gateway server is running at http://localhost:5000\nset_deployments_target("http://localhost:5000")\nmy_answer_similarity = mlflow.metrics.genai.answer_similarity(\n    model="endpoints:/my-endpoint"\n)\n'})}),"\n",(0,s.jsx)(n.h5,{id:"4-databricks-model-serving",children:"4. Databricks Model Serving"}),"\n",(0,s.jsxs)(n.p,{children:["If you have a model hosted on Databricks, you can use it as a judge model by setting ",(0,s.jsx)(n.code,{children:"endpoints:/<endpoint-name>"})," to the ",(0,s.jsx)(n.code,{children:"model"})," parameter in the metrics definition. The following code uses a Llama 3.1 405B model available via the ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/index.html",children:"Foundation Model API"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import set_deployments_target\n\nset_deployments_target("databricks")\nllama3_answer_similarity = mlflow.metrics.genai.answer_similarity(\n    model="endpoints:/databricks-llama-3-1-405b-instruct"\n)\n'})}),"\n",(0,s.jsx)(n.h4,{id:"overriding-default-judge-parameters",children:"Overriding Default Judge Parameters"}),"\n",(0,s.jsx)(n.p,{children:"By default, MLflow queries the judge LLM model with the following parameters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"temperature: 0.0\nmax_tokens: 200\ntop_p: 1.0\n"})}),"\n",(0,s.jsxs)(n.p,{children:["However, this might not be suitable for all LLM providers. For example, accessing Anthropic's Claude models on Amazon Bedrock requires an ",(0,s.jsx)(n.code,{children:"anthropic_version"})," parameter to be specified in the request payload. You can override these default parameters by passing the ",(0,s.jsx)(n.code,{children:"parameters"})," argument to the metrics definition."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'my_answer_similarity = mlflow.metrics.genai.answer_similarity(\n    model="bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0",\n    parameters={\n        "temperature": 0,\n        "max_tokens": 256,\n        "anthropic_version": "bedrock-2023-05-31",\n    },\n)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Note that the parameters dictionary you pass in the ",(0,s.jsx)(n.code,{children:"parameters"})," argument will ",(0,s.jsx)(n.strong,{children:"replace the default parameters"}),", instead of being merged with them. For example, ",(0,s.jsx)(n.code,{children:"top_p"})," will ",(0,s.jsx)(n.strong,{children:"not"})," be sent to the model in the above code example."]}),"\n",(0,s.jsx)(n.h4,{id:"creating-custom-llm-as-a-judge-metrics",children:"Creating Custom LLM-as-a-Judge Metrics"}),"\n",(0,s.jsxs)(n.p,{children:["You can also create your own LLM-as-a-Judge evaluation metrics with ",(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.make_genai_metric"})," API, which needs the following information:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"name"}),": the name of your custom metric."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"definition"}),": describe what's the metric doing."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"grading_prompt"}),": describe the scoring criteria."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"examples"})," (Optional): a few input/output examples with scores provided; used as a reference for the LLM judge."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["See the ",(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.make_genai_metric",children:"API documentation"})," for the full list of the configurations."]}),"\n",(0,s.jsxs)(n.p,{children:["Under the hood, ",(0,s.jsx)(n.code,{children:"definition"}),", ",(0,s.jsx)(n.code,{children:"grading_prompt"}),", ",(0,s.jsx)(n.code,{children:"examples"}),' together with evaluation data and model output will be\ncomposed into a long prompt and sent to LLM. If you are familiar with the concept of prompt engineering,\nSaaS LLM evaluation metric is basically trying to compose a "right" prompt containing instructions, data and model\noutput so that LLM, e.g., GPT4 can output the information we want.']}),"\n",(0,s.jsx)(n.p,{children:'Now let\'s create a custom GenAI metrics called "professionalism", which measures how professional our model output is.'}),"\n",(0,s.jsxs)(n.p,{children:["Let's first create a few examples with scores, these will be the reference samples LLM judge uses. To create such examples,\nwe will use ",(0,s.jsx)(o.B,{fn:"mlflow.metrics.genai.EvaluationExample"})," class, which has 4 fields:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"input: input text."}),"\n",(0,s.jsx)(n.li,{children:"output: output text."}),"\n",(0,s.jsx)(n.li,{children:"score: the score for output in the context of input."}),"\n",(0,s.jsxs)(n.li,{children:["justification: why do we give the ",(0,s.jsx)(n.code,{children:"score"})," for the data."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'professionalism_example_score_2 = mlflow.metrics.genai.EvaluationExample(\n    input="What is MLflow?",\n    output=(\n        "MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps "\n        "you track experiments, package your code and models, and collaborate with your team, making the whole ML "\n        "workflow smoother. It\'s like your Swiss Army knife for machine learning!"\n    ),\n    score=2,\n    justification=(\n        "The response is written in a casual tone. It uses contractions, filler words such as \'like\', and "\n        "exclamation points, which make it sound less professional. "\n    ),\n)\nprofessionalism_example_score_4 = mlflow.metrics.genai.EvaluationExample(\n    input="What is MLflow?",\n    output=(\n        "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was "\n        "developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is "\n        "designed to address the challenges that data scientists and machine learning engineers face when "\n        "developing, training, and deploying machine learning models.",\n    ),\n    score=4,\n    justification=("The response is written in a formal language and a neutral tone. "),\n)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Now let's define the ",(0,s.jsx)(n.code,{children:"professionalism"})," metric, you will see how each field is set up."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'professionalism = mlflow.metrics.genai.make_genai_metric(\n    name="professionalism",\n    definition=(\n        "Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is "\n        "tailored to the context and audience. It often involves avoiding overly casual language, slang, or "\n        "colloquialisms, and instead using clear, concise, and respectful language."\n    ),\n    grading_prompt=(\n        "Professionalism: If the answer is written using a professional tone, below are the details for different scores: "\n        "- Score 0: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for "\n        "professional contexts."\n        "- Score 1: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in "\n        "some informal professional settings."\n        "- Score 2: Language is overall formal but still have casual words/phrases. Borderline for professional contexts."\n        "- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. "\n        "- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal "\n        "business or academic settings. "\n    ),\n    examples=[professionalism_example_score_2, professionalism_example_score_4],\n    model="openai:/gpt-4o-mini",\n    parameters={"temperature": 0.0},\n    aggregations=["mean", "variance"],\n    greater_is_better=True,\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"prepare-your-target-models",children:"Prepare Your Target Models"}),"\n",(0,s.jsxs)(n.p,{children:["In order to evaluate your model with ",(0,s.jsx)(n.code,{children:"mlflow.evaluate()"}),", your model has to be one of the following types:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(o.B,{fn:"mlflow.pyfunc.PyFuncModel"})," instance or a URI pointing to a logged ",(0,s.jsx)(n.code,{children:"mlflow.pyfunc.PyFuncModel"})," model. In\ngeneral we call that MLflow model. The"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"A python function that takes in string inputs and outputs a single string. Your callable must match the signature of"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(o.B,{fn:"mlflow.pyfunc.PyFuncModel.predict"})," (without ",(0,s.jsx)(n.code,{children:"params"})," argument),\nbriefly it should:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Has ",(0,s.jsx)(n.code,{children:"data"})," as the only argument, which can be a ",(0,s.jsx)(n.code,{children:"pandas.Dataframe"}),", ",(0,s.jsx)(n.code,{children:"numpy.ndarray"}),", python list, dictionary or scipy matrix."]}),"\n",(0,s.jsxs)(n.li,{children:["Returns one of ",(0,s.jsx)(n.code,{children:"pandas.DataFrame"}),", ",(0,s.jsx)(n.code,{children:"pandas.Series"}),", ",(0,s.jsx)(n.code,{children:"numpy.ndarray"})," or list."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["An MLflow Deployments endpoint URI pointing to a local ",(0,s.jsx)(n.a,{href:"/llms/deployments/",children:"MLflow AI Gateway"}),", ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html",children:"Databricks Foundation Models API"}),", and ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/generative-ai/external-models/index.html",children:"External Models in Databricks Model Serving"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Set ",(0,s.jsx)(n.code,{children:"model=None"}),", and put model outputs in ",(0,s.jsx)(n.code,{children:"data"}),". Only applicable when the data is a Pandas dataframe."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"evaluating-with-an-mlflow-model",children:"Evaluating with an MLflow Model"}),"\n",(0,s.jsxs)(n.p,{children:["For detailed instruction on how to convert your model into a ",(0,s.jsx)(n.code,{children:"mlflow.pyfunc.PyFuncModel"})," instance, please read\n",(0,s.jsx)(n.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models",children:"this doc"}),". But in short,\nto evaluate your model as an MLflow model, we recommend following the steps below:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Log your model to MLflow server by ",(0,s.jsx)(n.code,{children:"log_model"}),". Each flavor (",(0,s.jsx)(n.code,{children:"opeanai"}),", ",(0,s.jsx)(n.code,{children:"pytorch"}),", ...)\nhas its own ",(0,s.jsx)(n.code,{children:"log_model"})," API, e.g., ",(0,s.jsx)(o.B,{fn:"mlflow.openai.log_model"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'with mlflow.start_run():\n    system_prompt = "Answer the following question in two sentences"\n    # Wrap "gpt-4o-mini" as an MLflow model.\n    logged_model_info = mlflow.openai.log_model(\n        model="gpt-4o-mini",\n        task=openai.chat.completions,\n        name="model",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": "{question}"},\n        ],\n    )\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Use the URI of logged model as the model instance in ",(0,s.jsx)(n.code,{children:"mlflow.evaluate()"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = mlflow.evaluate(\n    logged_model_info.model_uri,\n    eval_data,\n    targets="ground_truth",\n    model_type="question-answering",\n)\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"llm-eval-custom-function",children:"Evaluating with a Custom Function"}),"\n",(0,s.jsxs)(n.p,{children:["As of MLflow 2.8.0, ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," supports evaluating a python function without requiring\nlogging the model to MLflow. This is useful when you don't want to log the model and just want to evaluate\nit. The following example uses ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," to evaluate a function. You also need to set\nup OpenAI authentication to run the code below."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\nimport pandas as pd\nfrom typing import List\n\neval_data = pd.DataFrame(\n    {\n        "inputs": [\n            "What is MLflow?",\n            "What is Spark?",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.",\n            "Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks",\n        ],\n    }\n)\n\n\ndef openai_qa(inputs: pd.DataFrame) -> List[str]:\n    predictions = []\n    system_prompt = "Please answer the following question in formal language."\n\n    for _, row in inputs.iterrows():\n        completion = openai.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": row["inputs"]},\n            ],\n        )\n        predictions.append(completion.choices[0].message.content)\n\n    return predictions\n\n\nwith mlflow.start_run():\n    results = mlflow.evaluate(\n        model=openai_qa,\n        data=eval_data,\n        targets="ground_truth",\n        model_type="question-answering",\n    )\n\nprint(results.metrics)\n'})}),"\n",(0,s.jsx)(n.p,{children:"Output:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'{\n    "flesch_kincaid_grade_level/v1/mean": 14.75,\n    "flesch_kincaid_grade_level/v1/variance": 0.5625,\n    "flesch_kincaid_grade_level/v1/p90": 15.35,\n    "ari_grade_level/v1/mean": 18.15,\n    "ari_grade_level/v1/variance": 0.5625,\n    "ari_grade_level/v1/p90": 18.75,\n    "exact_match/v1": 0.0,\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"evaluating-with-an-mlflow-deployments-endpoint",children:"Evaluating with an MLflow Deployments Endpoint"}),"\n",(0,s.jsxs)(n.p,{children:["For MLflow >= 2.11.0, ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," supports evaluating a model endpoint by directly passing the MLflow Deployments endpoint URI to the ",(0,s.jsx)(n.code,{children:"model"})," argument.\nThis is particularly useful when you want to evaluate a deployed model hosted by a local ",(0,s.jsx)(n.a,{href:"/llms/deployments/",children:"MLflow AI Gateway"}),", ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html",children:"Databricks Foundation Models API"}),", and ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/generative-ai/external-models/index.html",children:"External Models in Databricks Model Serving"}),", without implementing custom prediction logic to wrap it as an MLflow model or a python function."]}),"\n",(0,s.jsxs)(n.p,{children:["Please don't forget to set the target deployment client by using ",(0,s.jsx)(o.B,{fn:"mlflow.deployments.set_deployments_target"})," before calling ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," with the endpoint URI, as shown in the example below. Otherwise, you will see an error message like ",(0,s.jsx)(n.code,{children:"MlflowException: No deployments target has been set..."}),"."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["When you want to use an endpoint ",(0,s.jsx)(n.strong,{children:"not"})," hosted by an MLflow AI Gateway or Databricks, you can create a custom Python function following the ",(0,s.jsx)(n.a,{href:"#llm-eval-custom-function",children:"Evaluating with a Custom Function"})," guide and use it as the ",(0,s.jsx)(n.code,{children:"model"})," argument."]})}),"\n",(0,s.jsx)(n.h4,{id:"supported-input-data-formats",children:"Supported Input Data Formats"}),"\n",(0,s.jsx)(n.p,{children:"The input data can be either of the following format when using an URI of the MLflow Deployment Endpoint as the model:"}),"\n",(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Data Format"}),(0,s.jsx)("th",{children:"Example"}),(0,s.jsx)("th",{children:"Additional Notes"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"A pandas DataFrame with a string column."}),(0,s.jsx)("td",{style:{maxWidth:"300px"},children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'pd.DataFrame(\n    {\n        "inputs": [\n            "What is MLflow?",\n            "What is Spark?",\n        ]\n    }\n)\n'})})}),(0,s.jsxs)("td",{children:["For this input format, MLflow will construct the appropriate request payload to the model endpoint type. For example, if your model is a chat endpoint (",(0,s.jsx)(n.code,{children:"llm/v1/chat"}),"), MLflow will wrap your input string with the chat messages format like ",(0,s.jsx)(n.code,{children:'{"messages": [{"role": "user", "content": "What is MLflow?"}]}'}),". If you want to customize the request payload e.g. including system prompt, please use the next format."]})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"A pandas DataFrame with a dictionary column."}),(0,s.jsx)("td",{style:{maxWidth:"300px"},children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'pd.DataFrame(\n    {\n        "inputs": [\n            {\n                "messages": [\n                    {"role": "system", "content": "Please answer."},\n                    {"role": "user", "content": "What is MLflow?"},\n                ],\n                "max_tokens": 100,\n            },\n            # ... more dictionary records\n        ]\n    }\n)\n'})})}),(0,s.jsxs)("td",{children:["In this format, the dictionary should have the correct request format for your model endpoint. Please refer to the ",(0,s.jsx)(n.a,{href:"/llms/deployments/#standard-deployments-parameters",children:"MLflow Deployments documentation"})," for more information about the request format for different model endpoint types."]})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"A list of input strings."}),(0,s.jsx)("td",{style:{maxWidth:"300px"},children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'[\n    "What is MLflow?",\n    "What is Spark?",\n]\n'})})}),(0,s.jsxs)("td",{children:["The ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," also accepts a list input."]})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"A list of request payload (dictionary)."}),(0,s.jsx)("td",{style:{maxWidth:"300px"},children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'[\n    {\n        "messages": [\n            {"role": "system", "content": "Please answer."},\n            {"role": "user", "content": "What is MLflow?"},\n        ],\n        "max_tokens": 100,\n    },\n    # ... more dictionary records\n]\n'})})}),(0,s.jsx)("td",{children:"Similarly to Pandas DataFrame input, the dictionary should have the correct request format for your model endpoint."})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"passing-inference-parameters",children:"Passing Inference Parameters"}),"\n",(0,s.jsxs)(n.p,{children:["You can pass additional inference parameters such as ",(0,s.jsx)(n.code,{children:"max_tokens"}),", ",(0,s.jsx)(n.code,{children:"temperature"}),", ",(0,s.jsx)(n.code,{children:"n"}),", to the model endpoint by setting the ",(0,s.jsx)(n.code,{children:"inference_params"})," argument in ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"}),". The ",(0,s.jsx)(n.code,{children:"inference_params"})," argument is a dictionary that contains the parameters to be passed to the model endpoint. The specified parameters are used for all the input record in the evaluation dataset."]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["When your input is a dictionary format that represents request payload, it can also include the parameters like ",(0,s.jsx)(n.code,{children:"max_tokens"}),". If there are overlapping parameters in both the ",(0,s.jsx)(n.code,{children:"inference_params"})," and the input data, the values in the ",(0,s.jsx)(n.code,{children:"inference_params"})," will take precedence."]})}),"\n",(0,s.jsx)(n.h4,{id:"examples",children:"Examples"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Chat Endpoint hosted by a local"})," ",(0,s.jsx)(n.a,{href:"/llms/deployments/",children:"MLflow AI Gateway"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.deployments import set_deployments_target\nimport pandas as pd\n\n# Point the client to the local MLflow AI Gateway\nset_deployments_target("http://localhost:5000")\n\neval_data = pd.DataFrame(\n    {\n        # Input data must be a string column and named "inputs".\n        "inputs": [\n            "What is MLflow?",\n            "What is Spark?",\n        ],\n        # Additional ground truth data for evaluating the answer\n        "ground_truth": [\n            "MLflow is an open-source platform ....",\n            "Apache Spark is an open-source, ...",\n        ],\n    }\n)\n\n\nwith mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        model="endpoints:/my-chat-endpoint",\n        data=eval_data,\n        targets="ground_truth",\n        inference_params={"max_tokens": 100, "temperature": 0.0},\n        model_type="question-answering",\n    )\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Completion Endpoint hosted on"})," ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html",children:"Databricks Foundation Models API"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.deployments import set_deployments_target\nimport pandas as pd\n\n# Point the client to Databricks Foundation Models API\nset_deployments_target("databricks")\n\neval_data = pd.DataFrame(\n    {\n        # Input data must be a string column and named "inputs".\n        "inputs": [\n            "Write 3 reasons why you should use MLflow?",\n            "Can you explain the difference between classification and regression?",\n        ],\n    }\n)\n\n\nwith mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        model="endpoints:/databricks-mpt-7b-instruct",\n        data=eval_data,\n        inference_params={"max_tokens": 100, "temperature": 0.0},\n        model_type="text",\n    )\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Evaluating ",(0,s.jsx)(n.a,{href:"https://docs.databricks.com/en/generative-ai/external-models/index.html",children:"External Models in Databricks Model Serving"})," can be done in the same way, you just need to specify the different URI that points to the serving endpoint like ",(0,s.jsx)(n.code,{children:'"endpoints:/your-chat-endpoint"'}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"llm-eval-static-dataset",children:"Evaluating with a Static Dataset"}),"\n",(0,s.jsxs)(n.p,{children:["For MLflow >= 2.8.0, ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"})," supports evaluating a static dataset without specifying a model.\nThis is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and\nwant to evaluate the static dataset without re-running the model."]}),"\n",(0,s.jsxs)(n.p,{children:["If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the\ntop-level ",(0,s.jsx)(n.code,{children:"predictions"})," parameter in ",(0,s.jsx)(o.B,{fn:"mlflow.evaluate"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\n\neval_data = pd.DataFrame(\n    {\n        "inputs": [\n            "What is MLflow?",\n            "What is Spark?",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. "\n            "It was developed by Databricks, a company that specializes in big data and machine learning solutions. "\n            "MLflow is designed to address the challenges that data scientists and machine learning engineers "\n            "face when developing, training, and deploying machine learning models.",\n            "Apache Spark is an open-source, distributed computing system designed for big data processing and "\n            "analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, "\n            "offering improvements in speed and ease of use. Spark provides libraries for various tasks such as "\n            "data ingestion, processing, and analysis through its components like Spark SQL for structured data, "\n            "Spark Streaming for real-time data processing, and MLlib for machine learning tasks",\n        ],\n        "predictions": [\n            "MLflow is an open-source platform that provides handy tools to manage Machine Learning workflow "\n            "lifecycle in a simple way",\n            "Spark is a popular open-source distributed computing system designed for big data processing and analytics.",\n        ],\n    }\n)\n\nwith mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        data=eval_data,\n        targets="ground_truth",\n        predictions="predictions",\n        extra_metrics=[mlflow.metrics.genai.answer_similarity()],\n        evaluators="default",\n    )\n    print(f"See aggregated evaluation results below: \\n{results.metrics}")\n\n    eval_table = results.tables["eval_results_table"]\n    print(f"See evaluation table below: \\n{eval_table}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"viewing-evaluation-results",children:"Viewing Evaluation Results"}),"\n",(0,s.jsx)(n.h3,{id:"view-evaluation-results-via-code",children:"View Evaluation Results via Code"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"mlflow.evaluate()"})," returns the evaluation results as an ",(0,s.jsx)(o.B,{fn:"mlflow.models.EvaluationResult"})," instance.\nTo see the score on selected metrics, you can check:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"metrics"}),": stores the aggregated results, like average/variance across the evaluation dataset. Let's take a second\npass on the code example above and focus on printing out the aggregated results."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'with mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        data=eval_data,\n        targets="ground_truth",\n        predictions="predictions",\n        extra_metrics=[mlflow.metrics.genai.answer_similarity()],\n        evaluators="default",\n    )\n    print(f"See aggregated evaluation results below: \\n{results.metrics}")\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:'tables["eval_results_table"]'}),": stores the per-row evaluation results."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'with mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        data=eval_data,\n        targets="ground_truth",\n        predictions="predictions",\n        extra_metrics=[mlflow.metrics.genai.answer_similarity()],\n        evaluators="default",\n    )\n    print(\n        f"See per-data evaluation results below: \\n{results.tables[\'eval_results_table\']}"\n    )\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"view-evaluation-results-via-the-mlflow-ui",children:"View Evaluation Results via the MLflow UI"}),"\n",(0,s.jsx)(n.p,{children:"Your evaluation result is automatically logged into MLflow server, so you can view your evaluation results directly from the\nMLflow UI. To view the evaluation results on MLflow UI, please follow the steps below:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Go to the experiment view of your MLflow experiment."}),"\n",(0,s.jsx)(n.li,{children:'Select the "Evaluation" tab.'}),"\n",(0,s.jsx)(n.li,{children:"Select the runs you want to check evaluation results."}),"\n",(0,s.jsx)(n.li,{children:"Select the metrics from the dropdown menu on the right side."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Please see the screenshot below for clarity:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Demo UI of MLflow evaluate",src:t(42008).A+"",width:"3034",height:"1716"})})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>_});var a=t(96540),s=t(34164),i=t(65627),l=t(56347),r=t(50372),o=t(30604),c=t(11861),d=t(78749);function h(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:s}}=e;return{value:n,label:t,attributes:a,default:s}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:t}=e;const s=(0,l.W6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,o.aZ)(i),(0,a.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(s.location.search);n.set(i,e),s.replace({...s.location,search:n.toString()})}),[i,s])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,i=m(e),[l,o]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:i}))),[c,h]=u({queryString:t,groupId:s}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[s,i]=(0,d.Dv)(t);return[s,(0,a.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:s}),w=(()=>{const e=c??f;return p({value:e,tabValues:i})?e:null})();(0,r.A)((()=>{w&&o(w)}),[w]);return{selectedValue:l,selectValue:(0,a.useCallback)((e=>{if(!p({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);o(e),h(e),g(e)}),[h,g,i]),tabValues:i}}var g=t(9136);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function v(e){let{className:n,block:t,selectedValue:a,selectValue:l,tabValues:r}=e;const o=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),d=e=>{const n=e.currentTarget,t=o.indexOf(n),s=r[t].value;s!==a&&(c(n),l(s))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:i}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{o.push(e)},onKeyDown:h,onClick:d,...i,className:(0,s.A)("tabs__item",w.tabItem,i?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function y(e){let{lazy:n,children:t,selectedValue:i}=e;const l=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=l.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:l.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function j(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,s.A)("tabs-container",w.tabList),children:[(0,x.jsx)(v,{...n,...e}),(0,x.jsx)(y,{...n,...e})]})}function _(e){const n=(0,g.A)();return(0,x.jsx)(j,{...e,children:h(e.children)},String(n))}},67756:(e,n,t)=>{t.d(n,{B:()=>o});t(96540);const a=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var s=t(29030),i=t(56289),l=t(74848);const r=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(a[e])return e}return null};function o(e){let{fn:n,children:t}=e;const o=r(n);if(!o)return(0,l.jsx)(l.Fragment,{children:t});const c=(0,s.Ay)(`/${a[o]}#${n}`);return(0,l.jsx)(i.A,{to:c,target:"_blank",children:t??(0,l.jsxs)("code",{children:[n,"()"]})})}},79329:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var a=t(34164);const s={tabItem:"tabItem_Ymn6"};var i=t(74848);function l(e){let{children:n,hidden:t,className:l}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,a.A)(s.tabItem,l),hidden:t,children:n})}}}]);