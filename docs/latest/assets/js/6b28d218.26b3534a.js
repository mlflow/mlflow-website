"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[5924],{28453:(e,t,r)=>{r.d(t,{R:()=>s,x:()=>o});var n=r(96540);const i={},a=n.createContext(i);function s(e){const t=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),n.createElement(a.Provider,{value:t},e.children)}},66927:(e,t,r)=>{r.d(t,{A:()=>s});r(96540);const n={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var i=r(86025),a=r(74848);function s({src:e,alt:t,width:r,caption:s,className:o}){return(0,a.jsxs)("div",{className:`${n.container} ${o||""}`,children:[(0,a.jsx)("div",{className:n.imageWrapper,style:r?{width:r}:{},children:(0,a.jsx)("img",{src:(0,i.Ay)(e),alt:t,className:n.image})}),s&&(0,a.jsx)("p",{className:n.caption,children:s})]})}},73484:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"getting-started/hyperparameter-tuning/index","title":"Tracking Hyperparameter Tuning with MLflow","description":"Hyperparameter tuning is an important process for improving the performance of a machine learning model, however, it can be cumbersome to manually track and compare the different trials. MLflow provides a powerful framework for hyperparameter tuning that allows you to systematically explore the hyperparameter space and find the best model.","source":"@site/docs/classic-ml/getting-started/hyperparameter-tuning/index.mdx","sourceDirName":"getting-started/hyperparameter-tuning","slug":"/getting-started/hyperparameter-tuning/","permalink":"/mlflow-website/docs/latest/ml/getting-started/hyperparameter-tuning/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"toc_max_heading_level":4,"sidebar_label":"Hyperparameter Tuning"},"sidebar":"classicMLSidebar","previous":{"title":"Quickstart","permalink":"/mlflow-website/docs/latest/ml/getting-started/quickstart"},"next":{"title":"Deep Learning","permalink":"/mlflow-website/docs/latest/ml/getting-started/deep-learning"}}');var i=r(74848),a=r(28453),s=r(66927);const o={sidebar_position:3,toc_max_heading_level:4,sidebar_label:"Hyperparameter Tuning"},l="Tracking Hyperparameter Tuning with MLflow",c={},p=[{value:"Prerequisites: Set up MLflow and Optuna",id:"prerequisites-set-up-mlflow-and-optuna",level:2},{value:"Step 1: Create a new experiment",id:"step-1-create-a-new-experiment",level:2},{value:"Step 2: Prepare Your Data",id:"step-2-prepare-your-data",level:2},{value:"Step 3: Define the objective function",id:"step-3-define-the-objective-function",level:2},{value:"Step 3: Run the hyperparameter tuning study",id:"step-3-run-the-hyperparameter-tuning-study",level:2},{value:"Step 4: View the results in the MLflow UI",id:"step-4-view-the-results-in-the-mlflow-ui",level:2},{value:"Step 5: Register Your Best Model",id:"step-5-register-your-best-model",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"tracking-hyperparameter-tuning-with-mlflow",children:"Tracking Hyperparameter Tuning with MLflow"})}),"\n",(0,i.jsx)(s.A,{src:"/images/tutorials/introductory/hyperparameter-tuning/ui-compare-metrics.png",alt:"MLflow UI Chart page",width:"80%"}),"\n",(0,i.jsx)(t.p,{children:"Hyperparameter tuning is an important process for improving the performance of a machine learning model, however, it can be cumbersome to manually track and compare the different trials. MLflow provides a powerful framework for hyperparameter tuning that allows you to systematically explore the hyperparameter space and find the best model."}),"\n",(0,i.jsx)(t.p,{children:"By the end of this tutorial, you'll know how to:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Set up your environment with MLflow tracking."}),"\n",(0,i.jsx)(t.li,{children:"Define a partial function that fits a machine learning model that can be used with a hyperparameter tuning library."}),"\n",(0,i.jsx)(t.li,{children:"Use Optuna for hyperparameter tuning."}),"\n",(0,i.jsx)(t.li,{children:"Leverage child runs within MLflow to keep track of each iteration during the hyperparameter tuning process."}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"prerequisites-set-up-mlflow-and-optuna",children:"Prerequisites: Set up MLflow and Optuna"}),"\n",(0,i.jsx)(t.p,{children:"MLflow is available on PyPI. Install MLflow and Optuna (Hyperparameter tuning library) with:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bash",children:"pip install mlflow optuna\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Then, follow the instructions in the ",(0,i.jsx)(t.a,{href:"/ml/getting-started/running-notebooks",children:"Set Up MLflow"})," guide to set up MLflow."]}),"\n",(0,i.jsx)(t.admonition,{title:"Team Collaboration and Managed Setup",type:"tip",children:(0,i.jsxs)(t.p,{children:["For production environments or team collaboration, consider hosting a shared ",(0,i.jsx)("ins",{children:(0,i.jsx)(t.a,{href:"/self-hosting",children:"MLflow Tracking Server"})}),". For a fully-managed solution, get started with Databricks Free Trial by visiting the ",(0,i.jsx)("ins",{children:(0,i.jsx)(t.a,{href:"https://signup.databricks.com/?destination_url=/ml/experiments-signup?source=OSS_DOCS&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW&utm_source=OSS_DOCS",children:"Databricks Trial Signup Page"})})," and follow the instructions outlined there."]})}),"\n",(0,i.jsx)(t.h2,{id:"step-1-create-a-new-experiment",children:"Step 1: Create a new experiment"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import mlflow\n\n# The set_experiment API creates a new experiment if it doesn\'t exist.\nmlflow.set_experiment("Hyperparameter Tuning Experiment")\n'})}),"\n",(0,i.jsx)(t.h2,{id:"step-2-prepare-your-data",children:"Step 2: Prepare Your Data"}),"\n",(0,i.jsx)(t.p,{children:"First, let's load a sample dataset and split it into training and validation sets:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\n\nX, y = fetch_california_housing(return_X_y=True)\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n"})}),"\n",(0,i.jsx)(t.h2,{id:"step-3-define-the-objective-function",children:"Step 3: Define the objective function"}),"\n",(0,i.jsxs)(t.p,{children:["In Optuna, a ",(0,i.jsx)(t.strong,{children:"study"})," is a single optimization task, representing the entire hyperparameter tuning\nsession consisting of multiple ",(0,i.jsx)(t.strong,{children:"trials"}),". A trial is a single execution of the objective function,\nnamely, training a model with a single combination of hyperparameters."]}),"\n",(0,i.jsxs)(t.p,{children:["In MLflow, this structure is represented by a ",(0,i.jsx)(t.strong,{children:"parent run"})," and ",(0,i.jsx)(t.strong,{children:"child runs"}),". A parent run is a\nrun that contains all the child runs for different trials. The parent-child relationship allows us\nto keep track of each trial during the hyperparameter tuning process and group them nicely in the\nMLflow UI."]}),"\n",(0,i.jsxs)(t.p,{children:["First, let's define the objective function that is executed for each trial. To log the parameters,\nmetrics, and model file, we use MLflow's API inside the objective function. An MLflow run is created\nwith the ",(0,i.jsx)(t.code,{children:"nested=True"})," flag to indicate it is a child run."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport optuna\nimport sklearn\n\n\ndef objective(trial):\n    # Setting nested=True will create a child run under the parent run.\n    with mlflow.start_run(nested=True, run_name=f"trial_{trial.number}") as child_run:\n        rf_max_depth = trial.suggest_int("rf_max_depth", 2, 32)\n        rf_n_estimators = trial.suggest_int("rf_n_estimators", 50, 300, step=10)\n        rf_max_features = trial.suggest_float("rf_max_features", 0.2, 1.0)\n        params = {\n            "max_depth": rf_max_depth,\n            "n_estimators": rf_n_estimators,\n            "max_features": rf_max_features,\n        }\n        # Log current trial\'s parameters\n        mlflow.log_params(params)\n\n        regressor_obj = sklearn.ensemble.RandomForestRegressor(**params)\n        regressor_obj.fit(X_train, y_train)\n\n        y_pred = regressor_obj.predict(X_val)\n        error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n        # Log current trial\'s error metric\n        mlflow.log_metrics({"error": error})\n\n        # Log the model file\n        mlflow.sklearn.log_model(regressor_obj, name="model")\n        # Make it easy to retrieve the best-performing child run later\n        trial.set_user_attr("run_id", child_run.info.run_id)\n        return error\n'})}),"\n",(0,i.jsx)(t.h2,{id:"step-3-run-the-hyperparameter-tuning-study",children:"Step 3: Run the hyperparameter tuning study"}),"\n",(0,i.jsx)(t.p,{children:"Now, let's run the hyperparameter tuning study using Optuna. We create a parent run named\n\"study\" and log the best trial's parameters and metrics there."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Create a parent run that contains all child runs for different trials\nwith mlflow.start_run(run_name="study") as run:\n    # Log the experiment settings\n    n_trials = 30\n    mlflow.log_param("n_trials", n_trials)\n\n    study = optuna.create_study(direction="minimize")\n    study.optimize(objective, n_trials=n_trials)\n\n    # Log the best trial and its run ID\n    mlflow.log_params(study.best_trial.params)\n    mlflow.log_metrics({"best_error": study.best_value})\n    if best_run_id := study.best_trial.user_attrs.get("run_id"):\n        mlflow.log_param("best_child_run_id", best_run_id)\n'})}),"\n",(0,i.jsx)(t.h2,{id:"step-4-view-the-results-in-the-mlflow-ui",children:"Step 4: View the results in the MLflow UI"}),"\n",(0,i.jsxs)(t.p,{children:["To see the results of training, you can access the MLflow UI by navigating to the URL of the Tracking Server. If you have not started one, open a new terminal and run the following command at the root of the MLflow project and access the UI at ",(0,i.jsx)(t.a,{href:"http://localhost:5000",children:"http://localhost:5000"})," (or the port number you specified)."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bash",children:"mlflow ui --port 5000\n"})}),"\n",(0,i.jsx)(t.p,{children:"When opening the site, you will see a screen similar to the following:"}),"\n",(0,i.jsx)(s.A,{src:"/images/tutorials/introductory/hyperparameter-tuning/ui-home.png",alt:"MLflow UI Home page"}),"\n",(0,i.jsx)(t.p,{children:'The "Experiments" section shows a list of (recently created) experiments. Click on the "Hyperparameter Tuning Experiment" experiment we\'ve created for this tutorial.'}),"\n",(0,i.jsx)(s.A,{src:"/images/tutorials/introductory/hyperparameter-tuning/ui-run-list.png",alt:"MLflow UI Run list page"}),"\n",(0,i.jsx)(t.p,{children:"Click the chart icon on the top left corner to view a visual representation of the tuning result. You can further click each child run to see the detailed metrics and parameters for each trial."}),"\n",(0,i.jsx)(s.A,{src:"/images/tutorials/introductory/hyperparameter-tuning/ui-compare-metrics.png",alt:"MLflow UI Chart page"}),"\n",(0,i.jsx)(t.h2,{id:"step-5-register-your-best-model",children:"Step 5: Register Your Best Model"}),"\n",(0,i.jsxs)(t.p,{children:["Once you identified the best trial, you can register the model into ",(0,i.jsx)(t.a,{href:"/ml/model-registry",children:"MLflow Model Registry"})," for promoting it to production."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"# Register the best model using the model URI\nmlflow.register_model(\n    model_uri=\"runs:/d0210c58afff4737a306a2fbc5f1ff8d/model\",\n    name=\"housing-price-predictor\",\n)\n\n# > Successfully registered model 'housing-price-predictor'.\n# > Created version '1' of model 'housing-price-predictor'.\n"})}),"\n",(0,i.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/ml/tracking/",children:"MLflow Tracking"}),": Learn more about the MLflow Tracking APIs."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/ml/model-registry",children:"MLflow Model Registry"}),": Learn how to register and manage model lifecycle in the MLflow Model Registry."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/ml/deep-learning",children:"MLflow for Deep Learning"}),": Learn how to use MLflow for deep learning frameworks such as PyTorch, TensorFlow, etc."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/self-hosting",children:"Self-hosting Guide"}),": Learn how to self-host the MLflow Tracking Server and set it up for team collaboration."]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);