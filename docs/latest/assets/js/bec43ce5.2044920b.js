"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["2006"],{10932(e,t,n){n.r(t),n.d(t,{metadata:()=>a,default:()=>y,frontMatter:()=>f,contentTitle:()=>u,toc:()=>w,assets:()=>h});var a=JSON.parse('{"id":"datasets/end-to-end-workflow","title":"End-to-End Workflow: Evaluation-Driven Development","description":"This guide demonstrates the complete workflow for building and evaluating GenAI applications using MLflow\'s evaluation-driven development approach.","source":"@site/docs/genai/datasets/end-to-end-workflow.mdx","sourceDirName":"datasets","slug":"/datasets/end-to-end-workflow","permalink":"/mlflow-website/docs/latest/genai/datasets/end-to-end-workflow","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"SDK Guide","permalink":"/mlflow-website/docs/latest/genai/datasets/sdk-guide"},"next":{"title":"Feedback Collection","permalink":"/mlflow-website/docs/latest/genai/assessments/feedback"}}'),r=n(74848),o=n(28453),l=n(54725),i=n(46077),s=n(10440),p=n(77541),c=n(93164),m=n(80827),d=n(99653);let f={},u="End-to-End Workflow: Evaluation-Driven Development",h={},w=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Build &amp; Trace Your Application",id:"step-1-build--trace-your-application",level:2},{value:"Step 2: Capture Production Traces",id:"step-2-capture-production-traces",level:2},{value:"Step 3: Add Ground Truth Expectations",id:"step-3-add-ground-truth-expectations",level:2},{value:"Step 4: Create an Evaluation Dataset",id:"step-4-create-an-evaluation-dataset",level:2},{value:"Step 5: Run Systematic Evaluation",id:"step-5-run-systematic-evaluation",level:2},{value:"Step 6: Iterate and Improve",id:"step-6-iterate-and-improve",level:2},{value:"Next Steps",id:"next-steps",level:2}];function _(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"end-to-end-workflow-evaluation-driven-development",children:"End-to-End Workflow: Evaluation-Driven Development"})}),"\n",(0,r.jsx)(t.p,{children:"This guide demonstrates the complete workflow for building and evaluating GenAI applications using MLflow's evaluation-driven development approach."}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Databricks Users"}),": To use Evaluation Datasets with Databricks Unity Catalog, MLflow requires the additional installation of the ",(0,r.jsx)(t.code,{children:"databricks-agents"})," package. This package uses Unity Catalog to store datasets. Install it with: ",(0,r.jsx)(t.code,{children:"pip install databricks-agents"})]})}),"\n",(0,r.jsx)(t.admonition,{title:"SQL Backend Required",type:"warning",children:(0,r.jsxs)(t.p,{children:["Evaluation Datasets require an MLflow Tracking Server with a ",(0,r.jsx)(t.strong,{children:(0,r.jsx)(t.a,{href:"/self-hosting/architecture/backend-store/#types-of-backend-stores",children:"SQL backend"})})," (PostgreSQL, MySQL, SQLite, or MSSQL).\nThis feature is ",(0,r.jsx)(t.strong,{children:"not available"})," with FileStore (local file system-based tracking)."]})}),"\n",(0,r.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"pip install --upgrade 'mlflow[genai]>=3.4' openai\n"})}),"\n",(0,r.jsx)(t.h2,{id:"step-1-build--trace-your-application",children:"Step 1: Build & Trace Your Application"}),"\n",(0,r.jsxs)(t.p,{children:["Start with a traced GenAI application. This example shows a customer support bot, but the pattern applies to any LLM application. You can use the ",(0,r.jsx)(l.B,{fn:"mlflow.trace",children:"mlflow.trace decorator"})," for manual instrumentation or ",(0,r.jsx)(l.B,{fn:"mlflow.openai.autolog",children:"enable automatic tracing for OpenAI"})," as shown below."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import mlflow\nimport openai\nimport os\n\n# Configure environment\nos.environ["OPENAI_API_KEY"] = "your-api-key-here"\nmlflow.set_experiment("Customer Support Bot")\n\n# Enable automatic tracing for OpenAI\nmlflow.openai.autolog()\n\n\nclass CustomerSupportBot:\n    def __init__(self):\n        self.client = openai.OpenAI()\n        self.knowledge_base = {\n            "refund": "Full refunds within 30 days with receipt.",\n            "shipping": "Standard: 5-7 days. Express available.",\n            "warranty": "1-year manufacturer warranty included.",\n        }\n\n    @mlflow.trace\n    def answer(self, question: str) -> str:\n        # Retrieve relevant context\n        context = self._get_context(question)\n\n        # Generate response\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {"role": "system", "content": "You are a helpful support assistant."},\n                {\n                    "role": "user",\n                    "content": f"Context: {context}\\n\\nQuestion: {question}",\n                },\n            ],\n            temperature=0.3,\n        )\n        return response.choices[0].message.content\n\n    def _get_context(self, question: str) -> str:\n        # Simple keyword matching for demo\n        for key, value in self.knowledge_base.items():\n            if key in question.lower():\n                return value\n        return "General customer support information."\n\n\nbot = CustomerSupportBot()\n'})}),"\n",(0,r.jsx)(t.h2,{id:"step-2-capture-production-traces",children:"Step 2: Capture Production Traces"}),"\n",(0,r.jsxs)(t.p,{children:["Run your application with real or test scenarios to capture traces. Later, you'll use ",(0,r.jsx)(l.B,{fn:"mlflow.search_traces",children:"mlflow.search_traces()"})," to retrieve these traces for annotation and dataset creation."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Test scenarios\ntest_questions = [\n    "What is your refund policy?",\n    "How long does shipping take?",\n    "Is my product under warranty?",\n    "Can I get express shipping?",\n]\n\n# Capture traces - automatically logged to the active experiment\nfor question in test_questions:\n    response = bot.answer(question)\n'})}),"\n",(0,r.jsx)(t.h2,{id:"step-3-add-ground-truth-expectations",children:"Step 3: Add Ground Truth Expectations"}),"\n",(0,r.jsxs)(t.p,{children:["Add expectations to your traces to define what you expect as a response coming from your application. Use ",(0,r.jsx)(l.B,{fn:"mlflow.log_expectation",children:"mlflow.log_expectation()"})," to annotate traces with ground truth values that will serve as your evaluation baseline. You can also directly apply expectations within the UI."]}),"\n",(0,r.jsx)(i.A,{src:"/images/add-expectation-ui.png",alt:"Adding Expectations in UI"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Search for recent traces (uses current active experiment by default)\ntraces = mlflow.search_traces(\n    max_results=10, return_type="list"  # Return list of Trace objects for iteration\n)\n\n# Add expectations to specific traces\nfor trace in traces:\n    # Get the question from the root span inputs\n    root_span = trace.data._get_root_span()\n    question = (\n        root_span.inputs.get("question", "") if root_span and root_span.inputs else ""\n    )\n\n    if "refund" in question.lower():\n        mlflow.log_expectation(\n            trace_id=trace.info.trace_id,\n            name="key_information",\n            value={"must_mention": ["30 days", "receipt"], "tone": "helpful"},\n        )\n    elif "shipping" in question.lower():\n        mlflow.log_expectation(\n            trace_id=trace.info.trace_id,\n            name="key_information",\n            value={"must_mention": ["5-7 days"], "offers_express": True},\n        )\n'})}),"\n",(0,r.jsx)(t.h2,{id:"step-4-create-an-evaluation-dataset",children:"Step 4: Create an Evaluation Dataset"}),"\n",(0,r.jsxs)(t.p,{children:["Transform your annotated traces into a reusable evaluation dataset. Use ",(0,r.jsx)(l.B,{fn:"mlflow.genai.datasets.create_dataset",children:"create_dataset()"})," to initialize your dataset and ",(0,r.jsx)(l.B,{fn:"mlflow.entities.EvaluationDataset.merge_records",children:"merge_records()"})," to add test cases from multiple sources."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from mlflow.genai.datasets import create_dataset\n\n# Create dataset from current experiment\ndataset = create_dataset(\n    name="customer_support_qa_v1",\n    experiment_id=mlflow.get_experiment_by_name("Customer Support Bot").experiment_id,\n    tags={"stage": "validation", "domain": "customer_support"},\n)\n\n# Re-fetch traces to get the attached expectations\n# The expectations are now part of the trace data\nannotated_traces = mlflow.search_traces(\n    max_results=100,\n    return_type="list",  # Need list for merge_records\n)\n\n# Add traces to dataset\ndataset.merge_records(annotated_traces)\n\n# Optionally add manual test cases\nmanual_tests = [\n    {\n        "inputs": {"question": "Can I return an item after 45 days?"},\n        "expectations": {"should_clarify": "30-day policy", "tone": "apologetic"},\n    },\n    {\n        "inputs": {"question": "Do you ship internationally?"},\n        "expectations": {"provides_alternatives": True},\n    },\n]\ndataset.merge_records(manual_tests)\n'})}),"\n",(0,r.jsx)(t.h2,{id:"step-5-run-systematic-evaluation",children:"Step 5: Run Systematic Evaluation"}),"\n",(0,r.jsxs)(t.p,{children:["Evaluate your application against the dataset using built-in and custom scorers. Use ",(0,r.jsx)(l.B,{fn:"mlflow.genai.evaluate",children:"mlflow.genai.evaluate()"})," to run comprehensive evaluations with scorers like ",(0,r.jsx)(l.B,{fn:"mlflow.genai.scorers.Correctness",children:"Correctness"})," for factual accuracy assessment. You can also create custom scorers using the ",(0,r.jsx)(l.B,{fn:"mlflow.genai.scorers.scorer",children:"@scorer decorator"})," to evaluate domain-specific criteria."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from mlflow.genai import evaluate\nfrom mlflow.genai.scorers import Correctness, Guidelines, scorer\n\n\n# Define custom scorer for your specific needs\n@scorer\ndef contains_required_info(outputs: str, expectations: dict) -> float:\n    """Check if response contains required information."""\n    if "must_mention" not in expectations:\n        return 1.0\n\n    output_lower = outputs.lower()\n    mentioned = [term for term in expectations["must_mention"] if term in output_lower]\n    return len(mentioned) / len(expectations["must_mention"])\n\n\n# Configure evaluation\nscorers = [\n    Correctness(name="factual_accuracy"),\n    Guidelines(\n        name="support_quality",\n        guidelines="Response must be helpful, accurate, and professional",\n    ),\n    contains_required_info,\n]\n\n# Run evaluation\nresults = evaluate(\n    data=dataset,\n    predict_fn=bot.answer,\n    scorers=scorers,\n    model_id="customer-support-bot-v1",\n)\n\n# Access results\nmetrics = results.metrics\ndetailed_results = results.tables["eval_results_table"]\n'})}),"\n",(0,r.jsx)(t.h2,{id:"step-6-iterate-and-improve",children:"Step 6: Iterate and Improve"}),"\n",(0,r.jsx)(t.p,{children:"Use evaluation results to improve your application, then re-evaluate using the same dataset."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Analyze results\nlow_scores = detailed_results[detailed_results["factual_accuracy/score"] < 0.8]\nif not low_scores.empty:\n    # Identify patterns in failures\n    failed_questions = low_scores["inputs.question"].tolist()\n\n    # Example improvements based on failure analysis\n    bot.knowledge_base[\n        "refund"\n    ] = "Full refunds available within 30 days with original receipt. Store credit offered after 30 days."\n    bot.client.temperature = 0.2  # Reduce temperature for more consistent responses\n\n    # Re-evaluate with same dataset for comparison\n    improved_results = evaluate(\n        data=dataset,\n        predict_fn=bot.answer,  # Updated bot\n        scorers=scorers,\n        model_id="customer-support-bot-v2",\n    )\n\n    # Compare versions\n    improvement = (\n        improved_results.metrics["factual_accuracy/score"]\n        - metrics["factual_accuracy/score"]\n    )\n'})}),"\n",(0,r.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsx)(p.A,{icon:c.A,iconSize:48,title:"Custom Scorers",description:"Build sophisticated scorers for complex evaluation criteria",href:"/genai/eval-monitor/scorers",linkText:"Learn more \u2192",containerHeight:64}),(0,r.jsx)(p.A,{icon:m.A,iconSize:48,title:"SDK Reference",description:"Deep dive into dataset management APIs",href:"/genai/datasets/sdk-guide",linkText:"View guide \u2192",containerHeight:64}),(0,r.jsx)(p.A,{icon:d.A,iconSize:48,title:"Production Monitoring",description:"Set up continuous evaluation for production",href:"/genai/tracing/prod-tracing",linkText:"Learn more \u2192",containerHeight:64})]})]})}function y(e={}){let{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(_,{...e})}):_(e)}},75689(e,t,n){n.d(t,{A:()=>s});var a=n(96540);let r=e=>{let t=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,n)=>n?n.toUpperCase():t.toLowerCase());return t.charAt(0).toUpperCase()+t.slice(1)},o=(...e)=>e.filter((e,t,n)=>!!e&&""!==e.trim()&&n.indexOf(e)===t).join(" ").trim();var l={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let i=(0,a.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:r,className:i="",children:s,iconNode:p,...c},m)=>(0,a.createElement)("svg",{ref:m,...l,width:t,height:t,stroke:e,strokeWidth:r?24*Number(n)/Number(t):n,className:o("lucide",i),...!s&&!(e=>{for(let t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0})(c)&&{"aria-hidden":"true"},...c},[...p.map(([e,t])=>(0,a.createElement)(e,t)),...Array.isArray(s)?s:[s]])),s=(e,t)=>{let n=(0,a.forwardRef)(({className:n,...l},s)=>(0,a.createElement)(i,{ref:s,iconNode:t,className:o(`lucide-${r(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,n),...l}));return n.displayName=r(e),n}},93164(e,t,n){n.d(t,{A:()=>a});let a=(0,n(75689).A)("code",[["path",{d:"m16 18 6-6-6-6",key:"eg8j8"}],["path",{d:"m8 6-6 6 6 6",key:"ppft3o"}]])},80827(e,t,n){n.d(t,{A:()=>a});let a=(0,n(75689).A)("file-text",[["path",{d:"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z",key:"1rqfz7"}],["path",{d:"M14 2v4a2 2 0 0 0 2 2h4",key:"tnqrlb"}],["path",{d:"M10 9H8",key:"b1mrlr"}],["path",{d:"M16 13H8",key:"t4e002"}],["path",{d:"M16 17H8",key:"z1uh3a"}]])},99653(e,t,n){n.d(t,{A:()=>a});let a=(0,n(75689).A)("rocket",[["path",{d:"M4.5 16.5c-1.5 1.26-2 5-2 5s3.74-.5 5-2c.71-.84.7-2.13-.09-2.91a2.18 2.18 0 0 0-2.91-.09z",key:"m3kijz"}],["path",{d:"m12 15-3-3a22 22 0 0 1 2-3.95A12.88 12.88 0 0 1 22 2c0 2.72-.78 7.5-6 11a22.35 22.35 0 0 1-4 2z",key:"1fmvmk"}],["path",{d:"M9 12H4s.55-3.03 2-4c1.62-1.08 5 0 5 0",key:"1f8sc4"}],["path",{d:"M12 15v5s3.03-.55 4-2c1.08-1.62 0-5 0-5",key:"qeys4"}]])},54725(e,t,n){n.d(t,{B:()=>l});var a=n(74848);n(96540);var r=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),o=n(66497);function l({fn:e,children:t,hash:n}){let l=(e=>{let t=e.split(".");for(let e=t.length;e>0;e--){let n=t.slice(0,e).join(".");if(r[n])return n}return null})(e);if(!l)return(0,a.jsx)(a.Fragment,{children:t});let i=(0,o.default)(`/${r[l]}#${n??e}`);return(0,a.jsx)("a",{href:i,target:"_blank",children:t??(0,a.jsxs)("code",{children:[e,"()"]})})}},46077(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var r=n(66497);function o({src:e,alt:t,width:n,caption:o,className:l}){return(0,a.jsxs)("div",{className:`container_JwLF ${l||""}`,children:[(0,a.jsx)("div",{className:"imageWrapper_RfGN",style:n?{width:n}:{},children:(0,a.jsx)("img",{src:(0,r.default)(e),alt:t,className:"image_bwOA"})}),o&&(0,a.jsx)("p",{className:"caption_jo2G",children:o})]})}},77541(e,t,n){n.d(t,{A:()=>p});var a=n(74848);n(96540);var r=n(95310),o=n(34164);let l="tileImage_O4So";var i=n(66497),s=n(92802);function p({icon:e,image:t,imageDark:n,imageWidth:p,imageHeight:c,iconSize:m=32,containerHeight:d,title:f,description:u,href:h,linkText:w="Learn more \u2192",className:_}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let y=d?{height:`${d}px`}:{},g={};return p&&(g.width=`${p}px`),c&&(g.height=`${c}px`),(0,a.jsxs)(r.A,{href:h,className:(0,o.A)("tileCard_NHsj",_),children:[(0,a.jsx)("div",{className:"tileIcon_pyoR",style:y,children:e?(0,a.jsx)(e,{size:m}):n?(0,a.jsx)(s.A,{sources:{light:(0,i.default)(t),dark:(0,i.default)(n)},alt:f,className:l,style:g}):(0,a.jsx)("img",{src:(0,i.default)(t),alt:f,className:l,style:g})}),(0,a.jsx)("h3",{children:f}),(0,a.jsx)("p",{children:u}),(0,a.jsx)("div",{className:"tileLink_iUbu",children:w})]})}},10440(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var r=n(34164);function o({children:e,className:t}){return(0,a.jsx)("div",{className:(0,r.A)("tilesGrid_hB9N",t),children:e})}},28453(e,t,n){n.d(t,{R:()=>l,x:()=>i});var a=n(96540);let r={},o=a.createContext(r);function l(e){let t=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),a.createElement(o.Provider,{value:t},e.children)}}}]);