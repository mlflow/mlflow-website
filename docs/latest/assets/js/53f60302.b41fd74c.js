"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4834],{16734:(e,n,i)=>{i.d(n,{d:()=>a});var t=i(58069);const s="codeBlock_oJcR";var o=i(74848);const a=e=>{let{children:n,executionCount:i}=e;return(0,o.jsx)("div",{style:{flexGrow:1,minWidth:0,marginTop:"var(--padding-md)",width:"100%"},children:(0,o.jsx)(t.A,{className:s,language:"python",children:n})})}},20723:(e,n,i)=>{i.d(n,{O:()=>o});var t=i(96540),s=i(74848);function o(e){let{children:n,href:i}=e;const o=(0,t.useCallback)((async e=>{if(e.preventDefault(),window.gtag)try{window.gtag("event","notebook-download",{href:i})}catch{}const n=await fetch(i),t=await n.blob(),s=window.URL.createObjectURL(t),o=document.createElement("a");o.style.display="none",o.href=s;const a=i.split("/").pop();o.download=a,document.body.appendChild(o),o.click(),window.URL.revokeObjectURL(s),document.body.removeChild(o)}),[i]);return(0,s.jsx)("a",{className:"button button--primary",style:{marginBottom:"1rem",display:"block",width:"min-content"},href:i,download:!0,onClick:o,children:n})}},61536:(e,n,i)=>{i.d(n,{p:()=>s});var t=i(74848);const s=e=>{let{children:n,isStderr:i}=e;return(0,t.jsx)("pre",{style:{margin:0,borderRadius:0,background:"none",fontSize:"0.85rem",flexGrow:1,padding:"var(--padding-sm)"},children:n})}},86563:(e,n,i)=>{i.d(n,{Q:()=>s});var t=i(74848);const s=e=>{let{children:n}=e;return(0,t.jsx)("div",{style:{flexGrow:1,minWidth:0,fontSize:"0.8rem",width:"100%"},children:n})}},90884:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>m,frontMatter:()=>d,metadata:()=>t,toc:()=>g});const t=JSON.parse('{"id":"llms/openai/notebooks/openai-embeddings-generation-ipynb","title":"Advanced Tutorial: Embeddings Support with OpenAI in MLflow","description":"Download this notebook","source":"@site/docs/llms/openai/notebooks/openai-embeddings-generation-ipynb.mdx","sourceDirName":"llms/openai/notebooks","slug":"/llms/openai/notebooks/openai-embeddings-generation","permalink":"/docs/latest/llms/openai/notebooks/openai-embeddings-generation","draft":false,"unlisted":false,"editUrl":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb","tags":[],"version":"current","frontMatter":{"custom_edit_url":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb","slug":"openai-embeddings-generation"},"sidebar":"docsSidebar","previous":{"title":"Building a Code Assistant with OpenAI & MLflow","permalink":"/docs/latest/llms/openai/notebooks/openai-code-helper"},"next":{"title":"Introduction to Using the OpenAI Flavor in MLflow","permalink":"/docs/latest/llms/openai/notebooks/openai-quickstart"}}');var s=i(74848),o=i(28453),a=i(16734),r=i(61536),l=(i(86563),i(20723));const d={custom_edit_url:"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb",slug:"openai-embeddings-generation"},c="Advanced Tutorial: Embeddings Support with OpenAI in MLflow",h={},g=[{value:"Understanding Embeddings",id:"understanding-embeddings",level:3},{value:"How Embeddings Work",id:"how-embeddings-work",level:3},{value:"In This Tutorial",id:"in-this-tutorial",level:3},{value:"Required packages",id:"required-packages",level:3},{value:"Integrating OpenAI Model with MLflow for Document Similarity",id:"integrating-openai-model-with-mlflow-for-document-similarity",level:3},{value:"Key Steps",id:"key-steps",level:4},{value:"Webpage Text Extraction for Embedding Analysis",id:"webpage-text-extraction-for-embedding-analysis",level:3},{value:"Overview of Functions",id:"overview-of-functions",level:4},{value:"Detailed Workflow:",id:"detailed-workflow",level:4},{value:"Measuring Similarity and Distance Between Embeddings",id:"measuring-similarity-and-distance-between-embeddings",level:3},{value:"Function Overviews",id:"function-overviews",level:4},{value:"Comparing Webpages Using Embeddings",id:"comparing-webpages-using-embeddings",level:3},{value:"Function Overview",id:"function-overview",level:4},{value:"How It Works",id:"how-it-works",level:4},{value:"Practical Application",id:"practical-application",level:4},{value:"Similarity Analysis Between MLflow Documentation Pages",id:"similarity-analysis-between-mlflow-documentation-pages",level:3},{value:"Process Overview",id:"process-overview",level:4},{value:"Results",id:"results",level:4},{value:"Interpretation",id:"interpretation",level:4},{value:"Conclusion",id:"conclusion",level:4},{value:"Brief Overview of Similarity Between MLflow LLMs and Plugins Pages",id:"brief-overview-of-similarity-between-mlflow-llms-and-plugins-pages",level:3},{value:"Analysis Execution",id:"analysis-execution",level:4},{value:"Results",id:"results-1",level:4},{value:"Tutorial Recap: Leveraging OpenAI Embeddings in MLflow",id:"tutorial-recap-leveraging-openai-embeddings-in-mlflow",level:3},{value:"Key Takeaways",id:"key-takeaways",level:4},{value:"Conclusion",id:"conclusion-1",level:4},{value:"What&#39;s Next?",id:"whats-next",level:3}];function u(e){const n={a:"a",code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"advanced-tutorial-embeddings-support-with-openai-in-mlflow",children:"Advanced Tutorial: Embeddings Support with OpenAI in MLflow"})}),"\n",(0,s.jsx)(l.O,{href:"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb",children:"Download this notebook"}),"\n",(0,s.jsx)(n.p,{children:"Welcome to this advanced guide on implementing OpenAI embeddings within the MLflow framework. This tutorial delves into the configuration and utilization of OpenAI's powerful embeddings, a key component in modern machine learning models."}),"\n",(0,s.jsx)(n.h3,{id:"understanding-embeddings",children:"Understanding Embeddings"}),"\n",(0,s.jsx)(n.p,{children:"Embeddings are a form of representation learning where words, phrases, or even entire documents are converted into vectors in a high-dimensional space. These vectors capture semantic meaning, enabling models to understand and process language more effectively. Embeddings are extensively used in natural language processing (NLP) for tasks like text classification, sentiment analysis, and language translation."}),"\n",(0,s.jsx)(n.h3,{id:"how-embeddings-work",children:"How Embeddings Work"}),"\n",(0,s.jsx)(n.p,{children:"Embeddings work by mapping textual data to vectors such that the distance and direction between vectors represent relationships between the words or phrases. For example, in a well-trained embedding space, synonyms are located closer together, while unrelated terms are farther apart. This spatial arrangement allows algorithms to recognize context and semantics, enhancing their ability to interpret and respond to natural language."}),"\n",(0,s.jsx)(n.h3,{id:"in-this-tutorial",children:"In This Tutorial"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding Endpoint Configuration"}),": Setting up and utilizing OpenAI's embedding endpoints in MLflow."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-world Application"}),": Practical example of comparing the text content of various web pages to one another to determine the amount of similarity in their contextually-specific content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficiency and Precision Enhancements"}),": Techniques for improving model performance using OpenAI embeddings."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"By the end of this tutorial, you'll have a thorough understanding of how to integrate and leverage OpenAI embeddings in your MLflow projects, harnessing the power of advanced NLP techniques. You'll also see a real-world application of using text embeddings of documents to compare their similarity. This use case is particularly useful for web content development as a critical task when performing search engine optimization (SEO) to ensure that site page contents are not too similar to one another (which could result in a downgrade in page rankings)."}),"\n",(0,s.jsx)(n.h3,{id:"required-packages",children:"Required packages"}),"\n",(0,s.jsxs)(n.p,{children:["In order to run this tutorial, you will need to install ",(0,s.jsx)(n.code,{children:"beautifulsoup4"})," from PyPI."]}),"\n",(0,s.jsx)(n.p,{children:"Let's dive into the world of embeddings and explore their transformative impact on machine learning models!"}),"\n",(0,s.jsx)(a.d,{executionCount:1,children:'import warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings("ignore", category=UserWarning)'}),"\n",(0,s.jsx)(a.d,{executionCount:2,children:'import os\n\nimport numpy as np\nimport openai\nimport requests\nfrom bs4 import BeautifulSoup\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema, TensorSpec\n\nassert "OPENAI_API_KEY" in os.environ, " OPENAI_API_KEY environment variable must be set"'}),"\n",(0,s.jsx)(n.h3,{id:"integrating-openai-model-with-mlflow-for-document-similarity",children:"Integrating OpenAI Model with MLflow for Document Similarity"}),"\n",(0,s.jsx)(n.p,{children:"In this tutorial segment, we demonstrate the process of setting up and utilizing an OpenAI embedding model within MLflow for document similarity tasks."}),"\n",(0,s.jsx)(n.h4,{id:"key-steps",children:"Key Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Setting an MLflow Experiment"}),": We begin by setting the experiment context in MLflow, specifically for document similarity, using ",(0,s.jsx)(n.code,{children:'mlflow.set_experiment("Documentation Similarity")'}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Logging the Model in MLflow"}),': We initiate an MLflow run and log metadata and access configuration parameters to communicate with a specific OpenAI endpoint. The OpenAI endpoint that we\'ve chosen here points to the model "text-embedding-ada-002", chosen for its robust embedding capabilities. During this step, we detail these access configurations, the embedding task, input/output schemas, and parameters like batch size.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Loading the Logged Model for Use"}),": After logging the MLflow model, we proceed to load it using MLflow's ",(0,s.jsx)(n.code,{children:"pyfunc"})," module. This is a critical step for applying the model to perform document similarity tasks within the MLflow ecosystem."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These steps are essential for integrating access to OpenAI's embedding model into MLflow, facilitating advanced NLP operations like document similarity analysis."}),"\n",(0,s.jsx)(a.d,{executionCount:3,children:'mlflow.set_experiment("Documenatation Similarity")\n\nwith mlflow.start_run():\n  model_info = mlflow.openai.log_model(\n      model="text-embedding-ada-002",\n      task=openai.embeddings,\n      name="model",\n      signature=ModelSignature(\n          inputs=Schema([ColSpec(type="string", name=None)]),\n          outputs=Schema([TensorSpec(type=np.dtype("float64"), shape=(-1,))]),\n          params=ParamSchema([ParamSpec(name="batch_size", dtype="long", default=1024)]),\n      ),\n  )\n\n# Load the model in pyfunc format\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)'}),"\n",(0,s.jsx)(n.h3,{id:"webpage-text-extraction-for-embedding-analysis",children:"Webpage Text Extraction for Embedding Analysis"}),"\n",(0,s.jsx)(n.p,{children:"This section of the tutorial introduces functions designed to extract and prepare text from webpages, a crucial step before applying embedding models for analysis."}),"\n",(0,s.jsx)(n.h4,{id:"overview-of-functions",children:"Overview of Functions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"insert_space_after_tags"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adds a space after specific HTML tags in a BeautifulSoup object for better text readability."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"extract_text_from_url"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Extracts text from a specified webpage section using its URL and a target ID. Filters and organizes the text from tags like ",(0,s.jsx)(n.code,{children:"<h>"}),", ",(0,s.jsx)(n.code,{children:"<li>"}),", and ",(0,s.jsx)(n.code,{children:"<p>"}),", excluding certain irrelevant sections."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These functions are integral to preprocessing web content, ensuring that the text fed into the embedding model is clean, relevant, and well-structured."}),"\n",(0,s.jsx)(a.d,{executionCount:4,children:'def insert_space_after_tags(soup, tags):\n  """\n  Insert a space after each tag specified in the provided BeautifulSoup object.\n\n  Args:\n      soup: BeautifulSoup object representing the parsed HTML.\n      tags: List of tag names (as strings) after which space should be inserted.\n  """\n  for tag_name in tags:\n      for tag in soup.find_all(tag_name):\n          tag.insert_after(" ")\n\n\ndef extract_text_from_url(url, id):\n  """\n  Extract and return text content from a specific section of a webpage.\n  """\n  try:\n      response = requests.get(url)\n      response.raise_for_status()  # Raises HTTPError for bad requests (4XX, 5XX)\n  except requests.exceptions.RequestException as e:\n      return f"Request failed: {e}"\n\n  soup = BeautifulSoup(response.text, "html.parser")\n  target_div = soup.find("div", {"class": "section", "id": id})\n  if not target_div:\n      return "Target element not found."\n\n  insert_space_after_tags(target_div, ["strong", "a"])\n\n  content_tags = target_div.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "li", "p"])\n  filtered_tags = [\n      tag\n      for tag in content_tags\n      if not (\n          (tag.name == "li" and tag.find("p") and tag.find("a", class_="reference external"))\n          or (tag.name == "p" and tag.find_parent("ul"))\n          or (tag.get_text(strip=True).lower() == "note")\n      )\n  ]\n\n  return "\n".join(tag.get_text(separator=" ", strip=True) for tag in filtered_tags)'}),"\n",(0,s.jsx)(n.h4,{id:"detailed-workflow",children:"Detailed Workflow:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The function ",(0,s.jsx)(n.code,{children:"extract_text_from_url"})," first fetches the webpage content using the ",(0,s.jsx)(n.code,{children:"requests"})," library."]}),"\n",(0,s.jsx)(n.li,{children:"It then parses the HTML content using BeautifulSoup."}),"\n",(0,s.jsx)(n.li,{children:"Specific HTML tags are targeted for text extraction, ensuring that the content is relevant and well-structured for embedding analysis."}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"insert_space_after_tags"})," function is called within ",(0,s.jsx)(n.code,{children:"extract_text_from_url"})," to improve text readability post-extraction."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"measuring-similarity-and-distance-between-embeddings",children:"Measuring Similarity and Distance Between Embeddings"}),"\n",(0,s.jsxs)(n.p,{children:["In this next part of the tutorial, we utilize two functions from ",(0,s.jsx)(n.code,{children:"sklearn"})," to measure the similarity and distance between document embeddings, essential for evaluating and comparing text-based machine learning models."]}),"\n",(0,s.jsx)(n.h4,{id:"function-overviews",children:"Function Overviews"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"cosine_similarity"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose"}),": Calculates the cosine similarity between two embedding vectors."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"How It Works"}),": This function computes similarity by finding the cosine of the angle between the two vectors, a common method for assessing how similar two documents are in terms of their content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance"}),": Very useful in NLP, especially for tasks like document retrieval and clustering, where the goal is to find documents with similar content."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"euclidean_distances"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose"}),": Computes the Euclidean distance between two embedding vectors."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Functionality"}),": Similar to ",(0,s.jsx)(n.code,{children:"cosine_similarity"}),' this function calculates the Euclidean distance, which is the "straight line" distance between the two points in the embedding space. This measure is useful for understanding how different two documents are.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance within NLP"}),": Offers a more intuitive physical distance metric, useful for tasks like document classification and anomaly detection."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These functions are crucial for analyzing and comparing the outputs of embedding models, providing insights into the relationships between different text data in terms of similarity and distinction."}),"\n",(0,s.jsx)(n.h3,{id:"comparing-webpages-using-embeddings",children:"Comparing Webpages Using Embeddings"}),"\n",(0,s.jsxs)(n.p,{children:["This section of the tutorial introduces a function, ",(0,s.jsx)(n.code,{children:"compare_pages"}),", designed to compare the content of two webpages using embedding models. This function is key for understanding how similar or different two given webpages are in terms of their textual content."]}),"\n",(0,s.jsx)(n.h4,{id:"function-overview",children:"Function Overview"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Function Name"}),": ",(0,s.jsx)(n.code,{children:"compare_pages"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose"}),": Compares two webpages and returns a similarity score based on their content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"url1"})," and ",(0,s.jsx)(n.code,{children:"url2"}),": URLs of the webpages to be compared."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"id1"})," and ",(0,s.jsx)(n.code,{children:"id2"}),": Target IDs for the main text content divs on each page."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text Extraction"}),": The function starts by extracting text from the specified sections of each webpage using the ",(0,s.jsx)(n.code,{children:"extract_text_from_url"})," function."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding Prediction"}),": It then uses the previously loaded OpenAI model to generate embeddings for the extracted texts."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Similarity and Distance Measurement"}),": The function calculates both the cosine similarity and Euclidean distance between the two embeddings. These metrics provide a quantifiable measure of how similar or dissimilar the webpage contents are."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Result"}),": Returns a tuple containing the cosine similarity score and the Euclidean distance. If text extraction fails, it returns an error message."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"practical-application",children:"Practical Application"}),"\n",(0,s.jsx)(n.p,{children:"This function is particularly useful in scenarios where comparing the content of different webpages is necessary, such as in content curation, plagiarism detection, or similarity analysis for SEO purposes."}),"\n",(0,s.jsxs)(n.p,{children:["By leveraging the power of embeddings and similarity metrics, ",(0,s.jsx)(n.code,{children:"compare_pages"})," provides a robust method for quantitatively assessing webpage content similarities and differences."]}),"\n",(0,s.jsx)(a.d,{executionCount:5,children:'def compare_pages(url1, url2, id1, id2):\n  """\n  Compare two webpages and return the similarity score.\n\n  Args:\n      url1: URL of the first webpage.\n      url2: URL of the second webpage.\n      id1: The target id for the div containing the main text content of the first page\n      id2: The target id for the div containing the main text content of the second page\n\n  Returns:\n      A tuple of floats representing the similarity score for cosine similarity and euclidean distance.\n  """\n  text1 = extract_text_from_url(url1, id1)\n  text2 = extract_text_from_url(url2, id2)\n\n  if text1 and text2:\n      embedding1 = model.predict([text1])\n      embedding2 = model.predict([text2])\n\n      return (\n          cosine_similarity(embedding1, embedding2),\n          euclidean_distances(embedding1, embedding2),\n      )\n  else:\n      return "Failed to retrieve content."'}),"\n",(0,s.jsx)(n.h3,{id:"similarity-analysis-between-mlflow-documentation-pages",children:"Similarity Analysis Between MLflow Documentation Pages"}),"\n",(0,s.jsxs)(n.p,{children:["In this tutorial segment, we demonstrate the practical application of the ",(0,s.jsx)(n.code,{children:"compare_pages"})," function by comparing two specific pages from the MLflow documentation. Our goal is to assess how similar the content of the main Large Language Models (LLMs) page is to the LLM Evaluation page within the 2.8.1 release of MLflow."]}),"\n",(0,s.jsx)(n.h4,{id:"process-overview",children:"Process Overview"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Target Webpages"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The main LLMs page: ",(0,s.jsx)(n.a,{href:"https://www.mlflow.org/docs/2.8.1/llms/index.html",children:"LLMs page for MLflow 2.8.1 release"})]}),"\n",(0,s.jsxs)(n.li,{children:["The LLM Evaluation page: ",(0,s.jsx)(n.a,{href:"https://www.mlflow.org/docs/2.8.1/llms/llm-evaluate/index.html",children:"LLM Evaluation for MLflow 2.8.1"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Content IDs"}),": We use 'llms' for the main LLMs page and 'mlflow-llm-evaluate' for the LLM Evaluation page to target specific content sections."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Comparison Execution"}),": The ",(0,s.jsx)(n.code,{children:"compare_pages"})," function is called with these URLs and content IDs to perform the analysis."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"results",children:"Results"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cosine Similarity and Euclidean Distance"}),": The function returns two key metrics:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cosine Similarity: Measures the cosine of the angle between the embedding vectors of the two pages. A higher value indicates greater similarity."}),"\n",(0,s.jsx)(n.li,{children:"Euclidean Distance: Represents the 'straight-line' distance between the two points in the embedding space, with lower values indicating closer similarity."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"interpretation",children:"Interpretation"}),"\n",(0,s.jsx)(n.p,{children:"The results show a high degree of cosine similarity (0.8792), suggesting that the content of the two pages is quite similar in terms of context and topics covered. The Euclidean distance of 0.4914, while relatively low, offers a complementary perspective, indicating some level of distinctiveness in the content."}),"\n",(0,s.jsx)(n.h4,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"This analysis highlights the effectiveness of using embeddings and similarity metrics for comparing webpage content. In practical terms, it helps in understanding the overlap and differences in documentation, aiding in content optimization, redundancy reduction, and ensuring comprehensive coverage of topics."}),"\n",(0,s.jsx)(a.d,{executionCount:6,children:'# Get the similarity between the main LLMs page in the MLflow Docs and the LLM Evaluation page for the 2.8.1 release of MLflow\n\nllm_cosine, llm_euclid = compare_pages(\n  url1="https://www.mlflow.org/docs/2.8.1/llms/index.html",\n  url2="https://www.mlflow.org/docs/2.8.1/llms/llm-evaluate/index.html",\n  id1="llms",\n  id2="mlflow-llm-evaluate",\n)\n\nprint(\n  f"The cosine similarity between the LLMs page and the LLM Evaluation page is: {llm_cosine} and the euclidean distance is: {llm_euclid}"\n)'}),"\n",(0,s.jsx)(r.p,{children:"The cosine similarity between the LLMs page and the LLM Evaluation page is: [[0.879243]] and the euclidean distance is: [[0.49144073]]"}),"\n",(0,s.jsx)(n.h3,{id:"brief-overview-of-similarity-between-mlflow-llms-and-plugins-pages",children:"Brief Overview of Similarity Between MLflow LLMs and Plugins Pages"}),"\n",(0,s.jsx)(n.p,{children:"This section demonstrates a quick similarity analysis between the MLflow Large Language Models (LLMs) page and the Plugins page from the 2.8.1 release."}),"\n",(0,s.jsx)(n.h4,{id:"analysis-execution",children:"Analysis Execution"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pages Compared"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["LLMs page: ",(0,s.jsx)(n.a,{href:"https://www.mlflow.org/docs/2.8.1/llms/index.html",children:"LLMs page for MLflow 2.8.1 release"})]}),"\n",(0,s.jsxs)(n.li,{children:["Plugins page: ",(0,s.jsx)(n.a,{href:"https://www.mlflow.org/docs/2.8.1/plugins.html",children:"Plugins page for MLflow 2.8.1 release"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IDs Used"}),": 'llms' for the LLMs page and 'mflow-plugins' for the Plugins page."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Function"}),": ",(0,s.jsx)(n.code,{children:"compare_pages"})," is utilized for the comparison."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"results-1",children:"Results"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cosine Similarity"}),": 0.6806, indicating moderate similarity in content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Euclidean Distance"}),": 0.7992, suggesting a noticeable difference in the context and topics covered by the two pages."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The results reflect a moderate level of similarity between the LLMs and Plugins pages, with a significant degree of distinctiveness in their content. This analysis is useful for understanding the relationship and content overlap between different sections of the MLflow documentation."}),"\n",(0,s.jsx)(a.d,{executionCount:7,children:'# Get the similarity between the main LLMs page in the MLflow Docs and the Plugins page for the 2.8.1 release of MLflow\n\nplugins_cosine, plugins_euclid = compare_pages(\n  url1="https://www.mlflow.org/docs/2.8.1/llms/index.html",\n  url2="https://www.mlflow.org/docs/2.8.1/plugins.html",\n  id1="llms",\n  id2="mflow-plugins",\n)\n\nprint(\n  f"The cosine similarity between the LLMs page and the MLflow Projects page is: {plugins_cosine} and the euclidean distance is: {plugins_euclid}"\n)'}),"\n",(0,s.jsx)(r.p,{children:"The cosine similarity between the LLMs page and the MLflow Projects page is: [[0.68062298]] and the euclidean distance is: [[0.79922088]]"}),"\n",(0,s.jsx)(n.h3,{id:"tutorial-recap-leveraging-openai-embeddings-in-mlflow",children:"Tutorial Recap: Leveraging OpenAI Embeddings in MLflow"}),"\n",(0,s.jsx)(n.p,{children:"As we conclude this tutorial, let's recap the key concepts and techniques we've explored regarding the use of OpenAI embeddings within the MLflow framework."}),"\n",(0,s.jsx)(n.h4,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Integrating OpenAI Models in MLflow"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'We learned how to log and load OpenAI\'s "text-embedding-ada-002" model within MLflow, an essential step for utilizing these embeddings in machine learning workflows.'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Text Extraction and Preprocessing"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The tutorial introduced methods for extracting and preprocessing text from webpages, ensuring the data is clean and structured for embedding analysis."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Calculating Similarity and Distance"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"We delved into functions for measuring cosine similarity and Euclidean distance between document embeddings, vital for comparing textual content."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-World Application: Webpage Content Comparison"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Practical application of these concepts was demonstrated through the comparison of different MLflow documentation pages. We analyzed the similarity and differences in their content using the embeddings generated by the OpenAI model."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Interpreting Results"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The tutorial provided insights into interpreting the results of similarity and distance metrics, highlighting their relevance in understanding content relationships."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"conclusion-1",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"This advanced tutorial aimed to enhance your skills in applying OpenAI embeddings in MLflow, focusing on real-world applications like document similarity analysis. By integrating these powerful NLP tools, we've showcased how to extract more value and insights from textual data, a crucial aspect of modern machine learning projects."}),"\n",(0,s.jsx)(n.p,{children:"We hope this guide has been informative and instrumental in advancing your understanding and application of OpenAI embeddings within the MLflow framework."}),"\n",(0,s.jsx)(n.h3,{id:"whats-next",children:"What's Next?"}),"\n",(0,s.jsxs)(n.p,{children:["To continue your learning journey, see the additional ",(0,s.jsx)(n.a,{href:"https://www.mlflow.org/docs/latest/llms/openai/index.html#advanced-tutorials",children:"advanced tutorials for MLflow's OpenAI flavor"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}}}]);