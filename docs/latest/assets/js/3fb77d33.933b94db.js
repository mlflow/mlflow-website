"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4318],{11470:(e,n,t)=>{t.d(n,{A:()=>v});var i=t(96540),r=t(34164),a=t(23104),s=t(56347),o=t(205),l=t(57485),c=t(31682),m=t(70679);function d(e){return i.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??function(e){return d(e).map((({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i})))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u({value:e,tabValues:n}){return n.some((n=>n.value===e))}function _({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(r),(0,i.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})}),[r,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=p(e),[s,l]=(0,i.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a}))),[c,d]=_({queryString:t,groupId:r}),[f,h]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,m.Dv)(n);return[t,(0,i.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:r}),g=(()=>{const e=c??f;return u({value:e,tabValues:a})?e:null})();(0,o.A)((()=>{g&&l(g)}),[g]);return{selectedValue:s,selectValue:(0,i.useCallback)((e=>{if(!u({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),h(e)}),[d,h,a]),tabValues:a}}var h=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function w({className:e,block:n,selectedValue:t,selectValue:i,tabValues:s}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,a.a_)(),c=e=>{const n=e.currentTarget,r=o.indexOf(n),a=s[r].value;a!==t&&(l(n),i(a))},m=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:s.map((({value:e,label:n,attributes:i})=>(0,y.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:m,onClick:c,...i,className:(0,r.A)("tabs__item",g.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function x({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find((e=>e.props.value===t));return e?(0,i.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:a.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function b(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,r.A)("tabs-container",g.tabList),children:[(0,y.jsx)(w,{...n,...e}),(0,y.jsx)(x,{...n,...e})]})}function v(e){const n=(0,h.A)();return(0,y.jsx)(b,{...e,children:d(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var i=t(34164);const r={tabItem:"tabItem_Ymn6"};var a=t(74848);function s({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,i.A)(r.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(96540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},49374:(e,n,t)=>{t.d(n,{B:()=>l});t(96540);const i=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=t(86025),a=t(28774),s=t(74848);const o=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(i[e])return e}return null};function l({fn:e,children:n}){const t=o(e);if(!t)return(0,s.jsx)(s.Fragment,{children:n});const l=(0,r.Ay)(`/${i[t]}#${e}`);return(0,s.jsx)(a.A,{to:l,target:"_blank",children:n??(0,s.jsxs)("code",{children:[e,"()"]})})}},65003:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"deep-learning/sentence-transformers/guide/index","title":"Sentence Transformers within MLflow","description":"Sentence Transformers have become the go-to solution for converting text into meaningful vector representations that capture semantic meaning. By combining the power of sentence transformers with MLflow\'s comprehensive experiment tracking, you create a robust workflow for developing, monitoring, and deploying semantic understanding applications.","source":"@site/docs/classic-ml/deep-learning/sentence-transformers/guide/index.mdx","sourceDirName":"deep-learning/sentence-transformers/guide","slug":"/deep-learning/sentence-transformers/guide/","permalink":"/docs/latest/ml/deep-learning/sentence-transformers/guide/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"Sentence Transformers","permalink":"/docs/latest/ml/deep-learning/sentence-transformers/"},"next":{"title":"Tutorials","permalink":"/docs/latest/ml/deep-learning/sentence-transformers/tutorials/"}}');var r=t(74848),a=t(28453),s=(t(49374),t(11470)),o=t(19365);const l={},c="Sentence Transformers within MLflow",m={},d=[{value:"Semantic Vector Magic",id:"semantic-vector-magic",level:4},{value:"Versatile Architecture Options",id:"versatile-architecture-options",level:4},{value:"Why MLflow + Sentence Transformers?",id:"why-mlflow--sentence-transformers",level:2},{value:"Core Workflows",id:"core-workflows",level:2},{value:"Loading and Logging Models",id:"loading-and-logging-models",level:3},{value:"Loading and Using Models",id:"loading-and-using-models",level:3},{value:"Building Semantic Search Systems",id:"building-semantic-search-systems",level:3},{value:"Using MLflow&#39;s Evaluation Framework",id:"using-mlflows-evaluation-framework",level:3},{value:"Domain-Specific Fine-tuning",id:"domain-specific-fine-tuning",level:3},{value:"Production-Ready Model Deployment",id:"production-ready-model-deployment",level:3},{value:"Batch Processing Pipeline",id:"batch-processing-pipeline",level:3},{value:"Advanced Workflows",id:"advanced-workflows",level:2},{value:"Systematic Multi-Model Evaluation",id:"systematic-multi-model-evaluation",level:3},{value:"Performance vs. Quality Trade-offs",id:"performance-vs-quality-trade-offs",level:3},{value:"Domain-Specific Evaluation Pipeline",id:"domain-specific-evaluation-pipeline",level:3},{value:"Best Practices and Optimization",id:"best-practices-and-optimization",level:2},{value:"Experiment Organization",id:"experiment-organization",level:3},{value:"Model Management",id:"model-management",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Efficient Batch Processing",id:"efficient-batch-processing",level:3},{value:"Production API Wrapper",id:"production-api-wrapper",level:3},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"sentence-transformers-within-mlflow",children:"Sentence Transformers within MLflow"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sentence Transformers"})," have become the go-to solution for converting text into meaningful vector representations that capture semantic meaning. By combining the power of sentence transformers with MLflow's comprehensive experiment tracking, you create a robust workflow for developing, monitoring, and deploying semantic understanding applications."]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Why Sentence Transformers Excel at Semantic Understanding"}),(0,r.jsx)(n.h4,{id:"semantic-vector-magic",children:"Semantic Vector Magic"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,r.jsx)(n.strong,{children:"Meaning-Based Representation"}),": Convert sentences into vectors where similar meanings cluster together"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udf10 ",(0,r.jsx)(n.strong,{children:"Multilingual Capabilities"}),": Work across 100+ languages with shared semantic space"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udccf ",(0,r.jsx)(n.strong,{children:"Fixed-Size Embeddings"}),": Transform variable-length text into consistent vector dimensions"]}),"\n",(0,r.jsxs)(n.li,{children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Efficient Inference"}),": Generate embeddings in milliseconds for real-time applications"]}),"\n"]}),(0,r.jsx)(n.h4,{id:"versatile-architecture-options",children:"Versatile Architecture Options"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfd7\ufe0f ",(0,r.jsx)(n.strong,{children:"Bi-Encoder Models"}),": Independent encoding for scalable similarity search and clustering"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Cross-Encoder Models"}),": Joint encoding for maximum accuracy in pairwise comparisons"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,r.jsx)(n.strong,{children:"Task-Specific Models"}),": Pre-trained models optimized for specific domains and use cases"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Flexible Pooling"}),": Multiple strategies to aggregate token representations into sentence embeddings"]}),"\n"]})]}),"\n",(0,r.jsx)(n.h2,{id:"why-mlflow--sentence-transformers",children:"Why MLflow + Sentence Transformers?"}),"\n",(0,r.jsx)(n.p,{children:"The integration of MLflow with sentence transformers creates a powerful workflow for semantic AI development:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Embedding Quality Tracking"}),": Monitor semantic similarity scores, embedding distributions, and model performance across different tasks"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Model Versioning"}),": Track embedding model evolution and compare performance across different architectures and fine-tuning approaches"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcc8 ",(0,r.jsx)(n.strong,{children:"Semantic Evaluation"}),": Capture similarity benchmarks, clustering metrics, and retrieval performance with comprehensive visualizations"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,r.jsx)(n.strong,{children:"Deployment Ready"}),": Package embedding models with proper signatures and dependencies for seamless production deployment"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udc65 ",(0,r.jsx)(n.strong,{children:"Collaborative Development"}),": Share embedding models, evaluation results, and semantic insights across teams through MLflow's intuitive interface"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\ude80 ",(0,r.jsx)(n.strong,{children:"Production Integration"}),": Deploy models for semantic search, document clustering, and recommendation systems with full lineage tracking"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"core-workflows",children:"Core Workflows"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsxs)(o.A,{value:"basic-usage",label:"Basic Usage",default:!0,children:[(0,r.jsx)(n.h3,{id:"loading-and-logging-models",children:"Loading and Logging Models"}),(0,r.jsx)(n.p,{children:"MLflow makes it incredibly easy to work with sentence transformer models:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.sentence_transformers\nfrom sentence_transformers import SentenceTransformer\n\n# Load a pre-trained model\nmodel = SentenceTransformer("all-MiniLM-L6-v2")\n\n# Generate sample embeddings for signature inference\nsample_texts = [\n    "MLflow makes machine learning development easier",\n    "Sentence transformers create semantic embeddings",\n]\nsample_embeddings = model.encode(sample_texts)\n\n# Infer model signature\nsignature = mlflow.models.infer_signature(sample_texts, sample_embeddings)\n\n# Log the model to MLflow\nwith mlflow.start_run():\n    model_info = mlflow.sentence_transformers.log_model(\n        model=model,\n        name="semantic_encoder",\n        signature=signature,\n        input_example=sample_texts,\n    )\n\nprint(f"Model logged with URI: {model_info.model_uri}")\n'})}),(0,r.jsx)(n.h3,{id:"loading-and-using-models",children:"Loading and Using Models"}),(0,r.jsx)(n.p,{children:"Once logged, you can easily load and use your models:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Load as a sentence transformer model (preserves all functionality)\nloaded_transformer = mlflow.sentence_transformers.load_model(model_info.model_uri)\nembeddings = loaded_transformer.encode(["New text to encode"])\n\n# Load as a generic MLflow model (for deployment)\nloaded_pyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\npredictions = loaded_pyfunc.predict(["New text to encode"])\n\nprint("Embeddings shape:", embeddings.shape)\nprint("Predictions shape:", predictions.shape)\n'})}),(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Understanding Model Signatures for Embeddings"}),(0,r.jsx)(n.p,{children:"Model signatures are crucial for sentence transformers as they define the expected input format and output structure:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom mlflow.models import infer_signature\n\nmodel = SentenceTransformer("all-MiniLM-L6-v2")\n\n# Single sentence input\nsingle_input = "This is a sample sentence."\nsingle_output = model.encode(single_input)\n\n# Multiple sentences input\nbatch_input = [\n    "First sentence for encoding.",\n    "Second sentence for batch processing.",\n    "Third sentence to demonstrate batching.",\n]\nbatch_output = model.encode(batch_input)\n\n# Infer signature for batch processing (recommended)\nsignature = infer_signature(batch_input, batch_output)\n\nwith mlflow.start_run():\n    mlflow.sentence_transformers.log_model(\n        model=model,\n        name="batch_encoder",\n        signature=signature,\n        input_example=batch_input,\n    )\n'})}),(0,r.jsx)(n.p,{children:"Benefits of proper signatures:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcdd ",(0,r.jsx)(n.strong,{children:"Input Validation"}),": Ensures correct data format during inference"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,r.jsx)(n.strong,{children:"API Documentation"}),": Clear specification of expected inputs and outputs"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\ude80 ",(0,r.jsx)(n.strong,{children:"Deployment Readiness"}),": Enables automatic endpoint generation and validation"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Type Safety"}),": Prevents runtime errors in production environments"]}),"\n"]})]})]}),(0,r.jsxs)(o.A,{value:"semantic-search",label:"Semantic Search",children:[(0,r.jsx)(n.h3,{id:"building-semantic-search-systems",children:"Building Semantic Search Systems"}),(0,r.jsx)(n.p,{children:"Here's a complete example of building and logging a semantic search system:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\nfrom mlflow.models import infer_signature\n\n# Sample document corpus\ndocuments = [\n    "Machine learning is a subset of artificial intelligence.",\n    "Deep learning uses neural networks with multiple layers.",\n    "Natural language processing helps computers understand text.",\n    "Computer vision enables machines to interpret visual information.",\n    "Reinforcement learning trains agents through trial and error.",\n    "Data science combines statistics and programming for insights.",\n    "Cloud computing provides scalable infrastructure resources.",\n    "MLflow helps manage the machine learning lifecycle.",\n]\n\n\ndef build_semantic_search_system():\n    """Build and log a complete semantic search system."""\n\n    with mlflow.start_run(run_name="semantic_search_system"):\n        # Load the sentence transformer\n        model = SentenceTransformer("all-MiniLM-L6-v2")\n\n        # Log model parameters\n        mlflow.log_params(\n            {\n                "model_name": "all-MiniLM-L6-v2",\n                "embedding_dimension": model.get_sentence_embedding_dimension(),\n                "max_seq_length": model.max_seq_length,\n                "corpus_size": len(documents),\n            }\n        )\n\n        # Encode the document corpus\n        print("Encoding document corpus...")\n        corpus_embeddings = model.encode(documents, convert_to_tensor=True)\n\n        # Save corpus and embeddings as artifacts\n        corpus_df = pd.DataFrame({"documents": documents})\n        corpus_df.to_csv("corpus.csv", index=False)\n        mlflow.log_artifact("corpus.csv")\n\n        # Example queries for testing\n        test_queries = [\n            "What is artificial intelligence?",\n            "How do neural networks work?",\n            "Tell me about text processing",\n            "What tools help with ML development?",\n        ]\n\n        # Perform semantic search for each query\n        search_results = []\n        for query in test_queries:\n            print(f"\\nSearching for: \'{query}\'")\n\n            # Encode the query\n            query_embedding = model.encode(query, convert_to_tensor=True)\n\n            # Calculate similarities\n            similarities = util.semantic_search(\n                query_embedding, corpus_embeddings, top_k=3\n            )[0]\n\n            # Store results\n            for hit in similarities:\n                search_results.append(\n                    {\n                        "query": query,\n                        "document": documents[hit["corpus_id"]],\n                        "similarity_score": hit["score"],\n                        "rank": len([r for r in search_results if r["query"] == query])\n                        + 1,\n                    }\n                )\n\n            # Print top results\n            for hit in similarities:\n                print(f"  Score: {hit[\'score\']:.4f} - {documents[hit[\'corpus_id\']]}")\n\n        # Log search results\n        results_df = pd.DataFrame(search_results)\n        results_df.to_csv("search_results.csv", index=False)\n        mlflow.log_artifact("search_results.csv")\n\n        # Calculate evaluation metrics\n        avg_top1_score = results_df[results_df["rank"] == 1]["similarity_score"].mean()\n        avg_top3_score = results_df["similarity_score"].mean()\n\n        mlflow.log_metrics(\n            {\n                "avg_top1_similarity": avg_top1_score,\n                "avg_top3_similarity": avg_top3_score,\n                "total_queries_tested": len(test_queries),\n            }\n        )\n\n        # Log the model with inference signature\n        signature = infer_signature(test_queries, model.encode(test_queries))\n\n        model_info = mlflow.sentence_transformers.log_model(\n            model=model,\n            name="semantic_search_model",\n            signature=signature,\n            input_example=test_queries[:2],\n        )\n\n        print(f"\\nModel logged successfully!")\n        print(f"Average top-1 similarity: {avg_top1_score:.4f}")\n        print(f"Average top-3 similarity: {avg_top3_score:.4f}")\n\n        return model_info\n\n\n# Run the semantic search system\nmodel_info = build_semantic_search_system()\n'})})]}),(0,r.jsxs)(o.A,{value:"model-evaluation",label:"Model Evaluation",children:[(0,r.jsx)(n.h3,{id:"using-mlflows-evaluation-framework",children:"Using MLflow's Evaluation Framework"}),(0,r.jsx)(n.p,{children:"MLflow's comprehensive evaluation API can be adapted for sentence transformer models to assess embedding quality and semantic understanding:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import make_metric\nimport pandas as pd\nimport numpy as np\nimport time\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.stats import pearsonr, spearmanr\n\n\ndef create_semantic_similarity_dataset():\n    """Create a labeled dataset for semantic similarity evaluation."""\n\n    # Sample similarity pairs with human-annotated scores (0-1 scale)\n    similarity_data = [\n        {\n            "text1": "The cat is sleeping",\n            "text2": "A cat is resting",\n            "similarity": 0.85,\n        },\n        {\n            "text1": "I love programming",\n            "text2": "Coding is my passion",\n            "similarity": 0.80,\n        },\n        {\n            "text1": "The weather is nice",\n            "text2": "It\'s raining heavily",\n            "similarity": 0.15,\n        },\n        {\n            "text1": "Machine learning is exciting",\n            "text2": "AI technology fascinates me",\n            "similarity": 0.75,\n        },\n        {\n            "text1": "Python is a language",\n            "text2": "The snake slithered away",\n            "similarity": 0.10,\n        },\n        {\n            "text1": "Data science projects",\n            "text2": "Analytics and statistics work",\n            "similarity": 0.70,\n        },\n    ]\n\n    return pd.DataFrame(similarity_data)\n\n\ndef evaluate_embedding_model_with_mlflow(model_name):\n    """Evaluate a sentence transformer using MLflow\'s evaluation framework."""\n\n    with mlflow.start_run(run_name=f"eval_{model_name.replace(\'/\', \'_\')}"):\n        # Load model\n        model = SentenceTransformer(model_name)\n\n        # Create evaluation dataset\n        eval_df = create_semantic_similarity_dataset()\n\n        # Create a wrapper model that outputs similarity predictions\n        class SimilarityPredictionModel(mlflow.pyfunc.PythonModel):\n            def __init__(self, sentence_transformer_model):\n                self.model = sentence_transformer_model\n\n            def predict(self, context, model_input):\n                """Predict similarity scores for text pairs."""\n                # Expect input DataFrame with \'text1\' and \'text2\' columns\n                embeddings1 = self.model.encode(model_input["text1"].tolist())\n                embeddings2 = self.model.encode(model_input["text2"].tolist())\n\n                similarities = []\n                for emb1, emb2 in zip(embeddings1, embeddings2):\n                    similarity = cosine_similarity([emb1], [emb2])[0][0]\n                    similarities.append(similarity)\n\n                return similarities\n\n        # Create wrapper model instance\n        similarity_model = SimilarityPredictionModel(model)\n\n        # Log the wrapper model for evaluation\n        input_example = eval_df[["text1", "text2"]].head(2)\n        signature = mlflow.models.infer_signature(\n            input_example, similarity_model.predict(None, input_example)\n        )\n\n        model_info = mlflow.pyfunc.log_model(\n            python_model=similarity_model,\n            name="similarity_model",\n            signature=signature,\n            input_example=input_example,\n        )\n\n        model_uri = model_info.model_uri\n\n        # Create custom metrics for MLflow evaluation\n\n        def pearson_correlation_metric(eval_df, builtin_metrics):\n            """Calculate Pearson correlation between predictions and targets."""\n            predictions = eval_df["prediction"]\n            targets = eval_df["similarity"]\n            correlation, _ = pearsonr(predictions, targets)\n            return correlation\n\n        def spearman_correlation_metric(eval_df, builtin_metrics):\n            """Calculate Spearman correlation between predictions and targets."""\n            predictions = eval_df["prediction"]\n            targets = eval_df["similarity"]\n            correlation, _ = spearmanr(predictions, targets)\n            return correlation\n\n        def accuracy_within_threshold_metric(eval_df, builtin_metrics, threshold=0.1):\n            """Calculate accuracy within similarity threshold."""\n            predictions = eval_df["prediction"]\n            targets = eval_df["similarity"]\n            accurate = np.abs(predictions - targets) <= threshold\n            return np.mean(accurate)\n\n        # Create MLflow metrics\n        pearson_metric = make_metric(\n            eval_fn=pearson_correlation_metric,\n            greater_is_better=True,\n            name="pearson_correlation",\n        )\n\n        spearman_metric = make_metric(\n            eval_fn=spearman_correlation_metric,\n            greater_is_better=True,\n            name="spearman_correlation",\n        )\n\n        accuracy_metric = make_metric(\n            eval_fn=lambda df, metrics: accuracy_within_threshold_metric(\n                df, metrics, 0.1\n            ),\n            greater_is_better=True,\n            name="accuracy_within_0.1",\n        )\n\n        # Prepare evaluation data for MLflow evaluate\n        eval_data_for_mlflow = eval_df[["text1", "text2", "similarity"]].copy()\n\n        # Use MLflow\'s evaluate API\n        result = mlflow.models.evaluate(\n            model_uri,\n            eval_data_for_mlflow,\n            targets="similarity",\n            model_type="regressor",  # Similarity prediction is a regression task\n            extra_metrics=[pearson_metric, spearman_metric, accuracy_metric],\n        )\n\n        # Extract our custom metrics\n        metrics = {\n            "pearson_correlation": result.metrics["pearson_correlation"],\n            "spearman_correlation": result.metrics["spearman_correlation"],\n            "accuracy_within_0.1": result.metrics["accuracy_within_0.1"],\n            "mean_absolute_error": result.metrics["mean_absolute_error"],\n            "root_mean_squared_error": result.metrics["root_mean_squared_error"],\n        }\n\n        print(f"Evaluation completed for {model_name}")\n        print(f"Pearson correlation: {metrics[\'pearson_correlation\']:.3f}")\n        print(f"Spearman correlation: {metrics[\'spearman_correlation\']:.3f}")\n        print(f"Mean Absolute Error: {metrics[\'mean_absolute_error\']:.3f}")\n\n        return metrics, result\n\n\n# Evaluate a single model\nmetrics, eval_result = evaluate_embedding_model_with_mlflow("all-MiniLM-L6-v2")\n'})})]}),(0,r.jsxs)(o.A,{value:"fine-tuning",label:"Fine-tuning",children:[(0,r.jsx)(n.h3,{id:"domain-specific-fine-tuning",children:"Domain-Specific Fine-tuning"}),(0,r.jsx)(n.p,{children:"Fine-tune sentence transformers for your specific domain while tracking the entire process:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n\ndef fine_tune_sentence_transformer():\n    """Fine-tune a sentence transformer for domain-specific data."""\n\n    # Sample training data (in practice, use much more data)\n    train_examples = [\n        InputExample(texts=["Python programming", "Coding in Python"], label=0.9),\n        InputExample(texts=["Machine learning model", "ML algorithm"], label=0.8),\n        InputExample(texts=["Data science project", "Analytics work"], label=0.7),\n        InputExample(texts=["Software development", "Cooking recipes"], label=0.1),\n        InputExample(texts=["Neural networks", "Deep learning"], label=0.9),\n        InputExample(texts=["Database query", "SQL programming"], label=0.8),\n        InputExample(texts=["Web development", "Frontend coding"], label=0.7),\n        InputExample(texts=["API integration", "Backend services"], label=0.6),\n    ]\n\n    with mlflow.start_run(run_name="fine_tuning_experiment"):\n        # Log training parameters\n        train_params = {\n            "base_model": "all-MiniLM-L6-v2",\n            "num_epochs": 3,\n            "batch_size": 16,\n            "learning_rate": 2e-5,\n            "warmup_steps": 100,\n            "training_examples": len(train_examples),\n        }\n        mlflow.log_params(train_params)\n\n        # Load base model\n        model = SentenceTransformer("all-MiniLM-L6-v2")\n\n        # Log original model performance\n        original_embedding_dim = model.get_sentence_embedding_dimension()\n        mlflow.log_metric("original_embedding_dimension", original_embedding_dim)\n\n        # Create data loader\n        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n        # Define loss function\n        train_loss = losses.CosineSimilarityLoss(model)\n\n        # Track training progress\n        class TrainingCallback:\n            def __init__(self):\n                self.step = 0\n\n            def __call__(self, score, epoch, steps):\n                self.step += 1\n                mlflow.log_metric("training_step", self.step)\n                if score is not None:\n                    mlflow.log_metric("evaluation_score", score, step=epoch)\n\n        callback = TrainingCallback()\n\n        # Fine-tune the model\n        print("Starting fine-tuning...")\n        model.fit(\n            train_objectives=[(train_dataloader, train_loss)],\n            epochs=3,\n            warmup_steps=100,\n            output_path="./fine_tuned_model",\n            callback=callback,\n            show_progress_bar=True,\n        )\n\n        # Log the fine-tuned model\n        model_info = mlflow.sentence_transformers.log_model(\n            model=model,\n            name="fine_tuned_model",\n            input_example=["Sample domain-specific text"],\n        )\n\n        # Test fine-tuned model on domain-specific examples\n        test_pairs = [\n            ("Python coding", "Programming in Python"),\n            ("Machine learning", "AI algorithms"),\n            ("Web development", "Cooking recipes"),  # Negative example\n        ]\n\n        for text1, text2 in test_pairs:\n            embeddings = model.encode([text1, text2])\n            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n            print(f"Similarity between \'{text1}\' and \'{text2}\': {similarity:.3f}")\n            mlflow.log_metric(f"similarity_{text1[:10]}_{text2[:10]}", similarity)\n\n        print("Fine-tuning completed and model logged!")\n        return model_info\n\n\n# Run fine-tuning\nfine_tuned_model_info = fine_tune_sentence_transformer()\n'})})]}),(0,r.jsxs)(o.A,{value:"production",label:"Production Deployment",children:[(0,r.jsx)(n.h3,{id:"production-ready-model-deployment",children:"Production-Ready Model Deployment"}),(0,r.jsx)(n.p,{children:"Create models ready for production deployment:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import ModelSignature\nfrom mlflow.types.schema import Schema, ColSpec\n\n\ndef create_production_ready_model():\n    """Create a production-ready semantic search model."""\n\n    with mlflow.start_run(run_name="production_semantic_search"):\n        model = SentenceTransformer("all-MiniLM-L6-v2")\n\n        # Define explicit signature for production\n        input_schema = Schema([ColSpec("string")])\n        output_schema = Schema([ColSpec("double", shape=(-1, 384))])\n        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n        # Log with production configuration\n        model_info = mlflow.sentence_transformers.log_model(\n            model=model,\n            name="production_embedder",\n            signature=signature,\n            input_example=["Production ready text embedding"],\n            pip_requirements=["sentence-transformers==4.1.0", "torch>=1.11.0"],\n            extra_pip_requirements=["numpy>=1.21.0"],\n        )\n\n        # Add production metadata\n        mlflow.set_tags(\n            {\n                "environment": "production",\n                "use_case": "semantic_search",\n                "deployment_ready": "true",\n            }\n        )\n\n        print(f"Production model ready: {model_info.model_uri}")\n        return model_info\n\n\n# Create production model\nproduction_model = create_production_ready_model()\n'})})]}),(0,r.jsxs)(o.A,{value:"batch-processing",label:"Batch Processing Pipeline",default:!0,children:[(0,r.jsx)(n.h3,{id:"batch-processing-pipeline",children:"Batch Processing Pipeline"}),(0,r.jsx)(n.p,{children:"Create efficient batch processing for large-scale embeddings:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\n\n\ndef create_batch_embedding_pipeline():\n    """Create a batch processing pipeline for large-scale embedding generation."""\n\n    with mlflow.start_run(run_name="batch_embedding_pipeline"):\n        model = SentenceTransformer("all-MiniLM-L6-v2")\n\n        # Simulate large dataset\n        large_text_dataset = [\n            f"Document {i}: This is sample text for embedding generation."\n            for i in range(1000)\n        ]\n\n        # Batch processing configuration\n        batch_config = {\n            "batch_size": 32,\n            "show_progress_bar": True,\n            "convert_to_numpy": True,\n            "normalize_embeddings": True,\n        }\n\n        mlflow.log_params(batch_config)\n        mlflow.log_param("total_documents", len(large_text_dataset))\n\n        # Process in batches\n        start_time = time.time()\n\n        embeddings = model.encode(\n            large_text_dataset,\n            batch_size=batch_config["batch_size"],\n            show_progress_bar=batch_config["show_progress_bar"],\n            convert_to_numpy=batch_config["convert_to_numpy"],\n            normalize_embeddings=batch_config["normalize_embeddings"],\n        )\n\n        processing_time = time.time() - start_time\n\n        # Log performance metrics\n        mlflow.log_metrics(\n            {\n                "processing_time_seconds": processing_time,\n                "documents_per_second": len(large_text_dataset) / processing_time,\n                "embedding_dimension": embeddings.shape[1],\n                "total_embeddings": embeddings.shape[0],\n            }\n        )\n\n        # Save embeddings as artifact\n        np.save("batch_embeddings.npy", embeddings)\n        mlflow.log_artifact("batch_embeddings.npy")\n\n        # Log optimized model for batch processing\n        mlflow.sentence_transformers.log_model(\n            model=model, name="batch_processor", input_example=large_text_dataset[:5]\n        )\n\n        print(\n            f"Processed {len(large_text_dataset)} documents in {processing_time:.2f} seconds"\n        )\n        print(f"Rate: {len(large_text_dataset) / processing_time:.1f} documents/second")\n\n\n# Run batch processing pipeline\ncreate_batch_embedding_pipeline()\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"advanced-workflows",children:"Advanced Workflows"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsxs)(o.A,{value:"model-comparison",label:"Model Comparison",default:!0,children:[(0,r.jsx)(n.h3,{id:"systematic-multi-model-evaluation",children:"Systematic Multi-Model Evaluation"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def comprehensive_model_comparison():\n    """Compare multiple sentence transformer models systematically."""\n\n    models_to_compare = [\n        "all-MiniLM-L6-v2",\n        "all-mpnet-base-v2",\n        "paraphrase-albert-small-v2",\n        "multi-qa-MiniLM-L6-cos-v1",\n    ]\n\n    # Parent run for the comparison experiment\n    with mlflow.start_run(run_name="multi_model_evaluation"):\n        all_results = {}\n\n        for model_name in models_to_compare:\n            print(f"\\nEvaluating {model_name}...")\n\n            # Nested run for each model\n            with mlflow.start_run(\n                run_name=f"eval_{model_name.replace(\'/\', \'_\')}", nested=True\n            ):\n                # Evaluate using our custom function\n                metrics, _ = evaluate_embedding_model_with_mlflow(model_name)\n                all_results[model_name] = metrics\n\n        # Create comparison summary\n        comparison_data = []\n        for model_name, metrics in all_results.items():\n            comparison_data.append(\n                {\n                    "model": model_name,\n                    "pearson_correlation": metrics["pearson_correlation"],\n                    "spearman_correlation": metrics["spearman_correlation"],\n                    "mean_absolute_error": metrics["mean_absolute_error"],\n                    "accuracy_within_0.1": metrics["accuracy_within_0.1"],\n                }\n            )\n\n        # Log comparison results\n        comparison_df = pd.DataFrame(comparison_data)\n        comparison_df.to_csv("model_comparison.csv", index=False)\n        mlflow.log_artifact("model_comparison.csv")\n\n        # Find best model\n        best_model = comparison_df.loc[comparison_df["pearson_correlation"].idxmax()]\n\n        mlflow.set_tag("best_model", best_model["model"])\n\n        print("\\n" + "=" * 60)\n        print("MODEL COMPARISON SUMMARY")\n        print("=" * 60)\n        print(comparison_df.round(3))\n        print(f"\\nBest model: {best_model[\'model\']}")\n        print(f"Best Pearson correlation: {best_model[\'pearson_correlation\']:.3f}")\n\n\n# Run comprehensive comparison\ncomprehensive_model_comparison()\n'})}),(0,r.jsx)(n.h3,{id:"performance-vs-quality-trade-offs",children:"Performance vs. Quality Trade-offs"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import matplotlib.pyplot as plt\n\n\ndef analyze_speed_quality_tradeoffs():\n    """Analyze the trade-off between model speed and quality."""\n\n    model_configs = [\n        {"name": "paraphrase-albert-small-v2", "category": "fast"},\n        {"name": "all-MiniLM-L6-v2", "category": "balanced"},\n        {"name": "all-mpnet-base-v2", "category": "quality"},\n    ]\n\n    with mlflow.start_run(run_name="speed_quality_analysis"):\n        results = []\n\n        for config in model_configs:\n            model_name = config["name"]\n            print(f"Analyzing {model_name}...")\n\n            with mlflow.start_run(\n                run_name=f"analysis_{model_name.replace(\'/\', \'_\')}", nested=True\n            ):\n                model = SentenceTransformer(model_name)\n\n                # Speed test\n                test_texts = ["Sample text for speed testing"] * 100\n                start_time = time.time()\n                embeddings = model.encode(test_texts)\n                encoding_time = time.time() - start_time\n\n                # Quality test (simplified)\n                test_pairs = [\n                    ("The cat is sleeping", "A cat is resting"),\n                    ("I love programming", "Coding is my passion"),\n                    ("The weather is nice", "It\'s raining heavily"),\n                ]\n\n                similarities = []\n                for text1, text2 in test_pairs:\n                    emb1, emb2 = model.encode([text1, text2])\n                    sim = cosine_similarity([emb1], [emb2])[0][0]\n                    similarities.append(sim)\n\n                # Calculate metrics\n                speed = len(test_texts) / encoding_time\n                avg_similarity = np.mean(similarities)\n\n                result = {\n                    "model": model_name,\n                    "category": config["category"],\n                    "speed_texts_per_sec": speed,\n                    "avg_similarity_quality": avg_similarity,\n                    "embedding_dim": model.get_sentence_embedding_dimension(),\n                    "encoding_time": encoding_time,\n                }\n\n                results.append(result)\n                mlflow.log_metrics(result)\n\n        # Create trade-off visualization\n        results_df = pd.DataFrame(results)\n\n        plt.figure(figsize=(10, 6))\n        scatter = plt.scatter(\n            results_df["speed_texts_per_sec"],\n            results_df["avg_similarity_quality"],\n            s=results_df["embedding_dim"] / 5,  # Size by embedding dimension\n            alpha=0.7,\n        )\n\n        for i, row in results_df.iterrows():\n            plt.annotate(\n                row["model"].split("/")[-1],\n                (row["speed_texts_per_sec"], row["avg_similarity_quality"]),\n                xytext=(5, 5),\n                textcoords="offset points",\n            )\n\n        plt.xlabel("Speed (texts/second)")\n        plt.ylabel("Quality (avg similarity)")\n        plt.title("Speed vs Quality Trade-off")\n        plt.grid(True, alpha=0.3)\n        plt.savefig("speed_quality_tradeoff.png")\n        mlflow.log_artifact("speed_quality_tradeoff.png")\n        plt.close()\n\n        results_df.to_csv("speed_quality_analysis.csv", index=False)\n        mlflow.log_artifact("speed_quality_analysis.csv")\n\n\n# Run speed-quality analysis\nanalyze_speed_quality_tradeoffs()\n'})})]}),(0,r.jsxs)(o.A,{value:"custom-workflows",label:"Custom Workflows",children:[(0,r.jsx)(n.h3,{id:"domain-specific-evaluation-pipeline",children:"Domain-Specific Evaluation Pipeline"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def create_domain_evaluation_pipeline(domain_name, test_cases):\n    """Create a domain-specific evaluation pipeline."""\n\n    with mlflow.start_run(run_name=f"domain_eval_{domain_name}"):\n        # Test multiple models on domain-specific tasks\n        models_to_test = [\n            "all-MiniLM-L6-v2",\n            "all-mpnet-base-v2",\n            "multi-qa-MiniLM-L6-cos-v1",\n        ]\n\n        domain_results = {}\n\n        for model_name in models_to_test:\n            print(f"Testing {model_name} on {domain_name} domain...")\n\n            model = SentenceTransformer(model_name)\n\n            # Domain-specific evaluation\n            domain_scores = []\n            for case in test_cases:\n                query = case["query"]\n                expected_doc = case["expected_match"]\n                distractor_docs = case["distractors"]\n\n                # Encode query and documents\n                query_emb = model.encode([query])\n                doc_embs = model.encode([expected_doc] + distractor_docs)\n\n                # Calculate similarities\n                similarities = cosine_similarity(query_emb, doc_embs)[0]\n\n                # Check if expected match has highest similarity\n                best_match_idx = np.argmax(similarities)\n                is_correct = best_match_idx == 0  # First doc is expected match\n                confidence = similarities[0]  # Similarity to expected match\n\n                domain_scores.append(\n                    {"correct": is_correct, "confidence": confidence, "query": query}\n                )\n\n            # Calculate domain metrics\n            accuracy = np.mean([score["correct"] for score in domain_scores])\n            avg_confidence = np.mean([score["confidence"] for score in domain_scores])\n\n            domain_results[model_name] = {\n                "accuracy": accuracy,\n                "avg_confidence": avg_confidence,\n                "detailed_scores": domain_scores,\n            }\n\n            # Log model-specific metrics\n            mlflow.log_metrics(\n                {\n                    f"{model_name}_accuracy": accuracy,\n                    f"{model_name}_confidence": avg_confidence,\n                }\n            )\n\n        # Find best model for this domain\n        best_model = max(\n            domain_results.keys(), key=lambda x: domain_results[x]["accuracy"]\n        )\n\n        mlflow.log_params(\n            {\n                "domain": domain_name,\n                "num_test_cases": len(test_cases),\n                "best_model_for_domain": best_model,\n            }\n        )\n\n        # Save detailed results\n        results_summary = pd.DataFrame(\n            [\n                {\n                    "model": model,\n                    "accuracy": results["accuracy"],\n                    "avg_confidence": results["avg_confidence"],\n                }\n                for model, results in domain_results.items()\n            ]\n        )\n\n        results_summary.to_csv(f"{domain_name}_evaluation_results.csv", index=False)\n        mlflow.log_artifact(f"{domain_name}_evaluation_results.csv")\n\n        print(f"Best model for {domain_name}: {best_model}")\n        print(f"Accuracy: {domain_results[best_model][\'accuracy\']:.3f}")\n\n        return domain_results\n\n\n# Example: Legal domain evaluation\nlegal_test_cases = [\n    {\n        "query": "contract termination clauses",\n        "expected_match": "Legal provisions regarding contract termination and breach",\n        "distractors": [\n            "Software development contracts and agreements",\n            "Real estate purchase agreements",\n            "Employment termination procedures",\n        ],\n    },\n    {\n        "query": "intellectual property rights",\n        "expected_match": "Patents, trademarks, and copyright protections",\n        "distractors": [\n            "Physical property ownership laws",\n            "Digital privacy and data protection",\n            "Software licensing agreements",\n        ],\n    },\n]\n\nlegal_results = create_domain_evaluation_pipeline("legal", legal_test_cases)\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices-and-optimization",children:"Best Practices and Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"experiment-organization",children:"Experiment Organization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83c\udff7\ufe0f ",(0,r.jsx)(n.strong,{children:"Consistent Tagging"}),": Use descriptive tags to organize experiments by use case, model type, and evaluation stage"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Comprehensive Metrics"}),": Track both technical metrics (encoding speed, embedding dimensions) and task-specific performance"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcdd ",(0,r.jsx)(n.strong,{children:"Documentation"}),": Include detailed descriptions of experimental setup, data sources, and intended use cases"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-management",children:"Model Management"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Version Control"}),": Maintain clear versioning for models, datasets, and evaluation protocols"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udce6 ",(0,r.jsx)(n.strong,{children:"Artifact Organization"}),": Store related artifacts (datasets, evaluation results, visualizations) together"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\ude80 ",(0,r.jsx)(n.strong,{children:"Deployment Readiness"}),": Ensure models include proper signatures, dependencies, and usage examples"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Batch Processing"}),": Use batch encoding for better throughput when processing multiple texts"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,r.jsx)(n.strong,{children:"Model Selection"}),": Choose models that balance quality and speed for your specific use case"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcbe ",(0,r.jsx)(n.strong,{children:"Caching Strategies"}),": Cache embeddings for frequently accessed content to improve response times"]}),"\n"]}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsxs)(o.A,{value:"optimization-tips",label:"Performance Tips",default:!0,children:[(0,r.jsx)(n.h3,{id:"efficient-batch-processing",children:"Efficient Batch Processing"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def optimized_batch_encoding():\n    """Demonstrate optimized batch processing techniques."""\n\n    with mlflow.start_run(run_name="batch_optimization"):\n        model = SentenceTransformer("all-MiniLM-L6-v2")\n\n        # Large dataset simulation\n        large_dataset = [\n            f"Document {i} with sample content for encoding." for i in range(5000)\n        ]\n\n        # Test different batch sizes\n        batch_sizes = [16, 32, 64, 128]\n        results = []\n\n        for batch_size in batch_sizes:\n            print(f"Testing batch size: {batch_size}")\n\n            start_time = time.time()\n            embeddings = model.encode(\n                large_dataset,\n                batch_size=batch_size,\n                show_progress_bar=False,\n                convert_to_tensor=False,\n                normalize_embeddings=True,\n            )\n            processing_time = time.time() - start_time\n\n            throughput = len(large_dataset) / processing_time\n\n            result = {\n                "batch_size": batch_size,\n                "processing_time": processing_time,\n                "throughput": throughput,\n                "memory_efficient": batch_size <= 64,\n            }\n\n            results.append(result)\n            mlflow.log_metrics(\n                {\n                    f"batch_{batch_size}_time": processing_time,\n                    f"batch_{batch_size}_throughput": throughput,\n                }\n            )\n\n        # Find optimal batch size\n        optimal_batch = max(results, key=lambda x: x["throughput"])\n\n        mlflow.log_params(\n            {\n                "optimal_batch_size": optimal_batch["batch_size"],\n                "optimal_throughput": optimal_batch["throughput"],\n                "dataset_size": len(large_dataset),\n            }\n        )\n\n        # Log results\n        results_df = pd.DataFrame(results)\n        results_df.to_csv("batch_optimization_results.csv", index=False)\n        mlflow.log_artifact("batch_optimization_results.csv")\n\n        print(f"Optimal batch size: {optimal_batch[\'batch_size\']}")\n        print(f"Best throughput: {optimal_batch[\'throughput\']:.1f} docs/sec")\n\n\noptimized_batch_encoding()\n'})})]}),(0,r.jsxs)(o.A,{value:"deployment-patterns",label:"Deployment Patterns",children:[(0,r.jsx)(n.h3,{id:"production-api-wrapper",children:"Production API Wrapper"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom typing import List, Dict, Optional\nimport numpy as np\n\n\nclass ProductionEmbeddingService:\n    """Production-ready embedding service with MLflow integration."""\n\n    def __init__(self, model_uri: str):\n        self.model = mlflow.sentence_transformers.load_model(model_uri)\n        self.model_uri = model_uri\n\n    def encode_texts(\n        self, texts: List[str], normalize: bool = True, batch_size: int = 32\n    ) -> np.ndarray:\n        """Encode texts with production optimizations."""\n\n        embeddings = self.model.encode(\n            texts,\n            batch_size=batch_size,\n            convert_to_numpy=True,\n            normalize_embeddings=normalize,\n            show_progress_bar=False,\n        )\n\n        return embeddings\n\n    def similarity_search(\n        self, query: str, documents: List[str], top_k: int = 5\n    ) -> List[Dict]:\n        """Perform similarity search with ranking."""\n\n        # Encode query and documents\n        query_embedding = self.model.encode([query])\n        doc_embeddings = self.model.encode(documents)\n\n        # Calculate similarities\n        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n\n        # Get top-k results\n        top_indices = np.argsort(similarities)[::-1][:top_k]\n\n        results = []\n        for i, idx in enumerate(top_indices):\n            results.append(\n                {\n                    "rank": i + 1,\n                    "document": documents[idx],\n                    "similarity_score": float(similarities[idx]),\n                    "document_index": int(idx),\n                }\n            )\n\n        return results\n\n    def health_check(self) -> Dict:\n        """Service health check."""\n        try:\n            # Test encoding\n            test_embedding = self.model.encode(["Health check test"])\n\n            return {\n                "status": "healthy",\n                "model_uri": self.model_uri,\n                "embedding_dimension": test_embedding.shape[1],\n                "test_successful": True,\n            }\n        except Exception as e:\n            return {"status": "unhealthy", "error": str(e), "test_successful": False}\n\n\ndef deploy_embedding_service():\n    """Deploy the embedding service with MLflow tracking."""\n\n    with mlflow.start_run(run_name="production_deployment"):\n        # Log a model for deployment\n        model = SentenceTransformer("all-MiniLM-L6-v2")\n\n        model_info = mlflow.sentence_transformers.log_model(\n            model=model,\n            name="production_embedder",\n            input_example=["Sample production text"],\n            pip_requirements=["sentence-transformers>=4.0.0"],\n        )\n\n        # Create service instance\n        service = ProductionEmbeddingService(model_info.model_uri)\n\n        # Test the service\n        health_status = service.health_check()\n        mlflow.log_params(health_status)\n\n        # Performance test\n        test_texts = ["Test document " + str(i) for i in range(100)]\n        start_time = time.time()\n        embeddings = service.encode_texts(test_texts)\n        encoding_time = time.time() - start_time\n\n        # Log performance metrics\n        mlflow.log_metrics(\n            {\n                "service_encoding_time": encoding_time,\n                "service_throughput": len(test_texts) / encoding_time,\n                "embedding_dimension": embeddings.shape[1],\n            }\n        )\n\n        mlflow.set_tags(\n            {\n                "deployment_ready": "true",\n                "service_type": "embedding_api",\n                "production_tested": "true",\n            }\n        )\n\n        print("Production service deployed and tested successfully!")\n        print(f"Health status: {health_status[\'status\']}")\n        print(f"Throughput: {len(test_texts) / encoding_time:.1f} texts/sec")\n\n        return service, model_info\n\n\n# Deploy the service\nservice, deployment_info = deploy_embedding_service()\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsx)(n.p,{children:"The MLflow-Sentence Transformers integration excels in practical scenarios such as:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,r.jsx)(n.strong,{children:"Document Search Systems"}),": Build intelligent search engines that understand user intent and find relevant documents based on semantic meaning"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udff7\ufe0f ",(0,r.jsx)(n.strong,{children:"Content Classification"}),": Automatically categorize and tag content with high accuracy using semantic similarity rather than keyword matching"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83e\udd16 ",(0,r.jsx)(n.strong,{children:"Chatbot Intent Recognition"}),": Understand user queries and match them to appropriate responses or actions"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcda ",(0,r.jsx)(n.strong,{children:"Knowledge Base Organization"}),": Cluster and organize large document collections for better information retrieval"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd17 ",(0,r.jsx)(n.strong,{children:"Recommendation Engines"}),": Build content recommendation systems that understand semantic relationships between items"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udf10 ",(0,r.jsx)(n.strong,{children:"Cross-lingual Applications"}),": Develop systems that work across multiple languages with shared semantic understanding"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Data Deduplication"}),": Identify similar or duplicate content even when expressed differently"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,r.jsx)(n.strong,{children:"Question Answering"}),": Match questions to relevant answers in knowledge bases or FAQs"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"The MLflow-Sentence Transformers integration provides a comprehensive foundation for building, tracking, and deploying semantic understanding applications. By combining sentence transformers' powerful semantic capabilities with MLflow's experiment management, you create workflows that are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,r.jsx)(n.strong,{children:"Semantically Aware"}),": Understand and work with the true meaning of text beyond simple keyword matching"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Reproducible"}),": Every embedding model and evaluation can be recreated exactly"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Comparable"}),": Different models and approaches can be evaluated side-by-side with clear metrics"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcc8 ",(0,r.jsx)(n.strong,{children:"Scalable"}),": From simple similarity tasks to complex semantic search systems"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udc65 ",(0,r.jsx)(n.strong,{children:"Collaborative"}),": Teams can share models, results, and insights effectively"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\ude80 ",(0,r.jsx)(n.strong,{children:"Production-Ready"}),": Seamless deployment of semantic models with proper monitoring and versioning"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Whether you're building your first semantic search system or deploying enterprise-scale text understanding applications, the MLflow-Sentence Transformers integration provides the foundation for organized, reproducible, and scalable semantic AI development."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}}}]);