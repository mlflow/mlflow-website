"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3023],{28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(96540);const r={},l=i.createContext(r);function a(e){const t=i.useContext(l);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(l.Provider,{value:t},e.children)}},33475:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>f,frontMatter:()=>o,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"llms/llm-tracking/index","title":"MLflow\'s LLM Tracking Capabilities","description":"MLflow\'s LLM Tracking system is an enhancement to the existing MLflow Tracking system, offerring additional capabilities for monitoring,","source":"@site/docs/llms/llm-tracking/index.mdx","sourceDirName":"llms/llm-tracking","slug":"/llms/llm-tracking/","permalink":"/docs/latest/llms/llm-tracking/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var r=n(74848),l=n(28453),a=n(67756);const o={},s="MLflow's LLM Tracking Capabilities",c={},m=[{value:"Introduction to LLM Tracking",id:"llm-tracking-introduction",level:2},{value:"Detailed Logging of LLM Interactions",id:"how-llm-data-is-captured",level:2},{value:"Structured Storage of LLM Tracking Data",id:"storage-of-llm-data",level:2}];function p(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"mlflows-llm-tracking-capabilities",children:"MLflow's LLM Tracking Capabilities"})}),"\n",(0,r.jsx)(t.p,{children:"MLflow's LLM Tracking system is an enhancement to the existing MLflow Tracking system, offerring additional capabilities for monitoring,\nmanaging, and interpreting interactions with Large Language Models (LLMs)."}),"\n",(0,r.jsx)(t.p,{children:"At its core, MLflow's LLM suite builds upon the standard logging capabilities familiar to professionals working with traditional\nMachine Learning (ML) and Deep Learning (DL). However, it introduces distinct features tailored for the unique intricacies of LLMs."}),"\n",(0,r.jsxs)(t.p,{children:["One such standout feature is the introduction of \"prompts\" \u2013 the queries or inputs directed towards an LLM \u2013 and the subsequent data\nthe model generates in response. While MLflow's offerings for other model types typically exclude built-in mechanisms for preserving\ninference results, LLMs necessitate this due to their dynamic and generative nature. Recognizing this, MLflow introduces the term\n'predictions' alongside the existing tracking components of ",(0,r.jsx)(t.strong,{children:"artifacts"}),", ",(0,r.jsx)(t.strong,{children:"parameters"}),", ",(0,r.jsx)(t.strong,{children:"tags"}),", and ",(0,r.jsx)(t.strong,{children:"metrics"}),", ensuring comprehensive\nlineage and quality tracking for text-generating models."]}),"\n",(0,r.jsx)(t.h2,{id:"llm-tracking-introduction",children:"Introduction to LLM Tracking"}),"\n",(0,r.jsxs)(t.p,{children:["The world of Large Language Models is vast, and as these models become more intricate and sophisticated, the need for a robust\ntracking system becomes paramount. MLflow's LLM Tracking is centered around the concept of ",(0,r.jsx)(t.em,{children:"runs"}),". In essence, a run is a\ndistinct execution or interaction with the LLM \u2014 whether it's a single query, a batch of prompts, or an entire fine-tuning session."]}),"\n",(0,r.jsx)(t.p,{children:"Each run meticulously records:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Parameters"}),": Key-value pairs that detail the input parameters for the LLM. These could range from model-specific parameters like ",(0,r.jsx)(t.em,{children:"top_k"})," and ",(0,r.jsx)(t.em,{children:"temperature"}),"\nto more generic ones. They provide context and configuration for each run. Parameters can be logged using both ",(0,r.jsx)(a.B,{fn:"mlflow.log_param"}),"\nfor individual entries and ",(0,r.jsx)(a.B,{fn:"mlflow.log_params"})," for bulk logging."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Metrics"}),": These are quantitative measures, often numeric, that give insights into the performance, accuracy, or any other measurable aspect of the LLM interaction.\nMetrics are dynamic and can be updated as the run progresses, offering a real-time or post-process insight into the model's behavior. Logging of metrics is facilitated\nthrough ",(0,r.jsx)(a.B,{fn:"mlflow.log_metric"})," and ",(0,r.jsx)(a.B,{fn:"mlflow.log_metrics"}),"."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Predictions"}),": To understand and evaluate LLM outputs, MLflow allows for the logging of predictions. This encompasses the prompts or inputs sent to the LLM and\nthe outputs or responses received. For structured storage and easy retrieval, these predictions are stored as artifacts in CSV format, ensuring that each interaction\nis preserved in its entirety. This logging is achieved using the dedicated ",(0,r.jsx)(a.B,{fn:"mlflow.log_table"}),"."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Artifacts"}),": Beyond predictions, MLflow's LLM Tracking can store a myriad of output files, ranging from visualization images (e.g., PNGs), serialized models\n(e.g., an ",(0,r.jsx)(t.em,{children:"openai"})," model), to structured data files (e.g., a ",(0,r.jsx)(t.a,{href:"https://parquet.apache.org",children:"Parquet"})," file). The ",(0,r.jsx)(a.B,{fn:"mlflow.log_artifact"})," function is\nat the heart of this, allowing users to log and organize their artifacts with ease."]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["Furthermore, to provide structured organization and comparative analysis capabilities, runs can be grouped into ",(0,r.jsx)(t.em,{children:"experiments"}),".\nThese experiments act as containers, grouping related runs, and providing a higher level of organization. This organization ensures\nthat related runs can be compared, analyzed, and managed as a cohesive unit."]}),"\n",(0,r.jsx)(t.h2,{id:"how-llm-data-is-captured",children:"Detailed Logging of LLM Interactions"}),"\n",(0,r.jsx)(t.p,{children:"MLflow's LLM Tracking doesn't just record data \u2014 it offers structured logging mechanisms tailored to the needs of LLM interactions:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Parameters"}),": Logging parameters is straightforward. Whether you're logging a single parameter using ",(0,r.jsx)(a.B,{fn:"mlflow.log_param"})," or multiple parameters simultaneously with ",(0,r.jsx)(a.B,{fn:"mlflow.log_params"}),", MLflow ensures that every detail is captured."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Metrics"}),": Quantitative insights are crucial. Whether it's tracking the accuracy of a fine-tuned LLM or understanding its response time, metrics provide this insight. They can be logged individually via ",(0,r.jsx)(a.B,{fn:"mlflow.log_metric"})," or in bulk using ",(0,r.jsx)(a.B,{fn:"mlflow.log_metrics"}),"."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Predictions"}),": Every interaction with an LLM yields a result \u2014 a prediction. Capturing this prediction, along with the inputs that led to it, is crucial. The ",(0,r.jsx)(a.B,{fn:"mlflow.log_table"})," function is specifically designed for this, ensuring that both inputs and outputs are logged cohesively."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Artifacts"}),": Artifacts act as the tangible outputs of an LLM run. They can be images, models, or any other form of data. Logging them is seamless with ",(0,r.jsx)(a.B,{fn:"mlflow.log_artifact"}),", which ensures that every piece of data, regardless of its format, is stored and linked to its respective run."]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"storage-of-llm-data",children:"Structured Storage of LLM Tracking Data"}),"\n",(0,r.jsx)(t.p,{children:"Every piece of data, every parameter, metric, prediction, and artifact is not just logged \u2014 it's structured and stored as part of an\nMLflow Experiment run. This organization ensures data integrity, easy retrieval, and a structured approach to analyzing and understanding\nLLM interactions in the grand scheme of machine learning workflows."})]})}function f(e={}){const{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},67756:(e,t,n)=>{n.d(t,{B:()=>s});n(96540);const i=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=n(29030),l=n(56289),a=n(74848);const o=e=>{const t=e.split(".");for(let n=t.length;n>0;n--){const e=t.slice(0,n).join(".");if(i[e])return e}return null};function s(e){let{fn:t,children:n}=e;const s=o(t);if(!s)return(0,a.jsx)(a.Fragment,{children:n});const c=(0,r.Ay)(`/${i[s]}#${t}`);return(0,a.jsx)(l.A,{to:c,target:"_blank",children:n??(0,a.jsxs)("code",{children:[t,"()"]})})}}}]);