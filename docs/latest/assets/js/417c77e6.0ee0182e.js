"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["5308"],{35648(e,n,t){t.r(n),t.d(n,{metadata:()=>l,default:()=>h,frontMatter:()=>s,contentTitle:()=>p,toc:()=>c,assets:()=>m});var l=JSON.parse('{"id":"tracing/integrations/listing/haystack","title":"Tracing Haystack","description":"Haystack is an open-source AI orchestration framework developed by deepset, designed to help Python developers build production-ready LLM-powered applications.","source":"@site/docs/genai/tracing/integrations/listing/haystack.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/haystack","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/haystack","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"sidebar_label":"Haystack"},"sidebar":"genAISidebar","previous":{"title":"Google ADK","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/google-adk"},"next":{"title":"Koog","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/koog"}}'),a=t(74848),o=t(28453),i=t(54725),r=t(46077);let s={sidebar_position:8,sidebar_label:"Haystack"},p="Tracing Haystack",m={},c=[{value:"Basic Example",id:"basic-example",level:3},{value:"Token usage",id:"token-usage",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:3}];function f(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"tracing-haystack",children:"Tracing Haystack"})}),"\n",(0,a.jsx)(r.A,{src:"/images/llms/haystack/haystack-tracing.png",alt:"Haystack Tracing via autolog"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/deepset-ai/haystack",children:"Haystack"})," is an open-source AI orchestration framework developed by deepset, designed to help Python developers build production-ready LLM-powered applications.\nIt features a modular architecture - built around components and pipelines for building everything from retrieval-augmented generation (RAG) workflows to autonomous agentic systems and scalable search engines."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"../../",children:"MLflow Tracing"})," provides automatic tracing capability when using Haystack pipelines and components.\nWhen Haystack auto-tracing is enabled by calling the ",(0,a.jsx)(i.B,{fn:"mlflow.haystack.autolog"})," function,\nusage of Haystack pipelines and components will automatically record generated traces during interactive development."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.haystack.autolog()\n"})}),"\n",(0,a.jsx)(n.p,{children:"MLflow trace automatically captures the following information:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Pipelines and Components"}),"\n",(0,a.jsx)(n.li,{children:"Latencies"}),"\n",(0,a.jsx)(n.li,{children:"Metadata about the different components added, such as tool names"}),"\n",(0,a.jsx)(n.li,{children:"Token usages and cost"}),"\n",(0,a.jsx)(n.li,{children:"Cache hit"}),"\n",(0,a.jsx)(n.li,{children:"Any exception if raised"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"basic-example",children:"Basic Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nfrom haystack import Document, Pipeline\nfrom haystack.components.builders.chat_prompt_builder import ChatPromptBuilder\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.components.retrievers.in_memory import InMemoryBM25Retriever\nfrom haystack.dataclasses import ChatMessage\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.utils import Secret\n\nmlflow.haystack.autolog()\nmlflow.set_experiment("Haystack Tracing")\n\n# Write documents to InMemoryDocumentStore\ndocument_store = InMemoryDocumentStore()\ndocument_store.write_documents(\n    [\n        Document(content="My name is Jean and I live in Paris."),\n        Document(content="My name is Mark and I live in Berlin."),\n        Document(content="My name is Giorgio and I live in Rome."),\n    ]\n)\n\n# Build a RAG pipeline\nprompt_template = [\n    ChatMessage.from_system("You are a helpful assistant."),\n    ChatMessage.from_user(\n        "Given these documents, answer the question.\\n"\n        "Documents:\\n{% for doc in documents %}{{ doc.content }}{% endfor %}\\n"\n        "Question: {{question}}\\n"\n        "Answer:"\n    ),\n]\n\n# Define required variables explicitly\nprompt_builder = ChatPromptBuilder(\n    template=prompt_template, required_variables={"question", "documents"}\n)\n\nretriever = InMemoryBM25Retriever(document_store=document_store)\nllm = OpenAIChatGenerator(api_key=Secret.from_env_var("OPENAI_API_KEY"))\n\nrag_pipeline = Pipeline()\nrag_pipeline.add_component("retriever", retriever)\nrag_pipeline.add_component("prompt_builder", prompt_builder)\nrag_pipeline.add_component("llm", llm)\nrag_pipeline.connect("retriever", "prompt_builder.documents")\nrag_pipeline.connect("prompt_builder", "llm.messages")\n\n# Ask a question\nquestion = "Who lives in Paris?"\nresults = rag_pipeline.run(\n    {\n        "retriever": {"query": question},\n        "prompt_builder": {"question": question},\n    }\n)\n\nprint(results["llm"]["replies"])\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Haystack Tracing via autolog",src:t(70793).A+"",width:"3136",height:"1786"})}),"\n",(0,a.jsx)(n.h2,{id:"token-usage",children:"Token usage"}),"\n",(0,a.jsxs)(n.p,{children:["MLflow >= 3.4.0 supports token usage tracking for Haystack. The token usage for each LLM call will be logged in the ",(0,a.jsx)(n.code,{children:"mlflow.chat.tokenUsage"})," attribute. The total token usage throughout the trace will be\navailable in the ",(0,a.jsx)(n.code,{children:"token_usage"})," field of the trace info object."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'question = "Who lives in Paris?"\nresults = rag_pipeline.run(\n    {\n        "retriever": {"query": question},\n        "prompt_builder": {"question": question},\n    }\n)\n\nprint(results["llm"]["replies"])\n\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Detailed usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 64\n  Output tokens: 5\n  Total tokens: 69\n\n== Detailed usage for each LLM call: ==\nOpenAIChatGenerator:\n  Input tokens: 64\n  Output tokens: 5\n"})}),"\n",(0,a.jsx)(n.h3,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,a.jsxs)(n.p,{children:["Auto tracing for Haystack can be disabled globally by calling ",(0,a.jsx)(n.code,{children:"mlflow.haystack.autolog(disable=True)"})," or ",(0,a.jsx)(n.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function h(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(f,{...e})}):f(e)}},70793(e,n,t){t.d(n,{A:()=>l});let l=t.p+"assets/images/haystack-basic-tracing-cc6d8017b6b96aefb71c5df7cdce81b1.png"},54725(e,n,t){t.d(n,{B:()=>i});var l=t(74848);t(96540);var a=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),o=t(66497);function i({fn:e,children:n,hash:t}){let i=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(a[t])return t}return null})(e);if(!i)return(0,l.jsx)(l.Fragment,{children:n});let r=(0,o.default)(`/${a[i]}#${t??e}`);return(0,l.jsx)("a",{href:r,target:"_blank",children:n??(0,l.jsxs)("code",{children:[e,"()"]})})}},46077(e,n,t){t.d(n,{A:()=>o});var l=t(74848);t(96540);var a=t(66497);function o({src:e,alt:n,width:t,caption:o,className:i}){return(0,l.jsxs)("div",{className:`container_JwLF ${i||""}`,children:[(0,l.jsx)("div",{className:"imageWrapper_RfGN",style:t?{width:t}:{},children:(0,l.jsx)("img",{src:(0,a.default)(e),alt:n,className:"image_bwOA"})}),o&&(0,l.jsx)("p",{className:"caption_jo2G",children:o})]})}},28453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var l=t(96540);let a={},o=l.createContext(a);function i(e){let n=l.useContext(o);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),l.createElement(o.Provider,{value:n},e.children)}}}]);