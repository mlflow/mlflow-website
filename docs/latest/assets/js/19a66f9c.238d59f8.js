"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[456],{11470:(e,n,t)=>{t.d(n,{A:()=>j});var s=t(96540),l=t(34164),o=t(23104),i=t(56347),a=t(205),r=t(57485),p=t(31682),c=t(70679);function m(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return m(e).map((({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s})))}(t);return function(e){const n=(0,p.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u({value:e,tabValues:n}){return n.some((n=>n.value===e))}function h({queryString:e=!1,groupId:n}){const t=(0,i.W6)(),l=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,r.aZ)(l),(0,s.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(t.location.search);n.set(l,e),t.replace({...t.location,search:n.toString()})}),[l,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:l}=e,o=d(e),[i,r]=(0,s.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o}))),[p,m]=h({queryString:t,groupId:l}),[f,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,l]=(0,c.Dv)(n);return[t,(0,s.useCallback)((e=>{n&&l.set(e)}),[n,l])]}({groupId:l}),_=(()=>{const e=p??f;return u({value:e,tabValues:o})?e:null})();(0,a.A)((()=>{_&&r(_)}),[_]);return{selectedValue:i,selectValue:(0,s.useCallback)((e=>{if(!u({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);r(e),m(e),g(e)}),[m,g,o]),tabValues:o}}var g=t(92303);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function w({className:e,block:n,selectedValue:t,selectValue:s,tabValues:i}){const a=[],{blockElementScrollPositionUntilNextRender:r}=(0,o.a_)(),p=e=>{const n=e.currentTarget,l=a.indexOf(n),o=i[l].value;o!==t&&(r(n),s(o))},c=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":n},e),children:i.map((({value:e,label:n,attributes:s})=>(0,y.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{a.push(e)},onKeyDown:c,onClick:p,...s,className:(0,l.A)("tabs__item",_.tabItem,s?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function x({lazy:e,children:n,selectedValue:t}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find((e=>e.props.value===t));return e?(0,s.cloneElement)(e,{className:(0,l.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function v(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,l.A)("tabs-container",_.tabList),children:[(0,y.jsx)(w,{...n,...e}),(0,y.jsx)(x,{...n,...e})]})}function j(e){const n=(0,g.A)();return(0,y.jsx)(v,{...e,children:m(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var s=t(34164);const l={tabItem:"tabItem_Ymn6"};var o=t(74848);function i({children:e,hidden:n,className:t}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,s.A)(l.tabItem,t),hidden:n,children:e})}},26210:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>c,default:()=>h,frontMatter:()=>p,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"serving/responses-agent","title":"ResponsesAgent for Model Serving","description":"The ResponsesAgent class in MLflow provides a specialized interface for serving generative AI models that handle structured responses with tool calling capabilities. This agent is designed to work seamlessly with MLflow\'s serving infrastructure while providing compatibility with OpenAI-style APIs.","source":"@site/docs/genai/serving/responses-agent.mdx","sourceDirName":"serving","slug":"/serving/responses-agent","permalink":"/docs/latest/genai/serving/responses-agent","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"MLflow Model Serving","permalink":"/docs/latest/genai/serving/"},"next":{"title":"Custom Apps","permalink":"/docs/latest/genai/serving/custom-apps"}}');var l=t(74848),o=t(28453),i=t(49374),a=t(11470),r=t(19365);const p={},c="ResponsesAgent for Model Serving",m={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Structured Response Handling",id:"structured-response-handling",level:3},{value:"OpenAI API Compatibility",id:"openai-api-compatibility",level:3},{value:"MLflow Integration",id:"mlflow-integration",level:3},{value:"Basic Usage",id:"basic-usage",level:2},{value:"Implementing a ResponsesAgent",id:"implementing-a-responsesagent",level:3},{value:"Getting started",id:"getting-started",level:4},{value:"Creating agent output",id:"creating-agent-output",level:4},{value:"Streaming agent output",id:"streaming-agent-output",level:4},{value:"Basic text streaming",id:"basic-text-streaming",level:5},{value:"Tool calling with streaming",id:"tool-calling-with-streaming",level:5},{value:"Logging and Serving",id:"logging-and-serving",level:3},{value:"Examples",id:"examples",level:2},{value:"Simple Chat Example",id:"simple-chat-example",level:3},{value:"Tool Calling Example",id:"tool-calling-example",level:3},{value:"Serving Options",id:"serving-options",level:2},{value:"Schema and Types",id:"schema-and-types",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging",id:"debugging",level:3}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"responsesagent-for-model-serving",children:"ResponsesAgent for Model Serving"})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.ResponsesAgent",children:"ResponsesAgent"})," class in MLflow provides a specialized interface for serving generative AI models that handle structured responses with tool calling capabilities. This agent is designed to work seamlessly with MLflow's serving infrastructure while providing compatibility with OpenAI-style APIs."]}),"\n",(0,l.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.ResponsesAgent",children:(0,l.jsx)(n.code,{children:"ResponsesAgent"})})," extends MLflow's PyFunc model\ninterface to support conversational AI applications that require advanced capabilities such as\nmulti-turn dialogue, tool-calling, multi-agent orchestration, and compatibility with OpenAI\u2019s\nResponses API and MLflow model tracking."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\ud83d\udce6 Structured request/response handling"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udee0\ufe0f Tool calling and function execution"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udcac Chat history management"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udcca Token usage tracking"}),"\n",(0,l.jsx)(n.li,{children:"\ud83e\udd16 OpenAI API compatibility"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udd01 Support for returning multiple output messages, including intermediate outputs from tool-calling"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udc65 Support for multi-agent scenarios"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udcda Compatibility with MLflow logging, tracing, and model serving"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\udd17 Ensure compatibility with OpenAI Responses API for seamless integration with downstream clients and UIs"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"This makes it ideal for building and deploying chatbots, virtual assistants, and other conversational AI applications."}),"\n",(0,l.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,l.jsx)(n.h3,{id:"structured-response-handling",children:"Structured Response Handling"}),"\n",(0,l.jsx)(n.p,{children:"The ResponsesAgent processes structured inputs and outputs that conform to chat completion standards:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Messages"}),": Handles conversation history with role-based messages (system, user, assistant)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Tool Calls"}),": Supports function calling with structured parameters"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Usage Tracking"}),": Monitors token consumption and model performance"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Metadata"}),": Captures additional context and configuration"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"openai-api-compatibility",children:"OpenAI API Compatibility"}),"\n",(0,l.jsx)(n.p,{children:"ResponsesAgent is designed with full OpenAI API compatibility in mind, allowing seamless integration with existing applications and tools built for OpenAI's chat completions API. This compatibility extends across request formats, response structures, and API endpoints."}),"\n",(0,l.jsx)(n.h3,{id:"mlflow-integration",children:"MLflow Integration"}),"\n",(0,l.jsx)(n.p,{children:"Full integration with MLflow's ecosystem including:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\ud83d\udcc8 Model tracking and versioning"}),"\n",(0,l.jsx)(n.li,{children:"\ud83e\uddea Experiment management"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\uddc3\ufe0f Model registry"}),"\n",(0,l.jsx)(n.li,{children:"\ud83d\ude80 Deployment options"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,l.jsx)(n.h3,{id:"implementing-a-responsesagent",children:"Implementing a ResponsesAgent"}),"\n",(0,l.jsx)(n.h4,{id:"getting-started",children:"Getting started"}),"\n",(0,l.jsxs)(n.p,{children:["To create your own agent, subclass ",(0,l.jsx)(n.code,{children:"mlflow.pyfunc.ResponsesAgent"})," and implement your agent logic in the ",(0,l.jsx)(n.code,{children:"predict"})," method. The implementation is framework-agnostic, allowing you to use any agent authoring framework. Note that ",(0,l.jsx)(n.code,{children:"pydantic>=2"})," is required to use ResponsesAgent. For example implementations, see the ",(0,l.jsx)(n.a,{href:"#simple-chat-example",children:"simple chat agent"})," and the ",(0,l.jsx)(n.a,{href:"#tool-calling-example",children:"tool calling agent"})," below."]}),"\n",(0,l.jsx)(n.h4,{id:"creating-agent-output",children:"Creating agent output"}),"\n",(0,l.jsxs)(n.p,{children:["When implementing your agent, you'll work with two main output types: ",(0,l.jsx)(i.B,{fn:"mlflow.types.responses.ResponsesAgentResponse",children:(0,l.jsx)(n.code,{children:"ResponsesAgentResponse"})})," and ",(0,l.jsx)(i.B,{fn:"mlflow.types.responses.ResponsesAgentResponse",children:(0,l.jsx)(n.code,{children:"ResponsesAgentStreamEvent"})}),". These are the only pydantic objects you should create directly. The remaining classes in ",(0,l.jsx)(n.code,{children:"mlflow.types.responses_helpers"})," are only for validating dictionaries."]}),"\n",(0,l.jsxs)(n.p,{children:["If you want to return outputs that don't fit into the standard interface, you can use the ",(0,l.jsx)(n.code,{children:"custom_outputs"})," field."]}),"\n",(0,l.jsx)(n.p,{children:"Below are some helper methods you can use to create common outputs within the ResponsesAgent interface:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.ResponsesAgent.create_text_output_item"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.ResponsesAgent.create_function_call_item"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.ResponsesAgent.create_function_call_output_item"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.ResponsesAgent.create_text_delta"})," (only for streaming)"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["Here's an example of a complete tool calling sequence using ",(0,l.jsx)(n.code,{children:"ResponsesAgentResponse"})," with a custom output:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse\n\n\nclass SimpleResponsesAgent(ResponsesAgent):\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n        return ResponsesAgentResponse(\n            output=[\n                self.create_function_call_item(\n                    id="fc_1",\n                    call_id="call_1",\n                    name="python_exec",\n                    arguments=\'{"code":"result = 4 * 3\\\\nprint(result)"}\',\n                ),\n                self.create_function_call_output_item(\n                    call_id="call_1",\n                    output="12\\n",\n                ),\n                self.create_text_output_item(\n                    text="The result of 4 * 3 in Python is 12.",\n                    id="msg_1",\n                ),\n            ],\n            custom_outputs={"key1": "custom-value1"},\n        )\n'})}),"\n",(0,l.jsx)(n.h4,{id:"streaming-agent-output",children:"Streaming agent output"}),"\n",(0,l.jsx)(n.p,{children:"For real-time processing, you can use streaming events instead of returning a complete response. Streaming allows you to send partial results as they become available, which is useful for long-running operations or when you want to show progress to users."}),"\n",(0,l.jsx)(n.h5,{id:"basic-text-streaming",children:"Basic text streaming"}),"\n",(0,l.jsx)(n.p,{children:"To stream text within the ResponsesAgent interface, you should:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["yield ",(0,l.jsx)(n.code,{children:"response.output_text.delta"})," events with the chunks as they become available","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["it must have an ",(0,l.jsx)(n.code,{children:"item_id"})," that corresponds related events to a single output item"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["yield a ",(0,l.jsx)(n.code,{children:"response.output_item.done"})," event to aggregate all chunks"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.types.responses import ResponsesAgentStreamEvent\n\n\nclass SimpleResponsesAgent(ResponsesAgent):\n    # ... continuing from above\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict_stream(\n        self, request: ResponsesAgentRequest\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        # stream text, all with the same item_id\n        yield ResponsesAgentStreamEvent(\n            **self.create_text_delta(delta="Hello", item_id="msg_1"),\n        )\n        yield ResponsesAgentStreamEvent(\n            **self.create_text_delta(delta="world", item_id="msg_1"),\n        )\n        yield ResponsesAgentStreamEvent(\n            **self.create_text_delta(delta="!", item_id="msg_1"),\n        )\n\n        # the text output item id should be the same\n        # item_id as the streamed text deltas\n        yield ResponsesAgentStreamEvent(\n            type="response.output_item.done",\n            item=self.create_text_output_item(\n                text="Hello world!",\n                id="msg_1",\n            ),\n        )\n'})}),"\n",(0,l.jsx)(n.h5,{id:"tool-calling-with-streaming",children:"Tool calling with streaming"}),"\n",(0,l.jsxs)(n.p,{children:["You can also stream tool calls and their results. Each tool call and its output are sent as separate ",(0,l.jsx)(n.code,{children:"response.output_item.done"})," events. This enables MLflow tracing and makes it easier for clients to reconstruct streamed message history."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.types.responses import ResponsesAgentStreamEvent\n\n\nclass SimpleResponsesAgent(ResponsesAgent):\n    # ... continuing from above\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict_stream(\n        self, request: ResponsesAgentRequest\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        yield ResponsesAgentStreamEvent(\n            type="response.output_item.done",\n            item=self.create_function_call_item(\n                id="fc_1",\n                call_id="call_1",\n                name="python_exec",\n                arguments=\'{"code":"result = 4 * 3\\\\nprint(result)"}\',\n            ),\n        )\n        yield ResponsesAgentStreamEvent(\n            type="response.output_item.done",\n            item=self.create_function_call_output_item(\n                call_id="call_1",\n                output="12\\n",\n            ),\n        )\n        yield ResponsesAgentStreamEvent(\n            type="response.output_item.done",\n            item=self.create_text_output_item(\n                text="The result of 4 * 3 in Python is 12.",\n                id="msg_1",\n            ),\n        )\n'})}),"\n",(0,l.jsx)(n.h3,{id:"logging-and-serving",children:"Logging and Serving"}),"\n",(0,l.jsxs)(n.p,{children:["Log your agent using the ",(0,l.jsx)(n.a,{href:"/ml/model/models-from-code",children:"Models-from-code"})," approach. This approach is framework-agnostic and supports all authoring frameworks:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'with mlflow.start_run():\n    logged_agent_info = mlflow.pyfunc.log_model(\n        python_model="agent.py",  # replace with your relative path to agent code\n        name="agent",\n    )\n'})}),"\n",(0,l.jsx)(n.p,{children:"For ease of use, MLflow has built in the following features:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Automatic model signature inference","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"An input and output signature that adheres to the ResponsesAgentRequest and ResponsesAgentResponse schemas will be set"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["Metadata","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:'{"task": "agent/v1/responses"}'})," will be automatically appended to any metadata that you may pass in when logging the model"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["Input Example","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Providing an input example is optional, ",(0,l.jsx)(n.code,{children:"mlflow.types.responses.RESPONSES_AGENT_INPUT_EXAMPLE"})," will be used by default"]}),"\n",(0,l.jsx)(n.li,{children:"If you do provide an input example, ensure it's a dictionary of the ResponsesAgentRequest schema"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"To test out a ResponsesAgent, you can pass a single input dictionary that follows the ResponsesAgentRequest schema both before and after logging it:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# load it back from mlflow\nloaded_model = mlflow.pyfunc.load_model(logged_agent_info.model_uri)\nloaded_model.predict(\n    {\n        "input": [{"role": "user", "content": "what is 4*3 in python"}],\n        "context": {"conversation_id": "123", "user_id": "456"},\n    }\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["To serve the model, refer to the ",(0,l.jsx)(n.a,{href:"#serving-options",children:(0,l.jsx)(n.code,{children:"serving options"})})," for detailed instructions."]}),"\n",(0,l.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,l.jsx)(n.h3,{id:"simple-chat-example",children:"Simple Chat Example"}),"\n",(0,l.jsx)(n.p,{children:"Here's an example of an agent that calls OpenAI's gpt-4o model with a simple tool:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# uncomment below if running inside a jupyter notebook\n# %%writefile agent.py\nimport os\nfrom typing import Generator\n\nimport mlflow\nfrom mlflow.entities.span import SpanType\nfrom mlflow.models import set_model\nfrom mlflow.pyfunc.model import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n    ResponsesAgentStreamEvent,\n)\nfrom openai import OpenAI\n\n\nclass SimpleResponsesAgent(ResponsesAgent):\n    def __init__(self, model: str):\n        self.client = OpenAI()\n        self.model = model\n\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n        response = self.client.responses.create(input=request.input, model=self.model)\n        return ResponsesAgentResponse(**response.to_dict())\n\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict_stream(\n        self, request: ResponsesAgentRequest\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        for event in self.client.responses.create(\n            input=request.input, stream=True, model=self.model\n        ):\n            yield ResponsesAgentStreamEvent(**event.to_dict())\n\n\nmlflow.openai.autolog()\nagent = SimpleResponsesAgent(model="gpt-4o")\nset_model(agent)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"tool-calling-example",children:"Tool Calling Example"}),"\n",(0,l.jsx)(n.p,{children:"Here's an example of an agent that calls OpenAI's gpt-4o model with a simple tool:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# uncomment below if running inside a jupyter notebook\n# %%writefile agent.py\nimport json\nfrom typing import Any, Callable, Generator\nimport os\nfrom uuid import uuid4\n\nimport backoff\nimport mlflow\nimport openai\nfrom mlflow.entities import SpanType\nfrom mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n    ResponsesAgentStreamEvent,\n)\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclass ToolInfo(BaseModel):\n    """\n    Class representing a tool for the agent.\n    - "name" (str): The name of the tool.\n    - "spec" (dict): JSON description of the tool (matches OpenAI Responses format)\n    - "exec_fn" (Callable): Function that implements the tool logic\n    """\n\n    name: str\n    spec: dict\n    exec_fn: Callable\n\n\nclass ToolCallingAgent(ResponsesAgent):\n    """\n    Class representing a tool-calling Agent\n    """\n\n    def __init__(self, model: str, tools: list[ToolInfo]):\n        """Initializes the ToolCallingAgent with tools."""\n        self.model = model\n        self.client: OpenAI = OpenAI()\n        self._tools_dict = {tool.name: tool for tool in tools}\n\n    def get_tool_specs(self) -> list[dict]:\n        """Returns tool specifications in the format OpenAI expects."""\n        return [tool_info.spec for tool_info in self._tools_dict.values()]\n\n    @mlflow.trace(span_type=SpanType.TOOL)\n    def execute_tool(self, tool_name: str, args: dict) -> Any:\n        """Executes the specified tool with the given arguments."""\n        return self._tools_dict[tool_name].exec_fn(**args)\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n    @mlflow.trace(span_type=SpanType.LLM)\n    def call_llm(self, input_messages) -> ResponsesAgentStreamEvent:\n        return (\n            self.client.responses.create(\n                model=self.model,\n                input=input_messages,\n                tools=self.get_tool_specs(),\n            )\n            .output[0]\n            .model_dump(exclude_none=True)\n        )\n\n    def handle_tool_call(self, tool_call: dict[str, Any]) -> ResponsesAgentStreamEvent:\n        """\n        Execute tool calls and return a ResponsesAgentStreamEvent w/ tool output\n        """\n        args = json.loads(tool_call["arguments"])\n        result = str(self.execute_tool(tool_name=tool_call["name"], args=args))\n\n        tool_call_output = {\n            "type": "function_call_output",\n            "call_id": tool_call["call_id"],\n            "output": result,\n        }\n        return ResponsesAgentStreamEvent(\n            type="response.output_item.done", item=tool_call_output\n        )\n\n    def call_and_run_tools(\n        self,\n        input_messages,\n        max_iter: int = 10,\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        for _ in range(max_iter):\n            last_msg = input_messages[-1]\n            if (\n                last_msg.get("type", None) == "message"\n                and last_msg.get("role", None) == "assistant"\n            ):\n                return\n            if last_msg.get("type", None) == "function_call":\n                tool_call_res = self.handle_tool_call(last_msg)\n                input_messages.append(tool_call_res.item)\n                yield tool_call_res\n            else:\n                llm_output = self.call_llm(input_messages=input_messages)\n                input_messages.append(llm_output)\n                yield ResponsesAgentStreamEvent(\n                    type="response.output_item.done",\n                    item=llm_output,\n                )\n\n        yield ResponsesAgentStreamEvent(\n            type="response.output_item.done",\n            item={\n                "id": str(uuid4()),\n                "content": [\n                    {\n                        "type": "output_text",\n                        "text": "Max iterations reached. Stopping.",\n                    }\n                ],\n                "role": "assistant",\n                "type": "message",\n            },\n        )\n\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n        outputs = [\n            event.item\n            for event in self.predict_stream(request)\n            if event.type == "response.output_item.done"\n        ]\n        return ResponsesAgentResponse(\n            output=outputs, custom_outputs=request.custom_inputs\n        )\n\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict_stream(\n        self, request: ResponsesAgentRequest\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        input_messages = [{"role": "system", "content": SYSTEM_PROMPT}] + [\n            i.model_dump() for i in request.input\n        ]\n        yield from self.call_and_run_tools(input_messages=input_messages)\n\n\ntools = [\n    ToolInfo(\n        name="get_weather",\n        spec={\n            "type": "function",\n            "name": "get_weather",\n            "description": "Get current temperature for provided coordinates in celsius.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "latitude": {"type": "number"},\n                    "longitude": {"type": "number"},\n                },\n                "required": ["latitude", "longitude"],\n                "additionalProperties": False,\n            },\n            "strict": True,\n        },\n        exec_fn=lambda latitude, longitude: 70,  # dummy tool implementation\n    )\n]\n\nos.environ["OPENAI_API_KEY"] = "your OpenAI API key"\n\nSYSTEM_PROMPT = "You are a helpful assistant that can call tools to get information."\nmlflow.openai.autolog()\nAGENT = ToolCallingAgent(model="gpt-4o", tools=tools)\nmlflow.models.set_model(AGENT)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"serving-options",children:"Serving Options"}),"\n",(0,l.jsxs)(a.A,{children:[(0,l.jsxs)(r.A,{value:"local-serving",label:"Local Serving",children:[(0,l.jsx)(n.p,{children:"Start a local server for development and testing:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow models serve -m models:/<model_id> -p 5000\n"})}),(0,l.jsx)(n.p,{children:"Test the served model:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import requests\n\nresponse = requests.post(\n    "http://localhost:5000/invocations",\n    json={"messages": [{"role": "user", "content": "What\'s the weather like?"}]},\n)\n\nprint(response.json())\n'})})]}),(0,l.jsx)(r.A,{value:"docker-deployment",label:"Docker Deployment",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow models build-docker -m runs:/<run_id>/responses_agent -n my-responses-agent\ndocker run -p 5000:8080 my-responses-agent\n"})})}),(0,l.jsxs)(r.A,{value:"databricks-model-serving",label:"Databricks Model Serving",children:[(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"Endpoint creation and management features are available in Databricks' managed MLflow service but not in open-source MLflow."})}),(0,l.jsx)(n.p,{children:"Prerequisite: register the logged model into Databricks-UC"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.register_model("model:/<model_id>", "<catalog>.<schema>.<model_name>")\n'})}),(0,l.jsx)(n.p,{children:"For managed MLflow on Databricks, ResponsesAgent models integrate seamlessly with Databricks Model Serving:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client("databricks")\nendpoint = client.create_endpoint(\n    name="unity-catalog-model-endpoint",\n    config={\n        "served_entities": [\n            {\n                "name": "ads-entity",\n                "entity_name": "catalog.schema.my-ads-model",\n                "entity_version": "3",\n                "workload_size": "Small",\n                "scale_to_zero_enabled": True,\n            }\n        ],\n        "traffic_config": {\n            "routes": [\n                {"served_model_name": "my-ads-model-3", "traffic_percentage": 100}\n            ]\n        },\n    },\n)\n'})})]})]}),"\n",(0,l.jsx)(n.h2,{id:"schema-and-types",children:"Schema and Types"}),"\n",(0,l.jsxs)(n.p,{children:["ResponsesAgent works with structured schemas for requests and responses, check ",(0,l.jsx)(i.B,{fn:"mlflow.types.responses.ResponsesAgentRequest",children:"ResponsesAgentRequest"})," and ",(0,l.jsx)(i.B,{fn:"mlflow.types.responses.ResponsesAgentResponse",children:"ResponsesAgentResponse"})," for detailed structure."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Example Request schema\n{\n    "input": [\n        {\n            "role": "user",\n            "content": "What is the weather like in Boston today?",\n        }\n    ],\n    "tools": [\n        {\n            "type": "function",\n            "name": "get_current_weather",\n            "parameters": {\n                "type": "object",\n                "properties": {"location": {"type": "string"}},\n                "required": ["location", "unit"],\n            },\n        }\n    ],\n}\n\n# Example Response schema\n{\n    "output": [\n        {\n            "type": "message",\n            "id": "some-id",\n            "status": "completed",\n            "role": "assistant",\n            "content": [\n                {\n                    "type": "output_text",\n                    "text": "rainy",\n                }\n            ],\n        }\n    ],\n}\n'})}),"\n",(0,l.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,l.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Import Errors"}),": Ensure all dependencies are included in the conda environment"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Schema Validation"}),": Verify input/output formats match expected schemas"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Logging Issues"}),": User ",(0,l.jsx)(n.code,{children:"models from code"})," feature to log the model"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Missing traces"}),": Enable tracing by ",(0,l.jsx)(n.code,{children:"mlflow.<flavor>.autolog"})," or manual tracing with ",(0,l.jsx)(n.code,{children:"mlflow.start_span"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"debugging",children:"Debugging"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsxs)(n.strong,{children:["Enable ",(0,l.jsx)(n.a,{href:"/genai/tracing",children:"tracing"})]})," for troubleshooting, if your model is traced:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# enable autologging if your agent internally uses MLflow tracing-supported libraries internally, such as LangChain, OpenAI, etc.\n# mlflow.<flavor>.autolog()\n\n# Test your agent locally before serving\nagent = MyResponsesAgent()\nagent.predict(\n    {\n        "input": [{"role": "user", "content": "what is 4*3 in python"}],\n        "context": {"conversation_id": "123", "user_id": "456"},\n    }\n)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(u,{...e})}):u(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var s=t(96540);const l={},o=s.createContext(l);function i(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),s.createElement(o.Provider,{value:n},e.children)}},49374:(e,n,t)=>{t.d(n,{B:()=>r});t(96540);const s=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=t(86025),o=t(28774),i=t(74848);const a=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(s[e])return e}return null};function r({fn:e,children:n}){const t=a(e);if(!t)return(0,i.jsx)(i.Fragment,{children:n});const r=(0,l.Ay)(`/${s[t]}#${e}`);return(0,i.jsx)(o.A,{to:r,target:"_blank",children:n??(0,i.jsxs)("code",{children:[e,"()"]})})}}}]);