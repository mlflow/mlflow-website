/*! For license information please see 7801048f.f575462a.js.LICENSE.txt */
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4077],{6789:(e,t,a)=>{a.d(t,{A:()=>c});a(96540);var s=a(28774),n=a(34164);const i={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var r=a(86025),o=a(21122),l=a(74848);function c({icon:e,image:t,imageDark:a,imageWidth:c,imageHeight:d,iconSize:u=32,containerHeight:m,title:h,description:p,href:g,linkText:x="Learn more \u2192",className:v}){if(!e&&!t)throw new Error("TileCard requires either an icon or image prop");const j=m?{height:`${m}px`}:{},y={};return c&&(y.width=`${c}px`),d&&(y.height=`${d}px`),(0,l.jsxs)(s.A,{href:g,className:(0,n.A)(i.tileCard,v),children:[(0,l.jsx)("div",{className:i.tileIcon,style:j,children:e?(0,l.jsx)(e,{size:u}):a?(0,l.jsx)(o.A,{sources:{light:(0,r.Ay)(t),dark:(0,r.Ay)(a)},alt:h,className:i.tileImage,style:y}):(0,l.jsx)("img",{src:(0,r.Ay)(t),alt:h,className:i.tileImage,style:y})}),(0,l.jsx)("h3",{children:h}),(0,l.jsx)("p",{children:p}),(0,l.jsx)("div",{className:i.tileLink,children:x})]})}},15977:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("refresh-cw",[["path",{d:"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8",key:"v9h5vc"}],["path",{d:"M21 3v5h-5",key:"1q7to0"}],["path",{d:"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16",key:"3uifl3"}],["path",{d:"M8 16H3v5",key:"1cv678"}]])},28453:(e,t,a)=>{a.d(t,{R:()=>r,x:()=>o});var s=a(96540);const n={},i=s.createContext(n);function r(e){const t=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),s.createElement(i.Provider,{value:t},e.children)}},47792:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},51004:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("database",[["ellipse",{cx:"12",cy:"5",rx:"9",ry:"3",key:"msslwz"}],["path",{d:"M3 5V19A9 3 0 0 0 21 19V5",key:"1wlel7"}],["path",{d:"M3 12A9 3 0 0 0 21 12",key:"mv7ke4"}]])},56955:(e,t,a)=>{a.d(t,{A:()=>P});var s=a(96540);const n="loopContainer_P7aD",i="loopTitle_JPUj",r="loopContent_d_OB",o="circleContainer_r3vu",l="svgCanvas_uDoP",c="arrowPath_C9al",d="arrowHead_pHvN",u="stepNode_dfTI",m="stepNodeContent_qttg",h="highlighted_oNtg",p="focusNode_z3RB",g="stepNumber_LNrP",x="stepLabel_vl8R",v="centerIcon_KAOa",j="loopIconWrapper_xBPW",y="loopText_T4eg",f="tooltip_UzKu",k="tooltipTitle_HAKW",w="tooltipDescription_EDYJ",_="tooltipArrow_WNhr",A="centerTooltip_R18b",N="centerTooltipDescription_ttXB",b="mobileLinearContent_PCYK",M="mobileStepItem_x9mX",L="mobileStepIndicator_zWzO",C="mobileStepNumber_HnjD",I="mobileFocusNode_FTRa",D="mobileStepConnector_kK9y",S="mobileStepContent_jmKx",q="mobileStepTitle_P2DM",E="mobileStepDescription_qbMN",z="mobileLoopBack_nXtn",B="mobileLoopIcon_FAGz",$="mobileLoopContent_BRFV",T="mobileLoopTitle_JcCt",H="mobileLoopDescription_5B8T";var R=a(74848);const P=({steps:e,title:t,loopBackIcon:a,loopBackText:P,loopBackDescription:W,circleSize:J=400})=>{const[F,U]=(0,s.useState)(null),[O,G]=(0,s.useState)(!1),[K,Q]=(0,s.useState)({x:0,y:0}),[V,X]=(0,s.useState)(!1),Z=(0,s.useRef)(null);s.useEffect(()=>{const e=()=>{X(window.innerWidth<=768)};return e(),window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)},[]);const Y=V?280:J,ee=(()=>{const t=J/2,a=V?100:140,s=V?130:220,n=Math.min(1.2,.8+.05*e.length),i=(t-(V?50:80))*n;return Math.max(a,Math.min(s,i))})(),te=Y/2,ae=Y/2,se=t=>{const a=2*t*Math.PI/e.length-Math.PI/2;return{x:te+ee*Math.cos(a),y:ae+ee*Math.sin(a)}},ne=(e,t)=>{const a=se(e),s=se(t),n=s.x-a.x,i=s.y-a.y,r=(a.x+s.x)/2,o=(a.y+s.y)/2,l=Math.sqrt(n*n+i*i),c=n/l,d=i/l;return`M ${r-10*c} ${o-10*d} L ${r+10*c} ${o+10*d}`},ie=()=>{U(null)};return V?(0,R.jsxs)("div",{className:n,children:[t&&(0,R.jsx)("h3",{className:i,children:t}),(0,R.jsxs)("div",{className:b,children:[e.map((t,a)=>(0,R.jsxs)("div",{className:M,children:[(0,R.jsxs)("div",{className:L,children:[(0,R.jsx)("div",{className:`${C} ${t.isFocus?I:""}`,children:t.icon?(0,R.jsx)(t.icon,{size:20}):(0,R.jsx)("span",{children:a+1})}),a<e.length-1&&(0,R.jsx)("div",{className:D})]}),(0,R.jsxs)("div",{className:S,children:[(0,R.jsx)("h4",{className:q,children:t.title}),(0,R.jsx)("p",{className:E,children:t.detailedDescription||t.description})]})]},a)),a&&W&&(0,R.jsxs)("div",{className:z,children:[(0,R.jsx)("div",{className:B,children:(0,R.jsx)(a,{size:24})}),(0,R.jsxs)("div",{className:$,children:[(0,R.jsx)("h4",{className:T,children:P||"Iterate"}),(0,R.jsx)("p",{className:H,children:W})]})]})]})]}):(0,R.jsxs)("div",{className:n,children:[t&&(0,R.jsx)("h3",{className:i,children:t}),(0,R.jsx)("div",{className:r,children:(0,R.jsxs)("div",{className:o,ref:Z,style:{width:`${Y}px`,height:`${Y}px`},children:[(0,R.jsxs)("svg",{width:Y,height:Y,className:l,children:[e.map((t,a)=>{const s=(a+1)%e.length;return(0,R.jsxs)("g",{children:[(0,R.jsx)("defs",{children:(0,R.jsx)("marker",{id:`arrowhead-${a}`,markerWidth:"6",markerHeight:"6",refX:"5",refY:"3",orient:"auto",children:(0,R.jsx)("path",{d:"M 0 0 L 6 3 L 0 6 L 1.5 3 Z",fill:"currentColor",opacity:"1",className:d})})}),(0,R.jsx)("path",{d:ne(a,s),fill:"none",stroke:"currentColor",strokeWidth:"2",strokeDasharray:"0",opacity:"0.9",markerEnd:`url(#arrowhead-${a})`,className:c})]},`arrow-${a}`)}),a&&(0,R.jsxs)("g",{className:v,onMouseEnter:()=>G(!0),onMouseLeave:()=>G(!1),style:{cursor:"pointer"},children:[(0,R.jsx)("foreignObject",{x:te-35,y:ae-35,width:"70",height:"70",children:(0,R.jsx)("div",{className:j,children:(0,R.jsx)(a,{size:32})})}),P&&(0,R.jsx)("text",{x:te,y:ae+50,textAnchor:"middle",className:y,children:P})]})]}),e.map((e,t)=>{const a=se(t);return(0,R.jsxs)("div",{className:`${u} ${e.highlight?h:""} ${e.isFocus?p:""}`,style:{left:`${a.x}px`,top:`${a.y}px`,transform:"translate(-50%, -50%)"},onMouseEnter:e=>(e=>{if(U(e),Z.current){Z.current.getBoundingClientRect();const t=se(e);Q({x:t.x,y:t.y})}})(t),onMouseLeave:ie,children:[(0,R.jsx)("div",{className:m,children:e.icon?(0,R.jsx)(e.icon,{size:24}):(0,R.jsx)("span",{className:g,children:t+1})}),(0,R.jsx)("div",{className:x,children:e.title})]},t)}),null!==F&&(0,R.jsxs)("div",{className:f,style:{left:`${K.x}px`,top:`${K.y}px`,transform:"translate(-50%, -120%)"},children:[(0,R.jsx)("h4",{className:k,children:e[F].title}),(0,R.jsx)("p",{className:w,children:e[F].detailedDescription||e[F].description}),(0,R.jsx)("div",{className:_})]}),O&&W&&(0,R.jsx)("div",{className:A,style:{left:`${te}px`,top:`${ae}px`,transform:"translate(-50%, -50%)"},children:(0,R.jsx)("p",{className:N,children:W})})]})})]})}},61878:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("git-branch",[["line",{x1:"6",x2:"6",y1:"3",y2:"15",key:"17qcm7"}],["circle",{cx:"18",cy:"6",r:"3",key:"1h7g24"}],["circle",{cx:"6",cy:"18",r:"3",key:"fqmcym"}],["path",{d:"M18 9a9 9 0 0 1-9 9",key:"n2h4wq"}]])},65592:(e,t,a)=>{a.d(t,{A:()=>r});a(96540);var s=a(34164);const n={tilesGrid:"tilesGrid_hB9N"};var i=a(74848);function r({children:e,className:t}){return(0,i.jsx)("div",{className:(0,s.A)(n.tilesGrid,t),children:e})}},70702:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>k,contentTitle:()=>f,default:()=>A,frontMatter:()=>y,metadata:()=>s,toc:()=>w});const s=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/datasets","title":"Judge Dataset Integration","description":"Using evaluation datasets with custom LLM judges for systematic testing and improvement","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/datasets.mdx","sourceDirName":"eval-monitor/scorers/llm-judge","slug":"/eval-monitor/scorers/llm-judge/datasets","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/datasets","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Judge Dataset Integration","sidebar_label":"Dataset Integration","description":"Using evaluation datasets with custom LLM judges for systematic testing and improvement","tag":"Advanced","keywords":["mlflow genai","mlflow judges","custom scorers","evaluation datasets","model evaluation","llm judge accuracy","alignment"]},"sidebar":"genAISidebar","previous":{"title":"Human Feedback Alignment","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/alignment"},"next":{"title":"Overview","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/agentic-overview"}}');var n=a(74848),i=a(28453),r=a(82238),o=a(56955),l=a(65592),c=a(6789),d=a(51004),u=a(47792),m=a(96393),h=a(93893),p=a(15977),g=a(96844),x=a(61878),v=a(80827),j=a(87073);const y={title:"Judge Dataset Integration",sidebar_label:"Dataset Integration",description:"Using evaluation datasets with custom LLM judges for systematic testing and improvement",tag:"Advanced",keywords:["mlflow genai","mlflow judges","custom scorers","evaluation datasets","model evaluation","llm judge accuracy","alignment"]},f="Judge Dataset Integration",k={},w=[{value:"Why Integrate Judges with Datasets?",id:"why-integrate-judges-with-datasets",level:2},{value:"Complete Example: Build \u2192 Evaluate \u2192 Improve",id:"complete-example-build--evaluate--improve",level:2},{value:"Key Integration Points",id:"key-integration-points",level:2},{value:"Dataset Operations",id:"dataset-operations",level:3},{value:"Evaluation with Datasets",id:"evaluation-with-datasets",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Learn More",id:"learn-more",level:2}];function _(e){const t={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"judge-dataset-integration",children:"Judge Dataset Integration"})}),"\n",(0,n.jsx)(t.p,{children:"Evaluation datasets enable systematic testing and improvement of your custom LLM judges. By building datasets from traces and adding ground truth labels, you can measure judge accuracy and identify areas for improvement."}),"\n",(0,n.jsx)(o.A,{steps:[{icon:d.A,title:"Build Dataset",description:"Collect traces & add labels",detailedDescription:"Build evaluation datasets from traces and add ground truth expectations for judge validation.",isFocus:!0},{icon:u.A,title:"Create & Run Judges",description:"Evaluate with judges",detailedDescription:"Design domain-specific judges and run them against the dataset to generate assessments.",isFocus:!0},{icon:m.A,title:"Analyze Accuracy",description:"Compare with expectations",detailedDescription:"Measure judge accuracy by comparing assessments against ground truth expectations in the dataset."},{icon:h.A,title:"Collect Feedback",description:"Gather human assessments",detailedDescription:"Collect expert feedback on judge outputs to identify areas for improvement."},{icon:p.A,title:"Align & Improve",description:"Refine judge accuracy",detailedDescription:"Use SIMBA optimizer to align judges with human feedback, improving accuracy."}],loopBackIcon:g.A,loopBackText:"Continuous Refinement",loopBackDescription:"Iterate to maintain and improve judge accuracy as your application evolves",circleSize:500}),"\n",(0,n.jsx)(t.h2,{id:"why-integrate-judges-with-datasets",children:"Why Integrate Judges with Datasets?"}),"\n",(0,n.jsx)(r.A,{features:[{icon:d.A,title:"Consistent Test Data",description:"Evaluation datasets provide reproducible test cases, ensuring consistent judge performance measurement across iterations."},{icon:u.A,title:"Ground Truth Comparison",description:"Expectations in datasets serve as ground truth, enabling automatic accuracy measurement of judge evaluations."},{icon:p.A,title:"Systematic Improvement",description:"Track judge performance over time, identify weaknesses, and systematically improve through alignment."},{icon:x.A,title:"Version Control",description:"Track datasets for complete evaluation reproducibility."}]}),"\n",(0,n.jsxs)(t.admonition,{title:"SQL Backend Required",type:"important",children:[(0,n.jsx)(t.p,{children:"Evaluation datasets require a SQL-based tracking backend. Set it up before using datasets:"}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'mlflow.set_tracking_uri("sqlite:///mlflow.db")  # Or PostgreSQL/MySQL\n'})})]}),"\n",(0,n.jsx)(t.h2,{id:"complete-example-build--evaluate--improve",children:"Complete Example: Build \u2192 Evaluate \u2192 Improve"}),"\n",(0,n.jsx)(t.p,{children:"Here's how to build a dataset from traces, evaluate your judge, and improve accuracy:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.judges import make_judge\nfrom mlflow.genai.datasets import create_dataset\n\n# Set up MLflow with SQL backend (required for datasets)\nmlflow.set_tracking_uri("sqlite:///mlflow.db")\n\n# Step 1: Build dataset from traces\ndataset = create_dataset(\n    name="judge_accuracy_test",\n    experiment_id="0",\n    tags={"purpose": "judge_validation", "version": "1.0"},\n)\n\n# Get traces from an experiment and add to dataset\ntraces_df = mlflow.search_traces(\n    experiment_ids=["0"], max_results=100\n)  # Returns DataFrame with trace data\n\n# Add traces directly - MLflow extracts inputs automatically\ndataset.merge_records(traces_df)\nprint(f"Added {len(traces_df)} records to evaluation dataset")\n\n# Step 2: Add ground truth expectations for accuracy measurement\nedge_cases = [\n    {\n        "inputs": {"question": ""},  # Empty input\n        "expectations": {"quality": "poor", "reason": "empty_input"},\n    },\n    {\n        "inputs": {"question": "How do I reset my password?"},\n        "expectations": {"quality": "good", "helpful": "yes"},\n    },\n    {\n        "inputs": {"question": "URGENT!!! HELP!!!"},\n        "expectations": {"quality": "poor", "reason": "no_clear_question"},\n    },\n]\ndataset.merge_records(edge_cases)\n\n# Step 3: Create judge and evaluate\nquality_judge = make_judge(\n    name="answer_quality",\n    instructions=(\n        "Evaluate if {{ outputs }} properly addresses {{ inputs }}. "\n        "Rate as \'good\', \'fair\', or \'poor\'."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n\ndef my_app(question):\n    # Your application logic\n    return {"answer": f"Response to: {question}"}\n\n\n# Evaluate judge performance\nresult = mlflow.genai.evaluate(data=dataset, scorers=[quality_judge], predict_fn=my_app)\n\n# Step 4: Iterate and improve\n# - Review results in MLflow UI\n# - Add more test cases based on errors\n# - Collect human feedback on judge outputs\n# - Use alignment to improve judge accuracy\n'})}),"\n",(0,n.jsx)(t.h2,{id:"key-integration-points",children:"Key Integration Points"}),"\n",(0,n.jsx)(t.h3,{id:"dataset-operations",children:"Dataset Operations"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'# Create dataset\ndataset = create_dataset(name="my_dataset", experiment_id="0")\n\n# Add traces\ntraces_df = mlflow.search_traces(experiment_ids=["0"])\ndataset.merge_records(traces_df)\n\n# Add manual test cases\ntest_cases = [\n    {"inputs": {...}, "expectations": {...}},\n    {"inputs": {...}, "expectations": {...}},\n]\ndataset.merge_records(test_cases)\n\n# Access dataset records\ndf = dataset.to_df()\n'})}),"\n",(0,n.jsx)(t.h3,{id:"evaluation-with-datasets",children:"Evaluation with Datasets"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"# Datasets work seamlessly with mlflow.genai.evaluate\nresult = mlflow.genai.evaluate(\n    data=dataset,  # Pass dataset directly\n    scorers=[judge1, judge2],\n    predict_fn=my_app,  # Generate outputs at evaluation time\n)\n"})}),"\n",(0,n.jsx)(t.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Start with Traces"}),": Bootstrap datasets using traces from development or QA testing"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Add Edge Cases"}),": Include problematic inputs to test judge robustness"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Label Strategically"}),": Focus ground truth labels on critical or ambiguous cases"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Iterate Regularly"}),": Continuously expand datasets as your application evolves"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Track Metrics"}),": Log judge accuracy metrics to monitor improvement over time"]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,n.jsxs)(l.A,{children:[(0,n.jsx)(c.A,{icon:p.A,iconSize:48,title:"Judge Alignment",description:"Learn how to align judges with human feedback for improved accuracy.",link:"/genai/eval-monitor/scorers/llm-judge/alignment",linkText:"Explore alignment \u2192",containerHeight:64}),(0,n.jsx)(c.A,{icon:v.A,iconSize:48,title:"Workflow Examples",description:"See complete production patterns for judge development and deployment.",link:"/genai/eval-monitor/scorers/llm-judge/workflow",linkText:"View workflows \u2192",containerHeight:64}),(0,n.jsx)(c.A,{icon:j.A,iconSize:48,title:"Custom LLM Judges",description:"Return to the overview to explore more judge features and capabilities.",link:"/genai/eval-monitor/scorers/llm-judge/",linkText:"Back to overview \u2192",containerHeight:64})]})]})}function A(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(_,{...e})}):_(e)}},80827:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("file-text",[["path",{d:"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z",key:"1rqfz7"}],["path",{d:"M14 2v4a2 2 0 0 0 2 2h4",key:"tnqrlb"}],["path",{d:"M10 9H8",key:"b1mrlr"}],["path",{d:"M16 13H8",key:"t4e002"}],["path",{d:"M16 17H8",key:"z1uh3a"}]])},82238:(e,t,a)=>{a.d(t,{A:()=>i});a(96540);const s={featureHighlights:"featureHighlights_Ardf",highlightItem:"highlightItem_XPnN",highlightIcon:"highlightIcon_SUR8",highlightContent:"highlightContent_d0XP"};var n=a(74848);function i({features:e}){return(0,n.jsx)("div",{className:s.featureHighlights,children:e.map((e,t)=>(0,n.jsxs)("div",{className:s.highlightItem,children:[e.icon&&(0,n.jsx)("div",{className:s.highlightIcon,children:(0,n.jsx)(e.icon,{size:24})}),(0,n.jsxs)("div",{className:s.highlightContent,children:[(0,n.jsx)("h4",{children:e.title}),(0,n.jsx)("p",{children:e.description})]})]},t))})}},84722:(e,t,a)=>{a.d(t,{A:()=>c});var s=a(96540);const n=e=>{const t=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,a)=>a?a.toUpperCase():t.toLowerCase()))(e);return t.charAt(0).toUpperCase()+t.slice(1)},i=(...e)=>e.filter((e,t,a)=>Boolean(e)&&""!==e.trim()&&a.indexOf(e)===t).join(" ").trim(),r=e=>{for(const t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0};var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const l=(0,s.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:a=2,absoluteStrokeWidth:n,className:l="",children:c,iconNode:d,...u},m)=>(0,s.createElement)("svg",{ref:m,...o,width:t,height:t,stroke:e,strokeWidth:n?24*Number(a)/Number(t):a,className:i("lucide",l),...!c&&!r(u)&&{"aria-hidden":"true"},...u},[...d.map(([e,t])=>(0,s.createElement)(e,t)),...Array.isArray(c)?c:[c]])),c=(e,t)=>{const a=(0,s.forwardRef)(({className:a,...r},o)=>{return(0,s.createElement)(l,{ref:o,iconNode:t,className:i(`lucide-${c=n(e),c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,a),...r});var c});return a.displayName=n(e),a}},87073:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("brain",[["path",{d:"M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z",key:"l5xja"}],["path",{d:"M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z",key:"ep3f8r"}],["path",{d:"M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4",key:"1p4c4q"}],["path",{d:"M17.599 6.5a3 3 0 0 0 .399-1.375",key:"tmeiqw"}],["path",{d:"M6.003 5.125A3 3 0 0 0 6.401 6.5",key:"105sqy"}],["path",{d:"M3.477 10.896a4 4 0 0 1 .585-.396",key:"ql3yin"}],["path",{d:"M19.938 10.5a4 4 0 0 1 .585.396",key:"1qfode"}],["path",{d:"M6 18a4 4 0 0 1-1.967-.516",key:"2e4loj"}],["path",{d:"M19.967 17.484A4 4 0 0 1 18 18",key:"159ez6"}]])},93893:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["path",{d:"M16 3.128a4 4 0 0 1 0 7.744",key:"16gr8j"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}]])},96393:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("chart-bar",[["path",{d:"M3 3v16a2 2 0 0 0 2 2h16",key:"c24i48"}],["path",{d:"M7 16h8",key:"srdodz"}],["path",{d:"M7 11h12",key:"127s9w"}],["path",{d:"M7 6h3",key:"w9rmul"}]])},96844:(e,t,a)=>{a.d(t,{A:()=>s});const s=(0,a(84722).A)("activity",[["path",{d:"M22 12h-2.48a2 2 0 0 0-1.93 1.46l-2.35 8.36a.25.25 0 0 1-.48 0L9.24 2.18a.25.25 0 0 0-.48 0l-2.35 8.36A2 2 0 0 1 4.49 12H2",key:"169zse"}]])}}]);