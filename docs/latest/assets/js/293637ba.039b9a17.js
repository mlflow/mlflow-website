"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2730],{10493:(e,a,n)=>{n.d(a,{Zp:()=>i,AC:()=>s,WO:()=>d,_C:()=>c,$3:()=>m,jK:()=>u});var l=n(34164);const t={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var r=n(28774),o=n(74848);const s=({children:e,isSmall:a,cols:n})=>(0,o.jsx)("div",{className:(0,l.A)(t.CardGroup,a?t.AutofillColumns:n?t[`Cols${n}`]:t.MaxThreeColumns),children:e}),i=({children:e,link:a=""})=>a?(0,o.jsx)(r.A,{className:(0,l.A)(t.Link,t.Card,t.CardBordered),to:a,children:e}):(0,o.jsx)("div",{className:(0,l.A)(t.Card,t.CardBordered),children:e}),c=({headerText:e,link:a,text:n})=>(0,o.jsx)(i,{link:a,children:(0,o.jsxs)("span",{children:[(0,o.jsx)("div",{className:(0,l.A)(t.CardTitle,t.BoxRoot,t.PaddingBottom4),style:{pointerEvents:"none"},children:(0,o.jsx)("div",{className:(0,l.A)(t.BoxRoot,t.FlexFlex,t.FlexAlignItemsCenter,t.FlexDirectionRow,t.FlexJustifyContentFlexStart,t.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,o.jsx)("div",{className:(0,l.A)(t.BoxRoot,t.BoxHideIfEmpty,t.MarginTop4,t.MarginLeft4),style:{pointerEvents:"auto"},children:(0,o.jsx)("span",{className:"",children:e})})})}),(0,o.jsx)("span",{className:(0,l.A)(t.TextColor,t.CardBody),children:(0,o.jsx)("p",{children:n})})]})}),d=({description:e,children:a,link:n})=>(0,o.jsx)(i,{link:n,children:(0,o.jsxs)("div",{className:t.LogoCardContent,children:[(0,o.jsx)("div",{className:t.LogoCardImage,children:a}),(0,o.jsx)("p",{className:t.TextColor,children:e})]})}),m=({children:e,link:a})=>(0,o.jsx)(i,{link:a,children:(0,o.jsx)("div",{className:t.SmallLogoCardContent,children:(0,o.jsx)("div",{className:(0,l.A)("max-height-img-container",t.SmallLogoCardImage),children:e})})}),u=({title:e,description:a,link:n=""})=>(0,o.jsx)(i,{link:n,children:(0,o.jsxs)("div",{className:t.TitleCardContent,children:[(0,o.jsx)("div",{className:(0,l.A)(t.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,o.jsx)("hr",{className:(0,l.A)(t.TitleCardSeparator),style:{margin:"12px 0"}}),(0,o.jsx)("p",{className:(0,l.A)(t.TextColor),children:a})]})})},11470:(e,a,n)=>{n.d(a,{A:()=>b});var l=n(96540),t=n(34164),r=n(23104),o=n(56347),s=n(205),i=n(57485),c=n(31682),d=n(70679);function m(e){return l.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:a,children:n}=e;return(0,l.useMemo)((()=>{const e=a??function(e){return m(e).map((({props:{value:e,label:a,attributes:n,default:l}})=>({value:e,label:a,attributes:n,default:l})))}(n);return function(e){const a=(0,c.XI)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,n])}function p({value:e,tabValues:a}){return a.some((a=>a.value===e))}function f({queryString:e=!1,groupId:a}){const n=(0,o.W6)(),t=function({queryString:e=!1,groupId:a}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:e,groupId:a});return[(0,i.aZ)(t),(0,l.useCallback)((e=>{if(!t)return;const a=new URLSearchParams(n.location.search);a.set(t,e),n.replace({...n.location,search:a.toString()})}),[t,n])]}function _(e){const{defaultValue:a,queryString:n=!1,groupId:t}=e,r=u(e),[o,i]=(0,l.useState)((()=>function({defaultValue:e,tabValues:a}){if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:a,tabValues:r}))),[c,m]=f({queryString:n,groupId:t}),[_,h]=function({groupId:e}){const a=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,t]=(0,d.Dv)(a);return[n,(0,l.useCallback)((e=>{a&&t.set(e)}),[a,t])]}({groupId:t}),g=(()=>{const e=c??_;return p({value:e,tabValues:r})?e:null})();(0,s.A)((()=>{g&&i(g)}),[g]);return{selectedValue:o,selectValue:(0,l.useCallback)((e=>{if(!p({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);i(e),m(e),h(e)}),[m,h,r]),tabValues:r}}var h=n(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=n(74848);function w({className:e,block:a,selectedValue:n,selectValue:l,tabValues:o}){const s=[],{blockElementScrollPositionUntilNextRender:i}=(0,r.a_)(),c=e=>{const a=e.currentTarget,t=s.indexOf(a),r=o[t].value;r!==n&&(i(a),l(r))},d=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=s.indexOf(e.currentTarget)+1;a=s[n]??s[0];break}case"ArrowLeft":{const n=s.indexOf(e.currentTarget)-1;a=s[n]??s[s.length-1];break}}a?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":a},e),children:o.map((({value:e,label:a,attributes:l})=>(0,v.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...l,className:(0,t.A)("tabs__item",g.tabItem,l?.className,{"tabs__item--active":n===e}),children:a??e},e)))})}function x({lazy:e,children:a,selectedValue:n}){const r=(Array.isArray(a)?a:[a]).filter(Boolean);if(e){const e=r.find((e=>e.props.value===n));return e?(0,l.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:r.map(((e,a)=>(0,l.cloneElement)(e,{key:a,hidden:e.props.value!==n})))})}function y(e){const a=_(e);return(0,v.jsxs)("div",{className:(0,t.A)("tabs-container",g.tabList),children:[(0,v.jsx)(w,{...a,...e}),(0,v.jsx)(x,{...a,...e})]})}function b(e){const a=(0,h.A)();return(0,v.jsx)(y,{...e,children:m(e.children)},String(a))}},19365:(e,a,n)=>{n.d(a,{A:()=>o});n(96540);var l=n(34164);const t={tabItem:"tabItem_Ymn6"};var r=n(74848);function o({children:e,hidden:a,className:n}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,l.A)(t.tabItem,n),hidden:a,children:e})}},28453:(e,a,n)=>{n.d(a,{R:()=>o,x:()=>s});var l=n(96540);const t={},r=l.createContext(t);function o(e){const a=l.useContext(r);return l.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),l.createElement(r.Provider,{value:a},e.children)}},49374:(e,a,n)=>{n.d(a,{B:()=>s});n(96540);const l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var t=n(86025),r=n(74848);const o=e=>{const a=e.split(".");for(let n=a.length;n>0;n--){const e=a.slice(0,n).join(".");if(l[e])return e}return null};function s({fn:e,children:a,hash:n}){const s=o(e);if(!s)return(0,r.jsx)(r.Fragment,{children:a});const i=(0,t.Ay)(`/${l[s]}#${n??e}`);return(0,r.jsx)("a",{href:i,target:"_blank",children:a??(0,r.jsxs)("code",{children:[e,"()"]})})}},86120:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>m,contentTitle:()=>d,default:()=>f,frontMatter:()=>c,metadata:()=>l,toc:()=>u});const l=JSON.parse('{"id":"evaluation/model-eval","title":"Model Evaluation","description":"This guide covers MLflow\'s core model evaluation capabilities for classification and regression tasks, showing how to comprehensively assess model performance with automated metrics, visualizations, and diagnostic tools.","source":"@site/docs/classic-ml/evaluation/model-eval.mdx","sourceDirName":"evaluation","slug":"/evaluation/model-eval","permalink":"/docs/latest/ml/evaluation/model-eval","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Model Evaluation","sidebar_position":1},"sidebar":"classicMLSidebar","previous":{"title":"Dataset Evaluation","permalink":"/docs/latest/ml/evaluation/dataset-eval"},"next":{"title":"Custom Metrics & Visualizations","permalink":"/docs/latest/ml/evaluation/metrics-visualizations"}}');var t=n(74848),r=n(28453),o=(n(28774),n(10493),n(49374)),s=n(11470),i=n(19365);const c={title:"Model Evaluation",sidebar_position:1},d="Model Evaluation",m={},u=[{value:"Quick Start: Evaluating a Classification Model",id:"quick-start-evaluating-a-classification-model",level:2},{value:"Supported Model Types",id:"supported-model-types",level:2},{value:"Advanced Evaluation Configurations",id:"advanced-evaluation-configurations",level:2},{value:"Specifying Evaluators",id:"specifying-evaluators",level:3},{value:"SHAP Configuration",id:"shap-configuration",level:4},{value:"Performance Options",id:"performance-options",level:4},{value:"Custom Metrics and Artifacts",id:"custom-metrics-and-artifacts",level:2},{value:"Working with Evaluation Results",id:"working-with-evaluation-results",level:2},{value:"Model Comparison and Advanced Workflows",id:"model-comparison-and-advanced-workflows",level:2},{value:"Model Validation and Quality Gates",id:"model-validation-and-quality-gates",level:2},{value:"Error Analysis and Debugging",id:"error-analysis-and-debugging",level:2},{value:"Best Practices and Optimization",id:"best-practices-and-optimization",level:2},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const a={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components},{Details:n}=a;return n||function(e,a){throw new Error("Expected "+(a?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"model-evaluation",children:"Model Evaluation"})}),"\n",(0,t.jsx)(a.p,{children:"This guide covers MLflow's core model evaluation capabilities for classification and regression tasks, showing how to comprehensively assess model performance with automated metrics, visualizations, and diagnostic tools."}),"\n",(0,t.jsx)(a.h2,{id:"quick-start-evaluating-a-classification-model",children:"Quick Start: Evaluating a Classification Model"}),"\n",(0,t.jsx)(a.p,{children:"The simplest way to evaluate a model is with MLflow's unified evaluation API:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'import mlflow\nimport xgboost as xgb\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom mlflow.models import infer_signature\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Train model\nmodel = xgb.XGBClassifier().fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["label"] = y_test\n\nwith mlflow.start_run():\n    # Log model with signature\n    signature = infer_signature(X_test, model.predict(X_test))\n    mlflow.sklearn.log_model(model, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n\n    # Comprehensive evaluation\n    result = mlflow.models.evaluate(\n        model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n        evaluators=["default"],\n    )\n\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\n    print(f"ROC AUC: {result.metrics[\'roc_auc\']:.3f}")\n'})}),"\n",(0,t.jsx)(a.p,{children:"This single call automatically generates:"}),"\n",(0,t.jsxs)("ul",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Performance Metrics"}),": Accuracy, precision, recall, F1-score, ROC-AUC"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Visualizations"}),": Confusion matrix, ROC curve, precision-recall curve"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Feature Importance"}),": SHAP values and feature contribution analysis"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Model Artifacts"}),": All plots and diagnostic information saved to MLflow"]})]}),"\n",(0,t.jsx)(a.h2,{id:"supported-model-types",children:"Supported Model Types"}),"\n",(0,t.jsx)(a.p,{children:"MLflow supports different model types, each with specialized metrics and evaluations:"}),"\n",(0,t.jsxs)("ul",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:(0,t.jsx)("code",{children:"classifier"})})," - Binary and multiclass classification models"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:(0,t.jsx)("code",{children:"regressor"})})," - Regression models for continuous target prediction"]})]}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsxs)(i.A,{value:"classification",label:"Classification",default:!0,children:[(0,t.jsx)(a.p,{children:"For classification tasks, MLflow automatically computes comprehensive metrics:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# Binary Classification\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",  # Automatically detects binary vs multiclass\n    evaluators=["default"],\n)\n\n# Access classification-specific metrics\nmetrics = result.metrics\nprint(f"Precision: {metrics[\'precision_score\']:.3f}")\nprint(f"Recall: {metrics[\'recall_score\']:.3f}")\nprint(f"F1 Score: {metrics[\'f1_score\']:.3f}")\nprint(f"ROC AUC: {metrics[\'roc_auc\']:.3f}")\n'})}),(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Automatic Classification Metrics:"})}),(0,t.jsxs)("ul",{children:[(0,t.jsx)("li",{children:"Accuracy, Precision, Recall, F1-Score"}),(0,t.jsx)("li",{children:"ROC-AUC and Precision-Recall AUC"}),(0,t.jsx)("li",{children:"Log Loss and Brier Score"}),(0,t.jsx)("li",{children:"Confusion Matrix and Classification Report"})]})]}),(0,t.jsxs)(i.A,{value:"regression",label:"Regression",children:[(0,t.jsx)(a.p,{children:"For regression tasks, MLflow provides comprehensive error analysis:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'from sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\n\n# Load regression dataset\nhousing = fetch_california_housing(as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    housing.data, housing.target, test_size=0.2, random_state=42\n)\n\n# Train regression model\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["target"] = y_test\n\nwith mlflow.start_run():\n    # Log and evaluate regression model\n    signature = infer_signature(X_train, reg_model.predict(X_train))\n    mlflow.sklearn.log_model(reg_model, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n\n    result = mlflow.models.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="regressor",\n        evaluators=["default"],\n    )\n\n    print(f"MAE: {result.metrics[\'mean_absolute_error\']:.3f}")\n    print(f"RMSE: {result.metrics[\'root_mean_squared_error\']:.3f}")\n    print(f"R\xb2 Score: {result.metrics[\'r2_score\']:.3f}")\n'})}),(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Automatic Regression Metrics:"})}),(0,t.jsxs)("ul",{children:[(0,t.jsx)("li",{children:"Mean Absolute Error (MAE)"}),(0,t.jsx)("li",{children:"Mean Squared Error (MSE) and Root MSE"}),(0,t.jsx)("li",{children:"R\xb2 Score and Adjusted R\xb2"}),(0,t.jsx)("li",{children:"Mean Absolute Percentage Error (MAPE)"}),(0,t.jsx)("li",{children:"Residual plots and distribution analysis"})]})]})]}),"\n",(0,t.jsx)(a.h2,{id:"advanced-evaluation-configurations",children:"Advanced Evaluation Configurations"}),"\n",(0,t.jsx)(a.h3,{id:"specifying-evaluators",children:"Specifying Evaluators"}),"\n",(0,t.jsx)(a.p,{children:"Control which evaluators run during assessment:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# Run only default metrics (fastest)\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    evaluators=["default"],\n)\n\n# Include SHAP explainer for feature importance\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    evaluators=["default"],\n    evaluator_config={"log_explainer": True},\n)\n'})}),"\n",(0,t.jsxs)(n,{children:[(0,t.jsx)("summary",{children:"Configuration Options Reference"}),(0,t.jsx)(a.h4,{id:"shap-configuration",children:"SHAP Configuration"}),(0,t.jsxs)("ul",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"log_explainer"}),": Whether to log the SHAP explainer as a model"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"explainer_type"}),': Type of SHAP explainer ("exact", "permutation", "partition")']}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"max_error_examples"}),": Maximum number of error examples to analyze"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"log_model_explanations"}),": Whether to log individual prediction explanations"]})]}),(0,t.jsx)(a.h4,{id:"performance-options",children:"Performance Options"}),(0,t.jsxs)("ul",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"pos_label"}),": Positive class label for binary classification metrics"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"average"}),': Averaging strategy for multiclass metrics ("macro", "micro", "weighted")']}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"sample_weights"}),": Sample weights for weighted metrics"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"normalize"}),': Normalization for confusion matrix ("true", "pred", "all")']})]})]}),"\n",(0,t.jsx)(a.h2,{id:"custom-metrics-and-artifacts",children:"Custom Metrics and Artifacts"}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsxs)(i.A,{value:"custom-metrics",label:"Custom Metrics",default:!0,children:[(0,t.jsxs)(a.p,{children:["MLflow provides a powerful framework for defining custom evaluation metrics using the ",(0,t.jsx)(a.code,{children:"make_metric"})," function:"]}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nfrom mlflow.models import make_metric\n\n\ndef weighted_accuracy(predictions, targets, metrics, sample_weights=None):\n    """Custom weighted accuracy metric."""\n    if sample_weights is None:\n        return (predictions == targets).mean()\n    else:\n        correct = predictions == targets\n        return np.average(correct, weights=sample_weights)\n\n\n# Create custom metric\ncustom_accuracy = make_metric(\n    eval_fn=weighted_accuracy, greater_is_better=True, name="weighted_accuracy"\n)\n\n# Use in evaluation\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    extra_metrics=[custom_accuracy],\n)\n'})})]}),(0,t.jsxs)(i.A,{value:"custom-artifacts",label:"Custom Artifacts",children:[(0,t.jsx)(a.p,{children:"Create custom visualization and analysis artifacts:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'import matplotlib.pyplot as plt\nimport os\n\n\ndef create_residual_plot(eval_df, builtin_metrics, artifacts_dir):\n    """Create custom residual plot for regression models."""\n\n    residuals = eval_df["target"] - eval_df["prediction"]\n\n    plt.figure(figsize=(10, 6))\n    plt.scatter(eval_df["prediction"], residuals, alpha=0.6)\n    plt.axhline(y=0, color="r", linestyle="--")\n    plt.xlabel("Predicted Values")\n    plt.ylabel("Residuals")\n    plt.title("Residual Plot")\n\n    plot_path = os.path.join(artifacts_dir, "residual_plot.png")\n    plt.savefig(plot_path)\n    plt.close()\n\n    return {"residual_plot": plot_path}\n\n\n# Use custom artifact\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="target",\n    model_type="regressor",\n    custom_artifacts=[create_residual_plot],\n)\n'})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"working-with-evaluation-results",children:"Working with Evaluation Results"}),"\n",(0,t.jsx)(a.p,{children:"The evaluation result object provides comprehensive access to all generated metrics and artifacts:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# Run evaluation\nresult = mlflow.models.evaluate(\n    model_uri, eval_data, targets="label", model_type="classifier"\n)\n\n# Access metrics\nprint("All Metrics:")\nfor metric_name, value in result.metrics.items():\n    print(f"  {metric_name}: {value}")\n\n# Access artifacts (plots, tables, etc.)\nprint("\\nGenerated Artifacts:")\nfor artifact_name, path in result.artifacts.items():\n    print(f"  {artifact_name}: {path}")\n\n# Access evaluation dataset\neval_table = result.tables["eval_results_table"]\nprint(f"\\nEvaluation table shape: {eval_table.shape}")\nprint(f"Columns: {list(eval_table.columns)}")\n'})}),"\n",(0,t.jsx)(a.h2,{id:"model-comparison-and-advanced-workflows",children:"Model Comparison and Advanced Workflows"}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsxs)(i.A,{value:"model-comparison",label:"Model Comparison",default:!0,children:[(0,t.jsx)(a.p,{children:"Compare multiple models systematically:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# Define models to compare\nmodels = {\n    "random_forest": RandomForestClassifier(n_estimators=100, random_state=42),\n    "logistic_regression": LogisticRegression(random_state=42),\n    "svm": SVC(probability=True, random_state=42),\n}\n\n# Evaluate each model\nresults = {}\n\nfor model_name, model in models.items():\n    with mlflow.start_run(run_name=f"eval_{model_name}"):\n        # Train model\n        model.fit(X_train, y_train)\n\n        # Log model\n        signature = infer_signature(X_train, model.predict(X_train))\n        mlflow.sklearn.log_model(model, name="model", signature=signature)\n        model_uri = mlflow.get_artifact_uri("model")\n\n        # Evaluate model\n        result = mlflow.models.evaluate(\n            model_uri, eval_data, targets="label", model_type="classifier"\n        )\n\n        results[model_name] = result.metrics\n\n        # Log comparison metrics\n        mlflow.log_metrics(\n            {\n                "accuracy": result.metrics["accuracy_score"],\n                "f1": result.metrics["f1_score"],\n                "roc_auc": result.metrics["roc_auc"],\n            }\n        )\n\n# Compare results\ncomparison_df = pd.DataFrame(results).T\nprint("Model Comparison:")\nprint(comparison_df[["accuracy_score", "f1_score", "roc_auc"]].round(3))\n'})})]}),(0,t.jsxs)(i.A,{value:"cross-validation",label:"Cross-Validation",children:[(0,t.jsx)(a.p,{children:"Combine MLflow evaluation with cross-validation:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'from sklearn.model_selection import cross_val_score, StratifiedKFold\n\n\ndef evaluate_with_cv(model, X, y, eval_data, cv_folds=5):\n    """Evaluate model with cross-validation and final test evaluation."""\n\n    with mlflow.start_run():\n        # Cross-validation scores\n        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n        cv_scores = cross_val_score(model, X, y, cv=cv, scoring="f1_weighted")\n\n        # Log CV results\n        mlflow.log_metrics(\n            {"cv_mean_f1": cv_scores.mean(), "cv_std_f1": cv_scores.std()}\n        )\n\n        # Train on full dataset\n        model.fit(X, y)\n\n        # Final evaluation\n        signature = infer_signature(X, model.predict(X))\n        mlflow.sklearn.log_model(model, name="model", signature=signature)\n        model_uri = mlflow.get_artifact_uri("model")\n\n        result = mlflow.models.evaluate(\n            model_uri, eval_data, targets="label", model_type="classifier"\n        )\n\n        # Compare CV and test performance\n        test_f1 = result.metrics["f1_score"]\n        cv_f1 = cv_scores.mean()\n\n        mlflow.log_metrics(\n            {\n                "cv_vs_test_diff": abs(cv_f1 - test_f1),\n                "potential_overfit": cv_f1 - test_f1 > 0.05,\n            }\n        )\n\n        return result\n\n\n# Usage\nresult = evaluate_with_cv(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    X_train,\n    y_train,\n    eval_data,\n)\n'})})]}),(0,t.jsxs)(i.A,{value:"automated-selection",label:"Automated Selection",children:[(0,t.jsx)(a.p,{children:"Automated model selection based on evaluation metrics:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'def evaluate_and_select_best_model(\n    models, X_train, y_train, eval_data, metric="f1_score"\n):\n    """Evaluate multiple models and select the best performer."""\n\n    results = {}\n    best_score = -1\n    best_model_name = None\n\n    for model_name, model in models.items():\n        with mlflow.start_run(run_name=f"candidate_{model_name}"):\n            # Train and evaluate\n            model.fit(X_train, y_train)\n\n            signature = infer_signature(X_train, model.predict(X_train))\n            mlflow.sklearn.log_model(model, name="model", signature=signature)\n            model_uri = mlflow.get_artifact_uri("model")\n\n            result = mlflow.models.evaluate(\n                model_uri, eval_data, targets="label", model_type="classifier"\n            )\n\n            score = result.metrics[metric]\n            results[model_name] = score\n\n            # Track best model\n            if score > best_score:\n                best_score = score\n                best_model_name = model_name\n\n            # Log selection metrics\n            mlflow.log_metrics(\n                {"selection_score": score, "is_best": score == best_score}\n            )\n\n    print(f"Best model: {best_model_name} (Score: {best_score:.3f})")\n    return best_model_name, results\n\n\n# Use automated selection\nbest_model, all_scores = evaluate_and_select_best_model(\n    models, X_train, y_train, eval_data, metric="f1_score"\n)\n'})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"model-validation-and-quality-gates",children:"Model Validation and Quality Gates"}),"\n",(0,t.jsx)(a.admonition,{title:"attention",type:"warning",children:(0,t.jsxs)(a.p,{children:["MLflow 2.18.0 has moved the model validation functionality from the ",(0,t.jsx)(o.B,{fn:"mlflow.models.evaluate"})," API\nto a dedicated ",(0,t.jsx)(o.B,{fn:"mlflow.validate_evaluation_results"})," API. The relevant parameters, such as baseline_model,\nare deprecated and will be removed from the older API in future versions."]})}),"\n",(0,t.jsxs)(a.p,{children:["With the ",(0,t.jsx)(o.B,{fn:"mlflow.validate_evaluation_results"})," API, you can validate metrics generated during model evaluation\nto assess the quality of your model against a baseline."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'from mlflow.models import MetricThreshold\n\n# Evaluate your model first\nresult = mlflow.models.evaluate(\n    model_uri, eval_data, targets="label", model_type="classifier"\n)\n\n# Define static performance thresholds\nstatic_thresholds = {\n    "accuracy_score": MetricThreshold(\n        threshold=0.85, greater_is_better=True  # Must achieve 85% accuracy\n    ),\n    "precision_score": MetricThreshold(\n        threshold=0.80, greater_is_better=True  # Must achieve 80% precision\n    ),\n    "recall_score": MetricThreshold(\n        threshold=0.75, greater_is_better=True  # Must achieve 75% recall\n    ),\n}\n\n# Validate against static thresholds\ntry:\n    mlflow.validate_evaluation_results(\n        candidate_result=result,\n        baseline_result=None,  # No baseline comparison\n        validation_thresholds=static_thresholds,\n    )\n    print("\u2705 Model meets all static performance thresholds.")\nexcept mlflow.exceptions.ModelValidationFailedException as e:\n    print(f"\u274c Model failed static validation: {e}")\n'})}),"\n",(0,t.jsxs)(a.p,{children:["More information on model validation behavior and outputs can be found in the ",(0,t.jsx)(o.B,{fn:"mlflow.validate_evaluation_results"})," API documentation."]}),"\n",(0,t.jsx)(a.h2,{id:"error-analysis-and-debugging",children:"Error Analysis and Debugging"}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsxs)(i.A,{value:"error-investigation",label:"Error Investigation",default:!0,children:[(0,t.jsx)(a.p,{children:"Analyze model errors in detail:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'def analyze_model_errors(result, eval_data, targets, top_n=20):\n    """Analyze model errors in detail."""\n\n    # Load evaluation results\n    eval_table = result.tables["eval_results_table"]\n\n    # Identify errors\n    errors = eval_table[eval_table["prediction"] != eval_table[targets]]\n\n    if len(errors) > 0:\n        print(f"Total errors: {len(errors)} out of {len(eval_table)} predictions")\n        print(f"Error rate: {len(errors) / len(eval_table) * 100:.2f}%")\n\n        # Most confident wrong predictions\n        if "prediction_score" in errors.columns:\n            confident_errors = errors.nlargest(top_n, "prediction_score")\n            print(f"\\nTop {top_n} most confident errors:")\n            print(confident_errors[["prediction", targets, "prediction_score"]].head())\n\n        # Error patterns by true class\n        error_by_class = errors.groupby(targets).size()\n        print(f"\\nErrors by true class:")\n        print(error_by_class)\n\n    return errors\n\n\n# Usage\nerrors = analyze_model_errors(result, eval_data, "label")\n'})})]}),(0,t.jsxs)(i.A,{value:"feature-analysis",label:"Feature Analysis",children:[(0,t.jsx)(a.p,{children:"Analyze how model errors relate to input features:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'def analyze_errors_by_features(model_uri, eval_data, targets, feature_columns):\n    """Analyze how model errors relate to input features."""\n\n    # Get model predictions\n    model = mlflow.pyfunc.load_model(model_uri)\n    predictions = model.predict(eval_data[feature_columns])\n\n    # Create analysis dataframe\n    analysis_df = eval_data.copy()\n    analysis_df["prediction"] = predictions\n    analysis_df["is_error"] = analysis_df["prediction"] != analysis_df[targets]\n\n    # Feature statistics for errors vs correct predictions\n    feature_stats = {}\n\n    for feature in feature_columns:\n        if analysis_df[feature].dtype in ["int64", "float64"]:\n            # Numerical features\n            correct_mean = analysis_df[~analysis_df["is_error"]][feature].mean()\n            error_mean = analysis_df[analysis_df["is_error"]][feature].mean()\n\n            feature_stats[feature] = {\n                "correct_mean": correct_mean,\n                "error_mean": error_mean,\n                "difference": abs(error_mean - correct_mean),\n                "relative_difference": abs(error_mean - correct_mean) / correct_mean\n                if correct_mean != 0\n                else 0,\n            }\n\n    # Sort features by impact on errors\n    numerical_features = [\n        (k, v["relative_difference"])\n        for k, v in feature_stats.items()\n        if "relative_difference" in v\n    ]\n    numerical_features.sort(key=lambda x: x[1], reverse=True)\n\n    print("Features most associated with errors:")\n    for feature, diff in numerical_features[:5]:\n        print(f"  {feature}: {diff:.3f}")\n\n    return feature_stats, analysis_df\n\n\n# Usage\nfeature_stats, analysis = analyze_errors_by_features(\n    model_uri,\n    eval_data,\n    "label",\n    feature_columns=eval_data.drop(columns=["label"]).columns.tolist(),\n)\n'})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"best-practices-and-optimization",children:"Best Practices and Optimization"}),"\n",(0,t.jsxs)(s.A,{children:[(0,t.jsxs)(i.A,{value:"best-practices",label:"Best Practices",default:!0,children:[(0,t.jsx)(a.p,{children:"Complete evaluation workflow with best practices:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'def comprehensive_model_evaluation(\n    model, X_train, y_train, eval_data, targets, model_type\n):\n    """Complete evaluation workflow with best practices."""\n\n    with mlflow.start_run():\n        # Train model\n        model.fit(X_train, y_train)\n\n        # Log training info\n        mlflow.log_params(\n            {"model_class": model.__class__.__name__, "training_samples": len(X_train)}\n        )\n\n        # Log model with signature\n        signature = infer_signature(X_train, model.predict(X_train))\n        mlflow.sklearn.log_model(model, name="model", signature=signature)\n        model_uri = mlflow.get_artifact_uri("model")\n\n        # Comprehensive evaluation\n        result = mlflow.models.evaluate(\n            model_uri,\n            eval_data,\n            targets=targets,\n            model_type=model_type,\n            evaluators=["default"],\n            evaluator_config={\n                "log_explainer": True,\n                "explainer_type": "exact",\n                "log_model_explanations": True,\n            },\n        )\n\n        return result\n'})})]}),(0,t.jsxs)(i.A,{value:"performance-optimization",label:"Performance Optimization",children:[(0,t.jsx)(a.p,{children:"Optimize evaluation for large datasets and complex models:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# Optimize evaluation performance\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data.sample(n=10000, random_state=42),  # Sample for faster evaluation\n    targets="label",\n    model_type="classifier",\n    evaluators=["default"],\n    evaluator_config={\n        "log_explainer": False,  # Skip SHAP for speed\n        "max_error_examples": 50,  # Reduce error analysis\n    },\n)\n\n\n# For very large datasets - evaluate in batches\ndef evaluate_in_batches(model_uri, large_eval_data, targets, batch_size=1000):\n    """Evaluate large datasets in batches to manage memory."""\n\n    all_predictions = []\n    all_targets = []\n\n    for i in range(0, len(large_eval_data), batch_size):\n        batch = large_eval_data.iloc[i : i + batch_size]\n\n        # Get predictions for batch\n        model = mlflow.pyfunc.load_model(model_uri)\n        batch_predictions = model.predict(batch.drop(columns=[targets]))\n\n        all_predictions.extend(batch_predictions)\n        all_targets.extend(batch[targets].values)\n\n    # Create final evaluation dataset\n    final_eval_data = pd.DataFrame(\n        {"prediction": all_predictions, "target": all_targets}\n    )\n\n    # Evaluate using static dataset approach\n    result = mlflow.models.evaluate(\n        data=final_eval_data,\n        predictions="prediction",\n        targets="target",\n        model_type="classifier",\n    )\n\n    return result\n'})})]}),(0,t.jsxs)(i.A,{value:"reproducible-evaluation",label:"Reproducible Evaluation",children:[(0,t.jsx)(a.p,{children:"Ensure consistent evaluation results:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'def reproducible_evaluation(model, eval_data, targets, random_seed=42):\n    """Ensure reproducible evaluation results."""\n\n    # Set random seeds\n    np.random.seed(random_seed)\n\n    with mlflow.start_run():\n        # Log evaluation configuration\n        mlflow.log_params(\n            {\n                "eval_random_seed": random_seed,\n                "eval_data_size": len(eval_data),\n                "eval_timestamp": pd.Timestamp.now().isoformat(),\n            }\n        )\n\n        # Consistent data ordering\n        eval_data_sorted = eval_data.sort_values(\n            by=eval_data.columns.tolist()\n        ).reset_index(drop=True)\n\n        # Run evaluation\n        result = mlflow.models.evaluate(\n            model,\n            eval_data_sorted,\n            targets=targets,\n            model_type="classifier",\n            evaluator_config={"random_seed": random_seed},\n        )\n\n        return result\n'})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(a.p,{children:"MLflow's model evaluation capabilities provide a comprehensive framework for assessing model performance across classification and regression tasks. The unified API simplifies complex evaluation workflows while providing deep insights into model behavior through automated metrics, visualizations, and diagnostic tools."}),"\n",(0,t.jsx)(a.p,{children:"Key benefits of MLflow model evaluation include:"}),"\n",(0,t.jsxs)("ul",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Comprehensive Assessment"}),": Automated generation of task-specific metrics and visualizations"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Reproducible Workflows"}),": Consistent evaluation processes with complete tracking and versioning"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Advanced Analysis"}),": Error investigation, feature impact analysis, and model comparison capabilities"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Production Integration"}),": Seamless integration with MLflow tracking for experiment organization and reporting"]})]}),"\n",(0,t.jsx)(a.p,{children:"Whether you're evaluating a single model or comparing multiple candidates, MLflow's evaluation framework provides the tools needed to make informed decisions about model performance and production readiness."})]})}function f(e={}){const{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);