"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9376],{16734:(e,n,i)=>{i.d(n,{d:()=>s});var t=i(58069);const o="codeBlock_oJcR";var r=i(74848);const s=e=>{let{children:n,executionCount:i}=e;return(0,r.jsx)("div",{style:{flexGrow:1,minWidth:0,marginTop:"var(--padding-md)",width:"100%"},children:(0,r.jsx)(t.A,{className:o,language:"python",children:n})})}},20723:(e,n,i)=>{i.d(n,{O:()=>r});var t=i(96540),o=i(74848);function r(e){let{children:n,href:i}=e;const r=(0,t.useCallback)((async e=>{if(e.preventDefault(),window.gtag)try{window.gtag("event","notebook-download",{href:i})}catch{}const n=await fetch(i),t=await n.blob(),o=window.URL.createObjectURL(t),r=document.createElement("a");r.style.display="none",r.href=o;const s=i.split("/").pop();r.download=s,document.body.appendChild(r),r.click(),window.URL.revokeObjectURL(o),document.body.removeChild(r)}),[i]);return(0,o.jsx)("a",{className:"button button--primary",style:{marginBottom:"1rem",display:"block",width:"min-content"},href:i,download:!0,onClick:r,children:n})}},61536:(e,n,i)=>{i.d(n,{p:()=>o});var t=i(74848);const o=e=>{let{children:n,isStderr:i}=e;return(0,t.jsx)("pre",{style:{margin:0,borderRadius:0,background:"none",fontSize:"0.85rem",flexGrow:1,padding:"var(--padding-sm)"},children:n})}},86563:(e,n,i)=>{i.d(n,{Q:()=>o});var t=i(74848);const o=e=>{let{children:n}=e;return(0,t.jsx)("div",{style:{flexGrow:1,minWidth:0,fontSize:"0.8rem",width:"100%"},children:n})}},88291:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>f,frontMatter:()=>d,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"llms/transformers/tutorials/audio-transcription/whisper-ipynb","title":"Introduction to MLflow and OpenAI\'s Whisper","description":"Download this notebook","source":"@site/docs/llms/transformers/tutorials/audio-transcription/whisper-ipynb.mdx","sourceDirName":"llms/transformers/tutorials/audio-transcription","slug":"/llms/transformers/tutorials/audio-transcription/whisper","permalink":"/docs/latest/llms/transformers/tutorials/audio-transcription/whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb","tags":[],"version":"current","frontMatter":{"custom_edit_url":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb","slug":"whisper"},"sidebar":"docsSidebar","previous":{"title":"Tutorials","permalink":"/docs/latest/llms/transformers/tutorials/"},"next":{"title":"Introduction to Conversational AI with MLflow and DialoGPT","permalink":"/docs/latest/llms/transformers/tutorials/conversational/conversational-model"}}');var o=i(74848),r=i(28453),s=i(16734),a=i(61536),l=(i(86563),i(20723));const d={custom_edit_url:"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb",slug:"whisper"},c="Introduction to MLflow and OpenAI's Whisper",h={},p=[{value:"What You Will Learn in This Tutorial",id:"what-you-will-learn-in-this-tutorial",level:3},{value:"What is Whisper?",id:"what-is-whisper",level:4},{value:"Why MLflow with Whisper?",id:"why-mlflow-with-whisper",level:4},{value:"Setting Up the Environment and Acquiring Audio Data",id:"setting-up-the-environment-and-acquiring-audio-data",level:3},{value:"Audio Acquisition",id:"audio-acquisition",level:4},{value:"Model and Pipeline Initialization",id:"model-and-pipeline-initialization",level:4},{value:"MLflow Environment Setup",id:"mlflow-environment-setup",level:4},{value:"Formatting the Transcription Output",id:"formatting-the-transcription-output",level:3},{value:"Executing the Transcription Pipeline",id:"executing-the-transcription-pipeline",level:3},{value:"Transcription Process",id:"transcription-process",level:4},{value:"Importance of Pre-Save Testing",id:"importance-of-pre-save-testing",level:4},{value:"Model Signature and Configuration",id:"model-signature-and-configuration",level:3},{value:"Handling Different Audio Formats",id:"handling-different-audio-formats",level:4},{value:"Model Configuration",id:"model-configuration",level:4},{value:"Creating an experiment",id:"creating-an-experiment",level:3},{value:"Logging the Model with MLflow",id:"logging-the-model-with-mlflow",level:3},{value:"Key Components of Model Logging",id:"key-components-of-model-logging",level:4},{value:"Using MLflow&#39;s <code>log_model</code> Function",id:"using-mlflows-log_model-function",level:4},{value:"Loading and Using the Model Pipeline",id:"loading-and-using-the-model-pipeline",level:3},{value:"Loading the Model",id:"loading-the-model",level:4},{value:"Using the Loaded Model",id:"using-the-loaded-model",level:4},{value:"Using the Pyfunc Flavor for Inference",id:"using-the-pyfunc-flavor-for-inference",level:3},{value:"Loading and Predicting with Pyfunc",id:"loading-and-predicting-with-pyfunc",level:4},{value:"Output Format Considerations",id:"output-format-considerations",level:4},{value:"Tutorial Roundup",id:"tutorial-roundup",level:3}];function u(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"introduction-to-mlflow-and-openais-whisper",children:"Introduction to MLflow and OpenAI's Whisper"})}),"\n",(0,o.jsx)(l.O,{href:"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb",children:"Download this notebook"}),"\n",(0,o.jsxs)(n.p,{children:["Discover the integration of ",(0,o.jsx)(n.a,{href:"https://huggingface.co/openai",children:"OpenAI's Whisper"}),", an ",(0,o.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Speech_recognition",children:"ASR system"}),", with MLflow in this tutorial."]}),"\n",(0,o.jsx)(n.h3,{id:"what-you-will-learn-in-this-tutorial",children:"What You Will Learn in This Tutorial"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Establish an audio transcription ",(0,o.jsx)(n.strong,{children:"pipeline"})," using the Whisper model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Log"})," and manage Whisper models with MLflow."]}),"\n",(0,o.jsxs)(n.li,{children:["Infer and understand Whisper model ",(0,o.jsx)(n.strong,{children:"signatures"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Load"})," and interact with Whisper models stored in MLflow."]}),"\n",(0,o.jsxs)(n.li,{children:["Utilize MLflow's ",(0,o.jsx)(n.strong,{children:"pyfunc"})," for Whisper model serving and transcription tasks."]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"what-is-whisper",children:"What is Whisper?"}),"\n",(0,o.jsx)(n.p,{children:"Whisper, developed by OpenAI, is a versatile ASR model trained for high-accuracy speech-to-text conversion. It stands out due to its training on diverse accents and environments, available via the Transformers library for easy use."}),"\n",(0,o.jsx)(n.h4,{id:"why-mlflow-with-whisper",children:"Why MLflow with Whisper?"}),"\n",(0,o.jsx)(n.p,{children:"Integrating MLflow with Whisper enhances ASR model management:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Experiment Tracking"}),": Facilitates tracking of model configurations and performance for optimal results."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Management"}),": Centralizes different versions of Whisper models, enhancing organization and accessibility."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reproducibility"}),": Ensures consistency in transcriptions by tracking all components required for reproducing model behavior."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deployment"}),": Streamlines the deployment of Whisper models in various production settings, ensuring efficient application."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Interested in learning more about Whisper? To read more about the significant breakthroughs in transcription capabilities that Whisper brought to the field of ASR, you can ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"read the white paper"})," and see more about the active development and ",(0,o.jsx)(n.a,{href:"https://openai.com/research/whisper",children:"read more about the progress"})," at OpenAI's research website."]}),"\n",(0,o.jsx)(n.p,{children:"Ready to enhance your speech-to-text capabilities? Let's explore automatic speech recognition using MLflow and Whisper!"}),"\n",(0,o.jsx)(s.d,{executionCount:1,children:'# Disable tokenizers warnings when constructing pipelines\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings("ignore", category=UserWarning)'}),"\n",(0,o.jsx)(a.p,{children:"env: TOKENIZERS_PARALLELISM=false"}),"\n",(0,o.jsx)(n.h3,{id:"setting-up-the-environment-and-acquiring-audio-data",children:"Setting Up the Environment and Acquiring Audio Data"}),"\n",(0,o.jsxs)(n.p,{children:["Initial steps for transcription using ",(0,o.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"Whisper"}),": acquiring ",(0,o.jsx)(n.a,{href:"https://www.nasa.gov/audio-and-ringtones/",children:"audio"})," and setting up MLflow."]}),"\n",(0,o.jsx)(n.p,{children:"Before diving into the audio transcription process with OpenAI's Whisper, there are a few preparatory steps to ensure everything is in place for a smooth and effective transcription experience."}),"\n",(0,o.jsx)(n.h4,{id:"audio-acquisition",children:"Audio Acquisition"}),"\n",(0,o.jsx)(n.p,{children:"The first step is to acquire an audio file to work with. For this tutorial, we use a publicly available audio file from NASA. This sample audio provides a practical example to demonstrate Whisper's transcription capabilities."}),"\n",(0,o.jsx)(n.h4,{id:"model-and-pipeline-initialization",children:"Model and Pipeline Initialization"}),"\n",(0,o.jsx)(n.p,{children:"We load the Whisper model, along with its tokenizer and feature extractor, from the Transformers library. These components are essential for processing the audio data and converting it into a format that the Whisper model can understand and transcribe.\nNext, we create a transcription pipeline using the Whisper model. This pipeline simplifies the process of feeding audio data into the model and obtaining the transcription."}),"\n",(0,o.jsx)(n.h4,{id:"mlflow-environment-setup",children:"MLflow Environment Setup"}),"\n",(0,o.jsx)(n.p,{children:"In addition to the model and audio data setup, we initialize our MLflow environment. MLflow is used to track and manage our experiments, offering an organized way to document the transcription process and results."}),"\n",(0,o.jsx)(n.p,{children:"The following code block covers these initial setup steps, providing the foundation for our audio transcription task with the Whisper model."}),"\n",(0,o.jsx)(s.d,{executionCount:2,children:'import requests\nimport transformers\n\nimport mlflow\n\n# Acquire an audio file that is in the public domain\nresp = requests.get(\n  "https://www.nasa.gov/wp-content/uploads/2015/01/590325main_ringtone_kennedy_WeChoose.mp3"\n)\nresp.raise_for_status()\naudio = resp.content\n\n# Set the task that our pipeline implementation will be using\ntask = "automatic-speech-recognition"\n\n# Define the model instance\narchitecture = "openai/whisper-large-v3"\n\n# Load the components and necessary configuration for Whisper ASR from the Hugging Face Hub\nmodel = transformers.WhisperForConditionalGeneration.from_pretrained(architecture)\ntokenizer = transformers.WhisperTokenizer.from_pretrained(architecture)\nfeature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(architecture)\nmodel.generation_config.alignment_heads = [[2, 2], [3, 0], [3, 2], [3, 3], [3, 4], [3, 5]]\n\n# Instantiate our pipeline for ASR using the Whisper model\naudio_transcription_pipeline = transformers.pipeline(\n  task=task, model=model, tokenizer=tokenizer, feature_extractor=feature_extractor\n)'}),"\n",(0,o.jsx)(a.p,{isStderr:!0,children:"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained."}),"\n",(0,o.jsx)(n.h3,{id:"formatting-the-transcription-output",children:"Formatting the Transcription Output"}),"\n",(0,o.jsx)(n.p,{children:"In this section, we introduce a utility function that is used solely for the purpose of enhancing the readability of the transcription output within this Jupyter notebook demo. It is important to note that this function is designed for demonstration purposes and should not be included in production code or used for any other purpose beyond this tutorial."}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"format_transcription"})," function takes a long string of transcribed text and formats it by splitting it into sentences and inserting newline characters. This makes the output easier to read when printed in the notebook environment."]}),"\n",(0,o.jsx)(s.d,{executionCount:3,children:'def format_transcription(transcription):\n  """\n  Function for formatting a long string by splitting into sentences and adding newlines.\n  """\n  # Split the transcription into sentences, ensuring we don\'t split on abbreviations or initials\n  sentences = [\n      sentence.strip() + ("." if not sentence.endswith(".") else "")\n      for sentence in transcription.split(". ")\n      if sentence\n  ]\n\n  # Join the sentences with a newline character\n  return "\n".join(sentences)'}),"\n",(0,o.jsx)(n.h3,{id:"executing-the-transcription-pipeline",children:"Executing the Transcription Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Perform audio transcription using the Whisper pipeline and review the output."}),"\n",(0,o.jsx)(n.p,{children:"After setting up the Whisper model and audio transcription pipeline, our next step is to process an audio file to extract its transcription. This part of the tutorial is crucial as it demonstrates the practical application of the Whisper model in converting spoken language into written text."}),"\n",(0,o.jsx)(n.h4,{id:"transcription-process",children:"Transcription Process"}),"\n",(0,o.jsxs)(n.p,{children:["The code block below feeds an audio file into the pipeline, which then produces the transcription. The ",(0,o.jsx)(n.code,{children:"format_transcription"})," function, defined earlier, enhances readability by formatting the output with sentence splits and newline characters."]}),"\n",(0,o.jsx)(n.h4,{id:"importance-of-pre-save-testing",children:"Importance of Pre-Save Testing"}),"\n",(0,o.jsx)(n.p,{children:"Testing the transcription pipeline before saving the model in MLflow is vital. This step verifies that the model works as expected, ensuring accuracy and reliability. Such validation avoids issues post-deployment and confirms that the model performs consistently with the training data it was exposed to. It also provides a benchmark to compare against the output after the model is loaded back from MLflow, ensuring consistency in performance."}),"\n",(0,o.jsx)(n.p,{children:"Execute the following code to transcribe the audio and assess the quality and accuracy of the transcription provided by the Whisper model."}),"\n",(0,o.jsx)(s.d,{executionCount:4,children:'# Verify that our pipeline is capable of processing an audio file and transcribing it\ntranscription = audio_transcription_pipeline(audio)\n\nprint(format_transcription(transcription["text"]))'}),"\n",(0,o.jsx)(a.p,{children:"We choose to go to the moon in this decade and do the other things.\nNot because they are easy, but because they are hard.\n3, 2, 1, 0.\nAll engines running.\nLiftoff.\nWe have a liftoff.\n32 minutes past the hour.\nLiftoff on Apollo 11."}),"\n",(0,o.jsx)(n.h3,{id:"model-signature-and-configuration",children:"Model Signature and Configuration"}),"\n",(0,o.jsx)(n.p,{children:"Generate a model signature for Whisper to understand its input and output data requirements."}),"\n",(0,o.jsx)(n.p,{children:"The model signature is critical for defining the schema for the Whisper model's inputs and outputs, clarifying the data types and structures expected. This step ensures the model processes inputs correctly and outputs structured data."}),"\n",(0,o.jsx)(n.h4,{id:"handling-different-audio-formats",children:"Handling Different Audio Formats"}),"\n",(0,o.jsxs)(n.p,{children:["While the default signature covers binary audio data, the ",(0,o.jsx)(n.code,{children:"transformers"})," flavor accommodates multiple formats, including numpy arrays and URL-based inputs. This flexibility allows Whisper to transcribe from various sources, although URL-based transcription isn't demonstrated here."]}),"\n",(0,o.jsx)(n.h4,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,o.jsxs)(n.p,{children:["Setting the model configuration involves parameters like ",(0,o.jsx)(n.em,{children:"chunk"})," and ",(0,o.jsx)(n.em,{children:"stride"})," lengths for audio processing. These settings are adjustable to suit different transcription needs, enhancing Whisper's performance for specific scenarios."]}),"\n",(0,o.jsx)(n.p,{children:"Run the next code block to infer the model's signature and configure key parameters, aligning Whisper's functionality with your project's requirements."}),"\n",(0,o.jsx)(s.d,{executionCount:5,children:'# Specify parameters and their defaults that we would like to be exposed for manipulation during inference time\nmodel_config = {\n  "chunk_length_s": 20,\n  "stride_length_s": [5, 3],\n}\n\n# Define the model signature by using the input and output of our pipeline, as well as specifying our inference parameters that will allow for those parameters to\n# be overridden at inference time.\nsignature = mlflow.models.infer_signature(\n  audio,\n  mlflow.transformers.generate_signature_output(audio_transcription_pipeline, audio),\n  params=model_config,\n)\n\n# Visualize the signature\nsignature'}),"\n",(0,o.jsx)(a.p,{children:"inputs: \n[binary]\noutputs: \n[string]\nparams: \n['chunk_length_s': long (default: 20), 'stride_length_s': long (default: [5, 3]) (shape: (-1,))]"}),"\n",(0,o.jsx)(n.h3,{id:"creating-an-experiment",children:"Creating an experiment"}),"\n",(0,o.jsx)(n.p,{children:"We create a new MLflow Experiment so that the run we're going to log our model to does not log to the default experiment and instead has its own contextually relevant entry."}),"\n",(0,o.jsx)(s.d,{executionCount:6,children:'# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri("http://127.0.0.1:8080")\n\nmlflow.set_experiment("Whisper Transcription ASR")'}),"\n",(0,o.jsx)(a.p,{children:"<Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/audio-transcription/mlruns/864092483920291025', creation_time=1701294423466, experiment_id='864092483920291025', last_update_time=1701294423466, lifecycle_stage='active', name='Whisper Transcription ASR', tags={}>"}),"\n",(0,o.jsx)(n.h3,{id:"logging-the-model-with-mlflow",children:"Logging the Model with MLflow"}),"\n",(0,o.jsx)(n.p,{children:"Learn how to log the Whisper model and its configurations with MLflow."}),"\n",(0,o.jsx)(n.p,{children:"Logging the Whisper model in MLflow is a critical step for capturing essential information for model reproduction, sharing, and deployment. This process involves:"}),"\n",(0,o.jsx)(n.h4,{id:"key-components-of-model-logging",children:"Key Components of Model Logging"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Information"}),": Includes the model, its signature, and an input example."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Configuration"}),": Any specific parameters set for the model, like ",(0,o.jsx)(n.em,{children:"chunk length"})," or ",(0,o.jsx)(n.em,{children:"stride length"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"using-mlflows-log_model-function",children:["Using MLflow's ",(0,o.jsx)(n.code,{children:"log_model"})," Function"]}),"\n",(0,o.jsx)(n.p,{children:"This function is utilized within an MLflow run to log the model and its configurations. It ensures that all necessary components for model usage are recorded."}),"\n",(0,o.jsx)(n.p,{children:"Executing the code in the next cell will log the Whisper model in the current MLflow experiment. This includes storing the model in a specified artifact path and documenting the default configurations that will be applied during inference."}),"\n",(0,o.jsx)(s.d,{executionCount:7,children:"# Log the pipeline\nwith mlflow.start_run():\n  model_info = mlflow.transformers.log_model(\n      transformers_model=audio_transcription_pipeline,\n      name=\"whisper_transcriber\",\n      signature=signature,\n      input_example=audio,\n      model_config=model_config,\n      # Since MLflow 2.11.0, you can save the model in 'reference-only' mode to reduce storage usage by not saving\n      # the base model weights but only the reference to the HuggingFace model hub. To enable this, uncomment the\n      # following line:\n      # save_pretrained=False,\n  )"}),"\n",(0,o.jsx)(n.h3,{id:"loading-and-using-the-model-pipeline",children:"Loading and Using the Model Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Explore how to load and use the Whisper model pipeline from MLflow."}),"\n",(0,o.jsx)(n.p,{children:"After logging the Whisper model in MLflow, the next crucial step is to load and use it for inference. This process ensures that our logged model operates as intended and can be effectively used for tasks like audio transcription."}),"\n",(0,o.jsx)(n.h4,{id:"loading-the-model",children:"Loading the Model"}),"\n",(0,o.jsxs)(n.p,{children:["The model is loaded in its native format using MLflow's ",(0,o.jsx)(n.code,{children:"load_model"})," function. This step verifies that the model can be retrieved and used seamlessly after being logged in MLflow."]}),"\n",(0,o.jsx)(n.h4,{id:"using-the-loaded-model",children:"Using the Loaded Model"}),"\n",(0,o.jsx)(n.p,{children:"Once loaded, the model is ready for inference. We demonstrate this by passing an MP3 audio file to the model and obtaining its transcription. This test is a practical demonstration of the model's capabilities post-logging."}),"\n",(0,o.jsx)(n.p,{children:"This step is a form of validation before moving to more complex deployment scenarios. Ensuring that the model functions correctly in its native format helps in troubleshooting and streamlines the deployment process, especially for large and complex models like Whisper."}),"\n",(0,o.jsx)(s.d,{executionCount:8,children:"# Load the pipeline in its native format\nloaded_transcriber = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n\n# Perform transcription with the native pipeline implementation\ntranscription = loaded_transcriber(audio)\n\nprint(f\"\nWhisper native output transcription:\n{format_transcription(transcription['text'])}\")"}),"\n",(0,o.jsx)(a.p,{isStderr:!0,children:"2023/11/30 12:51:43 INFO mlflow.transformers: 'runs:/f7503a09d20f4fb481544968b5ed28dd/whisper_transcriber' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/audio-transcription/mlruns/864092483920291025/f7503a09d20f4fb481544968b5ed28dd/artifacts/whisper_transcriber'"}),"\n",(0,o.jsx)(a.p,{children:"Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"}),"\n",(0,o.jsx)(a.p,{isStderr:!0,children:"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained."}),"\n",(0,o.jsx)(a.p,{children:"\nWhisper native output transcription:\nWe choose to go to the moon in this decade and do the other things.\nNot because they are easy, but because they are hard.\n3, 2, 1, 0.\nAll engines running.\nLiftoff.\nWe have a liftoff.\n32 minutes past the hour.\nLiftoff on Apollo 11."}),"\n",(0,o.jsx)(n.h3,{id:"using-the-pyfunc-flavor-for-inference",children:"Using the Pyfunc Flavor for Inference"}),"\n",(0,o.jsxs)(n.p,{children:["Learn how MLflow's ",(0,o.jsx)(n.code,{children:"pyfunc"})," flavor facilitates flexible model deployment."]}),"\n",(0,o.jsxs)(n.p,{children:["MLflow's ",(0,o.jsx)(n.code,{children:"pyfunc"})," flavor provides a generic interface for model inference, offering flexibility across various machine learning frameworks and deployment environments. This feature is beneficial for deploying models where the original framework may not be available, or a more adaptable interface is required."]}),"\n",(0,o.jsx)(n.h4,{id:"loading-and-predicting-with-pyfunc",children:"Loading and Predicting with Pyfunc"}),"\n",(0,o.jsxs)(n.p,{children:["The code below illustrates how to load the Whisper model as a ",(0,o.jsx)(n.code,{children:"pyfunc"})," and use it for prediction. This method highlights MLflow's capability to adapt and deploy models in diverse scenarios."]}),"\n",(0,o.jsx)(n.h4,{id:"output-format-considerations",children:"Output Format Considerations"}),"\n",(0,o.jsxs)(n.p,{children:["Note the difference in the output format when using ",(0,o.jsx)(n.code,{children:"pyfunc"})," compared to the native format. The ",(0,o.jsx)(n.code,{children:"pyfunc"})," output conforms to standard pyfunc output signatures, typically represented as a ",(0,o.jsx)(n.code,{children:"List[str]"})," type, aligning with broader MLflow standards for model outputs."]}),"\n",(0,o.jsx)(s.d,{executionCount:9,children:'# Load the saved transcription pipeline as a generic python function\npyfunc_transcriber = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# Ensure that the pyfunc wrapper is capable of transcribing passed-in audio\npyfunc_transcription = pyfunc_transcriber.predict([audio])\n\n# Note: the pyfunc return type if `return_timestamps` is set is a JSON encoded string.\nprint(f"\nPyfunc output transcription:\n{format_transcription(pyfunc_transcription[0])}")'}),"\n",(0,o.jsx)(a.p,{children:"Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"}),"\n",(0,o.jsx)(a.p,{isStderr:!0,children:"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n2023/11/30 12:52:02 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised."}),"\n",(0,o.jsx)(a.p,{children:"\nPyfunc output transcription:\nWe choose to go to the moon in this decade and do the other things.\nNot because they are easy, but because they are hard.\n3, 2, 1, 0.\nAll engines running.\nLiftoff.\nWe have a liftoff.\n32 minutes past the hour.\nLiftoff on Apollo 11."}),"\n",(0,o.jsx)(n.h3,{id:"tutorial-roundup",children:"Tutorial Roundup"}),"\n",(0,o.jsx)(n.p,{children:"Throughout this tutorial, we've explored how to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Set up an audio transcription pipeline using the OpenAI Whisper model."}),"\n",(0,o.jsx)(n.li,{children:"Format and prepare audio data for transcription."}),"\n",(0,o.jsx)(n.li,{children:"Log, load, and use the model with MLflow, leveraging both the native and pyfunc flavors for inference."}),"\n",(0,o.jsx)(n.li,{children:"Format the output for readability and practical use in a Jupyter Notebook environment."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"We've seen the benefits of using MLflow for managing the machine learning lifecycle, including experiment tracking, model versioning, reproducibility, and deployment. By integrating MLflow with the Transformers library, we've streamlined the process of working with state-of-the-art NLP models, making it easier to track, manage, and deploy cutting-edge NLP applications."})]})}function f(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}}}]);