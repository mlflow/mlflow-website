"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4102],{3703:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>m,contentTitle:()=>p,default:()=>y,frontMatter:()=>h,metadata:()=>i,toc:()=>f});const i=JSON.parse('{"id":"model/dependencies/index","title":"Managing Dependencies in MLflow Models","description":"MLflow Model is a standard format that packages a machine learning model with its dependencies and other metadata.","source":"@site/docs/model/dependencies/index.mdx","sourceDirName":"model/dependencies","slug":"/model/dependencies/","permalink":"/docs/latest/model/dependencies/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Dependency Management","sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Model Signature","permalink":"/docs/latest/model/signatures/"},"next":{"title":"Models From Code","permalink":"/docs/latest/model/models-from-code/"}}');var t=o(74848),l=o(28453),r=o(56289),d=o(65537),s=o(79329),a=o(61096),c=o(67756);const h={sidebar_label:"Dependency Management",sidebar_position:2},p="Managing Dependencies in MLflow Models",m={},f=[{value:"How MLflow Records Model Dependencies",id:"how-mlflow-records-dependencies",level:2},{value:"Example",id:"example",level:3},{value:"Adding Extra Dependencies to an MLflow Model",id:"adding-extra-dependencies-to-an-mlflow-model",level:2},{value:"Defining All Dependencies by Yourself",id:"defining-all-dependencies-by-yourself",level:2},{value:"Saving Extra Code dependencies with an MLflow Model - Automatic inference",id:"saving-code-dependencies",level:2},{value:"Restrictions with <code>infer_code_paths</code>",id:"restrictions-with-infer_code_paths",level:3},{value:"Why This Matters",id:"why-this-matters",level:4},{value:"Best Practices",id:"best-practices",level:4},{value:"Saving Extra Code with an MLflow Model - Manual Declaration",id:"saving-extra-code-with-an-mlflow-model---manual-declaration",level:2},{value:"Use of <code>code_paths</code> Option for a Custom Library",id:"use-of-code_paths-option-for-a-custom-library",level:3},{value:"Caveats of <code>code_paths</code> Option",id:"caveats-of-code_paths-option",level:3},{value:"Limitation of <code>code_paths</code> in loading multiple models with the same module name but different implementations",id:"limitation-of-code_paths-in-loading-multiple-models-with-the-same-module-name-but-different-implementations",level:3},{value:"Recommended Project Structure",id:"recommended-project-structure",level:3},{value:"Validating Environment for Prediction",id:"validating-environment-for-prediction",level:2},{value:"Testing offline prediction with a virtual environment",id:"testing-offline-prediction-with-a-virtual-environment",level:3},{value:"Testing online inference endpoint with a virtual environment",id:"testing-online-inference-endpoint-with-a-virtual-environment",level:3},{value:"Testing online inference endpoint with a Docker container",id:"testing-online-inference-endpoint-with-a-docker-container",level:3},{value:"Troubleshooting",id:"model-dependencies-troubleshooting",level:2},{value:"How to Fix Dependency Errors when Serving my Model",id:"how-to-fix-dependency-errors-in-model",level:3},{value:"1. Check the missing dependencies",id:"1-check-the-missing-dependencies",level:4},{value:"2. Try adding the dependencies using the <code>predict</code> API",id:"2-try-adding-the-dependencies-using-the-predict-api",level:4},{value:"3. Update the model metadata",id:"3-update-the-model-metadata",level:4},{value:"How to Migrate Anaconda Dependency for License Change",id:"how-to-migrate-anaconda-dependency-for-license-change",level:3}];function u(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"managing-dependencies-in-mlflow-models",children:"Managing Dependencies in MLflow Models"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/model",children:"MLflow Model"})," is a standard format that packages a machine learning model with its dependencies and other metadata.\nBuilding a model with its dependencies allows for reproducibility and portability across a variety of platforms and tools."]}),"\n",(0,t.jsxs)(n.p,{children:["When you create an MLflow model using the ",(0,t.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking APIs"}),", for instance, ",(0,t.jsx)(c.B,{fn:"mlflow.pytorch.log_model"}),",\nMLflow automatically infers the required dependencies for the model flavor you're using and records them as a part of Model metadata. Then, when you\nserve the model for prediction, MLflow automatically installs the dependencies to the environment. Therefore, you normally won't need to\nworry about managing dependencies in MLflow Model."]}),"\n",(0,t.jsx)(n.p,{children:"However, in some cases, you may need to add or modify some dependencies. This page provides a high-level description of how MLflow manages\ndependencies and guidance for how to customize dependencies for your use case."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["One tip for improving MLflow's dependency inference accuracy is to add an ",(0,t.jsx)(n.code,{children:"input_example"})," when saving your model. This enables MLflow to\nperform a model prediction before saving the model, thereby capturing the dependencies used during the prediction.\nPlease refer to ",(0,t.jsx)(n.a,{href:"/model/signatures#input-example",children:"Model Input Example"})," for additional, detailed usage of this parameter."]})}),"\n",(0,t.jsx)(a.A,{toc:f,maxHeadingLevel:2}),"\n",(0,t.jsx)(n.h2,{id:"how-mlflow-records-dependencies",children:"How MLflow Records Model Dependencies"}),"\n",(0,t.jsx)(n.p,{children:"An MLflow Model is saved within a specified directory with the following structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"my_model/\n\u251c\u2500\u2500 MLmodel\n\u251c\u2500\u2500 model.pkl\n\u251c\u2500\u2500 conda.yaml\n\u251c\u2500\u2500 python_env.yaml\n\u2514\u2500\u2500 requirements.txt\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Model dependencies are defined by the following files (For other files, please refer to the guidance provided in the section discussing ",(0,t.jsx)(n.a,{href:"/model#storage-format",children:"Storage Format"}),"):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"python_env.yaml"})," - This file contains the information required to restore the model environment using virtualenv (1) python version (2) build tools like pip, setuptools, and wheel (3) pip requirements of the model (a reference to requirements.txt)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"requirements.txt"})," - Defines the set of pip dependencies required to run the model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"conda.yaml"})," - Defines the conda environment required to run the model. This is used when you specify ",(0,t.jsx)(n.code,{children:"conda"})," as the environment manager for restoring the model environment."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Please note that ",(0,t.jsx)(n.strong,{children:"it is not recommended to edit these files manually"})," to add or remove dependencies.\nThey are automatically generated by MLflow and any change you make manually will be overwritten when you save the model again.\nInstead, you should use one of the recommended methods described in the following sections."]}),"\n",(0,t.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,t.jsxs)(n.p,{children:["The following shows an example of environment files generated by MLflow when logging a model with ",(0,t.jsx)(n.code,{children:"mlflow.sklearn.log_model"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"python_env.yaml"})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"python: 3.9.8\nbuild_dependencies:\n  - pip==23.3.2\n  - setuptools==69.0.3\n  - wheel==0.42.0\ndependencies:\n  - -r requirements.txt\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"requirements.txt"})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"mlflow==2.9.2\nscikit-learn==1.3.2\ncloudpickle==3.0.0\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"conda.yaml"})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"name: mlflow-env\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9.8\n  - pip\n  - pip:\n      - mlflow==2.9.2\n      - scikit-learn==1.3.2\n      - cloudpickle==3.0.0\n"})}),"\n",(0,t.jsx)(n.h2,{id:"adding-extra-dependencies-to-an-mlflow-model",children:"Adding Extra Dependencies to an MLflow Model"}),"\n",(0,t.jsxs)(n.p,{children:["MLflow infers dependencies required for the model flavor library, but your model may depend on other libraries e.g. data\npreprocessing. In this case, you can add extra dependencies to the model by specifying the ",(0,t.jsx)(n.strong,{children:"extra_pip_requirements"})," param\nwhen logging the model. For example,"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        # your model depends on pandas\n        import pandas as pd\n\n        ...\n        return prediction\n\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=CustomModel(),\n        name="model",\n        extra_pip_requirements=["pandas==2.0.3"],\n        input_example=input_data,\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The extra dependencies will be added to ",(0,t.jsx)(n.code,{children:"requirements.txt"})," as follows (and similarly to ",(0,t.jsx)(n.code,{children:"conda.yaml"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"mlflow==2.9.2\ncloudpickle==3.0.0\npandas==2.0.3 # added\n"})}),"\n",(0,t.jsx)(n.p,{children:"In this case, MLflow will install Pandas 2.0.3 in addition to the inferred dependencies when serving the model for prediction."}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Once you log the model with dependencies, it is advisable to test it in a sandbox environment to avoid any dependency\nissues when deploying the model to production. Since MLflow 2.10.0, you can use the ",(0,t.jsx)(c.B,{fn:"mlflow.models.predict"})," API to quickly test\nyour model in a virtual environment. Please refer to ",(0,t.jsx)(n.a,{href:"#validating-environment-for-prediction",children:"Validating Environment for Prediction"})," for more details."]})}),"\n",(0,t.jsx)(n.h2,{id:"defining-all-dependencies-by-yourself",children:"Defining All Dependencies by Yourself"}),"\n",(0,t.jsxs)(n.p,{children:["Alternatively, you can also define all dependencies from scratch rather than adding extra ones. To do so,\nspecify ",(0,t.jsx)(n.strong,{children:"pip_requirements"})," when logging the model. For example,"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        name="model",\n        pip_requirements=[\n            "mlflow-skinny==2.9.2",\n            "cloudpickle==2.5.8",\n            "scikit-learn==1.3.1",\n        ],\n    )\n'})}),"\n",(0,t.jsx)(n.p,{children:"The manually defined dependencies will override the default ones MLflow detects from the model flavor library:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"mlflow-skinny==2.9.2\ncloudpickle==2.5.8\nscikit-learn==1.3.1\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"Please be careful when declaring dependencies that are different from those used during training, as it can be dangerous\nand prone to unexpected behavior. The safest way to ensure consistency is to rely on the default dependencies inferred by MLflow."})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Once you log the model with dependencies, it is advisable to test it in a sandbox environment to avoid any dependency\nissues when deploying the model to production. Since MLflow 2.10.0, you can use the ",(0,t.jsx)(c.B,{fn:"mlflow.models.predict"})," API to quickly\ntest your model in a virtual environment. Please refer to ",(0,t.jsx)(n.a,{href:"#validating-environment-for-prediction",children:"Validating Environment for Prediction"})," for more details."]})}),"\n",(0,t.jsx)(n.h2,{id:"saving-code-dependencies",children:"Saving Extra Code dependencies with an MLflow Model - Automatic inference"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"Automatic code dependency inference is currently supported for Python Function Models only. Support for additional named model flavors will be coming in\nfuture releases of MLflow."})}),"\n",(0,t.jsxs)(n.p,{children:["In the MLflow 2.13.0 release, a new method of including custom dependent code was introduced that expands on the existing feature of declaring ",(0,t.jsx)(n.code,{children:"code_paths"})," when\nsaving or logging a model. This new feature utilizes import dependency analysis to automatically infer the code dependencies required by the model by checking which\nmodules are imported within the references of a Python Model's definition."]}),"\n",(0,t.jsxs)(n.p,{children:["In order to use this new feature, you can simply set the argument ",(0,t.jsx)(n.code,{children:"infer_code_paths"})," (Default ",(0,t.jsx)(n.code,{children:"False"}),") to ",(0,t.jsx)(n.code,{children:"True"})," when logging. You do not have to define\nfile locations explicitly via declaring ",(0,t.jsx)(n.code,{children:"code_paths"})," directory locations when utilizing this method of dependency inference, as you would have had to\nprior to MLflow 2.13.0."]}),"\n",(0,t.jsxs)(n.p,{children:["An example of using this feature is shown below, where we are logging a model that contains an external dependency.\nIn the first section, we are defining an external module named ",(0,t.jsx)(n.code,{children:"custom_code"})," that exists in a different than our model definition."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="custom_code.py"',children:'from typing import List\n\niris_types = ["setosa", "versicolor", "viginica"]\n\n\ndef map_iris_types(predictions: int) -> List[str]:\n    return [iris_types[pred] for pred in predictions]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["With this ",(0,t.jsx)(n.code,{children:"custom_code.py"})," module defined, it is ready for use in our Python Model:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="model.py"',children:'from typing import Any, Dict, List, Optional\n\nfrom custom_code import map_iris_types  # import the external reference\n\nimport mlflow\n\n\nclass FlowerMapping(mlflow.pyfunc.PythonModel):\n    """Custom model with an external dependency"""\n\n    def predict(\n        self, context, model_input, params: Optional[Dict[str, Any]] = None\n    ) -> List[str]:\n        predictions = [pred % 3 for pred in model_input]\n\n        # Call the external function\n        return map_iris_types(predictions)\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="flowers",\n        python_model=FlowerMapping(),\n        infer_code_paths=True,  # Enabling automatic code dependency inference\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["With ",(0,t.jsx)(n.code,{children:"infer_code_paths"})," set to ",(0,t.jsx)(n.code,{children:"True"}),", the dependency of ",(0,t.jsx)(n.code,{children:"map_iris_types"})," will be analyzed, its source declaration detected as originating in\nthe ",(0,t.jsx)(n.code,{children:"custom_code.py"})," module, and the code reference within ",(0,t.jsx)(n.code,{children:"custom_code.py"})," will be stored along with the model artifact. Note that defining the\nexternal code dependency by using the ",(0,t.jsx)(n.code,{children:"code_paths"})," argument (discussed in the next section) is not needed."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Only modules that are within the current working directory are accessible. Dependency inference will not work across module boundaries or if your\ncustom code is defined in an entirely different library. If your code base is structured in such a way that common modules are entirely external to\nthe path that your model logging code is executing within, the original ",(0,t.jsx)(n.code,{children:"code_paths"})," option is required in order to log these dependencies, as\n",(0,t.jsx)(n.code,{children:"infer_code_paths"})," dependency inference will not capture those requirements."]})}),"\n",(0,t.jsxs)(n.h3,{id:"restrictions-with-infer_code_paths",children:["Restrictions with ",(0,t.jsx)(n.code,{children:"infer_code_paths"})]}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["Before using dependency inference via ",(0,t.jsx)(n.code,{children:"infer_code_paths"}),", ensure that your dependent code modules do not have sensitive data hard-coded within the modules (e.g., passwords,\naccess tokens, or secrets). Code inference does not obfuscate sensitive information and will capture and log (save) the module, regardless of what it contains."]})}),"\n",(0,t.jsxs)(n.p,{children:["An important aspect to note about code structure when using ",(0,t.jsx)(n.code,{children:"infer_code_paths"})," is to avoid defining dependencies within a main entry point to your code.\nWhen a Python code file is loaded as the ",(0,t.jsx)(n.code,{children:"__main__"})," module, it cannot be inferred as a code path file. This means that if you run your script directly\n(e.g., using ",(0,t.jsx)(n.code,{children:"python script.py"}),"), the functions and classes defined in that script will be part of the ",(0,t.jsx)(n.code,{children:"__main__"})," module and not easily accessible by\nother modules."]}),"\n",(0,t.jsxs)(n.p,{children:["If your model depends on these classes or functions, this can pose a problem because they are not part of the standard module namespace and thus not\nstraightforward to serialize. To handle this situation, you should use ",(0,t.jsx)(n.code,{children:"cloudpickle"})," to serialize your model instance. ",(0,t.jsx)(n.code,{children:"cloudpickle"})," is an\nextended version of Python's ",(0,t.jsx)(n.code,{children:"pickle"})," module that can serialize a wider range of Python objects, including functions and classes defined in\nthe ",(0,t.jsx)(n.code,{children:"__main__"})," module."]}),"\n",(0,t.jsxs)("dl",{children:[(0,t.jsx)("dt",{children:(0,t.jsx)(n.h4,{id:"why-this-matters",children:"Why This Matters"})}),(0,t.jsx)("dd",{children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Path Inference"}),": MLflow uses the code path to understand and log the code associated with your model. When the script is executed as ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"__main__"})}),", the code path cannot be inferred, which complicates the tracking and reproducibility of your MLflow experiments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Serialization"}),": Standard serialization methods like ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"pickle"})})," may not work with ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"__main__"})})," module objects, leading to issues when trying to save and load models. ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"cloudpickle"})})," provides a workaround by enabling the serialization of these objects, ensuring that your model can be correctly saved and restored."]}),"\n"]})}),(0,t.jsx)("dt",{children:(0,t.jsx)(n.h4,{id:"best-practices",children:"Best Practices"})}),(0,t.jsx)("dd",{children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Avoid defining critical functions and classes in the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"__main__"})})," module. Instead, place them in separate module files that can be imported as needed."]}),"\n",(0,t.jsxs)(n.li,{children:["If you must define functions and classes in the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"__main__"})})," module, use ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"cloudpickle"})})," to serialize your model to ensure that all dependencies are correctly handled."]}),"\n"]})})]}),"\n",(0,t.jsx)(n.h2,{id:"saving-extra-code-with-an-mlflow-model---manual-declaration",children:"Saving Extra Code with an MLflow Model - Manual Declaration"}),"\n",(0,t.jsxs)(n.p,{children:["MLflow also supports saving your custom Python code as dependencies to the model. This is particularly useful\nwhen you want to deploy your custom modules that are required for prediction with the model.\nTo do so, specify ",(0,t.jsx)(n.strong,{children:"code_paths"})," when logging the model. For example, if you have the following file structure in your project:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"my_project/\n\u251c\u2500\u2500 utils.py\n\u2514\u2500\u2500 train.py\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="train.py"',children:'import mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        from utils import my_func\n\n        x = my_func(model_input)\n        # .. your prediction logic\n        return prediction\n\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        name="model",\n        input_example=input_data,\n        code_paths=["utils.py"],\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Then MLflow will save ",(0,t.jsx)(n.code,{children:"utils.py"})," under ",(0,t.jsx)(n.code,{children:"code/"})," directory in the model directory:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"model/\n\u251c\u2500\u2500 MLmodel\n\u251c\u2500\u2500 ...\n\u2514\u2500\u2500 code/\n    \u2514\u2500\u2500 utils.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["When MLflow loads the model for serving, the ",(0,t.jsx)(n.code,{children:"code"})," directory will be added to the system path so that you can use the module in your model\ncode like ",(0,t.jsx)(n.code,{children:"from utils import my_func"}),". You can also specify a directory path as ",(0,t.jsx)(n.code,{children:"code_paths"})," to save multiple files under the directory:"]}),"\n",(0,t.jsxs)(n.h3,{id:"use-of-code_paths-option-for-a-custom-library",children:["Use of ",(0,t.jsx)(n.code,{children:"code_paths"})," Option for a Custom Library"]}),"\n",(0,t.jsxs)(n.p,{children:["To include custom libraries that are not publicly available on PyPI when logging your model, the ",(0,t.jsx)(n.code,{children:"code_paths"})," argument can be used.\nThis option allows you to upload ",(0,t.jsx)(n.code,{children:".whl"})," files or other dependencies alongside your model, ensuring all required libraries are available during serving."]}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"The following example demonstrates a quick method for including custom libraries for development purposes.\nThis approach is not recommended for production environments.\nFor production usage, upload libraries to a custom PyPI server or a cloud storage to ensure reliable and secure access."})}),"\n",(0,t.jsx)(n.p,{children:"For example, suppose your project has the following file structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"my_project/\n|\u2500\u2500 train.py\n\u2514\u2500\u2500 custom_package.whl\n"})}),"\n",(0,t.jsx)(n.p,{children:"Then the following code can log your model with the custom package:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="train.py"',children:'import mlflow\nfrom custom_package import my_func\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        x = my_func(model_input)\n        # .. your prediction logic\n        return prediction\n\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        name="model",\n        extra_pip_requirements=["code/custom_package.whl"],\n        input_example=input_data,\n        code_paths=["custom_package.whl"],\n    )\n'})}),"\n",(0,t.jsxs)(n.h3,{id:"caveats-of-code_paths-option",children:["Caveats of ",(0,t.jsx)(n.code,{children:"code_paths"})," Option"]}),"\n",(0,t.jsxs)(n.p,{children:["When using the ",(0,t.jsx)(n.code,{children:"code_paths"})," option, please be aware of the limitation that the specified file or directory ",(0,t.jsx)(n.strong,{children:"must be in the same directory as your model script"}),".\nIf the specified file or directory is in a parent or child directory like ",(0,t.jsx)(n.code,{children:"my_project/src/utils.py"}),", model serving will fail with ",(0,t.jsx)(n.code,{children:"ModuleNotFoundError"}),".\nFor example, let's say that you have the following file structure in your project"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"my_project/\n|\u2500\u2500 train.py\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500  utils.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Then the following model code does ",(0,t.jsx)(n.strong,{children:"not"})," work:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        from src.utils import my_func\n\n        # .. your prediction logic\n        return prediction\n\n\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        name="model",\n        input_example=input_data,\n        code_paths=[\n            "src/utils.py"\n        ],  # the file will be saved at code/utils.py not code/src/utils.py\n    )\n\n# => Model serving will fail with ModuleNotFoundError: No module named \'src\'\n'})}),"\n",(0,t.jsxs)(n.p,{children:["This limitation is due to how MLflow saves and loads the specified files and directories. When it copies the specified files or directories in ",(0,t.jsx)(n.code,{children:"code/"})," target,\nit does ",(0,t.jsx)(n.strong,{children:"not"})," preserve the relative paths that they were originally residing within. For instance, in the above example, MLflow will copy ",(0,t.jsx)(n.code,{children:"utils.py"})," to ",(0,t.jsx)(n.code,{children:"code/utils.py"}),", not\n",(0,t.jsx)(n.code,{children:"code/src/utils.py"}),". As a result, it has to be imported as ",(0,t.jsx)(n.code,{children:"from utils import my_func"}),", instead of ",(0,t.jsx)(n.code,{children:"from src.utils import my_func"}),".\nHowever, this may not be pleasant, as the import path is different from the original training script."]}),"\n",(0,t.jsxs)(n.p,{children:["To workaround this issue, the ",(0,t.jsx)(n.code,{children:"code_paths"})," should specify the parent directory, which is ",(0,t.jsx)(n.code,{children:'code_paths=["src"]'})," in this example.\nThis way, MLflow will copy the entire ",(0,t.jsx)(n.code,{children:"src/"})," directory under ",(0,t.jsx)(n.code,{children:"code/"})," and your model code will be able to import ",(0,t.jsx)(n.code,{children:"src.utils"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        from src.utils import my_func\n\n        # .. your prediction logic\n        return prediction\n\n\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=model,\n        name="model",\n        input_example=input_data,\n        code_paths=["src"],  # the whole /src directory will be saved at code/src\n    )\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["By the same reason, the ",(0,t.jsx)(n.code,{children:"code_paths"})," option doesn't handle the relative import of ",(0,t.jsx)(n.code,{children:'code_paths=["../src"]'}),"."]})}),"\n",(0,t.jsxs)(n.h3,{id:"limitation-of-code_paths-in-loading-multiple-models-with-the-same-module-name-but-different-implementations",children:["Limitation of ",(0,t.jsx)(n.code,{children:"code_paths"})," in loading multiple models with the same module name but different implementations"]}),"\n",(0,t.jsxs)(n.p,{children:["The current implementation of the ",(0,t.jsx)(n.code,{children:"code_paths"})," option has a limitation that it doesn't support loading multiple models that depend on modules with the same name but different implementations within the same Python process, as illustrated in the following example:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import importlib\nimport sys\nimport tempfile\nfrom pathlib import Path\n\nimport mlflow\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    tmpdir = Path(tmpdir)\n    my_model_path = tmpdir / "my_model.py"\n    code_template = """\nimport mlflow\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return [{n}] * len(model_input)\n"""\n\n    my_model_path.write_text(code_template.format(n=1))\n\n    sys.path.insert(0, str(tmpdir))\n    import my_model\n\n    # model 1\n    model1 = my_model.MyModel()\n    assert model1.predict(context=None, model_input=[0]) == [1]\n\n    with mlflow.start_run():\n        info1 = mlflow.pyfunc.log_model(\n            name="model",\n            python_model=model1,\n            code_paths=[my_model_path],\n        )\n\n    # model 2\n    my_model_path.write_text(code_template.format(n=2))\n    importlib.reload(my_model)\n    model2 = my_model.MyModel()\n    assert model2.predict(context=None, model_input=[0]) == [2]\n\n    with mlflow.start_run():\n        info2 = mlflow.pyfunc.log_model(\n            name="model",\n            python_model=model2,\n            code_paths=[my_model_path],\n        )\n\n# To simulate a fresh Python process, remove the `my_model` module from the cache\nsys.modules.pop("my_model")\n\n# Now we have two models that depend on modules with the same name but different implementations.\n# Let\'s load them and check the prediction results.\npred = mlflow.pyfunc.load_model(info1.model_uri).predict([0])\nassert pred == [1], pred  # passes\n\n# As the `my_model` module was loaded and cached in the previous `load_model` call,\n# the next `load_model` call will reuse it and return the wrong prediction result.\nassert "my_model" in sys.modules\npred = mlflow.pyfunc.load_model(info2.model_uri).predict([0])\nassert pred == [2], pred  # doesn\'t pass, `pred` is [1]\n'})}),"\n",(0,t.jsx)(n.p,{children:"To work around this limitation, you can remove the module from the cache before loading the model. For example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'model1 = mlflow.pyfunc.load_model(info1.model_uri)\nsys.modules.pop("my_model")\nmodel2 = mlflow.pyfunc.load_model(info2.model_uri)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Another workaround is to use different module names for different implementations. For example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'mlflow.pyfunc.log_model(\n    name="model1",\n    python_model=model1,\n    code_paths=["my_model1.py"],\n)\n\nmlflow.pyfunc.log_model(\n    name="model",\n    python_model=model2,\n    code_paths=["my_model2.py"],\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"recommended-project-structure",children:"Recommended Project Structure"}),"\n",(0,t.jsxs)(n.p,{children:["With this limitation for ",(0,t.jsx)(n.code,{children:"code_paths"})," in mind, the recommended project structure looks like the following:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"my_project/\n|-- model.py # Defines the custom pyfunc model\n|\u2500\u2500 train.py # Trains and logs the model\n|\u2500\u2500 core/    # Required modules for prediction\n|   |\u2500\u2500 preprocessing.py\n|   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 helper/  # Other helper modules used for training, evaluation\n    |\u2500\u2500 evaluation.py\n    \u2514\u2500\u2500 ...\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This way you can log the model with ",(0,t.jsx)(n.code,{children:'code_paths=["core"]'})," to include the required modules for prediction, while excluding the helper modules\nthat are only used for development."]}),"\n",(0,t.jsx)(n.h2,{id:"validating-environment-for-prediction",children:"Validating Environment for Prediction"}),"\n",(0,t.jsxs)(n.p,{children:["Validating your model before deployment is a critical step to ensure production readiness.\nMLflow provides a few ways to test your model locally, either in a virtual environment or a Docker container.\nIf you find any dependency issues during validation, please follow the guidance in ",(0,t.jsx)(n.a,{href:"#how-to-fix-dependency-errors-in-model",children:"How to fix dependency errors when serving my model?"})]}),"\n",(0,t.jsx)(n.h3,{id:"testing-offline-prediction-with-a-virtual-environment",children:"Testing offline prediction with a virtual environment"}),"\n",(0,t.jsxs)(n.p,{children:["You can use MLflow Models ",(0,t.jsx)(n.strong,{children:"predict"})," API via Python or CLI to make test predictions with your model.\nThis will load your model from the model URI, create a virtual environment with the model dependencies (defined in MLflow Model),\nand run offline predictions with the model.\nPlease refer to ",(0,t.jsx)(c.B,{fn:"mlflow.models.predict"})," or the ",(0,t.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-models",target:"_blank",children:"CLI reference"})," for more detailed usage for the predict API."]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"The Python API is available since MLflow 2.10.0. If you are using an older version, please use the CLI option."})}),"\n",(0,t.jsxs)(d.A,{children:[(0,t.jsx)(s.A,{default:!0,label:"Python",value:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.models.predict(\n    model_uri="runs:/<run_id>/model",\n    input_data="<input_data>",\n)\n'})})}),(0,t.jsx)(s.A,{label:"Bash",value:"bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mlflow models predict -m runs:/<run_id>/model-i <input_path>\n"})})})]}),"\n",(0,t.jsxs)(n.p,{children:["Using the ",(0,t.jsx)(c.B,{fn:"mlflow.models.predict"})," API is convenient for testing your model and inference environment quickly.\nHowever, it may not be a perfect simulation of the serving because it does not start the online inference server. That\nsaid, it's a great way to test whether your prediction inputs are correctly formatted."]}),"\n",(0,t.jsxs)(n.p,{children:["Formatting is subject to the types supported by the ",(0,t.jsx)(n.code,{children:"predict()"})," method of your logged model. If the model was logged with a\nsignature, the input data should be viewable from the MLflow UI or via ",(0,t.jsx)(c.B,{fn:"mlflow.models.get_model_info"}),",\nwhich has the field ",(0,t.jsx)(n.code,{children:"signature"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"More generally, MLflow has the ability to support a variety of flavor-specific input types, such as a tensorflow tensor.\nMLflow also supports types that are not specific to a given flavor, such as a pandas DataFrame, numpy ndarray, python Dict,\npython List, scipy.sparse matrix, and spark data frame."}),"\n",(0,t.jsx)(n.h3,{id:"testing-online-inference-endpoint-with-a-virtual-environment",children:"Testing online inference endpoint with a virtual environment"}),"\n",(0,t.jsxs)(n.p,{children:["If you want to test your model by actually running the online inference server, you can use the MLflow ",(0,t.jsx)(n.code,{children:"serve"})," API.\nThis will create a virtual environment with your model and dependencies, similarly to the ",(0,t.jsx)(n.code,{children:"predict"})," API, but will start the inference server\nand expose the REST endpoints. Then you can send a test request and validate the response.\nPlease refer to the ",(0,t.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-models",target:"_blank",children:"CLI reference"})," for more detailed usage for the ",(0,t.jsx)(n.code,{children:"serve"})," API."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'mlflow models serve -m runs:/<run_id>/model -p <port>\n# In another terminal\ncurl -X POST -H "Content-Type: application/json" \\\n    --data \'{"inputs": [[1, 2], [3, 4]]}\' \\\n    http://localhost:<port>/invocations\n'})}),"\n",(0,t.jsx)(n.p,{children:"While this is a reliable way to test your model before deployment, one caveat is that the virtual environment doesn't absorb the OS-level differences\nbetween your machine and the production environment. For example, if you are using MacOS as a local dev machine but your deployment target is\nrunning on Linux, you may encounter some issues that are not reproducible in the virtual environment."}),"\n",(0,t.jsx)(n.p,{children:"In this case, you can use a Docker container to test your model. While it doesn't provide full OS-level isolation unlike virtual machines e.g. we\ncan't run Windows containers on Linux machines, Docker covers some popular test scenarios such as running different versions of Linux or simulating\nLinux environments on Mac or Windows."}),"\n",(0,t.jsx)(n.h3,{id:"testing-online-inference-endpoint-with-a-docker-container",children:"Testing online inference endpoint with a Docker container"}),"\n",(0,t.jsxs)(n.p,{children:["MLflow ",(0,t.jsx)(n.code,{children:"build-docker"})," API for CLI and Python is capable of building an Ubuntu-based Docker image for serving your model.\nThe image will contain your model and dependencies, as well as having an entrypoint that is used to start the inference server. Similarly to the ",(0,t.jsx)(n.em,{children:"serve"})," API,\nyou can send a test request and validate the response.\nPlease refer to the ",(0,t.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-models",target:"_blank",children:"CLI reference"})," for more detailed usage for the ",(0,t.jsx)(n.code,{children:"build-docker"})," API."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'mlflow models build-docker -m runs:/<run_id>/model -n <image_name>\ndocker run -p <port>:8080 <image_name>\n# In another terminal\ncurl -X POST -H "Content-Type: application/json" \\\n    --data \'{"inputs": [[1, 2], [3, 4]]}\' \\\n    http://localhost:<port>/invocations\n'})}),"\n",(0,t.jsx)(n.h2,{id:"model-dependencies-troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"how-to-fix-dependency-errors-in-model",children:"How to Fix Dependency Errors when Serving my Model"}),"\n",(0,t.jsx)(n.p,{children:'One of the most common issues experienced during model deployment centers around dependency issues. When logging or saving your model, MLflow tries to infer the\nmodel dependencies and save them as part of the MLflow Model metadata. However, this might not always be complete and miss some dependencies e.g. [extras] dependencies\nfor certain libraries. This can cause errors when serving your model, such as "ModuleNotFoundError" or "ImportError". Below are some steps that can help to diagnose\nand fix missing dependency errors.'}),"\n",(0,t.jsx)(n.admonition,{title:"hint",type:"tip",children:(0,t.jsxs)(n.p,{children:["To reduce the possibility of dependency errors, you can add ",(0,t.jsx)(n.code,{children:"input_example"})," when saving your model. This enables MLflow to\nperform a model prediction before saving the model, thereby capturing the dependencies used during the prediction.\nPlease refer to ",(0,t.jsx)(n.a,{href:"/model/signatures/#input-example",children:"Model Input Example"})," for additional, detailed usage of this parameter."]})}),"\n",(0,t.jsx)(n.h4,{id:"1-check-the-missing-dependencies",children:"1. Check the missing dependencies"}),"\n",(0,t.jsx)(n.p,{children:"The missing dependencies are listed in the error message. For example, if you see the following error message:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ModuleNotFoundError: No module named 'cv2'\n"})}),"\n",(0,t.jsxs)(n.h4,{id:"2-try-adding-the-dependencies-using-the-predict-api",children:["2. Try adding the dependencies using the ",(0,t.jsx)(n.code,{children:"predict"})," API"]}),"\n",(0,t.jsxs)(n.p,{children:["Now that you know the missing dependencies, you can create a new model version with the correct dependencies.\nHowever, creating a new model for trying new dependencies might be a bit tedious, particularly because you may need to\niterate multiple times to find the correct solution. Instead, you can use the ",(0,t.jsx)(c.B,{fn:"mlflow.models.predict"})," API to test your change without\nactually needing to re-log the model repeatedly while troubleshooting the installation errors."]}),"\n",(0,t.jsxs)(n.p,{children:["To do so, use the ",(0,t.jsx)(n.strong,{children:"pip-requirements-override"})," option to specify pip dependencies like ",(0,t.jsx)(n.code,{children:"opencv-python==4.8.0"}),"."]}),"\n",(0,t.jsxs)(d.A,{children:[(0,t.jsx)(s.A,{default:!0,label:"Python",value:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.models.predict(\n    model_uri="runs:/<run_id>/<model_path>",\n    input_data="<input_data>",\n    pip_requirements_override=["opencv-python==4.8.0"],\n)\n'})})}),(0,t.jsx)(s.A,{label:"Bash",value:"bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mlflow models predict \\\n    -m runs:/<run_id>/<model_path> \\\n    -I <input_path> \\\n    --pip-requirements-override opencv-python==4.8.0\n"})})})]}),"\n",(0,t.jsx)(n.p,{children:"The specified dependencies will be installed to the virtual environment in addition to (or instead of) the dependencies\ndefined in the model metadata. Since this doesn't mutate the model, you can iterate quickly and safely to find the correct dependencies."}),"\n",(0,t.jsxs)(n.p,{children:["Note that for ",(0,t.jsx)(n.code,{children:"input_data"})," parameter in the python implementation, the function takes a Python object that is supported by your\nmodel's ",(0,t.jsx)(n.code,{children:"predict()"})," function. Some examples may include flavor-specific input types, such as a\ntensorflow tensor, or more generic types such as a pandas DataFrame, numpy ndarray, python Dict, or\npython List. When working with the CLI, we cannot pass python objects and instead look to pass the path\nto a CSV or JSON file containing the input payload."]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"pip-requirements-override"})," option is available since MLflow 2.10.0."]})}),"\n",(0,t.jsx)(n.h4,{id:"3-update-the-model-metadata",children:"3. Update the model metadata"}),"\n",(0,t.jsxs)(n.p,{children:["Once you find the correct dependencies, you can update the logged model's dependencies using ",(0,t.jsx)(c.B,{fn:"mlflow.models.update_model_requirements"})," API."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.models.update_model_requirements(\n    model_uri="runs:/<run_id>/<model_path>",\n    operation="add",\n    requirement_list=["opencv-python==4.8.0"],\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Note that you can also leverage the CLI to update the model requirements:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'mlflow models update-pip-requirements -m runs:/<run_id>/<model_path> add "opencv-python==4.8.0"\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Alternatively, you can log a new model with the updated dependencies by specifying the ",(0,t.jsx)(n.code,{children:"extra_pip_requirements"})," option when logging the model."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.pyfunc.log_model(\n    name="model",\n    python_model=python_model,\n    extra_pip_requirements=["opencv-python==4.8.0"],\n    input_example=input_data,\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"how-to-migrate-anaconda-dependency-for-license-change",children:"How to Migrate Anaconda Dependency for License Change"}),"\n",(0,t.jsxs)(n.p,{children:["Anaconda Inc. updated their ",(0,t.jsx)(n.a,{href:"https://www.anaconda.com/terms-of-service",children:"terms of service"})," for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda\u2019s packaging and distribution. See ",(0,t.jsx)(n.a,{href:"https://www.anaconda.com/blog/anaconda-commercial-edition-faq",children:"Anaconda Commercial Edition FAQ"})," for more information. Your use of any Anaconda channels is governed by their terms of service."]}),"\n",(0,t.jsxs)(n.p,{children:["MLflow models logged before ",(0,t.jsx)(n.a,{href:"https://mlflow.org/news/2021/06/18/1.18.0-release/index.html",children:"v1.18"})," were by default logged with the conda ",(0,t.jsx)(n.code,{children:"defaults"})," channel (",(0,t.jsx)(n.a,{href:"https://repo.anaconda.com/pkgs",children:"https://repo.anaconda.com/pkgs"}),") as a dependency. Because of this license change, MLflow has stopped the use of the ",(0,t.jsx)(n.code,{children:"defaults"})," channel for models logged using MLflow v1.18 and above. The default channel logged is now ",(0,t.jsx)(n.code,{children:"conda-forge"}),", which points at the community managed ",(0,t.jsx)(n.a,{href:"https://conda-forge.org",children:"https://conda-forge.org"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["If you logged a model before MLflow v1.18 without excluding the ",(0,t.jsx)(n.code,{children:"defaults"})," channel from the conda environment for the model, that model may have a dependency on the ",(0,t.jsx)(n.code,{children:"defaults"})," channel that you may not have intended.\nTo manually confirm whether a model has this dependency, you can examine the ",(0,t.jsx)(n.code,{children:"channel"})," value in the ",(0,t.jsx)(n.code,{children:"conda.yaml"})," file that is packaged with the logged model. For example, a model's ",(0,t.jsx)(n.code,{children:"conda.yaml"})," with a ",(0,t.jsx)(n.code,{children:"defaults"})," channel dependency may look like this:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"name: mlflow-env\nchannels:\n  - defaults\ndependencies:\n  - python=3.8.8\n  - pip\n  - pip:\n      - mlflow==2.3\n      - scikit-learn==0.23.2\n      - cloudpickle==1.6.0\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If you would like to change the channel used in a model's environment, you can re-register the model to the model registry with a new ",(0,t.jsx)(n.code,{children:"conda.yaml"}),". You can do this by specifying the channel in the ",(0,t.jsx)(n.code,{children:"conda_env"})," parameter of ",(0,t.jsx)(n.code,{children:"log_model()"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["For more information on the ",(0,t.jsx)(n.code,{children:"log_model()"})," API, see the MLflow documentation for the model flavor you are working with, for example, ",(0,t.jsx)(c.B,{fn:"mlflow.sklearn.log_model"}),"."]})]})}function y(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},67756:(e,n,o)=>{o.d(n,{B:()=>s});o(96540);const i=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var t=o(29030),l=o(56289),r=o(74848);const d=e=>{const n=e.split(".");for(let o=n.length;o>0;o--){const e=n.slice(0,o).join(".");if(i[e])return e}return null};function s(e){let{fn:n,children:o}=e;const s=d(n);if(!s)return(0,r.jsx)(r.Fragment,{children:o});const a=(0,t.Ay)(`/${i[s]}#${n}`);return(0,r.jsx)(l.A,{to:a,target:"_blank",children:o??(0,r.jsxs)("code",{children:[n,"()"]})})}}}]);