"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["9581"],{18213(e,t,l){l.r(t),l.d(t,{metadata:()=>i,default:()=>v,frontMatter:()=>f,contentTitle:()=>u,toc:()=>g,assets:()=>w});var i=JSON.parse('{"id":"eval-monitor/legacy-llm-evaluation","title":"Migrating from Legacy LLM Evaluation","description":"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics.","source":"@site/docs/genai/eval-monitor/legacy-llm-evaluation.mdx","sourceDirName":"eval-monitor","slug":"/eval-monitor/legacy-llm-evaluation","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/legacy-llm-evaluation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"description":"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics."},"sidebar":"genAISidebar","previous":{"title":"AI Issue Discovery","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/ai-insights/ai-issue-discovery"},"next":{"title":"FAQ","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/faq"}}'),n=l(74848),a=l(28453),o=l(54725),r=l(66497),s=l(46077),c=l(81956),d=l(42640),h=l(51004),m=l(47792),p=l(85731);let f={description:"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics."},u="Migrating from Legacy LLM Evaluation",w={},g=[{value:"Why Migrate?",id:"why-migrate",level:2},{value:"1. Richer evaluation results",id:"1-richer-evaluation-results",level:5},{value:"2. More powerful and flexible LLM-as-a-Judge",id:"2-more-powerful-and-flexible-llm-as-a-judge",level:5},{value:"3. Integration with other MLflow GenAI capabilities",id:"3-integration-with-other-mlflow-genai-capabilities",level:5},{value:"4. Better future support",id:"4-better-future-support",level:5},{value:"Migration Steps",id:"migration-steps",level:2},{value:"1. Wrap Your Model in a Function",id:"1-wrap-your-model-in-a-function",level:3},{value:"2. Update the Dataset Format",id:"2-update-the-dataset-format",level:3},{value:"3. Migrate Metrics",id:"3-migrate-metrics",level:3},{value:"Example of custom LLM-as-a-Judge metrics",id:"example-of-custom-llm-as-a-judge-metrics",level:4},{value:"Example of custom heuristic metrics",id:"example-of-custom-heuristic-metrics",level:4},{value:"4. Run Evaluation",id:"4-run-evaluation",level:3},{value:"Other Changes",id:"other-changes",level:2},{value:"FAQ",id:"faq",level:2},{value:"Q: The feature I want is not supported in the new evaluation suite.",id:"q-the-feature-i-want-is-not-supported-in-the-new-evaluation-suite",level:3},{value:"Q: Where can I find the documentation for the legacy evaluation API?",id:"q-where-can-i-find-the-documentation-for-the-legacy-evaluation-api",level:3},{value:"Q: When will the legacy evaluation API be removed?",id:"q-when-will-the-legacy-evaluation-api-be-removed",level:3},{value:"Q: Should I migrate non-GenAI workloads to the new evaluation suite?",id:"q-should-i-migrate-non-genai-workloads-to-the-new-evaluation-suite",level:3}];function x(e){let t={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"migrating-from-legacy-llm-evaluation",children:"Migrating from Legacy LLM Evaluation"})}),"\n",(0,n.jsxs)(t.admonition,{type:"info",children:[(0,n.jsxs)(t.p,{children:["This is a migration guide for users who are using the legacy LLM evaluation capability through ",(0,n.jsx)(t.code,{children:"mlflow.evaluate"})," API and see the following warning while migrating to MLflow 3."]}),(0,n.jsxs)(t.blockquote,{children:["\n",(0,n.jsx)(t.p,{children:"The mlflow.evaluate API has been deprecated as of MLflow 3.0.0."}),"\n"]}),(0,n.jsxs)(t.p,{children:["If you are new to MLflow or its evaluation capabilities, start from the ",(0,n.jsx)("ins",{children:(0,n.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/index.html",children:"MLflow 3 GenAI Evaluation"})})," guide instead."]})]}),"\n",(0,n.jsx)(t.h2,{id:"why-migrate",children:"Why Migrate?"}),"\n",(0,n.jsxs)(t.p,{children:["MLflow 3 introduces a ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor",children:"new evaluation suite"})," that are optimized for evaluating LLMs and GenAI applications. Compared to the legacy evaluation through the ",(0,n.jsx)(t.code,{children:"mlflow.evaluate"})," API, the new suite offers the following benefits:"]}),"\n",(0,n.jsx)(t.h5,{id:"1-richer-evaluation-results",children:"1. Richer evaluation results"}),"\n",(0,n.jsx)(t.p,{children:"MLflow 3 displays the evaluation results with intuitive visualizations. Each prediction is recorded with a trace, which allows you to further investigate the result in details and identify the root cause of low quality predictions."}),"\n",(0,n.jsxs)("table",{children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("th",{children:"Old Results"}),(0,n.jsx)("td",{children:(0,n.jsx)(s.A,{src:"/images/mlflow-3/eval-monitor/legacy-eval-result.png",alt:"Legacy Evaluation",width:"80%"})})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("th",{children:"New Results"}),(0,n.jsx)("td",{children:(0,n.jsx)(s.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-compare.png",alt:"New Evaluation",width:"80%"})})]})]}),"\n",(0,n.jsx)(t.h5,{id:"2-more-powerful-and-flexible-llm-as-a-judge",children:"2. More powerful and flexible LLM-as-a-Judge"}),"\n",(0,n.jsxs)(t.p,{children:["A rich set of built-in ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges",children:"LLM Judges"})," and a flexible toolset to build your own LLM-as-a-Judge supports you to evaluate various aspects of your LLM applications. Furthermore, the new ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges",children:"Agents-as-a-Judge"})," capability evaluates complex trace with minimum context window consumption and boilerplate code."]}),"\n",(0,n.jsx)(t.h5,{id:"3-integration-with-other-mlflow-genai-capabilities",children:"3. Integration with other MLflow GenAI capabilities"}),"\n",(0,n.jsxs)(t.p,{children:["The new evaluation suite is tightly integrated with other MLflow GenAI capabilities, such as ",(0,n.jsx)(t.a,{href:"/genai/tracing",children:"tracing"}),", ",(0,n.jsx)(t.a,{href:"/genai/prompt-registry",children:"prompt management"}),", ",(0,n.jsx)(t.a,{href:"/genai/prompt-registry/optimize-prompts",children:"prompt optimization"}),", making it an end-to-end solution for building high-quality LLM applications."]}),"\n",(0,n.jsx)(t.h5,{id:"4-better-future-support",children:"4. Better future support"}),"\n",(0,n.jsxs)(t.p,{children:["MLflow is rapidly evolving (",(0,n.jsx)(t.a,{href:"https://github.com/mlflow/mlflow/releases",children:"changelog"}),") and will continue strengthening its evaluation capabilities with the north star of ",(0,n.jsx)(t.strong,{children:"Deliver production-ready AI"}),". Migrating your workload to the new evaluation suite will ensure you have instant access to the latest and greatest features."]}),"\n",(0,n.jsx)(t.h2,{id:"migration-steps",children:"Migration Steps"}),"\n",(0,n.jsx)(c.A,{width:"wide",steps:[{icon:d.A,title:"Wrap your model in a function",description:"If you are evaluating an MLflow Model, wrap the model in a function and pass it to the new evaluation API."},{icon:h.A,title:"Update dataset format",description:"Update the inputs and ground truth format to match the new evaluation dataset format."},{icon:m.A,title:"Migrate metrics",description:"Update the metrics to use the new built-in or custom scorers offered by MLflow 3."},{icon:p.A,title:"Run evaluation",description:"Execute the evaluation and make sure the results are as expected."}]}),"\n",(0,n.jsx)(t.admonition,{title:"Before you start the migration",type:"tip",children:(0,n.jsxs)(t.p,{children:["Before starting the migration, we highly recommend you to visit the ",(0,n.jsx)("ins",{children:(0,n.jsx)(t.a,{href:"/genai/eval-monitor",children:"GenAI Evaluation Guide"})})," and go through the ",(0,n.jsx)("ins",{children:(0,n.jsx)(t.a,{href:"/genai/eval-monitor/quickstart",children:"Quickstart"})})," to get a sense of the new evaluation suite. Basic understanding of the concepts will help you to migrate your existing workload smoothly."]})}),"\n",(0,n.jsx)(t.h3,{id:"1-wrap-your-model-in-a-function",children:"1. Wrap Your Model in a Function"}),"\n",(0,n.jsxs)(t.p,{children:["The old evaluation API accepts MLflow model URI as an evaluation target. The new evaluation API accepts a callable function as ",(0,n.jsx)(t.code,{children:"predict_fn"})," argument instead, to provide more flexibility and control. This also eliminates the need of logging the model in MLflow before evaluation."]}),"\n",(0,n.jsxs)("table",{style:{tableLayout:"fixed",width:"100%"},children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("th",{children:"Old Format"}),(0,n.jsx)("th",{children:"New Format"})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,n.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,n.jsx)("code",{className:"language-python",children:`# Log the model first before evaluation
with mlflow.start_run() as run:
    logged_model_info = mlflow.openai.log_model(
        model="gpt-5-mini",
        task=openai.chat.completions,
        artifact_path="model",
        messages=[
            {"role": "system", "content": "Answer the following question in two sentences"},
            {"role": "user", "content": "{question}"},
        ],
    )

# Pass the model URI to the evaluation API.
mlflow.evaluate(model=logged_model_info.model_uri, ...)
`})})}),(0,n.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,n.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,n.jsx)("code",{className:"language-python",children:`# Define a function that runs predictions.
def predict_fn(question: str) -> str:
  response = openai.OpenAI().chat.completions.create(
      model="gpt-5-mini",
      messages=[
          {"role": "system", "content": "Answer the following question in two sentences"},
          {"role": "user", "content": question},
      ],
  )
  return response.choices[0].message.content

mlflow.genai.evaluate(predict_fn=predict_fn, ...)
`})})})]})]}),"\n",(0,n.jsx)(t.p,{children:"If you want to evaluate a pre-logged model with the new evaluation API, simply call the loaded model in the function."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"# IMPORTANT: Load the model outside the predict_fn function. Otherwise the model will be loaded\n# for each input in the dataset and significantly slow down the evaluation.\nmodel = mlflow.pyfunc.load_model(model_uri)\n\n\ndef predict_fn(question: str) -> str:\n    return model.predict([question])[0]\n"})}),"\n",(0,n.jsx)(t.h3,{id:"2-update-the-dataset-format",children:"2. Update the Dataset Format"}),"\n",(0,n.jsx)(t.p,{children:"The dataset format has been changed to be more flexible and consistent. The new format requirements are:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"inputs"}),": The input to the predict_fn function. The key(s) must match the parameter name of the predict_fn function."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"expectations"}),": The expected output from the predict_fn function, namely, ground truth for the answer."]}),"\n",(0,n.jsxs)(t.li,{children:["Optionally, you can pass ",(0,n.jsx)(t.code,{children:"outputs"})," column or ",(0,n.jsx)(t.code,{children:"trace"})," column to evaluate pre-generated outputs and traces."]}),"\n"]}),"\n",(0,n.jsxs)("table",{style:{tableLayout:"fixed",width:"100%"},children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("th",{children:"Old Format"}),(0,n.jsx)("th",{children:"New Format"})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,n.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,n.jsx)("code",{className:"language-python",children:`eval_data = pd.DataFrame(
  {
      "inputs": [
          "What is MLflow?",
          "What is Spark?",
      ],
      "ground_truth": [
          "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle.",
          "Apache Spark is an open-source, distributed computing system designed for big data processing and analytics.",
      ],
      "predictions": [
        "MLflow is an open-source MLOps platform",
        "Apache Spark is an open-source distributed computing engine.",
      ]
  }
)

mlflow.evaluate(
  data=eval_data, # Needed to specify the ground truth and prediction # columns name, otherwise MLflow does not recognize them.
  targets="ground_truth",
  predictions="predictions",
  ...
)
`})})}),(0,n.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,n.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,n.jsx)("code",{className:"language-python",children:`eval_data = [
  {
      "inputs": {"question": "What is MLflow?"},
      "outputs": "MLflow is an open-source MLOps platform",
      "expectations": {"answer": "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle."},
  },
  {
      "inputs": {"question": "What is Spark?"},
      "outputs": "Apache Spark is an open-source distributed computing engine.",
      "expectations": {"answer": "Apache Spark is an open-source, distributed computing system designed for big data processing and analytics."},
  },
]

mlflow.genai.evaluate(
  data=eval_data,
  ...
)
`})})})]})]}),"\n",(0,n.jsx)(t.h3,{id:"3-migrate-metrics",children:"3. Migrate Metrics"}),"\n",(0,n.jsx)(t.p,{children:"The new evaluation API supports a rich set of built-in and custom LLM-as-a-Judge metrics. The table below shows the mapping between the legacy metrics and the new metrics."}),"\n",(0,n.jsxs)("table",{children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("th",{children:"Metric"}),(0,n.jsx)("th",{children:"Before"}),(0,n.jsx)("th",{children:"After"})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Latency"}),(0,n.jsx)("td",{children:(0,n.jsx)(o.B,{fn:"mlflow.metrics.latency",children:(0,n.jsx)(t.code,{children:"latency"})})}),(0,n.jsxs)("td",{children:["Traces record latency and also span-level break down. You don't need to specify a metric to evaluate latency when running the new ",(0,n.jsx)(o.B,{fn:"mlflow.genai.evaluate"})," API."]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Token Count"}),(0,n.jsx)("td",{children:(0,n.jsx)(o.B,{fn:"mlflow.metrics.token_count",children:(0,n.jsx)(t.code,{children:"token_count"})})}),(0,n.jsxs)("td",{children:["Traces record token count for LLM calls for most of popular LLM providers. For other cases, you can use a ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/custom",children:"custom scorer"})," to calculate the token count."]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Heuristic NLP metrics"}),(0,n.jsxs)("td",{children:[(0,n.jsx)(o.B,{fn:"mlflow.metrics.toxicity",children:(0,n.jsx)(t.code,{children:"toxicity"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.flesch_kincaid_grade_level",children:(0,n.jsx)(t.code,{children:"flesch_kincaid_grade_level"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.ari_grade_level",children:(0,n.jsx)(t.code,{children:"ari_grade_level"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.exact_match",children:(0,n.jsx)(t.code,{children:"exact_match"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.rouge1",children:(0,n.jsx)(t.code,{children:"rouge1"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.rouge2",children:(0,n.jsx)(t.code,{children:"rouge2"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.rougeL",children:(0,n.jsx)(t.code,{children:"rougeL"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.rougeLsum",children:(0,n.jsx)(t.code,{children:"rougeLsum"})})]}),(0,n.jsxs)("td",{children:["Use a ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/custom",children:"Code-based Scorer"})," to implement the equivalent metrics. See the example below for reference."]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Retrieval metrics"}),(0,n.jsxs)("td",{children:[(0,n.jsx)(o.B,{fn:"mlflow.metrics.precision_at_k",children:(0,n.jsx)(t.code,{children:"precision_at_k"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.recall_at_k",children:(0,n.jsx)(t.code,{children:"recall_at_k"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.ndcg_at_k",children:(0,n.jsx)(t.code,{children:"ndcg_at_k"})})]}),(0,n.jsxs)("td",{children:["Use the new ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges",children:"built-in retrieval metrics"})," or define a custom code-based scorer."]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Built-in LLM-as-a-Judge metrics"}),(0,n.jsxs)("td",{children:[(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.answer_similarity",children:(0,n.jsx)(t.code,{children:"answer_similarity"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.answer_correctness",children:(0,n.jsx)(t.code,{children:"answer_correctness"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.answer_relevance",children:(0,n.jsx)(t.code,{children:"answer_relevance"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.relevance",children:(0,n.jsx)(t.code,{children:"relevance"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.faithfulness",children:(0,n.jsx)(t.code,{children:"faithfulness"})})]}),(0,n.jsxs)("td",{children:["Use the new ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges",children:"built-in judges"}),". If the metric is not supported out of the box, define a custom LLM-as-a-Judge scorer using the ",(0,n.jsx)(o.B,{fn:"mlflow.genai.judges.make_judge",children:(0,n.jsx)(t.code,{children:"make_judge"})})," API, following the example below."]})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:"Custom LLM-as-a-Judge metrics"}),(0,n.jsxs)("td",{children:[(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.make_genai_metric",children:(0,n.jsx)(t.code,{children:"make_genai_metric"})}),", ",(0,n.jsx)(o.B,{fn:"mlflow.metrics.genai.make_genai_metric_from_prompt",children:(0,n.jsx)(t.code,{children:"make_genai_metric_from_prompt"})})]}),(0,n.jsxs)("td",{children:["Use the ",(0,n.jsx)(o.B,{fn:"mlflow.genai.judges.make_judge",children:(0,n.jsx)(t.code,{children:"make_judge"})})," API to define a custom LLM-as-a-Judge scorer, following the example below."]})]})]}),"\n",(0,n.jsx)(t.h4,{id:"example-of-custom-llm-as-a-judge-metrics",children:"Example of custom LLM-as-a-Judge metrics"}),"\n",(0,n.jsxs)(t.p,{children:["The new evaluation API supports defining custom LLM-as-a-Judge metrics from a custom prompt template. This eliminates a lot of complexity and over-abstractions from the previous ",(0,n.jsx)(t.code,{children:"make_genai_metric"})," and ",(0,n.jsx)(t.code,{children:"make_genai_metric_from_prompt"})," APIs."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'from mlflow.genai import make_judge\n\nanswer_similarity = make_judge(\n    name="answer_similarity",\n    instructions=(\n        "Evaluated on the degree of semantic similarity of the provided output to the expected answer.\\n\\n"\n        "Output: {{ outputs }}\\n\\n"\n        "Expected: {{ expectations }}"\n    ),\n    feedback_value_type=int,\n)\n\n# Pass the scorer to the evaluation API.\nmlflow.genai.evaluate(scorers=[answer_similarity, ...])\n'})}),"\n",(0,n.jsxs)(t.p,{children:["See the ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/#llms-as-judges",children:"LLM Judges"})," guide for more details."]}),"\n",(0,n.jsx)(t.h4,{id:"example-of-custom-heuristic-metrics",children:"Example of custom heuristic metrics"}),"\n",(0,n.jsxs)(t.p,{children:["Implementing a custom scorer for heuristic metrics is straightforward. You just need to define a function and decorate it with the ",(0,n.jsx)(o.B,{fn:"mlflow.genai.scorers.scorer",children:(0,n.jsx)(t.code,{children:"@scorer"})})," decorator. The example below shows how to implement the ",(0,n.jsx)(t.code,{children:"exact_match"})," metric."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'@scorer\ndef exact_match(outputs: dict, expectations: dict) -> bool:\n    return outputs == expectations["expected_response"]\n\n\n# Pass the scorer to the evaluation API.\nmlflow.genai.evaluate(scorers=[exact_match, ...])\n'})}),"\n",(0,n.jsxs)(t.p,{children:["See the ",(0,n.jsx)(t.a,{href:"/genai/eval-monitor/scorers/custom",children:"Code-based Scorers"})," guide for more details."]}),"\n",(0,n.jsx)(t.h3,{id:"4-run-evaluation",children:"4. Run Evaluation"}),"\n",(0,n.jsx)(t.p,{children:"Now you have migrated all components of the legacy evaluation API and are ready to run the evaluation!"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"mlflow.genai.evaluate(\n    data=eval_data,\n    predict_fn=predict_fn,\n    scorers=[answer_similarity, exact_match, ...],\n)\n"})}),"\n",(0,n.jsxs)(t.p,{children:["To view the evaluation results, click the link in the console output, or navigate to the ",(0,n.jsx)(t.strong,{children:"Evaluations"})," tab in the MLflow UI."]}),"\n",(0,n.jsx)("video",{src:(0,r.default)("/images/mlflow-3/eval-monitor/evaluation-result-video.mp4"),controls:!0,loop:!0,autoPlay:!0,muted:!0,"aria-label":"Prompt Evaluation"}),"\n",(0,n.jsx)(t.h2,{id:"other-changes",children:"Other Changes"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["When using Databricks Model Serving endpoint as a LLM-judge model, use ",(0,n.jsx)(t.code,{children:"databricks:/<endpoint-name>"})," as model provider, rather than ",(0,n.jsx)(t.code,{children:"endpoints:/<endpoint-name>"})]}),"\n",(0,n.jsxs)(t.li,{children:["The evaluation results are shown in the ",(0,n.jsx)(t.code,{children:"Evaluations"})," tab in the MLflow UI."]}),"\n",(0,n.jsxs)(t.li,{children:["Lots of configuration knobs such as ",(0,n.jsx)(t.code,{children:"model_type"}),", ",(0,n.jsx)(t.code,{children:"targets"}),", ",(0,n.jsx)(t.code,{children:"feature_names"}),", ",(0,n.jsx)(t.code,{children:"env_manager"}),", are removed in the new evaluation API."]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"faq",children:"FAQ"}),"\n",(0,n.jsx)(t.h3,{id:"q-the-feature-i-want-is-not-supported-in-the-new-evaluation-suite",children:"Q: The feature I want is not supported in the new evaluation suite."}),"\n",(0,n.jsxs)(t.p,{children:["Please open an feature request in ",(0,n.jsx)(t.a,{href:"https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml",children:"GitHub"}),"."]}),"\n",(0,n.jsx)(t.h3,{id:"q-where-can-i-find-the-documentation-for-the-legacy-evaluation-api",children:"Q: Where can I find the documentation for the legacy evaluation API?"}),"\n",(0,n.jsxs)(t.p,{children:["See ",(0,n.jsx)(t.a,{href:"https://mlflow.org/docs/2.22.1/llms/llm-evaluate",children:"MLflow 2 documentation"})," for the legacy evaluation API."]}),"\n",(0,n.jsx)(t.h3,{id:"q-when-will-the-legacy-evaluation-api-be-removed",children:"Q: When will the legacy evaluation API be removed?"}),"\n",(0,n.jsx)(t.p,{children:"It will likely be removed in MLflow 3.7.0 or a few releases after that."}),"\n",(0,n.jsx)(t.h3,{id:"q-should-i-migrate-non-genai-workloads-to-the-new-evaluation-suite",children:"Q: Should I migrate non-GenAI workloads to the new evaluation suite?"}),"\n",(0,n.jsxs)(t.p,{children:["No. The new evaluation suite is only for GenAI workloads. If you are not using GenAI, you should use the ",(0,n.jsx)(o.B,{fn:"mlflow.models.evaluate"})," API, which offers perfect compatibility with ",(0,n.jsx)(t.code,{children:"mlflow.evaluate"})," API but drops the GenAI-specific features."]})]})}function v(e={}){let{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(x,{...e})}):x(e)}},75689(e,t,l){l.d(t,{A:()=>s});var i=l(96540);let n=e=>{let t=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,l)=>l?l.toUpperCase():t.toLowerCase());return t.charAt(0).toUpperCase()+t.slice(1)},a=(...e)=>e.filter((e,t,l)=>!!e&&""!==e.trim()&&l.indexOf(e)===t).join(" ").trim();var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let r=(0,i.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:l=2,absoluteStrokeWidth:n,className:r="",children:s,iconNode:c,...d},h)=>(0,i.createElement)("svg",{ref:h,...o,width:t,height:t,stroke:e,strokeWidth:n?24*Number(l)/Number(t):l,className:a("lucide",r),...!s&&!(e=>{for(let t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,t])=>(0,i.createElement)(e,t)),...Array.isArray(s)?s:[s]])),s=(e,t)=>{let l=(0,i.forwardRef)(({className:l,...o},s)=>(0,i.createElement)(r,{ref:s,iconNode:t,className:a(`lucide-${n(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,l),...o}));return l.displayName=n(e),l}},42640(e,t,l){l.d(t,{A:()=>i});let i=(0,l(75689).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},51004(e,t,l){l.d(t,{A:()=>i});let i=(0,l(75689).A)("database",[["ellipse",{cx:"12",cy:"5",rx:"9",ry:"3",key:"msslwz"}],["path",{d:"M3 5V19A9 3 0 0 0 21 19V5",key:"1wlel7"}],["path",{d:"M3 12A9 3 0 0 0 21 12",key:"mv7ke4"}]])},85731(e,t,l){l.d(t,{A:()=>i});let i=(0,l(75689).A)("play",[["polygon",{points:"6 3 20 12 6 21 6 3",key:"1oa8hb"}]])},47792(e,t,l){l.d(t,{A:()=>i});let i=(0,l(75689).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},54725(e,t,l){l.d(t,{B:()=>o});var i=l(74848);l(96540);var n=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),a=l(66497);function o({fn:e,children:t,hash:l}){let o=(e=>{let t=e.split(".");for(let e=t.length;e>0;e--){let l=t.slice(0,e).join(".");if(n[l])return l}return null})(e);if(!o)return(0,i.jsx)(i.Fragment,{children:t});let r=(0,a.default)(`/${n[o]}#${l??e}`);return(0,i.jsx)("a",{href:r,target:"_blank",children:t??(0,i.jsxs)("code",{children:[e,"()"]})})}},46077(e,t,l){l.d(t,{A:()=>a});var i=l(74848);l(96540);var n=l(66497);function a({src:e,alt:t,width:l,caption:a,className:o}){return(0,i.jsxs)("div",{className:`container_JwLF ${o||""}`,children:[(0,i.jsx)("div",{className:"imageWrapper_RfGN",style:l?{width:l}:{},children:(0,i.jsx)("img",{src:(0,n.default)(e),alt:t,className:"image_bwOA"})}),a&&(0,i.jsx)("p",{className:"caption_jo2G",children:a})]})}},81956(e,t,l){l.d(t,{A:()=>a});var i=l(74848);l(96540);var n=l(46077);let a=({steps:e,title:t,screenshot:l,width:a="normal"})=>(0,i.jsxs)("div",{className:"workflowContainer__N1v",children:[t&&(0,i.jsx)("h3",{className:"workflowTitle_QrAr",children:t}),l&&(0,i.jsx)("div",{className:"screenshotContainer_OwzZ",children:(0,i.jsx)(n.A,{src:l.src,alt:l.alt,width:l.width||"90%"})}),(0,i.jsx)("div",{className:"stepsContainer_IGeu",style:{maxWidth:"wide"===a?"850px":"700px"},children:e.map((t,l)=>(0,i.jsxs)("div",{className:"stepItem_GyHJ",children:[(0,i.jsxs)("div",{className:"stepIndicator_U2Wb",children:[(0,i.jsx)("div",{className:"stepNumber_vINc",children:t.icon?(0,i.jsx)(t.icon,{size:16}):(0,i.jsx)("span",{className:"stepNumberText_eLd7",children:l+1})}),l<e.length-1&&(0,i.jsx)("div",{className:"stepConnector_Si86"})]}),(0,i.jsxs)("div",{className:"stepContent_D0CA",children:[(0,i.jsx)("h4",{className:"stepTitle_wujx",children:t.title}),(0,i.jsx)("p",{className:"stepDescription_PIaE",children:t.description})]})]},l))})]})},28453(e,t,l){l.d(t,{R:()=>o,x:()=>r});var i=l(96540);let n={},a=i.createContext(n);function o(e){let t=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);