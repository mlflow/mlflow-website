"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["8315"],{10508(e,n,t){t.r(n),t.d(n,{metadata:()=>s,default:()=>u,frontMatter:()=>l,contentTitle:()=>d,toc:()=>h,assets:()=>c});var s=JSON.parse('{"id":"governance/ai-gateway/endpoints/query-endpoints","title":"Query Endpoints","description":"Once you\'ve created an endpoint, you can call it through several different API styles depending on your needs.","source":"@site/docs/genai/governance/ai-gateway/endpoints/query-endpoints.mdx","sourceDirName":"governance/ai-gateway/endpoints","slug":"/governance/ai-gateway/endpoints/query-endpoints","permalink":"/mlflow-website/docs/latest/genai/governance/ai-gateway/endpoints/query-endpoints","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Query Endpoints"},"sidebar":"genAISidebar","previous":{"title":"Create and Manage Endpoints","permalink":"/mlflow-website/docs/latest/genai/governance/ai-gateway/endpoints/create-and-manage"},"next":{"title":"Model Providers","permalink":"/mlflow-website/docs/latest/genai/governance/ai-gateway/endpoints/model-providers"}}'),a=t(74848),o=t(28453),r=t(78010),i=t(57250);let l={title:"Query Endpoints"},d="Query Endpoints",c={},h=[{value:"Viewing Usage Examples",id:"viewing-usage-examples",level:2},{value:"Unified APIs",id:"unified-apis",level:2},{value:"MLflow Invocations API",id:"mlflow-invocations-api",level:3},{value:"API Specification",id:"api-specification",level:4},{value:"OpenAI-Compatible Chat Completions API",id:"openai-compatible-chat-completions-api",level:3},{value:"Passthrough APIs",id:"passthrough-apis",level:2},{value:"OpenAI Passthrough",id:"openai-passthrough",level:3},{value:"Anthropic Passthrough",id:"anthropic-passthrough",level:3},{value:"Google Gemini Passthrough",id:"google-gemini-passthrough",level:3},{value:"Framework Integrations",id:"framework-integrations",level:2},{value:"Using Gateway Endpoints with MLflow Judges",id:"using-gateway-endpoints-with-mlflow-judges",level:2}];function p(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"query-endpoints",children:"Query Endpoints"})}),"\n",(0,a.jsx)(n.p,{children:"Once you've created an endpoint, you can call it through several different API styles depending on your needs."}),"\n",(0,a.jsx)(n.h2,{id:"viewing-usage-examples",children:"Viewing Usage Examples"}),"\n",(0,a.jsx)(n.p,{children:"To see code examples for your endpoint, navigate to the Endpoints list and click either the Use button or the endpoint name itself. This opens a modal with comprehensive usage examples tailored to your specific endpoint."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Usage Modal",src:t(78542).A+"",width:"3456",height:"1916"})}),"\n",(0,a.jsx)(n.p,{children:"The usage modal organizes examples into two categories: unified APIs that work across any provider, and passthrough APIs that expose provider-specific features."}),"\n",(0,a.jsx)(n.h2,{id:"unified-apis",children:"Unified APIs"}),"\n",(0,a.jsx)(n.p,{children:"Unified APIs provide a consistent interface regardless of the underlying model provider. These APIs make it easy to switch between different models or providers without changing your application code."}),"\n",(0,a.jsx)(n.h3,{id:"mlflow-invocations-api",children:"MLflow Invocations API"}),"\n",(0,a.jsx)(n.p,{children:"The MLflow Invocations API is the native interface for calling gateway endpoints. This API seamlessly handles model switching and advanced routing features like traffic splitting and fallbacks:"}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"curl",label:"cURL",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/my-endpoint/mlflow/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "temperature": 0.7\n  }\'\n'})})}),(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import requests\n\nresponse = requests.post(\n    "http://localhost:5000/gateway/my-endpoint/mlflow/invocations",\n    json={"messages": [{"role": "user", "content": "Hello!"}], "temperature": 0.7},\n)\nprint(response.json())\n'})})})]}),"\n",(0,a.jsx)(n.h4,{id:"api-specification",children:"API Specification"}),"\n",(0,a.jsx)(n.p,{children:"The MLflow Invocations API supports both OpenAI-style chat completions and embeddings endpoints."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Endpoint URL Pattern:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"POST /gateway/{endpoint_name}/mlflow/invocations\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Chat Completions Request Body:"})}),"\n",(0,a.jsxs)(n.p,{children:["The request body follows the OpenAI chat completions format with these supported parameters. See ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat",children:"OpenAI Chat Completions API Reference"})," for complete documentation."]}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Parameter"}),(0,a.jsx)(n.th,{children:"Type"}),(0,a.jsx)(n.th,{children:"Required"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"messages"})}),(0,a.jsx)(n.td,{children:"array"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsxs)(n.td,{children:["Array of message objects with ",(0,a.jsx)(n.code,{children:"role"})," and ",(0,a.jsx)(n.code,{children:"content"})," fields"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"temperature"})}),(0,a.jsx)(n.td,{children:"number"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Sampling temperature between 0 and 2. Higher values make output more random."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"max_tokens"})}),(0,a.jsx)(n.td,{children:"integer"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Maximum number of tokens to generate."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"top_p"})}),(0,a.jsx)(n.td,{children:"number"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Nucleus sampling parameter between 0 and 1. Alternative to temperature."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"n"})}),(0,a.jsx)(n.td,{children:"integer"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Number of completions to generate. Default is 1."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"stream"})}),(0,a.jsx)(n.td,{children:"boolean"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Whether to stream responses."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"stream_options"})}),(0,a.jsx)(n.td,{children:"object"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Options for streaming responses."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"stop"})}),(0,a.jsx)(n.td,{children:"array"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"List of sequences where the API will stop generating tokens."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"presence_penalty"})}),(0,a.jsx)(n.td,{children:"number"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Penalizes new tokens based on presence in text so far. Range: -2.0 to 2.0."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"frequency_penalty"})}),(0,a.jsx)(n.td,{children:"number"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Penalizes new tokens based on frequency in text so far. Range: -2.0 to 2.0."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"tools"})}),(0,a.jsx)(n.td,{children:"array"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsxs)(n.td,{children:["List of tools the model can call. Each tool includes ",(0,a.jsx)(n.code,{children:"type"}),", ",(0,a.jsx)(n.code,{children:"function"})," with ",(0,a.jsx)(n.code,{children:"name"}),", ",(0,a.jsx)(n.code,{children:"description"}),", and ",(0,a.jsx)(n.code,{children:"parameters"}),"."]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"response_format"})}),(0,a.jsx)(n.td,{children:"object"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:'Format for the model output. Can specify "text", "json_object", or "json_schema" with schema definition.'})]})]})]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Response Format:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-123",\n  "object": "chat.completion",\n  "created": 1677652288,\n  "model": "gpt-5",\n  "choices": [{\n    "index": 0,\n    "message": {\n      "role": "assistant",\n      "content": "Hello! How can I assist you today?"\n    },\n    "finish_reason": "stop"\n  }],\n  "usage": {\n    "prompt_tokens": 9,\n    "completion_tokens": 12,\n    "total_tokens": 21\n  }\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Streaming Responses:"})}),"\n",(0,a.jsxs)(n.p,{children:["When ",(0,a.jsx)(n.code,{children:"stream: true"})," is set, the response is sent as Server-Sent Events (SSE):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-5","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}\n\ndata: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-5","choices":[{"index":0,"delta":{"content":"!"},"finish_reason":null}]}\n\ndata: [DONE]\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Embeddings Request Body:"})}),"\n",(0,a.jsxs)(n.p,{children:["For embeddings endpoints, the request body follows the OpenAI embeddings format. See ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/embeddings",children:"OpenAI Embeddings API Reference"})," for complete documentation."]}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Parameter"}),(0,a.jsx)(n.th,{children:"Type"}),(0,a.jsx)(n.th,{children:"Required"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"input"})}),(0,a.jsx)(n.td,{children:"string or array"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"Input text(s) to embed. Can be a single string or array of strings."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"encoding_format"})}),(0,a.jsx)(n.td,{children:"string"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:'Format to return embeddings. Options: "float" (default) or "base64".'})]})]})]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Embeddings Response Format:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [{\n    "object": "embedding",\n    "embedding": [0.0023064255, -0.009327292, ...],\n    "index": 0\n  }],\n  "model": "text-embedding-ada-002",\n  "usage": {\n    "prompt_tokens": 8,\n    "total_tokens": 8\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"openai-compatible-chat-completions-api",children:"OpenAI-Compatible Chat Completions API"}),"\n",(0,a.jsx)(n.p,{children:"For teams already using the OpenAI chat completion style APIs, the gateway provides an OpenAI-compatible interface. Simply point your OpenAI client to the gateway's base URL and use your endpoint name as the model parameter. This lets you leverage existing OpenAI-based code while gaining the gateway's routing capabilities."}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat",children:"OpenAI Chat Completions API Reference"})," for complete documentation."]}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"curl",label:"cURL",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/mlflow/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "my-endpoint",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "temperature": 0.7\n  }\'\n'})})}),(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:5000/gateway/mlflow/v1",\n    api_key="",  # API key not needed, configured server-side\n)\n\nresponse = client.chat.completions.create(\n    model="my-endpoint",\n    messages=[{"role": "user", "content": "Hello!"}],\n    temperature=0.7,\n)\nprint(response.choices[0].message.content)\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"passthrough-apis",children:"Passthrough APIs"}),"\n",(0,a.jsx)(n.p,{children:"The Passthrough API relays requests to the provider's LLM endpoint using its native formats, allowing you to use their native client SDKs with the MLflow Gateway. While unified APIs work for most use cases, passthrough APIs give you full access to provider-specific features that may not be available through the unified interface."}),"\n",(0,a.jsxs)(n.p,{children:["For detailed information on passthrough APIs for each provider, see ",(0,a.jsx)(n.a,{href:"/genai/governance/ai-gateway/endpoints/model-providers",children:"Model Providers"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"openai-passthrough",children:"OpenAI Passthrough"}),"\n",(0,a.jsxs)(n.p,{children:["The OpenAI passthrough API exposes the full OpenAI API including Chat Completions, Embeddings, and Responses endpoints. See ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Reference"})," for complete documentation."]}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"curl",label:"cURL",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/openai/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "my-endpoint",\n    "messages": [{"role": "user", "content": "Hello!"}]\n  }\'\n'})})}),(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:5000/gateway/openai/v1",\n    api_key="dummy",  # API key not needed, configured server-side\n)\n\nresponse = client.chat.completions.create(\n    model="my-endpoint", messages=[{"role": "user", "content": "Hello!"}]\n)\nprint(response.choices[0].message.content)\n'})})})]}),"\n",(0,a.jsx)(n.h3,{id:"anthropic-passthrough",children:"Anthropic Passthrough"}),"\n",(0,a.jsxs)(n.p,{children:["Access Anthropic's Messages API directly through the gateway. See ",(0,a.jsx)(n.a,{href:"https://docs.anthropic.com/en/api",children:"Anthropic API Reference"})," for complete documentation."]}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"curl",label:"cURL",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/anthropic/v1/messages \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "my-endpoint",\n    "max_tokens": 1024,\n    "messages": [{"role": "user", "content": "Hello!"}]\n  }\'\n'})})}),(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import anthropic\n\nclient = anthropic.Anthropic(\n    base_url="http://localhost:5000/gateway/anthropic",\n    api_key="dummy",  # API key not needed, configured server-side\n)\n\nresponse = client.messages.create(\n    model="my-endpoint",\n    max_tokens=1024,\n    messages=[{"role": "user", "content": "Hello!"}],\n)\nprint(response.content[0].text)\n'})})})]}),"\n",(0,a.jsx)(n.h3,{id:"google-gemini-passthrough",children:"Google Gemini Passthrough"}),"\n",(0,a.jsxs)(n.p,{children:["The Gemini passthrough API follows Google's API structure. See ",(0,a.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs",children:"Google Gemini API Reference"})," for complete documentation."]}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"curl",label:"cURL",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/gemini/v1beta/models/my-endpoint:generateContent \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "contents": [{\n      "parts": [{"text": "Hello!"}]\n    }]\n  }\'\n'})})}),(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from google import genai\n\nclient = genai.Client(\n    api_key="dummy",\n    http_options={\n        "base_url": "http://localhost:5000/gateway/gemini",\n    },\n)\n\nresponse = client.models.generate_content(\n    model="my-endpoint",\n    contents={"text": "Hello!"},\n)\nclient.close()\nprint(response.candidates[0].content.parts[0].text)\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"framework-integrations",children:"Framework Integrations"}),"\n",(0,a.jsx)(n.p,{children:"The MLflow AI Gateway's OpenAI-compatible API makes it easy to integrate with popular LLM frameworks. Simply point your framework to the gateway's base URL and use your endpoint name as the model."}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"litellm",label:"LiteLLM",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import litellm\n\nresponse = litellm.completion(\n    model="openai/my-endpoint",\n    messages=[{"role": "user", "content": "Hello!"}],\n    api_base="http://localhost:5000/gateway/mlflow/v1",\n    api_key="not-needed",\n)\nprint(response.choices[0].message.content)\n'})})}),(0,a.jsx)(i.A,{value:"langchain",label:"LangChain",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model="my-endpoint",\n    base_url="http://localhost:5000/gateway/mlflow/v1",\n    api_key="not-needed",\n)\nresponse = llm.invoke("Hello!")\nprint(response.content)\n'})})}),(0,a.jsx)(i.A,{value:"langgraph",label:"LangGraph",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nllm = ChatOpenAI(\n    model="my-endpoint",\n    base_url="http://localhost:5000/gateway/mlflow/v1",\n    api_key="not-needed",\n)\ngraph = create_react_agent(llm, tools=[])\nresult = graph.invoke({"messages": [{"role": "user", "content": "Hello!"}]})\nprint(result["messages"][-1].content)\n'})})}),(0,a.jsx)(i.A,{value:"dspy",label:"DSPy",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import dspy\n\nlm = dspy.LM(\n    model="openai/my-endpoint",\n    api_base="http://localhost:5000/gateway/mlflow/v1",\n    api_key="not-needed",\n)\ndspy.configure(lm=lm)\nprogram = dspy.Predict("question -> answer")\nprint(program(question="What is MLflow?").answer)\n'})})}),(0,a.jsx)(i.A,{value:"openai-agents",label:"OpenAI Agents SDK",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nfrom agents import Agent, Runner, set_default_openai_client\n\nclient = openai.AsyncOpenAI(\n    base_url="http://localhost:5000/gateway/openai/v1",\n    api_key="not-needed",\n)\nset_default_openai_client(client)\n\nagent = Agent(name="Assistant", instructions="You are helpful.", model="my-endpoint")\nresult = await Runner.run(agent, input="Hello!")\nprint(result.final_output)\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"using-gateway-endpoints-with-mlflow-judges",children:"Using Gateway Endpoints with MLflow Judges"}),"\n",(0,a.jsxs)(n.p,{children:["AI Gateway endpoints can be used as the backing LLM for MLflow's ",(0,a.jsx)(n.a,{href:"/genai/eval-monitor/scorers/#llms-as-judges",children:"LLM Judges"}),". This allows you to run judge evaluations through the gateway, benefiting from centralized API key management and cost tracking."]}),"\n",(0,a.jsxs)(n.p,{children:["To use a gateway endpoint as a judge model, use the ",(0,a.jsx)(n.code,{children:"gateway:/"})," prefix followed by your endpoint name:"]}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(i.A,{value:"built-in",label:"Built-in Judges",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import Correctness\n\n# Use a gateway endpoint for the Correctness judge\nscorer = Correctness(model="gateway:/my-chat-endpoint")\n'})})}),(0,a.jsx)(i.A,{value:"make-judge",label:"Custom Judges",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\nfrom typing import Literal\n\n# Create a custom judge using a gateway endpoint\ncoherence_judge = make_judge(\n    name="coherence",\n    instructions=(\n        "Evaluate if the response is coherent and maintains a clear flow.\\n"\n        "Question: {{ inputs }}\\n"\n        "Response: {{ outputs }}\\n"\n    ),\n    feedback_value_type=Literal["coherent", "somewhat coherent", "incoherent"],\n    model="gateway:/my-chat-endpoint",\n)\n'})})})]}),"\n",(0,a.jsxs)(n.p,{children:["For more details on creating and using LLM judges, see the ",(0,a.jsx)(n.a,{href:"/genai/eval-monitor/scorers/#llms-as-judges",children:"LLM Judges documentation"}),"."]})]})}function u(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},78542(e,n,t){t.d(n,{A:()=>s});let s=t.p+"assets/images/usage-modal-0e1b25b064247119b5494f8641452c88.png"},57250(e,n,t){t.d(n,{A:()=>o});var s=t(74848);t(96540);var a=t(34164);function o({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)("tabItem_Ymn6",t),hidden:n,children:e})}},78010(e,n,t){t.d(n,{A:()=>y});var s=t(74848),a=t(96540),o=t(34164),r=t(88287),i=t(28584),l=t(56347),d=t(99989),c=t(96629),h=t(80618),p=t(41367);function u(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}var g=t(19863);function x({className:e,block:n,selectedValue:t,selectValue:a,tabValues:r}){let l=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.a_)(),c=e=>{let n=e.currentTarget,s=r[l.indexOf(n)].value;s!==t&&(d(n),a(s))},h=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{let t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{let t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1]}}n?.focus()};return(0,s.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:a})=>(0,s.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:h,onClick:c,...a,className:(0,o.A)("tabs__item","tabItem_LNqP",a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:t}){let r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=r.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,s.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function f(e){let n=function(e){let n,{defaultValue:t,queryString:s=!1,groupId:o}=e,r=function(e){let{values:n,children:t}=e;return(0,a.useMemo)(()=>{let e=n??u(t).map(({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s})),s=(0,h.XI)(e,(e,n)=>e.value===n.value);if(s.length>0)throw Error(`Docusaurus error: Duplicate values "${s.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,t])}(e),[i,g]=(0,a.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let t=n.find(e=>e.default)??n[0];if(!t)throw Error("Unexpected error: 0 tabValues");return t.value})({defaultValue:t,tabValues:r})),[x,j]=function({queryString:e=!1,groupId:n}){let t=(0,l.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(s),(0,a.useCallback)(e=>{if(!s)return;let n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})},[s,t])]}({queryString:s,groupId:o}),[f,y]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[t,s]=(0,p.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:o}),v=m({value:n=x??f,tabValues:r})?n:null;return(0,d.A)(()=>{v&&g(v)},[v]),{selectedValue:i,selectValue:(0,a.useCallback)(e=>{if(!m({value:e,tabValues:r}))throw Error(`Can't select invalid tab value=${e}`);g(e),j(e),y(e)},[j,y,r]),tabValues:r}}(e);return(0,s.jsxs)("div",{className:(0,o.A)(r.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,s.jsx)(x,{...n,...e}),(0,s.jsx)(j,{...n,...e})]})}function y(e){let n=(0,g.A)();return(0,s.jsx)(f,{...e,children:u(e.children)},String(n))}},28453(e,n,t){t.d(n,{R:()=>r,x:()=>i});var s=t(96540);let a={},o=s.createContext(a);function r(e){let n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);