"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1545],{7712:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/langchain-retrievalqa-062004e33f90253db67a531616dbe0b0.png"},11759:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/langchain-code-model-e83e8e4a2dadcb329c2bb7e0fa1d1bd7.png"},14709:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/stateful-chains-0848141cc5a49caea27639994dbc0b6d.png"},24268:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>m,contentTitle:()=>p,default:()=>f,frontMatter:()=>d,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"llms/langchain/index","title":"MLflow LangChain Flavor","description":"The langchain flavor is under active development and is marked as Experimental. Public APIs are","source":"@site/docs/llms/langchain/index.mdx","sourceDirName":"llms/langchain","slug":"/llms/langchain/","permalink":"/docs/latest/llms/langchain/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Detailed Guide","permalink":"/docs/latest/llms/openai/guide/"},"next":{"title":"MLflow Langchain Autologging","permalink":"/docs/latest/llms/langchain/autologging"}}');var i=a(74848),o=a(28453),l=a(65537),r=a(79329),s=a(86294),c=a(67756),h=a(56289);const d={},p="MLflow LangChain Flavor",m={},u=[{value:"Why use MLflow with LangChain?",id:"why-use-mlflow-with-langchain",level:2},{value:"Experiment Tracking",id:"experiment-tracking",level:3},{value:"Dependency Management",id:"dependency-management",level:3},{value:"MLflow Evaluate",id:"mlflow-evaluate",level:3},{value:"Observability",id:"observability",level:3},{value:"Automatic Logging",id:"automatic-logging",level:2},{value:"Supported Elements in MLflow LangChain Integration",id:"supported-elements-in-mlflow-langchain-integration",level:2},{value:"Overview of Chains, Agents, and Retrievers",id:"overview-of-chains-agents-and-retrievers",level:2},{value:"Getting Started with the MLflow LangChain Flavor - Tutorials and Guides",id:"getting-started-with-the-mlflow-langchain-flavor---tutorials-and-guides",level:2},{value:"Introductory Tutorial",id:"introductory-tutorial",level:3},{value:"Advanced Tutorials",id:"advanced-tutorials",level:3},{value:"Logging models from Code",id:"logging-models-from-code",level:3},{value:"Detailed Documentation",id:"detailed-documentation",level:2},{value:"FAQ",id:"faq",level:2},{value:"I can&#39;t load my chain!",id:"i-cant-load-my-chain",level:3},{value:"I can&#39;t save my chain, agent, or retriever with MLflow.",id:"i-cant-save-my-chain-agent-or-retriever-with-mlflow",level:3},{value:"I&#39;m getting an AttributeError when saving my model",id:"im-getting-an-attributeerror-when-saving-my-model",level:3},{value:"How can I use a streaming API with LangChain?",id:"how-can-i-use-a-streaming-api-with-langchain",level:3},{value:"How can I log an agent built with LangGraph to MLflow?",id:"how-can-i-log-an-agent-built-with-langgraph-to-mlflow",level:3},{value:"How can I evaluate a LangGraph Agent?",id:"how-can-i-evaluate-a-langgraph-agent",level:3},{value:"How to control whether my input is converted to List[langchain.schema.BaseMessage] in PyFunc predict?",id:"how-to-control-whether-my-input-is-converted-to-listlangchainschemabasemessage-in-pyfunc-predict",level:3}];function g(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"mlflow-langchain-flavor",children:"MLflow LangChain Flavor"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"langchain"})," flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change, and new features may be added as the flavor evolves."]})}),"\n",(0,i.jsxs)(n.p,{children:["Welcome to the developer guide for the integration of ",(0,i.jsx)(n.a,{href:"https://www.langchain.com/",children:"LangChain"})," with MLflow. This guide serves as a comprehensive\nresource for understanding and leveraging the combined capabilities of LangChain and MLflow in developing advanced language model applications."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.langchain.com/",children:"LangChain"})," is a versatile framework designed for building applications powered by language models. It excels in creating context-aware applications\nthat utilize language models for reasoning and generating responses, enabling the development of sophisticated NLP applications."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://langchain-ai.github.io/langgraph/",children:"LangGraph"})," is a complementary agent-based framework from the creators of Langchain, supporting the creation of\nstateful agent and multi-agent GenAI applications. LangGraph utilizes LangChain in order to interface with GenAI agent components."]}),"\n",(0,i.jsx)(n.h2,{id:"why-use-mlflow-with-langchain",children:"Why use MLflow with LangChain?"}),"\n",(0,i.jsx)(n.p,{children:"Aside from the benefits of using MLflow for managing and deploying machine learning models, the integration of LangChain with MLflow provides a number of\nbenefits that are associated with using LangChain within the broader MLflow ecosystem."}),"\n",(0,i.jsx)(n.h3,{id:"experiment-tracking",children:"Experiment Tracking"}),"\n",(0,i.jsxs)(n.p,{children:["LangChain's flexibility in experimenting with various agents, tools, and retrievers becomes even more powerful when paired with ",(0,i.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking"}),". This combination allows for rapid experimentation and iteration. You can effortlessly compare runs, making it easier to refine models and accelerate the journey from development to production deployment."]}),"\n",(0,i.jsx)(n.h3,{id:"dependency-management",children:"Dependency Management"}),"\n",(0,i.jsxs)(n.p,{children:["Deploy your LangChain application with confidence, leveraging MLflow's ability to ",(0,i.jsx)(n.a,{href:"/model/dependencies",children:"manage and record code and environment dependencies"})," automatically.\nYou can also explicitly declare external resource dependencies, like the LLM serving endpoint or vector search index queried by your LangChain application.\nThese dependencies are tracked by MLflow as model metadata, so that downstream serving systems can ensure authentication from your\ndeployed LangChain application to these dependent resources just works."]}),"\n",(0,i.jsx)(n.p,{children:"These features ensure consistency between development and production environments, reducing deployment risks with less manual intervention."}),"\n",(0,i.jsx)(n.h3,{id:"mlflow-evaluate",children:"MLflow Evaluate"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/llms/llm-evaluate",children:"MLflow Evaluate"})," provides native capabilities within MLflow to evaluate language models. With this feature you can easily utilize automated evaluation algorithms on the results of your LangChain application's inference results. This capability facilitates the efficient assessment of inference results from your LangChain application, ensuring robust performance analytics."]}),"\n",(0,i.jsx)(n.h3,{id:"observability",children:"Observability"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/tracing",children:"MLflow Tracing"})," is a new feature of MLflow that allows you to trace how data flows through your LangChain chain/agents/etc. This feature provides a visual representation of the data flow, making it easier to understand the behavior of your LangChain application and identify potential bottlenecks or issues. With its powerful ",(0,i.jsx)(n.a,{href:"/tracing/#automatic-tracing",children:"Automatic Tracing"})," capability, you can instrument your LangChain application without any code change but just running ",(0,i.jsx)(n.code,{children:"mlflow.langchain.autolog()"})," command once."]}),"\n",(0,i.jsx)(n.h2,{id:"automatic-logging",children:"Automatic Logging"}),"\n",(0,i.jsxs)(n.p,{children:["Autologging is a powerful one stop solution to achieve all the above benefits with just one line of code ",(0,i.jsx)(n.code,{children:"mlflow.langchain.autolog()"}),". By enabling autologging, you can automatically log all the components of your LangChain application, including chains, agents, and retrievers, with minimal effort. This feature simplifies the process of tracking and managing your LangChain application, allowing you to focus on developing and improving your models. For more information on how to use this feature, refer to the ",(0,i.jsx)(n.a,{href:"/llms/langchain/autologging",children:"MLflow LangChain Autologging Documentation"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"supported-elements-in-mlflow-langchain-integration",children:"Supported Elements in MLflow LangChain Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/agents/",children:"Agents"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/data_connection/retrievers/",children:"Retrievers"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://python.langchain.com/v0.1/docs/expression_language/interface/",children:"Runnables"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://langchain-ai.github.io/langgraph/reference/graphs/",children:"LangGraph Complied Graph"})," (only supported via ",(0,i.jsx)(n.a,{href:"#logging-models-from-code",children:"Model-from-Code"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/foundational/llm_chain",children:"LLMChain"})," (deprecated, only support for ",(0,i.jsx)(n.code,{children:"langchain<0.3.0"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://js.langchain.com/docs/modules/chains/popular/vector_db_qa",children:"RetrievalQA"})," (deprecated, only support for ",(0,i.jsx)(n.code,{children:"langchain<0.3.0"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["There is a known deserialization issue when logging chains or agents dependent upon LangChain components from ",(0,i.jsx)(n.a,{href:"https://python.langchain.com/v0.1/docs/integrations/platforms/#partner-packages",children:"the partner packages"})," such as ",(0,i.jsx)(n.code,{children:"langchain-openai"}),". If you log such models using the legacy serialization based logging, some components may be loaded from the respective ",(0,i.jsx)(n.code,{children:"langchain-community"})," package instead of the partner package library, which can lead to unexpected behavior or import errors when executing your code.\nTo avoid this issue, we strongly recommend using the ",(0,i.jsx)(n.a,{href:"#logging-models-from-code",children:"Model-from-Code"})," method for logging such models. This method allows you to bypass the model serialization and robustly save the model definition."]})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Logging chains/agents that include ",(0,i.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/chat/openai",children:"ChatOpenAI"})," and ",(0,i.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/chat/azure_chat_openai",children:"AzureChatOpenAI"})," requires ",(0,i.jsx)(n.code,{children:"MLflow>=2.12.0"})," and ",(0,i.jsx)(n.code,{children:"LangChain>=0.0.307"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"overview-of-chains-agents-and-retrievers",children:"Overview of Chains, Agents, and Retrievers"}),"\n",(0,i.jsxs)(l.A,{children:[(0,i.jsxs)(r.A,{label:"Chain",value:"chain",default:!0,children:[(0,i.jsx)(n.p,{children:"Sequences of actions or steps hardcoded in code. Chains in LangChain combine various components like prompts, models, and output parsers to create a flow of processing steps."}),(0,i.jsx)(n.p,{children:'The figure below shows an example of interfacing directly with a SaaS LLM via API calls with no context to the history of the conversation in the top portion. The\nbottom portion shows the same queries being submitted to a LangChain chain that incorporates a conversation history state such that the entire conversation\'s history\nis included with each subsequent input. Preserving conversational context in this manner is key to creating a "chat bot".'}),(0,i.jsx)("div",{style:{width:"80%",margin:"auto"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"The importance of stateful storage of conversation history for chat applications",src:a(14709).A+"",width:"1437",height:"1356"})})})]}),(0,i.jsxs)(r.A,{label:"Agents",value:"agents",children:[(0,i.jsx)(n.p,{children:"Dynamic constructs that use language models to choose a sequence of actions. Unlike chains, agents decide the order of actions based on inputs, tools available, and intermediate outcomes."}),(0,i.jsx)("div",{style:{width:"80%",margin:"auto"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Complex LLM queries with LangChain agents",src:a(45546).A+"",width:"1428",height:"863"})})})]}),(0,i.jsxs)(r.A,{label:"Retrievers",value:"retrievers",children:[(0,i.jsx)(n.p,{children:"Components in RetrievalQA chains responsible for sourcing relevant documents or data. Retrievers are key in applications where LLMs need to reference specific external information for accurate responses."}),(0,i.jsx)("div",{style:{width:"80%",margin:"auto"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"MLflow LangChain RetrievalQA architecture",src:a(7712).A+"",width:"1282",height:"1001"})})})]})]}),"\n",(0,i.jsx)(n.h2,{id:"getting-started-with-the-mlflow-langchain-flavor---tutorials-and-guides",children:"Getting Started with the MLflow LangChain Flavor - Tutorials and Guides"}),"\n",(0,i.jsx)(n.h3,{id:"introductory-tutorial",children:"Introductory Tutorial"}),"\n",(0,i.jsx)(n.p,{children:"In this introductory tutorial, you will learn the most fundamental components of LangChain and how to leverage the integration with MLflow to store, retrieve, and\nuse a chain."}),"\n",(0,i.jsx)(s.AC,{children:(0,i.jsx)(s._C,{link:"/llms/langchain/notebooks/langchain-quickstart",headerText:"LangChain Quickstart",text:["Get started with MLflow and LangChain by exploring the simplest possible chain configuration of a prompt and model chained to create a single-purpose utility application."]})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-tutorials",children:"Advanced Tutorials"}),"\n",(0,i.jsx)(n.p,{children:"In these tutorials, you can learn about more complex usages of LangChain with MLflow. It is highly advised to read through the introductory tutorial prior to\nexploring these more advanced use cases."}),"\n",(0,i.jsx)(s.AC,{children:(0,i.jsx)(s._C,{link:"/llms/langchain/notebooks/langchain-retriever",headerText:"RAG tutorial with LangChain",text:["Learn how to build a LangChain RAG with MLflow integration to answer highly specific questions about the legality of business ventures."]})}),"\n",(0,i.jsx)(n.h3,{id:"logging-models-from-code",children:"Logging models from Code"}),"\n",(0,i.jsx)(n.p,{children:"Since MLflow 2.12.2, MLflow introduced the ability to log LangChain models directly from a code definition."}),"\n",(0,i.jsx)(n.p,{children:"The feature provides several benefits to manage LangChain models:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Avoid Serialization Complication"}),": File handles, sockets, external connections, dynamic references, lambda functions and system resources are unpicklable. Some LangChain components do not support native serialization, e.g. ",(0,i.jsx)(n.code,{children:"RunnableLambda"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"No Pickling"}),": Loading a pickle or cloudpickle file in a Python version that was different than the one used to serialize the object does not guarantee compatibility."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Readability"}),": The serialized objects are often hardly readable by humans. Model-from-code allows you to review your model definition via code."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Refer to the ",(0,i.jsx)(n.a,{href:"/model/#models-from-code",children:"Models From Code feature documentation"})," for more information about this feature."]}),"\n",(0,i.jsxs)(n.p,{children:["In order to use this feature, you will utilize the ",(0,i.jsx)(c.B,{fn:"mlflow.models.set_model"})," API to define the chain that you would like to log as an MLflow model.\nAfter having this set within your code that defines your chain, when logging your model, you will specify the ",(0,i.jsx)(n.strong,{children:"path"})," to the file that defines your chain."]}),"\n",(0,i.jsx)(n.p,{children:"The following example demonstrates how to log a simple chain with this method:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Define the chain in a separate Python file"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["If you are using Jupyter Notebook, you can use the ",(0,i.jsx)(n.code,{children:"%%writefile"})," magic command to write the code cell directly to a file, without leaving the notebook to create it manually."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# %%writefile chain.py\n\nimport os\nfrom operator import itemgetter\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import OpenAI\n\nimport mlflow\n\nmlflow.set_experiment("Homework Helper")\n\nmlflow.langchain.autolog()\n\nprompt = PromptTemplate(\n    template="You are a helpful tutor that evaluates my homework assignments and provides suggestions on areas for me to study further."\n    " Here is the question: {question} and my answer which I got wrong: {answer}",\n    input_variables=["question", "answer"],\n)\n\n\ndef get_question(input):\n    default = "What is your name?"\n    if isinstance(input_data[0], dict):\n        return input_data[0].get("content").get("question", default)\n    return default\n\n\ndef get_answer(input):\n    default = "My name is Bobo"\n    if isinstance(input_data[0], dict):\n        return input_data[0].get("content").get("answer", default)\n    return default\n\n\nmodel = OpenAI(temperature=0.95)\n\nchain = (\n    {\n        "question": itemgetter("messages") | RunnableLambda(get_question),\n        "answer": itemgetter("messages") | RunnableLambda(get_answer),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nmlflow.models.set_model(chain)\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Then from the main notebook, log the model via supplying the path to the file that defines the chain:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pprint import pprint\n\nimport mlflow\n\nchain_path = "chain.py"\n\nwith mlflow.start_run():\n    info = mlflow.langchain.log_model(lc_model=chain_path, name="chain")\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The model defined in ",(0,i.jsx)(n.code,{children:"chain.py"})," is now logged to MLflow. You can load the model back and run inference:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Load the model and run inference\nhomework_chain = mlflow.langchain.load_model(model_uri=info.model_uri)\n\nexam_question = {\n    "messages": [\n        {\n            "role": "user",\n            "content": {\n                "question": "What is the primary function of control rods in a nuclear reactor?",\n                "answer": "To stir the primary coolant so that the neutrons are mixed well.",\n            },\n        },\n    ]\n}\n\nresponse = homework_chain.invoke(exam_question)\n\npprint(response)\n'})}),"\n",(0,i.jsx)(n.p,{children:"You can see the model is logged as a code on MLflow UI:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Logging a LangChain model from a code script file",src:a(11759).A+"",width:"2920",height:"1739"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"When logging models from code, make sure that your code does not contain any sensitive information, such as API keys, passwords, or other confidential data. The code will be stored in plain text in the MLflow model artifact, and anyone with access to the artifact will be able to view the code."})}),"\n",(0,i.jsx)(n.h2,{id:"detailed-documentation",children:(0,i.jsx)(n.a,{href:"/llms/langchain/guide/",children:"Detailed Documentation"})}),"\n",(0,i.jsx)(n.p,{children:"To learn more about the details of the MLflow LangChain flavor, read the detailed guide below."}),"\n",(0,i.jsx)(h.A,{to:"/llms/langchain/guide/",children:(0,i.jsx)("button",{className:"button button--primary",children:"View the Comprehensive Guide"})}),"\n",(0,i.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,i.jsx)(n.h3,{id:"i-cant-load-my-chain",children:"I can't load my chain!"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Allowing for Dangerous Deserialization"}),": Pickle opt-in logic in LangChain will prevent components from being loaded via MLflow. You might see an error like this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ValueError: This code relies on the pickle module. You will need to set allow_dangerous_deserialization=True if you want to opt-in to\nallow deserialization of data using pickle. Data can be compromised by a malicious actor if not handled properly to include a malicious\npayload that when deserialized with pickle can execute arbitrary code on your machine.\n"})}),"\n",(0,i.jsxs)(n.p,{children:["A change within LangChain that ",(0,i.jsx)(n.a,{href:"https://github.com/langchain-ai/langchain/pull/18696",children:"forces users to opt-in to pickle deserialization"})," can create\nsome issues with loading chains, vector stores, retrievers, and agents that have been logged using MLflow. Because the option is not exposed per component\nto set this argument on the loader function, you will need to ensure that you are setting this option directly within the defined loader function when\nlogging the model. LangChain components that do not set this value will be saved without issue, but a ",(0,i.jsx)(n.code,{children:"ValueError"})," will be raised when loading if unset."]}),"\n",(0,i.jsxs)(n.p,{children:["To fix this, simply re-log your model, specifying the option ",(0,i.jsx)(n.code,{children:"allow_dangerous_deserialization=True"})," in your defined loader function. See the tutorial\n",(0,i.jsx)(n.a,{href:"/llms/langchain/notebooks/langchain-retriever/#establishing-retrievalqa-chain-and-logging-with-mlflow",children:"for LangChain retrievers"})," for an example of specifying this\noption when logging a ",(0,i.jsx)(n.code,{children:"FAISS"})," vector store instance within a ",(0,i.jsx)(n.code,{children:"loader_fn"})," declaration."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"i-cant-save-my-chain-agent-or-retriever-with-mlflow",children:"I can't save my chain, agent, or retriever with MLflow."}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["If you're encountering issues with logging or saving LangChain components with MLflow, see the ",(0,i.jsx)(n.a,{href:"/model/#models-from-code",children:"models from code"}),"\nfeature documentation to determine if logging your model from a script file provides a simpler and more robust logging solution!"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Serialization Challenges with Cloudpickle"}),": Serialization with cloudpickle can encounter limitations depending on the complexity of the objects."]}),"\n",(0,i.jsx)(n.p,{children:"Some objects, especially those with intricate internal states or dependencies on external system resources, are not inherently pickleable. This limitation\narises because serialization essentially requires converting an object to a byte stream, which can be complex for objects tightly coupled with system states\nor those having external I/O operations. Try upgrading PyDantic to 2.x version to resolve this issue."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Verifying Native Serialization Support"}),": Ensure that the langchain object (chain, agent, or retriever) is serializable natively using langchain APIs if saving or logging with MLflow doesn't work."]}),"\n",(0,i.jsxs)(n.p,{children:["Due to their complex structures, not all langchain components are readily serializable. If native serialization\nis not supported and MLflow doesn't support saving the model, you can file an issue ",(0,i.jsx)(n.a,{href:"https://github.com/langchain-ai/langchain/issues",children:"in the LangChain repository"})," or\nask for guidance in the ",(0,i.jsx)(n.a,{href:"https://github.com/langchain-ai/langchain/discussions",children:"LangChain Discussions board"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Keeping Up with New Features in MLflow"}),": MLflow might not immediately support the latest LangChain features immediately."]}),"\n",(0,i.jsxs)(n.p,{children:["If a new feature is not supported in MLflow, consider ",(0,i.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/issues",children:"filing a feature request on the MLflow GitHub issues page"}),".\nWith the rapid pace of changes in libraries that are in heavy active development (such as ",(0,i.jsx)(n.a,{href:"https://pypi.org/project/langchain/#history",children:"LangChain's release velocity"}),"),\nbreaking changes, API refactoring, and fundamental functionality support for even existing features can cause integration issues. If there is a chain, agent,\nretriever, or any future structure within LangChain that you'd like to see supported, please let us know!"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"im-getting-an-attributeerror-when-saving-my-model",children:"I'm getting an AttributeError when saving my model"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Handling Dependency Installation in LangChain and MLflow"}),": LangChain and MLflow do not automatically install all dependencies."]}),"\n",(0,i.jsx)(n.p,{children:"Other packages that might be required for specific agents, retrievers, or tools may need to be explicitly defined when saving or logging your model.\nIf your model relies on these external component libraries (particularly for tools) that not included in the standard LangChain package, these dependencies\nwill not be automatically logged as part of the model at all times (see below for guidance on how to include them)."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Declaring Extra Dependencies"}),": Use the ",(0,i.jsx)(n.code,{children:"extra_pip_requirements"})," parameter when saving and logging."]}),"\n",(0,i.jsxs)(n.p,{children:["When saving or logging your model that contains external dependencies that are not part of the core langchain installation, you will need these additional\ndependencies. The model flavor contains two options for declaring these dependencies: ",(0,i.jsx)(n.code,{children:"extra_pip_requirements"})," and ",(0,i.jsx)(n.code,{children:"pip_requirements"}),". While specifying\n",(0,i.jsx)(n.code,{children:"pip_requirements"})," is entirely valid, we recommend using ",(0,i.jsx)(n.code,{children:"extra_pip_requirements"})," as it does not rely on defining all of the core dependent packages that\nare required to use the langchain model for inference (the other core dependencies will be inferred automatically)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"how-can-i-use-a-streaming-api-with-langchain",children:"How can I use a streaming API with LangChain?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Streaming with LangChain Models"}),": Ensure that the LangChain model supports a streaming response and use an MLflow version >= 2.12.2."]}),"\n",(0,i.jsxs)(n.p,{children:["As of the MLflow 2.12.2 release, LangChain models that support streaming responses that have been saved using MLflow 2.12.2 (or higher) can be loaded and used for\nstreamable inference using the ",(0,i.jsx)(n.code,{children:"predict_stream"})," API. Ensure that you are consuming the return type correctly, as the return from these models is a ",(0,i.jsx)(n.code,{children:"Generator"})," object.\nTo learn more, refer to the ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/models.html#how-to-load-and-score-python-function-models",children:"predict_stream guide"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"how-can-i-log-an-agent-built-with-langgraph-to-mlflow",children:"How can I log an agent built with LangGraph to MLflow?"}),"\n",(0,i.jsxs)(n.p,{children:["The LangGraph integration with MLflow is designed to utilize the ",(0,i.jsx)(n.a,{href:"/model/models-from-code",children:"Models From Code feature"}),"\nin MLflow to broaden and simplify the support of agent serialization."]}),"\n",(0,i.jsxs)(n.p,{children:["To log a LangGraph agent, you can define your agent code within a script, as shown below, saved to a file ",(0,i.jsx)(n.code,{children:"langgraph.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from typing import Literal\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nimport mlflow\n\n\n@tool\ndef get_weather(city: Literal["seattle", "sf"]):\n    """Use this to get weather information."""\n    if city == "seattle":\n        return "It\'s probably raining. Again."\n    elif city == "sf":\n        return "It\'s always sunny in sf"\n\n\nllm = ChatOpenAI()\ntools = [get_weather]\ngraph = create_react_agent(llm, tools)\n\n# specify the Agent as the model interface to be loaded when executing the script\nmlflow.models.set_model(graph)\n'})}),"\n",(0,i.jsx)(n.p,{children:"When you're ready to log this agent script definition to MLflow, you can refer to\nthis saved script directly when defining the model:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\ninput_example = {\n    "messages": [{"role": "user", "content": "what is the weather in seattle today?"}]\n}\n\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(\n        lc_model="./langgraph.py",  # specify the path to the LangGraph agent script definition\n        name="langgraph",\n        input_example=input_example,\n    )\n'})}),"\n",(0,i.jsx)(n.p,{children:"When the agent is loaded from MLflow, the script will be executed and the defined agent will be\nmade available for use for invocation."}),"\n",(0,i.jsx)(n.p,{children:"The agent can be loaded and used for inference as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'agent = mlflow.langchain.load_model(model_info.model_uri)\nquery = {\n    "messages": [\n        {\n            "role": "user",\n            "content": "Should I bring an umbrella today when I go to work in San Francisco?",\n        }\n    ]\n}\nagent.invoke(query)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"how-can-i-evaluate-a-langgraph-agent",children:"How can I evaluate a LangGraph Agent?"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/model-evaluation/index.html",children:"mlflow.evaluate"})," function provides\na robust way to evaluate model performance."]}),"\n",(0,i.jsxs)(n.p,{children:["LangGraph agents, especially those with chat functionality, can return multiple messages in one\ninference call. Given ",(0,i.jsx)(n.code,{children:"mlflow.evaluate"})," performs naive comparisons between raw predictions and a specified\nground truth value, it is the user's responsibility to reconcile potential differences prediction output\nand ground truth."]}),"\n",(0,i.jsxs)(n.p,{children:["Often, the best approach is to use a ",(0,i.jsx)(n.a,{href:"https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#evaluating-with-a-custom-function",children:"custom function"}),"\nto process the response. Below we provide an example of a custom function that extracts the last chat\nmessage from a LangGraph model. This function is then used in mlflow.evaluate to return a single\nstring response, which can be compared to the ",(0,i.jsx)(n.code,{children:'"ground_truth"'})," column."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom typing import List\n\n# Note that we assume the `model_uri` variable is present\n# Also note that registering and loading the model is optional and you\n# can simply leverage your langgraph object in the custom function.\nloaded_model = mlflow.langchain.load_model(model_uri)\n\neval_data = pd.DataFrame(\n    {\n        "inputs": [\n            "What is MLflow?",\n            "What is Spark?",\n        ],\n        "ground_truth": [\n            "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.",\n            "Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks",\n        ],\n    }\n)\n\n\ndef custom_langgraph_wrapper(inputs: pd.DataFrame) -> List[str]:\n    """Extract the predictions from a chat message sequence."""\n    answers = []\n    for content in inputs["inputs"]:\n        prediction = loaded_model.invoke(\n            {"messages": [{"role": "user", "content": content}]}\n        )\n        last_message_content = prediction["messages"][-1].content\n        answers.append(last_message_content)\n\n    return answers\n\n\nwith mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        custom_langgraph_wrapper,  # Pass our function defined above\n        data=eval_data,\n        targets="ground_truth",\n        model_type="question-answering",\n        extra_metrics=[\n            mlflow.metrics.latency(),\n            mlflow.metrics.genai.answer_correctness("openai:/gpt-4o"),\n        ],\n    )\nprint(results.metrics)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'{\n    "latency/mean": 1.8976624011993408,\n    "latency/variance": 0.10328687906900313,\n    "latency/p90": 2.1547686100006103,\n    "flesch_kincaid_grade_level/v1/mean": 12.1,\n    "flesch_kincaid_grade_level/v1/variance": 0.25,\n    "flesch_kincaid_grade_level/v1/p90": 12.5,\n    "ari_grade_level/v1/mean": 15.850000000000001,\n    "ari_grade_level/v1/variance": 0.06250000000000044,\n    "ari_grade_level/v1/p90": 16.05,\n    "exact_match/v1": 0.0,\n    "answer_correctness/v1/mean": 5.0,\n    "answer_correctness/v1/variance": 0.0,\n    "answer_correctness/v1/p90": 5.0,\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["For a complete example of a LangGraph model that works with this evaluation example, see the\n",(0,i.jsx)(n.a,{href:"https://mlflow.org/blog/langgraph-model-from-code",children:"MLflow LangGraph blog"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"how-to-control-whether-my-input-is-converted-to-listlangchainschemabasemessage-in-pyfunc-predict",children:"How to control whether my input is converted to List[langchain.schema.BaseMessage] in PyFunc predict?"}),"\n",(0,i.jsxs)(n.p,{children:["By default, MLflow converts chat request format input ",(0,i.jsx)(n.code,{children:'{"messages": [{"role": "user", "content": "some_question"}]}'})," to\nList[langchain.schema.BaseMessage] like ",(0,i.jsx)(n.code,{children:'[HumanMessage(content="some_question")]'})," for certain model types.\nTo force the conversion, set the environment variable ",(0,i.jsx)(n.code,{children:"MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN"})," to ",(0,i.jsx)(n.code,{children:"True"}),".\nTo disable this behavior, set the environment variable ",(0,i.jsx)(n.code,{children:"MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN"})," to ",(0,i.jsx)(n.code,{children:"False"})," as demonstrated below:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import json\nimport mlflow\nimport os\nfrom operator import itemgetter\nfrom langchain.schema.runnable import RunnablePassthrough\n\nmodel = RunnablePassthrough.assign(\n    problem=lambda x: x["messages"][-1]["content"]\n) | itemgetter("problem")\n\ninput_example = {\n    "messages": [\n        {\n            "role": "user",\n            "content": "Hello",\n        }\n    ]\n}\n# this model accepts the input_example\nassert model.invoke(input_example) == "Hello"\n\n# set this environment variable to avoid input conversion\nos.environ["MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN"] = "false"\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(\n        model, name="model", input_example=input_example\n    )\n\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nassert pyfunc_model.predict(input_example) == ["Hello"]\n'})})]})}function f(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>l,x:()=>r});var t=a(96540);const i={},o=t.createContext(i);function l(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(o.Provider,{value:n},e.children)}},45546:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/langchain-agents-250b50942032f0cd16fd75c653dfc606.png"},65537:(e,n,a)=>{a.d(n,{A:()=>_});var t=a(96540),i=a(34164),o=a(65627),l=a(56347),r=a(50372),s=a(30604),c=a(11861),h=a(78749);function d(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:a}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return d(e).map((e=>{let{props:{value:n,label:a,attributes:t,default:i}}=e;return{value:n,label:a,attributes:t,default:i}}))}(a);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function m(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:a}=e;const i=(0,l.W6)(),o=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,s.aZ)(o),(0,t.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(i.location.search);n.set(o,e),i.replace({...i.location,search:n.toString()})}),[o,i])]}function g(e){const{defaultValue:n,queryString:a=!1,groupId:i}=e,o=p(e),[l,s]=(0,t.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o}))),[c,d]=u({queryString:a,groupId:i}),[g,f]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,o]=(0,h.Dv)(a);return[i,(0,t.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:i}),w=(()=>{const e=c??g;return m({value:e,tabValues:o})?e:null})();(0,r.A)((()=>{w&&s(w)}),[w]);return{selectedValue:l,selectValue:(0,t.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),f(e)}),[d,f,o]),tabValues:o}}var f=a(9136);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=a(74848);function y(e){let{className:n,block:a,selectedValue:t,selectValue:l,tabValues:r}=e;const s=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),h=e=>{const n=e.currentTarget,a=s.indexOf(n),i=r[a].value;i!==t&&(c(n),l(i))},d=e=>{let n=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const a=s.indexOf(e.currentTarget)+1;n=s[a]??s[0];break}case"ArrowLeft":{const a=s.indexOf(e.currentTarget)-1;n=s[a]??s[s.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":a},n),children:r.map((e=>{let{value:n,label:a,attributes:o}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:e=>{s.push(e)},onKeyDown:d,onClick:h,...o,className:(0,i.A)("tabs__item",w.tabItem,o?.className,{"tabs__item--active":t===n}),children:a??n},n)}))})}function v(e){let{lazy:n,children:a,selectedValue:o}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=l.find((e=>e.props.value===o));return e?(0,t.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:l.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function j(e){const n=g(e);return(0,x.jsxs)("div",{className:(0,i.A)("tabs-container",w.tabList),children:[(0,x.jsx)(y,{...n,...e}),(0,x.jsx)(v,{...n,...e})]})}function _(e){const n=(0,f.A)();return(0,x.jsx)(j,{...e,children:d(e.children)},String(n))}},67756:(e,n,a)=>{a.d(n,{B:()=>s});a(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var i=a(29030),o=a(56289),l=a(74848);const r=e=>{const n=e.split(".");for(let a=n.length;a>0;a--){const e=n.slice(0,a).join(".");if(t[e])return e}return null};function s(e){let{fn:n,children:a}=e;const s=r(n);if(!s)return(0,l.jsx)(l.Fragment,{children:a});const c=(0,i.Ay)(`/${t[s]}#${n}`);return(0,l.jsx)(o.A,{to:c,target:"_blank",children:a??(0,l.jsxs)("code",{children:[n,"()"]})})}},79329:(e,n,a)=>{a.d(n,{A:()=>l});a(96540);var t=a(34164);const i={tabItem:"tabItem_Ymn6"};var o=a(74848);function l(e){let{children:n,hidden:a,className:l}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,t.A)(i.tabItem,l),hidden:a,children:n})}},86294:(e,n,a)=>{a.d(n,{Zp:()=>s,AC:()=>r,WO:()=>h,tf:()=>p,_C:()=>c,$3:()=>d,jK:()=>m});var t=a(34164);const i={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var o=a(56289),l=a(74848);const r=e=>{let{children:n,isSmall:a,cols:o}=e;return(0,l.jsx)("div",{className:(0,t.A)(i.CardGroup,a?i.AutofillColumns:o?i[`Cols${o}`]:i.MaxThreeColumns),children:n})},s=e=>{let{children:n,link:a=""}=e;return a?(0,l.jsx)(o.A,{className:(0,t.A)(i.Link,i.Card,i.CardBordered),to:a,children:n}):(0,l.jsx)("div",{className:(0,t.A)(i.Card,i.CardBordered),children:n})},c=e=>{let{headerText:n,link:a,text:o}=e;return(0,l.jsx)(s,{link:a,children:(0,l.jsxs)("span",{children:[(0,l.jsx)("div",{className:(0,t.A)(i.CardTitle,i.BoxRoot,i.PaddingBottom4),style:{pointerEvents:"none"},children:(0,l.jsx)("div",{className:(0,t.A)(i.BoxRoot,i.FlexFlex,i.FlexAlignItemsCenter,i.FlexDirectionRow,i.FlexJustifyContentFlexStart,i.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,l.jsx)("div",{className:(0,t.A)(i.BoxRoot,i.BoxHideIfEmpty,i.MarginTop4,i.MarginLeft4),style:{pointerEvents:"auto"},children:(0,l.jsx)("span",{className:"",children:n})})})}),(0,l.jsx)("span",{className:(0,t.A)(i.TextColor,i.CardBody),children:(0,l.jsx)("p",{children:o})})]})})},h=e=>{let{description:n,children:a,link:t}=e;return(0,l.jsx)(s,{link:t,children:(0,l.jsxs)("div",{className:i.LogoCardContent,children:[(0,l.jsx)("div",{className:i.LogoCardImage,children:a}),(0,l.jsx)("p",{className:i.TextColor,children:n})]})})},d=e=>{let{children:n,link:a}=e;return(0,l.jsx)(s,{link:a,children:(0,l.jsx)("div",{className:i.SmallLogoCardContent,children:(0,l.jsx)("div",{className:(0,t.A)("max-height-img-container",i.SmallLogoCardImage),children:n})})})},p=e=>{let{children:n,description:a,name:t,releaseVersion:r,learnMoreLink:c=""}=e;return(0,l.jsx)(s,{children:(0,l.jsxs)("div",{className:i.NewFeatureCardWrapper,children:[(0,l.jsxs)("div",{className:i.NewFeatureCardContent,children:[(0,l.jsxs)("div",{className:i.NewFeatureCardHeading,children:[t,(0,l.jsx)("br",{}),(0,l.jsx)("hr",{className:i.NewFeatureCardHeadingSeparator})]}),(0,l.jsx)("div",{className:i.LogoCardImage,children:n}),(0,l.jsx)("br",{}),(0,l.jsx)("p",{children:a}),(0,l.jsx)("br",{})]}),(0,l.jsxs)("div",{className:i.NewFeatureCardTags,children:[(0,l.jsx)("div",{children:c&&(0,l.jsx)(o.A,{className:"button button--outline button--sm button--primary",to:c,children:"Learn more"})}),(0,l.jsxs)(o.A,{className:"button button--outline button--sm button--primary",to:`https://github.com/mlflow/mlflow/releases/tag/v${r}`,children:["released in ",r]})]})]})})},m=e=>{let{title:n,description:a,link:o=""}=e;return(0,l.jsx)(s,{link:o,children:(0,l.jsxs)("div",{className:i.TitleCardContent,children:[(0,l.jsx)("div",{className:(0,t.A)(i.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:n}),(0,l.jsx)("hr",{className:(0,t.A)(i.TitleCardSeparator),style:{margin:"12px 0"}}),(0,l.jsx)("p",{className:(0,t.A)(i.TextColor),children:a})]})})}}}]);