"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[388],{17723:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"llms/langchain/autologging","title":"MLflow Langchain Autologging","description":"MLflow LangChain flavor supports autologging, a powerful feature that allows you to log crucial details about the LangChain model and execution without the need for explicit logging statements. MLflow LangChain autologging covers various aspects of the model, including traces, models, signatures and more.","source":"@site/docs/llms/langchain/autologging.mdx","sourceDirName":"llms/langchain","slug":"/llms/langchain/autologging","permalink":"/docs/latest/llms/langchain/autologging","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"MLflow LangChain Flavor","permalink":"/docs/latest/llms/langchain/"},"next":{"title":"LangChain within MLflow (Experimental)","permalink":"/docs/latest/llms/langchain/guide/"}}');var o=a(74848),l=a(28453),i=a(67756);const r={},s="MLflow Langchain Autologging",c={},h=[{value:"Quickstart",id:"quickstart",level:2},{value:"Configure Autologging",id:"configure-autologging",level:2},{value:"Example Code of LangChain Autologging",id:"example-code-of-langchain-autologging",level:2},{value:"Tracing LangGraph",id:"tracing-langgraph",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"MLflow Tracing Callbacks",id:"mlflow-tracing-callbacks",level:3},{value:"Customize Callback",id:"customize-callback",level:3},{value:"Patch Functions for Logging Artifacts",id:"patch-functions-for-logging-artifacts",level:3},{value:"FAQ",id:"faq",level:2},{value:"How to suppress the warning messages during autologging?",id:"how-to-suppress-the-warning-messages-during-autologging",level:3},{value:"I can&#39;t load the model logged by mlflow langchain autologging",id:"i-cant-load-the-model-logged-by-mlflow-langchain-autologging",level:3},{value:"How to customize span names in the traces?",id:"how-to-customize-span-names-in-the-traces",level:3},{value:"How to add extra metadata to a span?",id:"how-to-add-extra-metadata-to-a-span",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"mlflow-langchain-autologging",children:"MLflow Langchain Autologging"})}),"\n",(0,o.jsx)(n.p,{children:"MLflow LangChain flavor supports autologging, a powerful feature that allows you to log crucial details about the LangChain model and execution without the need for explicit logging statements. MLflow LangChain autologging covers various aspects of the model, including traces, models, signatures and more."}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsx)(n.p,{children:"MLflow LangChain Autologging is verified to be compatible with LangChain versions between 0.1.0 and 0.2.3. Outside of this range, the feature may not work as expected. To install the compatible version of LangChain, please run the following command:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"pip install mlflow[langchain] --upgrade\n"})}),"\n",(0,o.jsx)(n.h2,{id:"quickstart",children:"Quickstart"}),"\n",(0,o.jsxs)(n.p,{children:["To enable autologging for LangChain models, call ",(0,o.jsx)(i.B,{fn:"mlflow.langchain.autolog"})," at the beginning of your script or notebook. This will automatically log the traces by default as well as other artifacts such as models, input examples, and model signatures if you explicitly enable them. For more information about the configuration, please refer to the ",(0,o.jsx)(n.a,{href:"#configure-autologging",children:"Configure Autologging"})," section."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.langchain.autolog()\n\n# Enable other optional logging\n# mlflow.langchain.autolog(log_models=True, log_input_examples=True)\n\n# Your LangChain model code here\n...\n"})}),"\n",(0,o.jsx)(n.p,{children:"Once you have invoked the chain, you can view the logged traces and artifacts in the MLflow UI."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"LangChain Tracing via autolog",src:a(36458).A+"",width:"1876",height:"1080"})}),"\n",(0,o.jsx)(n.h2,{id:"configure-autologging",children:"Configure Autologging"}),"\n",(0,o.jsxs)(n.p,{children:["MLflow LangChain autologging can log various information about the model and its inference. ",(0,o.jsx)(n.strong,{children:"By default, only trace logging is enabled"}),", but you can enable autologging of other information by setting the corresponding parameters when calling ",(0,o.jsx)(i.B,{fn:"mlflow.langchain.autolog"}),". For other configurations, please refer to the API documentation."]}),"\n",(0,o.jsxs)("table",{children:[(0,o.jsx)("thead",{children:(0,o.jsxs)("tr",{children:[(0,o.jsx)("th",{children:"Target"}),(0,o.jsx)("th",{children:"Default"}),(0,o.jsx)("th",{children:"Parameter"}),(0,o.jsx)("th",{children:"Description"})]})}),(0,o.jsxs)("tbody",{children:[(0,o.jsxs)("tr",{children:[(0,o.jsx)("td",{children:"Traces"}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"true"})}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"log_traces"})}),(0,o.jsxs)("td",{children:["Whether to generate and log traces for the model. See ",(0,o.jsx)(n.a,{href:"/tracing/",children:"MLflow Tracing"})," for more details about tracing feature."]})]}),(0,o.jsxs)("tr",{children:[(0,o.jsx)("td",{children:"Model Artifacts"}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"false"})}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"log_models"})}),(0,o.jsxs)("td",{children:["If set to ",(0,o.jsx)(n.code,{children:"True"}),", the LangChain model will be logged when it is invoked. Supported models are ",(0,o.jsx)(n.code,{children:"Chain"}),", ",(0,o.jsx)(n.code,{children:"AgentExecutor"}),", ",(0,o.jsx)(n.code,{children:"BaseRetriever"}),", ",(0,o.jsx)(n.code,{children:"SimpleChatModel"}),", ",(0,o.jsx)(n.code,{children:"ChatPromptTemplate"}),", and subset of ",(0,o.jsx)(n.code,{children:"Runnable"})," types. Please refer to the ",(0,o.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/d2955cc90b6c5d7c931a8476b85f66e63990ca96/mlflow/langchain/utils.py#L183",children:"MLflow repository"})," for the full list of supported models."]})]}),(0,o.jsxs)("tr",{children:[(0,o.jsx)("td",{children:"Model Signatures"}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"false"})}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"log_model_signatures"})}),(0,o.jsxs)("td",{children:["If set to ",(0,o.jsx)(n.code,{children:"True"}),", ",(0,o.jsx)(i.B,{fn:"mlflow.models.ModelSignature",children:"ModelSignatures"})," describing model inputs and outputs are collected and logged along with Langchain model artifacts during inference. This option is only available when ",(0,o.jsx)(n.code,{children:"log_models"})," is enabled."]})]}),(0,o.jsxs)("tr",{children:[(0,o.jsx)("td",{children:"Input Example"}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"false"})}),(0,o.jsx)("td",{children:(0,o.jsx)(n.code,{children:"log_input_examples"})}),(0,o.jsxs)("td",{children:["If set to ",(0,o.jsx)(n.code,{children:"True"}),", input examples from inference data are collected and logged along with LangChain model artifacts during inference. This option is only available when ",(0,o.jsx)(n.code,{children:"log_models"})," is enabled."]})]})]})]}),"\n",(0,o.jsx)(n.p,{children:"For example, to disable logging of traces, and instead enable model logging, run the following code:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.langchain.autolog(\n    log_traces=False,\n    log_models=True,\n)\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["MLflow does not support automatic model logging for chains that contain retrievers. Saving retrievers requires additional ",(0,o.jsx)(n.code,{children:"loader_fn"})," and ",(0,o.jsx)(n.code,{children:"persist_dir"})," information for loading the model. If you want to log the model with retrievers, please log the model manually as shown in the ",(0,o.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/langchain/retriever_chain.py",children:"retriever_chain"})," example."]})}),"\n",(0,o.jsx)(n.h2,{id:"example-code-of-langchain-autologging",children:"Example Code of LangChain Autologging"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import os\nfrom operator import itemgetter\n\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda\n\nimport mlflow\n\n# Uncomment the following to use the full abilities of langchain autologgin\n# %pip install `langchain_community>=0.0.16`\n# These two libraries enable autologging to log text analysis related artifacts\n# %pip install textstat spacy\n\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\n\n# Enable mlflow langchain autologging\n# Note: We only support auto-logging models that do not contain retrievers\nmlflow.langchain.autolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    registered_model_name="lc_model",\n)\n\nprompt_with_history_str = """\nHere is a history between you and a human: {chat_history}\nNow, please answer this question: {question}\n"""\nprompt_with_history = PromptTemplate(\n    input_variables=["chat_history", "question"], template=prompt_with_history_str\n)\n\n\ndef extract_question(input):\n    return input[-1]["content"]\n\n\ndef extract_history(input):\n    return input[:-1]\n\n\nllm = OpenAI(temperature=0.9)\n\n# Build a chain with LCEL\nchain_with_history = (\n    {\n        "question": itemgetter("messages") | RunnableLambda(extract_question),\n        "chat_history": itemgetter("messages") | RunnableLambda(extract_history),\n    }\n    | prompt_with_history\n    | llm\n    | StrOutputParser()\n)\n\ninputs = {"messages": [{"role": "user", "content": "Who owns MLflow?"}]}\n\nprint(chain_with_history.invoke(inputs))\n# sample output:\n# "1. Databricks\\n2. Microsoft\\n3. Google\\n4. Amazon\\n\\nEnter your answer: 1\\n\\n\n# Correct! MLflow is an open source project developed by Databricks. ...\n\n# We automatically log the model and trace related artifacts\n# A model with name `lc_model` is registered, we can load it back as a PyFunc model\nmodel_name = "lc_model"\nmodel_version = 1\nloaded_model = mlflow.pyfunc.load_model(f"models:/{model_name}/{model_version}")\nprint(loaded_model.predict(inputs))\n'})}),"\n",(0,o.jsx)(n.h2,{id:"tracing-langgraph",children:"Tracing LangGraph"}),"\n",(0,o.jsxs)(n.p,{children:["MLflow support automatic tracing for LangGraph, an open-source library from LangChain for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. To enable auto-tracing for LangGraph, use the same ",(0,o.jsx)(i.B,{fn:"mlflow.langchain.autolog"})," function."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from typing import Literal\n\nimport mlflow\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\n# Enabling tracing for LangGraph (LangChain)\nmlflow.langchain.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("LangGraph")\n\n\n@tool\ndef get_weather(city: Literal["nyc", "sf"]):\n    """Use this to get weather information."""\n    if city == "nyc":\n        return "It might be cloudy in nyc"\n    elif city == "sf":\n        return "It\'s always sunny in sf"\n\n\nllm = ChatOpenAI(model="gpt-4o-mini")\ntools = [get_weather]\ngraph = create_react_agent(llm, tools)\n\n# Invoke the graph\nresult = graph.invoke(\n    {"messages": [{"role": "user", "content": "what is the weather in sf?"}]}\n)\n'})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsx)(n.p,{children:"MLflow does not support other auto-logging features for LangGraph, such as automatic model logging. Only traces are logged for LangGraph."})}),"\n",(0,o.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,o.jsxs)(n.p,{children:["MLflow LangChain Autologging uses two ways to log traces and other artifacts. Tracing is made possible via the ",(0,o.jsx)(n.a,{href:"https://python.langchain.com/v0.1/docs/modules/callbacks/",children:"Callbacks"})," framework of LangChain. Other artifacts are recorded by patching the invocation functions of the supported models. In typical scenarios, you don't need to care about the internal implementation details, but this section provides a brief overview of how it works under the hood."]}),"\n",(0,o.jsx)(n.h3,{id:"mlflow-tracing-callbacks",children:"MLflow Tracing Callbacks"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/mlflow/langchain/langchain_tracer.py",children:"MlflowLangchainTracer"})," is a callback handler that is injected into the langchain model inference process to log traces automatically. It starts a new span upon a set of actions of the chain such as ",(0,o.jsx)(n.code,{children:"on_chain_start"}),", ",(0,o.jsx)(n.code,{children:"on_llm_start"}),", and concludes it when the action is finished. Various metadata such as span type, action name, input, output, latency, are automatically recorded to the span."]}),"\n",(0,o.jsx)(n.h3,{id:"customize-callback",children:"Customize Callback"}),"\n",(0,o.jsxs)(n.p,{children:["Sometimes you may want to customize what information is logged in the traces. You can achieve this by creating a custom callback handler that inherits from ",(0,o.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/mlflow/langchain/langchain_tracer.py",children:"MlflowLangchainTracer"}),". The following example demonstrates how to record an additional attribute to the span when a chat model starts running."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from mlflow.langchain.langchain_tracer import MlflowLangchainTracer\n\n\nclass CustomLangchainTracer(MlflowLangchainTracer):\n    # Override the handler functions to customize the behavior. The method signature is defined by LangChain Callbacks.\n    def on_chat_model_start(\n        self,\n        serialized: Dict[str, Any],\n        messages: List[List[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[List[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ):\n        """Run when a chat model starts running."""\n        attributes = {\n            **kwargs,\n            **metadata,\n            # Add additional attribute to the span\n            "version": "1.0.0",\n        }\n\n        # Call the _start_span method at the end of the handler function to start a new span.\n        self._start_span(\n            span_name=name or self._assign_span_name(serialized, "chat model"),\n            parent_run_id=parent_run_id,\n            span_type=SpanType.CHAT_MODEL,\n            run_id=run_id,\n            inputs=messages,\n            attributes=kwargs,\n        )\n'})}),"\n",(0,o.jsx)(n.h3,{id:"patch-functions-for-logging-artifacts",children:"Patch Functions for Logging Artifacts"}),"\n",(0,o.jsx)(n.p,{children:"Other artifacts such as models are logged by patching the invocation functions of the supported models to insert the logging call. MLflow patches the following functions:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"invoke"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"batch"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"stream"})}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"get_relevant_documents"})," (for retrievers)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"__call__"})," (for Chains and AgentExecutors)"]}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"ainvoke"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"abatch"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"astream"})}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsxs)(n.p,{children:["MLflow supports autologging for async functions (e.g., ",(0,o.jsx)(n.code,{children:"ainvoke"}),", ",(0,o.jsx)(n.code,{children:"abatch"}),", ",(0,o.jsx)(n.code,{children:"astream"}),"), however, the logging operation is not\nasynchronous and may block the main thread. The invocation function itself is still not blocking and returns a coroutine object, but\nthe logging overhead may slow down the model inference process. Please be aware of this side effect when using async functions with autologging."]})}),"\n",(0,o.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,o.jsxs)(n.p,{children:["If you encounter any issues with MLflow LangChain flavor, please also refer to ",(0,o.jsx)(n.a,{href:"/llms/langchain/#faq",children:"FAQ"}),". If you still have questions, please feel free to open an issue in ",(0,o.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/issues",children:"MLflow Github repo"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"how-to-suppress-the-warning-messages-during-autologging",children:"How to suppress the warning messages during autologging?"}),"\n",(0,o.jsxs)(n.p,{children:["MLflow Langchain Autologging calls various logging functions and LangChain utilities under the hood. Some of them may\ngenerate warning messages that are not critical to the autologging process. If you want to suppress these warning messages, pass ",(0,o.jsx)(n.code,{children:"silent=True"})," to the ",(0,o.jsx)(i.B,{fn:"mlflow.langchain.autolog"})," function."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.langchain.autolog(silent=True)\n\n# No warning messages will be emitted from autologging\n"})}),"\n",(0,o.jsx)(n.h3,{id:"i-cant-load-the-model-logged-by-mlflow-langchain-autologging",children:"I can't load the model logged by mlflow langchain autologging"}),"\n",(0,o.jsx)(n.p,{children:"There are a few type of models that MLflow LangChain autologging does not support native saving or loading."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Model contains langchain retrievers"})}),"\n",(0,o.jsxs)(n.p,{children:["LangChain retrievers are not supported by MLflow autologging. If your model contains a retriever, you will need to manually log the model using the ",(0,o.jsx)(n.code,{children:"mlflow.langchain.log_model"})," API.\nAs loading those models requires specifying ",(0,o.jsx)(n.code,{children:"loader_fn"})," and ",(0,o.jsx)(n.code,{children:"persist_dir"})," parameters, please check examples in\n",(0,o.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/langchain/retriever_chain.py",children:"retriever_chain"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Can't pickle certain objects"})}),"\n",(0,o.jsxs)(n.p,{children:["For certain models that LangChain does not support native saving or loading, we will pickle the object when saving it. Due to this functionality, your cloudpickle version must be\nconsistent between the saving and loading environments to ensure that object references resolve properly. For further guarantees of correct object representation, you should ensure that your\nenvironment has ",(0,o.jsx)(n.code,{children:"pydantic"})," installed with at least version 2."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"how-to-customize-span-names-in-the-traces",children:"How to customize span names in the traces?"}),"\n",(0,o.jsxs)(n.p,{children:["By default, MLflow creates span names based on the class name in LangChain, such as ",(0,o.jsx)(n.code,{children:"ChatOpenAI"}),", ",(0,o.jsx)(n.code,{children:"RunnableLambda"}),", etc. If you want to customize the span names, you can do the following:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Pass ",(0,o.jsx)(n.code,{children:"name"})," parameter to the constructor of the LangChain class. This is useful when you want to set a specific name for a single component."]}),"\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.code,{children:"with_config"})," method to set the name for the runnables. You can pass the ",(0,o.jsx)(n.code,{children:'"run_name"'})," key to the config dictionary to set a name for a sub chain that contains multiple components."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Enable auto-tracing for LangChain\nmlflow.langchain.autolog()\n\n# Method 1: Pass `name` parameter to the constructor\nmodel = ChatOpenAI(name="custom-llm", model="gpt-4o-mini")\n# Method 2: Use `with_config` method to set the name for the runnables\nrunnable = (model | StrOutputParser()).with_config({"run_name": "custom-chain"})\n\nrunnable.invoke("Hi")\n'})}),"\n",(0,o.jsx)(n.p,{children:"The above code will create a trace like the following:"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Customize Span Names in LangChain Traces",src:a(80378).A+"",width:"1368",height:"459"})}),"\n",(0,o.jsx)(n.h3,{id:"how-to-add-extra-metadata-to-a-span",children:"How to add extra metadata to a span?"}),"\n",(0,o.jsxs)(n.p,{children:["You can record extra metadata to the span by passing the ",(0,o.jsx)(n.code,{children:"metadata"})," parameter of the LangChain's ",(0,o.jsx)(n.code,{children:"RunnableConfig"})," dictionary, either to the constructor or at runtime."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom langchain_openai import ChatOpenAI\n\n# Enable auto-tracing for LangChain\nmlflow.langchain.autolog()\n\n# Pass metadata to the constructor using `with_config` method\nmodel = ChatOpenAI(model="gpt-4o-mini").with_config({"metadata": {"key1": "value1"}})\n\n# Pass metadata at runtime using the `config` parameter\nmodel.invoke("Hi", config={"metadata": {"key2": "value2"}})\n'})}),"\n",(0,o.jsxs)(n.p,{children:["The metadata can be accessed in the ",(0,o.jsx)(n.code,{children:"Attributes"})," tab in the MLflow UI."]})]})}function m(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>r});var t=a(96540);const o={},l=t.createContext(o);function i(e){const n=t.useContext(l);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(l.Provider,{value:n},e.children)}},36458:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/tracing-top-dcca046565ab33be6afe0447dd328c22.gif"},67756:(e,n,a)=>{a.d(n,{B:()=>s});a(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var o=a(29030),l=a(56289),i=a(74848);const r=e=>{const n=e.split(".");for(let a=n.length;a>0;a--){const e=n.slice(0,a).join(".");if(t[e])return e}return null};function s(e){let{fn:n,children:a}=e;const s=r(n);if(!s)return(0,i.jsx)(i.Fragment,{children:a});const c=(0,o.Ay)(`/${t[s]}#${n}`);return(0,i.jsx)(l.A,{to:c,target:"_blank",children:a??(0,i.jsxs)("code",{children:[n,"()"]})})}},80378:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/langchain-name-customize-ca9014274c05b7f16d671e87f2ee7d5b.png"}}]);