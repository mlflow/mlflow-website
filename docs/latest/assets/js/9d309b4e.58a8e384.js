"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["6934"],{82954(e,n,t){t.r(n),t.d(n,{metadata:()=>i,default:()=>p,frontMatter:()=>d,contentTitle:()=>c,toc:()=>m,assets:()=>h});var i=JSON.parse('{"id":"flavors/llama-index/notebooks/llama_index_quickstart-ipynb","title":"Introduction to Using LlamaIndex with MLflow","description":"Download this notebook","source":"@site/docs/genai/flavors/llama-index/notebooks/llama_index_quickstart-ipynb.mdx","sourceDirName":"flavors/llama-index/notebooks","slug":"/flavors/llama-index/notebooks/llama_index_quickstart","permalink":"/mlflow-website/docs/latest/genai/flavors/llama-index/notebooks/llama_index_quickstart","draft":false,"unlisted":false,"editUrl":"https://github.com/mlflow/mlflow/edit/master/docs/docs/genai/flavors/llama-index/notebooks/llama_index_quickstart.ipynb","tags":[],"version":"current","frontMatter":{"custom_edit_url":"https://github.com/mlflow/mlflow/edit/master/docs/docs/genai/flavors/llama-index/notebooks/llama_index_quickstart.ipynb","slug":"llama_index_quickstart"},"sidebar":"genAISidebar","previous":{"title":"MLflow LlamaIndex Flavor","permalink":"/mlflow-website/docs/latest/genai/flavors/llama-index/"},"next":{"title":"Agents with LlamaIndex","permalink":"/mlflow-website/docs/latest/genai/flavors/llama-index/notebooks/llama_index_workflow_tutorial"}}'),a=t(74848),o=t(28453),s=t(75940),r=t(75453);t(66354);var l=t(42676);let d={custom_edit_url:"https://github.com/mlflow/mlflow/edit/master/docs/docs/genai/flavors/llama-index/notebooks/llama_index_quickstart.ipynb",slug:"llama_index_quickstart"},c="Introduction to Using LlamaIndex with MLflow",h={},m=[{value:"What you will learn",id:"what-you-will-learn",level:3},{value:"Setup",id:"setup",level:3},{value:"Create a Index",id:"create-a-index",level:3},{value:"Log the Index with MLflow",id:"log-the-index-with-mlflow",level:3},{value:"Load the Index and Perform Inference",id:"load-the-index-and-perform-inference",level:3},{value:"Explore the MLflow UI",id:"explore-the-mlflow-ui",level:3},{value:"Customization and Next Steps",id:"customization-and-next-steps",level:2}];function u(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"introduction-to-using-llamaindex-with-mlflow",children:"Introduction to Using LlamaIndex with MLflow"})}),"\n",(0,a.jsx)(l.O,{href:"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/genai/flavors/llama-index/notebooks/llama_index_quickstart.ipynb",children:"Download this notebook"}),"\n",(0,a.jsxs)(n.p,{children:["Welcome to this interactive tutorial designed to introduce you to ",(0,a.jsx)(n.a,{href:"https://www.llamaindex.ai/",children:"LlamaIndex"})," and its integration with MLflow. This tutorial is structured as a notebook to provide a hands-on, practical learning experience with the simplest and most core features of LlamaIndex."]}),"\n",(0,a.jsx)(n.h3,{id:"what-you-will-learn",children:"What you will learn"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this tutorial you will have:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Created an MVP VectorStoreIndex in LlamaIndex."}),"\n",(0,a.jsx)(n.li,{children:"Logged that index to the MLflow tracking server."}),"\n",(0,a.jsx)(n.li,{children:"Registered that index to the MLflow model registry."}),"\n",(0,a.jsx)(n.li,{children:"Loaded the model and performed inference."}),"\n",(0,a.jsx)(n.li,{children:"Explored the MLflow UI to learn about logged artifacts."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"These basics will familiarize you with the LlamaIndex user journey in MLlfow."}),"\n",(0,a.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,a.jsx)(n.p,{children:"First, we must ensure we have the required dependecies and environment variables. By default, LlamaIndex uses OpenAI as the source for LLMs and embeding models, so we'll do the same. Let's start by installing the requisite libraries and providing an OpenAI API key."}),"\n",(0,a.jsx)(s.d,{executionCount:" ",children:"%pip install 'mlflow[genai]>=2.15' llama-index>=0.10.44 -q"}),"\n",(0,a.jsx)(s.d,{executionCount:2,children:`import os
from getpass import getpass

from llama_index.core import Document, VectorStoreIndex
from llama_index.core.llms import ChatMessage

import mlflow

os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")`}),"\n",(0,a.jsx)(s.d,{executionCount:21,children:'assert "OPENAI_API_KEY" in os.environ, "Please set the OPENAI_API_KEY environment variable."'}),"\n",(0,a.jsx)(n.h3,{id:"create-a-index",children:"Create a Index"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/",children:"Vector store indexes"})," are one of the core components in LlamaIndex. They contain embedding vectors of ingested document chunks (and sometimes the document chunks as well). These vectors enable various types of inference, such as query engines, chat engines, and retrievers, each serving different purposes in LlamaIndex."]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Query Engine:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Usage:"})," Perform straightforward queries to retrieve relevant information based on a user's question."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scenario:"})," Ideal for fetching concise answers or documents matching specific queries, similar to a search engine."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Chat Engine:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Usage:"})," Engage in conversational AI tasks that require maintaining context and history over multiple interactions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scenario:"})," Suitable for interactive applications like customer support bots or virtual assistants, where conversation context is important."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Retriever:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Usage:"})," Retrieve documents or text segments that are semantically similar to a given input."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scenario:"})," Useful in retrieval-augmented generation (RAG) systems to fetch relevant context or background information, enhancing the quality of generated responses in tasks like summarization or question answering."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"By leveraging these different types of inference, LlamaIndex allows you to build robust AI applications tailored to various use cases, enhancing interaction between users and large language models."}),"\n",(0,a.jsx)(s.d,{executionCount:4,children:`print("------------- Example Document used to Enrich LLM Context -------------")
llama_index_example_document = Document.example()
print(llama_index_example_document)

index = VectorStoreIndex.from_documents([llama_index_example_document])

print("
------------- Example Query Engine -------------")
query_response = index.as_query_engine().query("What is llama_index?")
print(query_response)

print("
------------- Example Chat Engine  -------------")
chat_response = index.as_chat_engine().chat(
  "What is llama_index?",
  chat_history=[ChatMessage(role="system", content="You are an expert on RAG!")],
)
print(chat_response)


print("
------------- Example Retriever   -------------")
retriever_response = index.as_retriever().retrieve("What is llama_index?")
print(retriever_response)`}),"\n",(0,a.jsx)(r.p,{children:`------------- Example Document used to Enrich LLM Context -------------
Doc ID: e4c638ce-6757-482e-baed-096574550602
Text: Context LLMs are a phenomenal piece of technology for knowledge
generation and reasoning. They are pre-trained on large amounts of
publicly available data. How do we best augment LLMs with our own
private data? We need a comprehensive toolkit to help perform this
data augmentation for LLMs.  Proposed Solution That's where LlamaIndex
comes in. Ll...

------------- Example Query Engine -------------
LlamaIndex is a "data framework" designed to assist in building LLM apps by offering tools such as data connectors for various data sources, ways to structure data for easy use with LLMs, an advanced retrieval/query interface, and integrations with different application frameworks. It caters to both beginner and advanced users, providing a high-level API for simple data ingestion and querying, as well as lower-level APIs for customization and extension of different modules to suit individual needs.

------------- Example Chat Engine  -------------
LlamaIndex is a data framework designed to assist in building LLM apps by providing tools such as data connectors for various data sources, ways to structure data for easy use with LLMs, an advanced retrieval/query interface, and integrations with different application frameworks. It caters to both beginner and advanced users with a high-level API for easy data ingestion and querying, as well as lower-level APIs for customization and extension of different modules to suit specific needs.

------------- Example Retriever   -------------
[NodeWithScore(node=TextNode(id_='d18bb1f1-466a-443d-98d9-6217bf71ee5a', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e4c638ce-6757-482e-baed-096574550602', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'README.md', 'category': 'codebase'}, hash='3183371414f6a23e9a61e11b45ec45f808b148f9973166cfed62226e3505eb05')}, text='Context
LLMs are a phenomenal piece of technology for knowledge generation and reasoning.
They are pre-trained on large amounts of publicly available data.
How do we best augment LLMs with our own private data?
We need a comprehensive toolkit to help perform this data augmentation for LLMs.

Proposed Solution
That's where LlamaIndex comes in. LlamaIndex is a "data framework" to help
you build LLM  apps. It provides the following tools:

Offers data connectors to ingest your existing data sources and data formats
(APIs, PDFs, docs, SQL, etc.)
Provides ways to structure your data (indices, graphs) so that this data can be
easily used with LLMs.
Provides an advanced retrieval/query interface over your data:
Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.
Allows easy integrations with your outer application framework
(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).
LlamaIndex provides tools for both beginner users and advanced users.
Our high-level API allows beginner users to use LlamaIndex to ingest and
query their data in 5 lines of code. Our lower-level APIs allow advanced users to
customize and extend any module (data connectors, indices, retrievers, query engines,
reranking modules), to fit their needs.', mimetype='text/plain', start_char_idx=1, end_char_idx=1279, text_template='{metadata_str}

{content}', metadata_template='{key}: {value}', metadata_seperator='
'), score=0.850998849877966)]`}),"\n",(0,a.jsx)(n.h3,{id:"log-the-index-with-mlflow",children:"Log the Index with MLflow"}),"\n",(0,a.jsxs)(n.p,{children:["The below code logs a LlamaIndex model with MLflow, allowing you to persist and manage it across different environments. By using MLflow, you can track, version, and reproduce your model reliably. The script logs parameters, an example input, and registers the model under a specific name. The ",(0,a.jsx)(n.code,{children:"model_uri"})," provides a unique identifier for retrieving the model later. This persistence is essential for ensuring consistency and reproducibility in development, testing, and production. Managing the model with MLflow simplifies loading, deployment, and sharing, maintaining an organized workflow."]}),"\n",(0,a.jsx)(n.p,{children:"Key Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"engine_type"}),": defines the pyfunc and spark_udf inference type"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"input_example"}),": defines the the input signature and infers the output signature via a prediction"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"registered_model_name"}),": defines the name of the model in the MLflow model registry"]}),"\n"]}),"\n",(0,a.jsx)(s.d,{executionCount:5,children:`mlflow.llama_index.autolog()  # This is for enabling tracing

with mlflow.start_run() as run:
  mlflow.llama_index.log_model(
      index,
      name="llama_index",
      engine_type="query",  # Defines the pyfunc and spark_udf inference type
      input_example="hi",  # Infers signature
      registered_model_name="my_llama_index_vector_store",  # Stores an instance in the model registry
  )

  run_id = run.info.run_id
  model_uri = f"runs:/{run_id}/llama_index"
  print(f"Unique identifier for the model location for loading: {model_uri}")`}),"\n",(0,a.jsx)(r.p,{isStderr:!0,children:`2024/07/24 17:58:27 INFO mlflow.llama_index.serialize_objects: API key(s) will be removed from the global Settings object during serialization to protect against key leakage. At inference time, the key(s) must be passed as environment variables.
/Users/michael.berk/opt/anaconda3/envs/mlflow-dev/lib/python3.8/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.
warnings.warn("Setuptools is replacing distutils.")
Successfully registered model 'my_llama_index_vector_store'.
Created version '1' of model 'my_llama_index_vector_store'.`}),"\n",(0,a.jsx)(r.p,{children:"Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"}),"\n",(0,a.jsx)(r.p,{children:"Unique identifier for the model location for loading: runs:/036936a7ac964f0cb6ab99fa908d6421/llama_index"}),"\n",(0,a.jsx)(n.h3,{id:"load-the-index-and-perform-inference",children:"Load the Index and Perform Inference"}),"\n",(0,a.jsx)(n.p,{children:"The below code demonstrates three core types of inference that can be done with the loaded model."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Load and Perform Inference via LlamaIndex:"})," This method loads the model using ",(0,a.jsx)(n.code,{children:"mlflow.llama_index.load_model"})," and performs direct querying, chat, or retrieval. It is ideal when you want to leverage the full capabilities of the underlying llama index object."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Load and Perform Inference via MLflow PyFunc:"})," This method loads the model using ",(0,a.jsx)(n.code,{children:"mlflow.pyfunc.load_model"}),", enabling model predictions in a generic PyFunc format, with the engine type specified at logging time. It is useful for evaluating the model with ",(0,a.jsx)(n.code,{children:"mlflow.genai.evaluate"})," or deploying the model for serving."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Load and Perform Inference via MLflow Spark UDF:"})," This method uses ",(0,a.jsx)(n.code,{children:"mlflow.pyfunc.spark_udf"})," to load the model as a Spark UDF, facilitating distributed inference across large datasets in a Spark DataFrame. It is ideal for handling large-scale data processing and, like with PyFunc inference, only supports the engine type defined when logging."]}),"\n"]}),"\n",(0,a.jsx)(s.d,{executionCount:8,children:`print("
------------- Inference via Llama Index   -------------")
index = mlflow.llama_index.load_model(model_uri)
query_response = index.as_query_engine().query("hi")
print(query_response)

print("
------------- Inference via MLflow PyFunc -------------")
index = mlflow.pyfunc.load_model(model_uri)
query_response = index.predict("hi")
print(query_response)`}),"\n",(0,a.jsx)(r.p,{isStderr:!0,children:"2024/07/24 18:02:21 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API."}),"\n",(0,a.jsx)(r.p,{children:`
------------- Inference via Llama Index   -------------`}),"\n",(0,a.jsx)(r.p,{isStderr:!0,children:"2024/07/24 18:02:22 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API."}),"\n",(0,a.jsx)(r.p,{children:`Hello! How can I assist you today?

------------- Inference via MLflow PyFunc -------------
Hello! How can I assist you today?`}),"\n",(0,a.jsx)(s.d,{executionCount:6,children:`# Optional: Spark UDF inference
show_spark_udf_inference = False
if show_spark_udf_inference:
  print("
------------- Inference via MLflow Spark UDF -------------")
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.getOrCreate()

  udf = mlflow.pyfunc.spark_udf(spark, model_uri, result_type="string")
  df = spark.createDataFrame([("hi",), ("hello",)], ["text"])
  df.withColumn("response", udf("text")).toPandas()`}),"\n",(0,a.jsx)(n.h3,{id:"explore-the-mlflow-ui",children:"Explore the MLflow UI"}),"\n",(0,a.jsxs)(n.p,{children:["Finally, let's explore what's happening under the hood. To open the MLflow UI, run the following\ncell. Note that you can also run this in a new CLI window at the same directory that contains\nyour ",(0,a.jsx)(n.code,{children:"mlruns"})," folder, which by default will be this notebook's directory."]}),"\n",(0,a.jsx)(s.d,{executionCount:7,children:`import os
import subprocess

from IPython.display import IFrame

# Start the MLflow UI in a background process
mlflow_ui_command = ["mlflow", "ui", "--port", "5000"]
subprocess.Popen(
  mlflow_ui_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid
)`}),"\n",(0,a.jsx)(r.p,{children:"<subprocess.Popen at 0x7fbe09399ee0>"}),"\n",(0,a.jsx)(s.d,{executionCount:" ",children:`# Wait for the MLflow server to start then run the following command
# Note that cached results don't render, so you need to run this to see the UI
IFrame(src="http://localhost:5000", width=1000, height=600)`}),"\n",(0,a.jsx)(n.p,{children:"Let's navigate to the experiments tab in the top left of the screen and click on our most recent\nrun, as shown in the image below."}),"\n",(0,a.jsxs)(n.p,{children:["MLflow logs artifacts associated with your model and its environment during the MLflow run.\nMost of the logged files, such as the ",(0,a.jsx)(n.code,{children:"conda.yaml"}),", ",(0,a.jsx)(n.code,{children:"python_env.yml"}),", and\n",(0,a.jsx)(n.code,{children:"requirements.txt"})," are standard to all MLflow logging and facilitate reproducibility between\nenvironments. However, there are two sets of artifacts that are specific to LlamaIndex:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"index"}),": a directory that stores the serialized vector store. For more details, visit ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/storing/save_load/",children:"LlamaIndex's serialization docs"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"settings.json"}),": the serialized ",(0,a.jsx)(n.code,{children:"llama_index.core.Settings"})," service context. For more details, visit ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/",children:"LlamaIndex's Settings docs"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"By storing these objects, MLflow is able to recreate the environment in which you logged your model."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"llama_index_mlflow_ui_run",src:t(77270).A+"",width:"1762",height:"911"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Important:"})," MLflow will not serialize API keys. Those must be present in your model loading\nenvironment as environment variables."]}),"\n",(0,a.jsxs)(n.p,{children:["We also created a record of the model in the model registry. By simply specifying\n",(0,a.jsx)(n.code,{children:"registered_model_name"})," and ",(0,a.jsx)(n.code,{children:"input_example"})," when logging the model, we get robust signature\ninference and an instance in the model registry, as shown below."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"llama_index_mlflow_ui_registered_model",src:t(71899).A+"",width:"1607",height:"680"})}),"\n",(0,a.jsxs)(n.p,{children:["Finally, let's explore the traces we logged. In the ",(0,a.jsx)(n.code,{children:"Experiments"})," tab we can click on ",(0,a.jsx)(n.code,{children:"Tracing"})," to\nview the logged traces for our two inference calls. Tracing effectively shows a callback-based\nstacktrace for what ocurred in our inference system."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"llama_index_tracing_quickstart",src:t(39692).A+"",width:"1454",height:"784"})}),"\n",(0,a.jsx)(n.p,{children:"If we click on our first trace, we can see some really cool details about our inputs, outputs,\nand the duration of each step in the chain."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"llama_index_single_trace_quickstart",src:t(10422).A+"",width:"2922",height:"1656"})}),"\n",(0,a.jsx)(n.h2,{id:"customization-and-next-steps",children:"Customization and Next Steps"}),"\n",(0,a.jsxs)(n.p,{children:["When working with production systems, typically users leverage a customized service context, which can be done via LlamaIndex's ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/",children:"Settings"})," object."]})]})}function p(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},71899(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/llama_index_mlflow_ui_registered_model-95e77775bc60bf8f810e13699394e949.png"},77270(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/llama_index_mlflow_ui_run-d2570e3471609c9a78e643e999ea8c05.png"},10422(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/llama_index_single_trace_quickstart-d214fc3b3e5d95040d4e74cea5f02bf1.png"},39692(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/llama_index_tracing_quickstart-b3265e68e3b0d35172bc038f2b75d3ef.png"},75453(e,n,t){t.d(n,{p:()=>a});var i=t(74848);let a=({children:e,isStderr:n})=>(0,i.jsx)("pre",{style:{margin:0,borderRadius:0,background:"none",fontSize:"0.85rem",flexGrow:1,padding:"var(--padding-sm)"},children:e})},75940(e,n,t){t.d(n,{d:()=>o});var i=t(74848),a=t(37449);let o=({children:e,executionCount:n})=>(0,i.jsx)("div",{style:{flexGrow:1,minWidth:0,marginTop:"var(--padding-md)",width:"100%"},children:(0,i.jsx)(a.A,{className:"codeBlock_oJcR",language:"python",children:e})})},42676(e,n,t){t.d(n,{O:()=>s});var i=t(74848),a=t(96540);let o="3.9.1.dev0";function s({children:e,href:n}){let t=(0,a.useCallback)(async e=>{if(e.preventDefault(),window.gtag)try{window.gtag("event","notebook-download",{href:n})}catch{}o.includes("dev")||(n=n.replace(/\/master\//,`/v${o}/`));let t=await fetch(n),i=await t.blob(),a=window.URL.createObjectURL(i),s=document.createElement("a");s.style.display="none",s.href=a,s.download=n.split("/").pop(),document.body.appendChild(s),s.click(),window.URL.revokeObjectURL(a),document.body.removeChild(s)},[n]);return(0,i.jsx)("a",{className:"button button--primary",style:{marginBottom:"1rem",display:"block",width:"min-content"},href:n,download:!0,onClick:t,children:e})}},66354(e,n,t){t.d(n,{Q:()=>a});var i=t(74848);let a=({children:e})=>(0,i.jsx)("div",{style:{flexGrow:1,minWidth:0,fontSize:"0.8rem",width:"100%"},children:e})},52915(e,n,t){t.d(n,{A:()=>h});var i=t(74848);t(96540);var a=t(34164),o=t(71643),s=t(66697),r=t(92949),l=t(64560),d=t(47819);function c({language:e}){return(0,i.jsxs)("div",{className:(0,a.A)("codeBlockHeader_C_1e"),"aria-label":`Code block header for ${e} code with copy and toggle buttons`,children:[(0,i.jsx)("span",{className:"languageLabel_zr_I",children:e}),(0,i.jsx)(d.A,{})]})}function h({className:e}){let{metadata:n}=(0,o.Ph)(),t=n.language||"text";return(0,i.jsxs)(s.A,{as:"div",className:(0,a.A)(e,n.className),children:[n.title&&(0,i.jsx)("div",{className:"codeBlockTitle_d3dP",children:(0,i.jsx)(r.A,{children:n.title})}),(0,i.jsxs)("div",{className:"codeBlockContent_bxn0",children:[(0,i.jsx)(c,{language:t}),(0,i.jsx)(l.A,{})]})]})}}}]);