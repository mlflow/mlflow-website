"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["3406"],{60481(e,t,n){n.r(t),n.d(t,{metadata:()=>a,default:()=>L,frontMatter:()=>C,contentTitle:()=>G,toc:()=>M,assets:()=>D});var a=JSON.parse('{"id":"assessments/expectations","title":"Ground Truth Expectations","description":"MLflow Expectations provide a systematic way to capture ground truth - the correct or desired outputs that your AI should produce. By establishing these reference points, you create the foundation for meaningful evaluation and continuous improvement of your GenAI applications.","source":"@site/docs/genai/assessments/expectations.mdx","sourceDirName":"assessments","slug":"/assessments/expectations","permalink":"/mlflow-website/docs/latest/genai/assessments/expectations","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"sidebar_label":"Ground Truth Expectations"},"sidebar":"genAISidebar","previous":{"title":"Feedback Collection","permalink":"/mlflow-website/docs/latest/genai/assessments/feedback"},"next":{"title":"AI Issue Discovery","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/ai-insights/ai-issue-discovery"}}'),i=n(74848),o=n(28453),s=n(33508),r=n(34742),l=n(81956),c=n(98130),p=n(95986),d=n(10440),h=n(77541),m=n(54725),u=n(78010),f=n(57250),x=n(47792),_=n(46858),g=n(51004),y=n(22492),w=n(46534),v=n(80697),j=n(72216),b=n(3549),A=n(80964),N=n(57906),k=n(47504),T=n(76316),E=n(93893),S=n(45244),I=n(22864);let q=n.p+"assets/images/add_expectation_ui-5d57bfe9d75f1e2bedc36c493aacbd01.png",C={sidebar_label:"Ground Truth Expectations"},G="Ground Truth Expectations",D={},M=[{value:"What are Expectations?",id:"what-are-expectations",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Why Annotate Ground Truth?",id:"why-annotate-ground-truth",level:2},{value:"Types of Expectations",id:"types-of-expectations",level:2},{value:"Factual Expectations",id:"factual-expectations",level:3},{value:"Structured Expectations",id:"structured-expectations",level:3},{value:"Behavioral Expectations",id:"behavioral-expectations",level:3},{value:"Span-Level Expectations",id:"span-level-expectations",level:3},{value:"Step-by-Step Guides",id:"step-by-step-guides",level:2},{value:"Add Ground Truth Annotation via UI",id:"add-ground-truth-annotation-via-ui",level:3},{value:"Log Ground Truth via API",id:"log-ground-truth-via-api",level:3},{value:"Expectation Annotation Workflows",id:"expectation-annotation-workflows",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Be Specific and Measurable",id:"be-specific-and-measurable",level:3},{value:"Document Your Reasoning",id:"document-your-reasoning",level:3},{value:"Maintain Consistency",id:"maintain-consistency",level:3},{value:"Managing Expectations",id:"managing-expectations",level:2},{value:"Retrieving Expectations",id:"retrieving-expectations",level:3},{value:"Updating Expectations",id:"updating-expectations",level:3},{value:"Deleting Expectations",id:"deleting-expectations",level:3},{value:"Integration with Evaluation",id:"integration-with-evaluation",level:2},{value:"Next Steps",id:"next-steps",level:2}];function U(e){let t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"ground-truth-expectations",children:"Ground Truth Expectations"})}),"\n",(0,i.jsx)(t.p,{children:"MLflow Expectations provide a systematic way to capture ground truth - the correct or desired outputs that your AI should produce. By establishing these reference points, you create the foundation for meaningful evaluation and continuous improvement of your GenAI applications."}),"\n",(0,i.jsxs)(t.p,{children:["For complete API documentation and implementation details, see the ",(0,i.jsx)(m.B,{fn:"mlflow.log_expectation"})," reference."]}),"\n",(0,i.jsx)(t.h2,{id:"what-are-expectations",children:"What are Expectations?"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"/genai/concepts/expectations",children:"Expectations"}),' define the "gold standard" for what your AI should produce given specific inputs. They represent the correct answer, desired behavior, or ideal output as determined by domain experts. Think of expectations as the answer key against which actual AI performance is measured.']}),"\n",(0,i.jsxs)(t.p,{children:["Unlike ",(0,i.jsx)(t.a,{href:"/genai/assessments/feedback",children:"feedback"})," that evaluates what happened, expectations establish what should happen. They're always created by humans who have the expertise to define correct outcomes."]}),"\n",(0,i.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(t.p,{children:["Before using the ",(0,i.jsx)(m.B,{fn:"mlflow.log_expectation",children:"Expectations API"}),", ensure you have:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"MLflow 3.2.0 or later installed"}),"\n",(0,i.jsx)(t.li,{children:"An active MLflow tracking server or local tracking setup"}),"\n",(0,i.jsx)(t.li,{children:"Traces that have been logged from your GenAI application to an MLflow Experiment"}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"why-annotate-ground-truth",children:"Why Annotate Ground Truth?"}),"\n",(0,i.jsx)(s.A,{features:[{icon:x.A,title:"Create Evaluation Baselines",description:"Establish reference points for objective accuracy measurement. Without ground truth, you can't measure how well your AI performs against known correct answers."},{icon:_.A,title:"Enable Systematic Testing",description:"Transform ad-hoc testing into systematic evaluation by building datasets of expected outputs to consistently measure performance across versions and configurations."},{icon:g.A,title:"Support Fine-Tuning and Training",description:"Create high-quality training data from ground truth annotations. Essential for fine-tuning models and training automated evaluators."},{icon:y.A,title:"Establish Quality Standards",description:"Codify quality requirements and transform implicit knowledge into explicit, measurable criteria that everyone can understand and follow."}]}),"\n",(0,i.jsx)(t.h2,{id:"types-of-expectations",children:"Types of Expectations"}),"\n",(0,i.jsx)(p.A,{children:(0,i.jsxs)(u.A,{children:[(0,i.jsxs)(f.A,{value:"factual",label:"Factual",default:!0,children:[(0,i.jsx)(t.h3,{id:"factual-expectations",children:"Factual Expectations"}),(0,i.jsx)(t.p,{children:"For questions with definitive answers:"}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'mlflow.log_expectation(\n    trace_id=trace_id,\n    name="expected_answer",\n    value="The speed of light in vacuum is 299,792,458 meters per second",\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.HUMAN,\n        source_id="physics_expert@university.edu",\n    ),\n)\n'})})]}),(0,i.jsxs)(f.A,{value:"structured",label:"Structured",children:[(0,i.jsx)(t.h3,{id:"structured-expectations",children:"Structured Expectations"}),(0,i.jsx)(t.p,{children:"For complex outputs with multiple components:"}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'mlflow.log_expectation(\n    trace_id=trace_id,\n    name="expected_extraction",\n    value={\n        "company": "TechCorp Inc.",\n        "sentiment": "positive",\n        "key_topics": ["product_launch", "quarterly_earnings", "market_expansion"],\n        "action_required": True,\n    },\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.HUMAN, source_id="business_analyst@company.com"\n    ),\n)\n'})})]}),(0,i.jsxs)(f.A,{value:"behavioral",label:"Behavioral",children:[(0,i.jsx)(t.h3,{id:"behavioral-expectations",children:"Behavioral Expectations"}),(0,i.jsx)(t.p,{children:"For defining how the AI should act:"}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'mlflow.log_expectation(\n    trace_id=trace_id,\n    name="expected_behavior",\n    value={\n        "should_escalate": True,\n        "required_elements": ["empathy", "solution_offer", "follow_up"],\n        "max_response_length": 150,\n        "tone": "professional_friendly",\n    },\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.HUMAN,\n        source_id="customer_success_lead@company.com",\n    ),\n)\n'})})]}),(0,i.jsxs)(f.A,{value:"span_level",label:"Span-Level",children:[(0,i.jsx)(t.h3,{id:"span-level-expectations",children:"Span-Level Expectations"}),(0,i.jsx)(t.p,{children:"For specific operations within your AI pipeline:"}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Expected documents for RAG retrieval\nmlflow.log_expectation(\n    trace_id=trace_id,\n    span_id=retrieval_span_id,\n    name="expected_documents",\n    value=["policy_doc_2024", "faq_section_3", "user_guide_ch5"],\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.HUMAN,\n        source_id="information_architect@company.com",\n    ),\n)\n'})})]})]})}),"\n",(0,i.jsx)(t.h2,{id:"step-by-step-guides",children:"Step-by-Step Guides"}),"\n",(0,i.jsx)(t.h3,{id:"add-ground-truth-annotation-via-ui",children:"Add Ground Truth Annotation via UI"}),"\n",(0,i.jsx)(t.p,{children:"The MLflow UI provides an intuitive way to add expectations directly to traces. This approach is ideal for domain experts who need to define ground truth without writing code, and for collaborative annotation workflows where multiple stakeholders contribute different perspectives."}),"\n",(0,i.jsx)(c.A,{title:"Show Step-by-Step Instructions (8 steps)",defaultExpanded:!1,children:(0,i.jsx)(l.A,{steps:[{icon:w.A,title:"Navigate to your experiment",description:"Select the trace containing the interaction you want to annotate"},{icon:v.A,title:'Click "Add Assessment" button',description:"Access the assessment creation form on the trace detail page"},{icon:j.A,title:'Select "Expectation" from dropdown',description:"Choose Assessment Type to define ground truth rather than feedback"},{icon:b.A,title:"Enter a descriptive name",description:'Use clear names like "expected_answer", "correct_classification", or "expected_documents"'},{icon:A.A,title:"Choose the appropriate data type",description:"Select String for text, JSON for structured data, Boolean for binary expectations"},{icon:N.A,title:"Enter the ground truth value",description:"Define what the AI should have produced for this specific input"},{icon:k.A,title:"Add rationale explaining correctness",description:"Document why this is the correct or expected output for future reference"},{icon:x.A,title:'Click "Create" to record expectation',description:"Save your ground truth annotation to establish the quality baseline"}],screenshot:{src:q,alt:"Add Expectation"}})}),"\n",(0,i.jsx)(t.p,{children:"The expectation will be immediately attached to the trace, establishing the ground truth reference for future evaluation."}),"\n",(0,i.jsx)(t.h3,{id:"log-ground-truth-via-api",children:"Log Ground Truth via API"}),"\n",(0,i.jsxs)(t.p,{children:["Use the programmatic ",(0,i.jsx)(m.B,{fn:"mlflow.log_expectation"})," API when you need to automate expectation creation, integrate with existing annotation tools, or build custom ground truth collection workflows."]}),"\n",(0,i.jsxs)(u.A,{children:[(0,i.jsxs)(f.A,{value:"single",label:"Single Annotations",default:!0,children:[(0,i.jsx)(t.p,{children:"Programmatically create expectations for systematic ground truth collection:"}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"1. Set up your annotation environment:"})}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import AssessmentSource\nfrom mlflow.entities.assessment_source import AssessmentSourceType\n\n# Define your domain expert source\nexpert_source = AssessmentSource(\n    source_type=AssessmentSourceType.HUMAN, source_id="domain_expert@company.com"\n)\n'})}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"2. Create expectations for different data types:"})}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'def log_factual_expectation(trace_id, question, correct_answer):\n    """Log expectation for factual questions."""\n    mlflow.log_expectation(\n        trace_id=trace_id,\n        name="expected_factual_answer",\n        value=correct_answer,\n        source=expert_source,\n        metadata={\n            "question": question,\n            "expectation_type": "factual",\n            "confidence": "high",\n            "verified_by": "subject_matter_expert",\n        },\n    )\n\n\ndef log_structured_expectation(trace_id, expected_extraction):\n    """Log expectation for structured data extraction."""\n    mlflow.log_expectation(\n        trace_id=trace_id,\n        name="expected_extraction",\n        value=expected_extraction,\n        source=expert_source,\n        metadata={\n            "expectation_type": "structured",\n            "schema_version": "v1.0",\n            "annotation_guidelines": "company_extraction_standards_v2",\n        },\n    )\n\n\ndef log_behavioral_expectation(trace_id, expected_behavior):\n    """Log expectation for AI behavior patterns."""\n    mlflow.log_expectation(\n        trace_id=trace_id,\n        name="expected_behavior",\n        value=expected_behavior,\n        source=expert_source,\n        metadata={\n            "expectation_type": "behavioral",\n            "behavior_category": "customer_service",\n            "compliance_requirement": "company_policy_v3",\n        },\n    )\n'})}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"3. Use the functions in your annotation workflow:"})}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Example: Annotating a customer service interaction\ntrace_id = "tr-customer-service-001"\n\n# Define what the AI should have said\nfactual_answer = "Your account balance is $1,234.56 as of today."\nlog_factual_expectation(trace_id, "What is my account balance?", factual_answer)\n\n# Define expected data extraction\nexpected_extraction = {\n    "intent": "account_balance_inquiry",\n    "account_type": "checking",\n    "urgency": "low",\n    "requires_authentication": True,\n}\nlog_structured_expectation(trace_id, expected_extraction)\n\n# Define expected behavior\nexpected_behavior = {\n    "should_verify_identity": True,\n    "tone": "professional_helpful",\n    "should_offer_additional_help": True,\n    "escalation_required": False,\n}\nlog_behavioral_expectation(trace_id, expected_behavior)\n'})})]}),(0,i.jsxs)(f.A,{value:"batch",label:"Batch Annotations",children:[(0,i.jsx)(t.p,{children:"For large-scale ground truth collection, use batch annotation:"}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"1. Define the batch annotation function:"})}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'def annotate_batch_expectations(annotation_data):\n    """Annotate multiple traces with ground truth expectations."""\n    for item in annotation_data:\n        try:\n            mlflow.log_expectation(\n                trace_id=item["trace_id"],\n                name=item["expectation_name"],\n                value=item["expected_value"],\n                source=AssessmentSource(\n                    source_type=AssessmentSourceType.HUMAN,\n                    source_id=item["annotator_id"],\n                ),\n                metadata={\n                    "batch_id": item["batch_id"],\n                    "annotation_session": item["session_id"],\n                    "quality_checked": True,\n                },\n            )\n            print(f"\u2713 Annotated {item[\'trace_id\']}")\n        except Exception as e:\n            print(f"\u2717 Failed to annotate {item[\'trace_id\']}: {e}")\n'})}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"2. Prepare your annotation data:"})}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Example batch annotation data\nbatch_data = [\n    {\n        "trace_id": "tr-001",\n        "expectation_name": "expected_answer",\n        "expected_value": "Paris is the capital of France",\n        "annotator_id": "expert1@company.com",\n        "batch_id": "geography_qa_batch_1",\n        "session_id": "session_2024_01_15",\n    },\n    {\n        "trace_id": "tr-002",\n        "expectation_name": "expected_answer",\n        "expected_value": "The speed of light is 299,792,458 m/s",\n        "annotator_id": "expert2@company.com",\n        "batch_id": "physics_qa_batch_1",\n        "session_id": "session_2024_01_15",\n    },\n]\n'})}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"3. Execute batch annotation:"})}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"annotate_batch_expectations(batch_data)\n"})})]})]}),"\n",(0,i.jsx)(t.h2,{id:"expectation-annotation-workflows",children:"Expectation Annotation Workflows"}),"\n",(0,i.jsx)(t.p,{children:"Different stages of your AI development lifecycle require different approaches to expectation annotation. The following workflows help you systematically create and maintain ground truth expectations that align with your development process and quality goals."}),"\n",(0,i.jsx)(r.A,{concepts:[{icon:A.A,title:"Development Phase",description:"Define success criteria by identifying test scenarios, creating expectations with domain experts, testing AI outputs, and iterating on configurations until expectations are met."},{icon:T.A,title:"Production Monitoring",description:"Enable systematic quality tracking by sampling production traces, adding expectations to create evaluation datasets, and tracking performance trends over time."},{icon:E.A,title:"Collaborative Annotation",description:"Use team-based annotation where domain experts define initial expectations, review committees validate and refine, and consensus building resolves disagreements."}]}),"\n",(0,i.jsx)(t.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(t.h3,{id:"be-specific-and-measurable",children:"Be Specific and Measurable"}),"\n",(0,i.jsx)(t.p,{children:"Vague expectations lead to inconsistent evaluation. Define clear, specific criteria that can be objectively verified."}),"\n",(0,i.jsx)(t.h3,{id:"document-your-reasoning",children:"Document Your Reasoning"}),"\n",(0,i.jsx)(t.p,{children:"Use metadata to explain why an expectation is defined a certain way:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'mlflow.log_expectation(\n    trace_id=trace_id,\n    name="expected_diagnosis",\n    value={\n        "primary": "Type 2 Diabetes",\n        "risk_factors": ["obesity", "family_history"],\n        "recommended_tests": ["HbA1c", "fasting_glucose"],\n    },\n    metadata={\n        "guideline_version": "ADA_2024",\n        "confidence": "high",\n        "based_on": "clinical_presentation_and_history",\n    },\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.HUMAN, source_id="endocrinologist@hospital.org"\n    ),\n)\n'})}),"\n",(0,i.jsx)(t.h3,{id:"maintain-consistency",children:"Maintain Consistency"}),"\n",(0,i.jsx)(t.p,{children:"Use standardized naming and structure across your expectations to enable meaningful analysis and comparison."}),"\n",(0,i.jsx)(t.h2,{id:"managing-expectations",children:"Managing Expectations"}),"\n",(0,i.jsx)(t.p,{children:"Once you've defined expectations for your traces, you may need to retrieve, update, or delete them to maintain accurate ground truth data."}),"\n",(0,i.jsx)(t.h3,{id:"retrieving-expectations",children:"Retrieving Expectations"}),"\n",(0,i.jsx)(t.p,{children:"Retrieve specific expectations to analyze your ground truth data:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Get a specific expectation by ID\nexpectation = mlflow.get_assessment(\n    trace_id="tr-1234567890abcdef", assessment_id="a-0987654321abcdef"\n)\n\n# Access expectation details\nname = expectation.name\nvalue = expectation.value\nsource_type = expectation.source.source_type\nmetadata = expectation.metadata if hasattr(expectation, "metadata") else None\n'})}),"\n",(0,i.jsx)(t.h3,{id:"updating-expectations",children:"Updating Expectations"}),"\n",(0,i.jsx)(t.p,{children:"Update existing expectations when ground truth needs refinement:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from mlflow.entities import Expectation\n\n# Update expectation with corrected information\nupdated_expectation = Expectation(\n    name="expected_answer",\n    value="The capital of France is Paris, located in the \xcele-de-France region",\n)\n\nmlflow.update_assessment(\n    trace_id="tr-1234567890abcdef",\n    assessment_id="a-0987654321abcdef",\n    assessment=updated_expectation,\n)\n'})}),"\n",(0,i.jsx)(t.h3,{id:"deleting-expectations",children:"Deleting Expectations"}),"\n",(0,i.jsx)(t.p,{children:"Remove expectations that were logged incorrectly:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Delete specific expectation\nmlflow.delete_assessment(\n    trace_id="tr-1234567890abcdef", assessment_id="a-5555666677778888"\n)\n'})}),"\n",(0,i.jsx)(t.h2,{id:"integration-with-evaluation",children:"Integration with Evaluation"}),"\n",(0,i.jsx)(t.p,{children:"Expectations are most powerful when combined with systematic evaluation:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Automated scoring"})," against expectations"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Human feedback"})," on expectation achievement"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Gap analysis"})," between expected and actual"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Performance metrics"})," based on expectation matching"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(d.A,{children:[(0,i.jsx)(h.A,{icon:S.A,iconSize:48,title:"Expectations Concepts",description:"Deep dive into expectations architecture and schema",href:"/genai/concepts/expectations",linkText:"Learn more \u2192",containerHeight:64}),(0,i.jsx)(h.A,{icon:k.A,iconSize:48,title:"Automated and Human Feedback",description:"Learn how to collect quality evaluations from multiple sources",href:"/genai/assessments/feedback",linkText:"Start collecting \u2192",containerHeight:64}),(0,i.jsx)(h.A,{icon:I.A,iconSize:48,title:"LLM Evaluation",description:"Learn how to systematically evaluate and improve your GenAI applications",href:"/genai/eval-monitor",linkText:"Start evaluating \u2192",containerHeight:64})]})]})}function L(e={}){let{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(U,{...e})}):U(e)}},46858(e,t,n){n.d(t,{A:()=>a});let a=(0,n(75689).A)("zap",[["path",{d:"M4 14a1 1 0 0 1-.78-1.63l9.9-10.2a.5.5 0 0 1 .86.46l-1.92 6.02A1 1 0 0 0 13 10h7a1 1 0 0 1 .78 1.63l-9.9 10.2a.5.5 0 0 1-.86-.46l1.92-6.02A1 1 0 0 0 11 14z",key:"1xq2db"}]])},54725(e,t,n){n.d(t,{B:()=>s});var a=n(74848);n(96540);var i=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),o=n(66497);function s({fn:e,children:t,hash:n}){let s=(e=>{let t=e.split(".");for(let e=t.length;e>0;e--){let n=t.slice(0,e).join(".");if(i[n])return n}return null})(e);if(!s)return(0,a.jsx)(a.Fragment,{children:t});let r=(0,o.default)(`/${i[s]}#${n??e}`);return(0,a.jsx)("a",{href:r,target:"_blank",children:t??(0,a.jsxs)("code",{children:[e,"()"]})})}},98130(e,t,n){n.d(t,{A:()=>r});var a=n(74848),i=n(96540),o=n(72102),s=n(75107);let r=({children:e,title:t,defaultExpanded:n=!1})=>{let[r,l]=(0,i.useState)(n);return(0,a.jsxs)("div",{className:"collapsibleContainer_Oabu",children:[(0,a.jsx)("div",{className:"header_N69Q",children:(0,a.jsxs)("button",{className:"toggleButton_gd1o",onClick:()=>l(!r),"aria-expanded":r,children:[(0,a.jsx)("span",{className:"toggleText_VQQb",children:t}),r?(0,a.jsx)(o.A,{size:20}):(0,a.jsx)(s.A,{size:20})]})}),r&&(0,a.jsx)("div",{className:"content_HWPC",children:e})]})}},34742(e,t,n){n.d(t,{A:()=>i});var a=n(74848);n(96540);function i({concepts:e,title:t}){return(0,a.jsxs)("div",{className:"conceptOverview_x8T_",children:[t&&(0,a.jsx)("h3",{className:"overviewTitle_HyAI",children:t}),(0,a.jsx)("div",{className:"conceptGrid_uJNV",children:e.map((e,t)=>(0,a.jsxs)("div",{className:"conceptCard_oday",children:[(0,a.jsxs)("div",{className:"conceptHeader_HCk5",children:[e.icon&&(0,a.jsx)("div",{className:"conceptIcon_gejw",children:(0,a.jsx)(e.icon,{size:20})}),(0,a.jsx)("h4",{className:"conceptTitle_TGMM",children:e.title})]}),(0,a.jsx)("p",{className:"conceptDescription_ZyDn",children:e.description})]},t))})]})}},33508(e,t,n){n.d(t,{A:()=>i});var a=n(74848);n(96540);function i({features:e,col:t=2}){return(0,a.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${t}, 1fr)`},children:e.map((e,t)=>(0,a.jsxs)("div",{className:"highlightItem_XPnN",children:[e.icon&&(0,a.jsx)("div",{className:"highlightIcon_SUR8",children:(0,a.jsx)(e.icon,{size:24})}),(0,a.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,a.jsx)("h4",{children:e.title}),(0,a.jsx)("p",{children:e.description})]})]},t))})}},46077(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var i=n(66497);function o({src:e,alt:t,width:n,caption:o,className:s}){return(0,a.jsxs)("div",{className:`container_JwLF ${s||""}`,children:[(0,a.jsx)("div",{className:"imageWrapper_RfGN",style:n?{width:n}:{},children:(0,a.jsx)("img",{src:(0,i.default)(e),alt:t,className:"image_bwOA"})}),o&&(0,a.jsx)("p",{className:"caption_jo2G",children:o})]})}},95986(e,t,n){n.d(t,{A:()=>i});var a=n(74848);n(96540);function i({children:e}){return(0,a.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,t,n){n.d(t,{A:()=>c});var a=n(74848);n(96540);var i=n(95310),o=n(34164);let s="tileImage_O4So";var r=n(66497),l=n(92802);function c({icon:e,image:t,imageDark:n,imageWidth:c,imageHeight:p,iconSize:d=32,containerHeight:h,title:m,description:u,href:f,linkText:x="Learn more \u2192",className:_}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let g=h?{height:`${h}px`}:{},y={};return c&&(y.width=`${c}px`),p&&(y.height=`${p}px`),(0,a.jsxs)(i.A,{href:f,className:(0,o.A)("tileCard_NHsj",_),children:[(0,a.jsx)("div",{className:"tileIcon_pyoR",style:g,children:e?(0,a.jsx)(e,{size:d}):n?(0,a.jsx)(l.A,{sources:{light:(0,r.default)(t),dark:(0,r.default)(n)},alt:m,className:s,style:y}):(0,a.jsx)("img",{src:(0,r.default)(t),alt:m,className:s,style:y})}),(0,a.jsx)("h3",{children:m}),(0,a.jsx)("p",{children:u}),(0,a.jsx)("div",{className:"tileLink_iUbu",children:x})]})}},10440(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var i=n(34164);function o({children:e,className:t}){return(0,a.jsx)("div",{className:(0,i.A)("tilesGrid_hB9N",t),children:e})}},81956(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var i=n(46077);let o=({steps:e,title:t,screenshot:n,width:o="normal"})=>(0,a.jsxs)("div",{className:"workflowContainer__N1v",children:[t&&(0,a.jsx)("h3",{className:"workflowTitle_QrAr",children:t}),n&&(0,a.jsx)("div",{className:"screenshotContainer_OwzZ",children:(0,a.jsx)(i.A,{src:n.src,alt:n.alt,width:n.width||"90%"})}),(0,a.jsx)("div",{className:"stepsContainer_IGeu",style:{maxWidth:"wide"===o?"850px":"700px"},children:e.map((t,n)=>(0,a.jsxs)("div",{className:"stepItem_GyHJ",children:[(0,a.jsxs)("div",{className:"stepIndicator_U2Wb",children:[(0,a.jsx)("div",{className:"stepNumber_vINc",children:t.icon?(0,a.jsx)(t.icon,{size:16}):(0,a.jsx)("span",{className:"stepNumberText_eLd7",children:n+1})}),n<e.length-1&&(0,a.jsx)("div",{className:"stepConnector_Si86"})]}),(0,a.jsxs)("div",{className:"stepContent_D0CA",children:[(0,a.jsx)("h4",{className:"stepTitle_wujx",children:t.title}),(0,a.jsx)("p",{className:"stepDescription_PIaE",children:t.description})]})]},n))})]})}}]);