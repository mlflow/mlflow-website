"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[302],{10493:(e,n,i)=>{i.d(n,{Zp:()=>s,AC:()=>r,WO:()=>c,_C:()=>d,$3:()=>p,jK:()=>m});var o=i(34164);const t={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var l=i(28774),a=i(74848);const r=({children:e,isSmall:n,cols:i})=>(0,a.jsx)("div",{className:(0,o.A)(t.CardGroup,n?t.AutofillColumns:i?t[`Cols${i}`]:t.MaxThreeColumns),children:e}),s=({children:e,link:n=""})=>n?(0,a.jsx)(l.A,{className:(0,o.A)(t.Link,t.Card,t.CardBordered),to:n,children:e}):(0,a.jsx)("div",{className:(0,o.A)(t.Card,t.CardBordered),children:e}),d=({headerText:e,link:n,text:i})=>(0,a.jsx)(s,{link:n,children:(0,a.jsxs)("span",{children:[(0,a.jsx)("div",{className:(0,o.A)(t.CardTitle,t.BoxRoot,t.PaddingBottom4),style:{pointerEvents:"none"},children:(0,a.jsx)("div",{className:(0,o.A)(t.BoxRoot,t.FlexFlex,t.FlexAlignItemsCenter,t.FlexDirectionRow,t.FlexJustifyContentFlexStart,t.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,a.jsx)("div",{className:(0,o.A)(t.BoxRoot,t.BoxHideIfEmpty,t.MarginTop4,t.MarginLeft4),style:{pointerEvents:"auto"},children:(0,a.jsx)("span",{className:"",children:e})})})}),(0,a.jsx)("span",{className:(0,o.A)(t.TextColor,t.CardBody),children:(0,a.jsx)("p",{children:i})})]})}),c=({description:e,children:n,link:i})=>(0,a.jsx)(s,{link:i,children:(0,a.jsxs)("div",{className:t.LogoCardContent,children:[(0,a.jsx)("div",{className:t.LogoCardImage,children:n}),(0,a.jsx)("p",{className:t.TextColor,children:e})]})}),p=({children:e,link:n})=>(0,a.jsx)(s,{link:n,children:(0,a.jsx)("div",{className:t.SmallLogoCardContent,children:(0,a.jsx)("div",{className:(0,o.A)("max-height-img-container",t.SmallLogoCardImage),children:e})})}),m=({title:e,description:n,link:i=""})=>(0,a.jsx)(s,{link:i,children:(0,a.jsxs)("div",{className:t.TitleCardContent,children:[(0,a.jsx)("div",{className:(0,o.A)(t.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,a.jsx)("hr",{className:(0,o.A)(t.TitleCardSeparator),style:{margin:"12px 0"}}),(0,a.jsx)("p",{className:(0,o.A)(t.TextColor),children:n})]})})},13122:(e,n,i)=>{i.d(n,{A:()=>o});const o=i.p+"assets/images/dspy-integration-architecture-ee130c78236e38d9728f2a818bf024e9.png"},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var o=i(96540);const t={},l=o.createContext(t);function a(e){const n=o.useContext(l);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(l.Provider,{value:n},e.children)}},41947:(e,n,i)=>{i.d(n,{A:()=>o});const o=i.p+"assets/images/dspy-artifacts-e5f816ebc8919118392f3e066edece68.png"},49374:(e,n,i)=>{i.d(n,{B:()=>s});i(96540);const o=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var t=i(86025),l=i(28774),a=i(74848);const r=e=>{const n=e.split(".");for(let i=n.length;i>0;i--){const e=n.slice(0,i).join(".");if(o[e])return e}return null};function s({fn:e,children:n}){const i=r(e);if(!i)return(0,a.jsx)(a.Fragment,{children:n});const s=(0,t.Ay)(`/${o[i]}#${e}`);return(0,a.jsx)(l.A,{to:s,target:"_blank",children:n??(0,a.jsxs)("code",{children:[e,"()"]})})}},51067:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"flavors/dspy/index","title":"MLflow DSPy Flavor","description":"The dspy flavor is under active development and is marked as Experimental. Public APIs are","source":"@site/docs/genai/flavors/dspy/index.mdx","sourceDirName":"flavors/dspy","slug":"/flavors/dspy/","permalink":"/docs/latest/genai/flavors/dspy/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Embeddings Support with OpenAI in MLflow","permalink":"/docs/latest/genai/flavors/openai/notebooks/openai-embeddings-generation"},"next":{"title":"DSPy Quickstart","permalink":"/docs/latest/genai/flavors/dspy/notebooks/dspy_quickstart"}}');var t=i(74848),l=i(28453),a=i(49374),r=i(10493);const s={},d="MLflow DSPy Flavor",c={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Why use DSPy with MLflow?",id:"why-use-dspy-with-mlflow",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Concepts",id:"concepts",level:2},{value:"<code>Module</code>",id:"module",level:4},{value:"<code>Signature</code>",id:"signature",level:4},{value:"<code>Optimizer</code>",id:"optimizer",level:4},{value:"<code>Program</code>",id:"program",level:4},{value:"Automatic Tracing",id:"automatic-tracing",level:2},{value:"Tracking DSPy Program in MLflow Experiment",id:"tracking-dspy-program-in-mlflow-experiment",level:2},{value:"Creating a DSPy Program",id:"creating-a-dspy-program",level:4},{value:"Logging the Program to MLflow",id:"logging-the-program-to-mlflow",level:4},{value:"Loading the Module for inference",id:"loading-the-module-for-inference",level:4},{value:"Optimizer Autologging",id:"optimizer-autologging",level:2},{value:"FAQ",id:"faq",level:2},{value:"How can I save a compiled vs. uncompiled model?",id:"how-can-i-save-a-compiled-vs-uncompiled-model",level:3},{value:"What can be serialized by MLflow?",id:"what-can-be-serialized-by-mlflow",level:3},{value:"How do I manage secrets?",id:"how-do-i-manage-secrets",level:3},{value:"How is the DSPy <code>settings</code> object saved?",id:"how-is-the-dspy-settings-object-saved",level:3}];function m(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"mlflow-dspy-flavor",children:"MLflow DSPy Flavor"})}),"\n",(0,t.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"dspy"})," flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change and new features may be added as the flavor evolves."]})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://dspy-docs.vercel.app/",children:"DSPy"})," is a framework for algorithmically optimizing LM prompts and weights. It's designed to\nimprove the process of prompt engineering by replacing hand-crafted prompt strings with modular\ncomponents. These modules are concise, well-defined, and maintain high quality and expressive power,\nmaking prompt creation more efficient and scalable. By parameterizing these modules and treating\nprompting as an optimization problem, DSPy can adapt better to different language models,\npotentially outperforming prompts crafted by experts. This modularity also enables easier\nexploration of complex pipelines, allowing for fine-tuning performance based on specific tasks or\nnuanced metrics."]}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"70%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Overview of DSPy and MLflow integration",src:i(13122).A+"",width:"1094",height:"1522"})})}),"\n",(0,t.jsx)(n.h2,{id:"why-use-dspy-with-mlflow",children:"Why use DSPy with MLflow?"}),"\n",(0,t.jsx)(n.p,{children:"The native integration of the DSPy library with MLflow helps users manage the development lifecycle with DSPy. The following are some of the key benefits of using DSPy with MLflow:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/ml/tracking",children:"MLflow Tracking"})," allows you to track your DSPy program's training and execution. With the MLflow APIs, you can log a variety of artifacts and organize training runs, thereby increasing visibility into your model performance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/ml/model",children:"MLflow Model"})," packages your compiled DSPy program along with its dependency versions, input and output interfaces and other essential metadata. This allows you to deploy your compiled DSPy program with ease, knowing that the environment is consistent across different stages of the ML lifecycle."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/genai/eval-monitor",children:"MLflow Evaluate"})," provides native capabilities within MLflow to evaluate GenAI applications. This capability facilitates the efficient assessment of inference results from your DSPy compiled program, ensuring robust performance analytics and facilitating quick iterations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/genai/tracing",children:"MLflow Tracing"})," is a powerful observability tool for monitoring and debugging what happens inside the DSPy models, helping you identify potential bottlenecks or issues quickly. With its powerful automatic logging capability, you can instrument your DSPy application without needing to add any code apart from running a single command."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsx)(n.p,{children:"In this introductory tutorial, you will learn the most fundamental components of DSPy and how to leverage the integration with MLflow to store, retrieve, and\nuse a DSPy program."}),"\n",(0,t.jsx)(r.AC,{children:(0,t.jsx)(r._C,{headerText:"DSPy Quickstart",link:"/genai/flavors/dspy/notebooks/dspy_quickstart/",text:"Get started with MLflow and DSPy by exploring the simplest possible configuration of a DSPy program."})}),"\n",(0,t.jsx)(n.h2,{id:"concepts",children:"Concepts"}),"\n",(0,t.jsx)(n.h4,{id:"module",children:(0,t.jsx)(n.code,{children:"Module"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://dspy.ai/learn/programming/modules",children:"Modules"})," are components that handle specific text transformations, like answering questions or summarizing. They replace traditional hand-written prompts and can learn from examples, making them more adaptable."]}),"\n",(0,t.jsx)(n.h4,{id:"signature",children:(0,t.jsx)(n.code,{children:"Signature"})}),"\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.a,{href:"https://dspy.ai/learn/programming/signatures",children:"signature"})," is a natural language description of a module's input and output behavior. For example, ",(0,t.jsx)(n.em,{children:'"question -> answer"'})," specifies that the module should take a question as input and return an answer."]}),"\n",(0,t.jsx)(n.h4,{id:"optimizer",children:(0,t.jsx)(n.code,{children:"Optimizer"})}),"\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.a,{href:"https://dspy.ai/learn/optimization/optimizers",children:"optimizer"})," improves LM pipelines by adjusting modules to meet a performance metric, either by generating better prompts or fine-tuning models."]}),"\n",(0,t.jsx)(n.h4,{id:"program",children:(0,t.jsx)(n.code,{children:"Program"})}),"\n",(0,t.jsx)(n.p,{children:"A program is a a set of modules connected into a pipeline to perform complex tasks. DSPy programs are flexible, allowing you to optimize and adapt them using the compiler."}),"\n",(0,t.jsx)(n.h2,{id:"automatic-tracing",children:"Automatic Tracing"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"DSPy Tracing via autolog",src:i(62561).A+"",width:"1192",height:"720"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/genai/tracing",children:"MLflow Tracing"})," tracing is a powerful feature that allows you to monitor and debug your DSPy programs. With MLflow, you can enable auto tracing just by calling the ",(0,t.jsx)(a.B,{fn:"mlflow.dspy.autolog"})," function in your code."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.dspy.autolog()\n"})}),"\n",(0,t.jsx)(n.p,{children:"Once enabled, MLflow will generate traces whenever your DSPy program is executed and record them in your MLflow Experiment."}),"\n",(0,t.jsxs)(n.p,{children:["Learn more about MLflow DSPy tracing capabilities ",(0,t.jsx)(n.a,{href:"/genai/tracing/integrations/listing/dspy",children:"here"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"tracking-dspy-program-in-mlflow-experiment",children:"Tracking DSPy Program in MLflow Experiment"}),"\n",(0,t.jsx)(n.h4,{id:"creating-a-dspy-program",children:"Creating a DSPy Program"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://dspy.ai/learn/programming/modules",children:"Module"})," object is the centerpiece of\nthe DSPy and MLflow integration. With DSPy, you can create complex agentic logic via a module or\nset of modules."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"pip install mlflow dspy -U\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import dspy\n\n# Define our language model\nlm = dspy.LM(model="openai/gpt-4o-mini", max_tokens=250)\ndspy.settings.configure(lm=lm)\n\n\n# Define a Chain of Thought module\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought("question -> answer")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\ndspy_model = CoT()\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Typically you'd want to leverage a\n",(0,t.jsx)(n.a,{href:"https://dspy.ai/learn/optimization/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them",children:"compiled DSPy"}),"\nmodule. MLflow will natively supports logging both compiled and uncompiled DSPy modules. Above\nwe show an uncompiled version for simplicity, but in production you'd want to leverage an\n",(0,t.jsx)(n.a,{href:"https://dspy.ai/learn/optimization/optimizers",children:"optimizer"})," and log the\noutputted object instead."]})}),"\n",(0,t.jsx)(n.h4,{id:"logging-the-program-to-mlflow",children:"Logging the Program to MLflow"}),"\n",(0,t.jsxs)(n.p,{children:["You can log the ",(0,t.jsx)(n.code,{children:"dspy.Module"})," object to an MLflow run using the ",(0,t.jsx)(a.B,{fn:"mlflow.dspy.log_model"})," function."]}),"\n",(0,t.jsxs)(n.p,{children:["We will also specify a ",(0,t.jsx)(n.a,{href:"/ml/model/signatures#automatic-signature-inference",children:"model signature"}),".\nAn MLflow model signature defines the expected schema for model inputs and outputs, ensuring\nconsistency and correctness during model inference."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Log the model\n    model_info = mlflow.dspy.log_model(\n        dspy_model,\n        name="model",\n        input_example="what is 2 + 2?",\n    )\n'})}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLflow artifacts for the DSPy program",src:i(41947).A+"",width:"2316",height:"1240"})})}),"\n",(0,t.jsx)(n.h4,{id:"loading-the-module-for-inference",children:"Loading the Module for inference"}),"\n",(0,t.jsxs)(n.p,{children:["The saved module can be loaded back for inference using the ",(0,t.jsx)(a.B,{fn:"mlflow.pyfunc.load_model"})," function. This function\ngives an MLflow Python Model backed by the DSPy module."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Load the model as an MLflow PythonModel\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Predict with the object\nresponse = model.predict("What kind of bear is best?")\nprint(response)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Output"',children:'{\n    "reasoning": """The question "What kind of bear is best?" is often associated with a\n    humorous reference from the television show "The Office," where the character Jim\n    Halpert jokingly states, "Bears, beets, Battlestar Galactica." However, if we consider\n    the question seriously, it depends on the context. Different species of bears have\n    different characteristics and adaptations that make them "best" in various ways.\n    For example, the American black bear is known for its adaptability, while the polar bear is\n    the largest land carnivore and is well adapted to its Arctic environment. Ultimately, the\n    answer can vary based on personal preference or specific criteria such as strength,\n    intelligence, or adaptability.""",\n    "answer": """There isn\\\'t a definitive answer, as it depends on the context. However, many\n    people humorously refer to the American black bear or the polar bear when discussing\n    "the best" kind of bear.""",\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"The MLflow PythonModel for DSPy supports streaming. To enable streaming, ensure the following conditions are met:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Install ",(0,t.jsx)(n.code,{children:"dspy"})," version greater than 2.6.23."]}),"\n",(0,t.jsx)(n.li,{children:"Log your model with a signature."}),"\n",(0,t.jsx)(n.li,{children:"Ensure all outputs of your model are strings."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'stream_response = model.predict_stream("What kind of bear is best?")\nfor output in stream_response:\n    print(output)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Output"',children:'{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": "The"}\n{\n    "predict_name": "prog.predict",\n    "signature_field_name": "reasoning",\n    "chunk": " question",\n}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " of"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " what"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " kind"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " of"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " bear"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " is"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " best"}\n{"predict_name": "prog.predict", "signature_field_name": "reasoning", "chunk": " is"}\n...\n'})}),"\n",(0,t.jsxs)(n.p,{children:["To load the DSPy program itself back instead of the PyFunc-wrapped model, use the ",(0,t.jsx)(a.B,{fn:"mlflow.dspy.load_model"})," function."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"model = mlflow.dspy.load_model(model_uri)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"optimizer-autologging",children:"Optimizer Autologging"}),"\n",(0,t.jsxs)(n.p,{children:["The MLflow DSPy flavor supports autologging for DSPy optimizers. See the ",(0,t.jsx)(n.a,{href:"/genai/flavors/dspy/optimizer",children:"Optimizer Autologging"})," page for details."]}),"\n",(0,t.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,t.jsx)(n.h3,{id:"how-can-i-save-a-compiled-vs-uncompiled-model",children:"How can I save a compiled vs. uncompiled model?"}),"\n",(0,t.jsx)(n.p,{children:"DSPy compiles models by updating various LLM parameters, such as prompts, hyperparameters, and\nmodel weights, to optimize training. While MLflow allows logging both compiled and uncompiled\nmodels, it's generally preferable to use a compiled model, as it is expected to perform better in\npractice."}),"\n",(0,t.jsx)(n.h3,{id:"what-can-be-serialized-by-mlflow",children:"What can be serialized by MLflow?"}),"\n",(0,t.jsxs)(n.p,{children:["When using ",(0,t.jsx)(a.B,{fn:"mlflow.dspy.log_model"})," or ",(0,t.jsx)(a.B,{fn:"mlflow.dspy.save_model"})," in MLflow, the\nDSPy program is serialized and saved to the tracking server as a ",(0,t.jsx)(n.code,{children:".pkl"})," file. This enables easy\ndeployment. Under the hood, MLflow uses ",(0,t.jsx)(n.code,{children:"cloudpickle"})," to serialize the DSPy object, but some\nDSPy artifacts are note serializable. Relevant examples are listed below."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"API tokens. These should be managed separately and passed securely via environment variables."}),"\n",(0,t.jsx)(n.li,{children:"The DSPy trace object, which is primarily used during training, not inference."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-do-i-manage-secrets",children:"How do I manage secrets?"}),"\n",(0,t.jsx)(n.p,{children:"When serializing using the MLflow DSPy flavor, tokens are dropped from the settings objects. It is\nthe user's responsibility to securely pass the required secrets to the deployment environment."}),"\n",(0,t.jsxs)(n.h3,{id:"how-is-the-dspy-settings-object-saved",children:["How is the DSPy ",(0,t.jsx)(n.code,{children:"settings"})," object saved?"]}),"\n",(0,t.jsx)(n.p,{children:"To ensure program reproducibility, the service context is converted to a Python dictionary and\npickled with the model artifact. Service context is a concept that has been popularized in GenAI\nframeworks. Put simply, it stores a configuration that is global to your project. For DSPy\nspecifically, we can set information such as the language model, reranker, adapter, etc."}),"\n",(0,t.jsxs)(n.p,{children:["DSPy stores this service context in a ",(0,t.jsx)(n.code,{children:"Settings"})," singleton class. Sensitive API access keys that\nare set within the ",(0,t.jsx)(n.code,{children:"Settings"})," object are not persisted when logging your model. When deploying\nyour DSPy model, you must ensure that the deployment environment has these keys set so that your\nDSPy model can make remote calls to services that require access keys."]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},62561:(e,n,i)=>{i.d(n,{A:()=>o});const o=i.p+"assets/images/dspy-tracing-957d61580cca35522155c70e79cdbe42.gif"}}]);