"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["1652"],{28491(e,n,t){t.r(n),t.d(n,{metadata:()=>r,default:()=>M,frontMatter:()=>v,contentTitle:()=>b,toc:()=>A,assets:()=>k});var r=JSON.parse('{"id":"concepts/scorers","title":"Scorer Concepts","description":"What are Scorers?","source":"@site/docs/genai/concepts/scorers.mdx","sourceDirName":"concepts","slug":"/concepts/scorers","permalink":"/mlflow-website/docs/latest/genai/concepts/scorers","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Expectations","permalink":"/mlflow-website/docs/latest/genai/concepts/expectations"},"next":{"title":"Evaluation Datasets","permalink":"/mlflow-website/docs/latest/genai/concepts/evaluation-datasets"}}'),l=t(74848),i=t(28453),a=t(78010),s=t(57250),o=t(54725),c=t(95986),d=t(33508),p=t(34742),m=t(10440),h=t(77541),u=t(87073),f=t(22492),g=t(61878),y=t(96393),w=t(96844),_=t(93893),x=t(93164),j=t(47792);let v={},b="Scorer Concepts",k={},A=[{value:"What are Scorers?",id:"what-are-scorers",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Types of Scorers",id:"types-of-scorers",level:2},{value:"Scorer Output Structure",id:"scorer-output-structure",level:2},{value:"Common Scorer Patterns",id:"common-scorer-patterns",level:2},{value:"Judge Alignment",id:"judge-alignment",level:2},{value:"How Alignment Works",id:"how-alignment-works",level:3},{value:"Key Benefits of Alignment",id:"key-benefits-of-alignment",level:3},{value:"The Plugin Architecture",id:"the-plugin-architecture",level:3},{value:"Integration with MLflow Evaluation",id:"integration-with-mlflow-evaluation",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function L(e){let n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"scorer-concepts",children:"Scorer Concepts"})}),"\n",(0,l.jsx)(n.h2,{id:"what-are-scorers",children:"What are Scorers?"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Scorers"})," in MLflow are evaluation functions that assess the quality of your GenAI application outputs. They provide a systematic way to measure performance across different dimensions like correctness, relevance, safety, and adherence to guidelines."]}),"\n",(0,l.jsx)(n.p,{children:"Scorers transform subjective quality assessments into measurable metrics, enabling you to track performance, compare models, and ensure your applications meet quality standards. They range from simple rule-based checks to sophisticated LLM judges that can evaluate nuanced aspects of language generation."}),"\n",(0,l.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,l.jsx)(d.A,{features:[{icon:u.A,title:"Automated Quality Assessment",description:"Replace manual review processes with automated scoring that can evaluate thousands of outputs consistently and at scale, using either deterministic rules or LLM-based evaluation."},{icon:f.A,title:"Safety & Compliance Validation",description:"Systematically check for harmful content, bias, PII leakage, and regulatory compliance. Ensure your applications meet organizational and legal standards before deployment."},{icon:g.A,title:"A/B Testing & Model Comparison",description:"Compare different models, prompts, or configurations using consistent evaluation criteria. Make data-driven decisions about which approach performs best for your use case."},{icon:y.A,title:"Continuous Quality Monitoring",description:"Track quality metrics over time in production, detect degradations early, and maintain high standards as your application evolves and scales."}]}),"\n",(0,l.jsx)(n.h2,{id:"types-of-scorers",children:"Types of Scorers"}),"\n",(0,l.jsx)(n.p,{children:"MLflow provides several types of scorers to address different evaluation needs:"}),"\n",(0,l.jsx)(p.A,{concepts:[{icon:w.A,title:"Agent-as-a-Judge",description:"Autonomous agents that analyze execution traces to evaluate not just outputs, but the entire process. They can assess tool usage, reasoning chains, and error handling."},{icon:_.A,title:"Human-Aligned Judges",description:"LLM judges that have been aligned with human feedback using the built-in align() method to match your specific quality standards. These provide the consistency of automation with the nuance of human judgment."},{icon:u.A,title:"LLM-based Scorers (LLM-as-a-Judge)",description:"Use large language models to evaluate subjective qualities like helpfulness, coherence, and style. These scorers can understand context and nuance that rule-based systems miss."},{icon:x.A,title:"Code-based Scorers",description:"Custom Python functions for deterministic evaluation. Perfect for metrics that can be calculated algorithmically like ROUGE scores, exact match, or custom business logic."}]}),"\n",(0,l.jsx)(n.h2,{id:"scorer-output-structure",children:"Scorer Output Structure"}),"\n",(0,l.jsxs)(n.p,{children:["All scorers in MLflow produce standardized output that integrates seamlessly with the evaluation framework. Scorers return a ",(0,l.jsx)(o.B,{fn:"mlflow.entities.Feedback"})," object containing:"]}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Field"}),(0,l.jsx)(n.th,{children:"Type"}),(0,l.jsx)(n.th,{children:"Description"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"name"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"str"})}),(0,l.jsx)(n.td,{children:'Unique identifier for the scorer (e.g., "correctness", "safety")'})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"value"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"Any"})}),(0,l.jsx)(n.td,{children:"The evaluation result - can be numeric, boolean, or categorical"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"rationale"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"Optional[str]"})}),(0,l.jsx)(n.td,{children:"Explanation of why this score was given (especially useful for LLM judges)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"metadata"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"Optional[dict]"})}),(0,l.jsx)(n.td,{children:"Additional information about the evaluation (confidence, sub-scores, etc.)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"error"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"Optional[str]"})}),(0,l.jsx)(n.td,{children:"Error message if the scorer failed to evaluate"})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"common-scorer-patterns",children:"Common Scorer Patterns"}),"\n",(0,l.jsx)(n.p,{children:"MLflow's scorer system is highly flexible, supporting everything from simple rule-based checks to sophisticated AI agents that analyze entire execution traces. The examples below demonstrate the breadth of evaluation capabilities available - from detecting inefficiencies in multi-step workflows to assessing text readability, measuring response latency, and ensuring output quality. Each pattern can be customized to your specific use case and combined with others for comprehensive evaluation."}),"\n",(0,l.jsx)(c.A,{children:(0,l.jsxs)(a.A,{children:[(0,l.jsx)(s.A,{value:"agent-judge",label:"Agent-as-a-Judge (Trace Analysis)",default:!0,children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\nimport mlflow\n\n# Create an Agent-as-a-Judge that analyzes execution patterns\nfrom typing import Literal\n\nefficiency_judge = make_judge(\n    name="efficiency_analyzer",\n    instructions=(\n        "Analyze the {{ trace }} for inefficiencies.\\n\\n"\n        "Check for:\\n"\n        "- Redundant API calls or database queries\\n"\n        "- Sequential operations that could be parallelized\\n"\n        "- Unnecessary data processing\\n\\n"\n        "Rate as: \'efficient\', \'acceptable\', or \'inefficient\'"\n    ),\n    feedback_value_type=Literal["efficient", "acceptable", "inefficient"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Example: RAG application with retrieval and generation\nfrom mlflow.entities import SpanType\nimport time\n\n\n@mlflow.trace(span_type=SpanType.RETRIEVER)\ndef retrieve_context(query: str):\n    # Simulate vector database retrieval\n    time.sleep(0.5)  # Retrieval latency\n    return [\n        {"doc": "MLflow is an open-source platform", "score": 0.95},\n        {"doc": "It manages the ML lifecycle", "score": 0.89},\n        {"doc": "Includes tracking and deployment", "score": 0.87},\n    ]\n\n\n@mlflow.trace(span_type=SpanType.RETRIEVER)\ndef retrieve_user_history(user_id: str):\n    # Another retrieval that could be parallelized\n    time.sleep(0.5)  # Could run parallel with above\n    return {"previous_queries": ["What is MLflow?", "How to log models?"]}\n\n\n@mlflow.trace(span_type=SpanType.LLM)\ndef generate_response(query: str, context: list, history: dict):\n    # Simulate LLM generation\n    return f"Based on context about \'{query}\': MLflow is a platform for ML lifecycle management."\n\n\n@mlflow.trace(span_type=SpanType.AGENT)\ndef rag_agent(query: str, user_id: str):\n    # Sequential operations that could be optimized\n    context = retrieve_context(query)\n    history = retrieve_user_history(user_id)  # Could be parallel with above\n    response = generate_response(query, context, history)\n    return response\n\n\n# Run the RAG agent\nresult = rag_agent("What is MLflow?", "user123")\ntrace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id)\n\n# Judge analyzes the trace to identify inefficiencies\nfeedback = efficiency_judge(trace=trace)\nprint(f"Efficiency: {feedback.value}")\nprint(f"Analysis: {feedback.rationale}")\n'})})}),(0,l.jsx)(s.A,{value:"llm-judge",label:"LLM Judge (Field-Based)",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\n\ncorrectness_judge = make_judge(\n    name="correctness",\n    instructions=(\n        "Evaluate if the response in {{ outputs }} "\n        "correctly answers the question in {{ inputs }}."\n    ),\n    feedback_value_type=bool,\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Example usage\nfeedback = correctness_judge(\n    inputs={"question": "What is MLflow?"},\n    outputs={\n        "response": "MLflow is an open-source platform for ML lifecycle management."\n    },\n)\nprint(f"Correctness: {feedback.value}")\n'})})}),(0,l.jsx)(s.A,{value:"reading-level",label:"Reading Level Assessment",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import textstat\nfrom mlflow.genai.scorers import scorer\nfrom mlflow.entities import Feedback\n\n\n@scorer\ndef reading_level(outputs: str) -> Feedback:\n    """Evaluate text complexity using Flesch Reading Ease."""\n    score = textstat.flesch_reading_ease(outputs)\n\n    if score >= 60:\n        level = "easy"\n        rationale = f"Reading ease score of {score:.1f} - accessible to most readers"\n    elif score >= 30:\n        level = "moderate"\n        rationale = f"Reading ease score of {score:.1f} - college level complexity"\n    else:\n        level = "difficult"\n        rationale = f"Reading ease score of {score:.1f} - expert level required"\n\n    return Feedback(value=level, rationale=rationale, metadata={"score": score})\n'})})}),(0,l.jsx)(s.A,{value:"perplexity",label:"Language Perplexity Scoring",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom mlflow.genai.scorers import scorer\n\n\n@scorer\ndef perplexity_score(outputs: str) -> float:\n    """Calculate perplexity to measure text quality and coherence."""\n    model = AutoModelForCausalLM.from_pretrained("gpt2")\n    tokenizer = AutoTokenizer.from_pretrained("gpt2")\n\n    inputs = tokenizer(outputs, return_tensors="pt")\n    with torch.no_grad():\n        outputs = model(**inputs, labels=inputs["input_ids"])\n\n    perplexity = torch.exp(outputs.loss).item()\n    return perplexity  # Lower is better - indicates more natural text\n'})})}),(0,l.jsx)(s.A,{value:"latency",label:"Response Latency Tracking",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import scorer\nfrom mlflow.entities import Feedback, Trace\n\n\n@scorer\ndef response_time(trace: Trace) -> Feedback:\n    """Evaluate response time from trace spans."""\n    root_span = trace.data.spans[0]\n    latency_ms = (root_span.end_time - root_span.start_time) / 1e6\n\n    if latency_ms < 100:\n        value = "fast"\n    elif latency_ms < 500:\n        value = "acceptable"\n    else:\n        value = "slow"\n\n    return Feedback(\n        value=value,\n        rationale=f"Response took {latency_ms:.0f}ms",\n        metadata={"latency_ms": latency_ms},\n    )\n'})})})]})}),"\n",(0,l.jsx)(n.h2,{id:"judge-alignment",children:"Judge Alignment"}),"\n",(0,l.jsxs)(n.p,{children:["One of the most powerful features of MLflow scorers is the ability to ",(0,l.jsx)(n.strong,{children:"align LLM judges with human preferences"}),". This transforms generic evaluation models into domain-specific experts that understand your unique quality standards."]}),"\n",(0,l.jsx)(n.h3,{id:"how-alignment-works",children:"How Alignment Works"}),"\n",(0,l.jsx)(n.p,{children:"Judge alignment uses human feedback to improve the accuracy and consistency of LLM-based scorers:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\nimport mlflow\n\n# Create an initial judge\nquality_judge = make_judge(\n    name="quality",\n    instructions="Evaluate if {{ outputs }} meets quality standards for {{ inputs }}.",\n    feedback_value_type=bool,\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Collect traces with both judge assessments and human feedback\ntraces_with_feedback = mlflow.search_traces(\n    experiment_ids=[experiment_id], max_results=20  # Minimum 10 required for alignment\n)\n\n# Align the judge with human preferences (uses default DSPy-SIMBA optimizer)\naligned_judge = quality_judge.align(traces_with_feedback)\n\n# The aligned judge now better matches your team\'s quality standards\nfeedback = aligned_judge(inputs={"query": "..."}, outputs={"response": "..."})\n'})}),"\n",(0,l.jsx)(n.h3,{id:"key-benefits-of-alignment",children:"Key Benefits of Alignment"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Domain Expertise"}),": Judges learn your specific quality criteria from expert feedback"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Consistency"}),": Aligned judges apply standards uniformly across evaluations"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Cost Efficiency"}),": Once aligned, smaller/cheaper models can match expert judgment"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Continuous Improvement"}),": Re-align as your standards evolve"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"the-plugin-architecture",children:"The Plugin Architecture"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow's alignment system uses a plugin architecture, allowing you to create custom optimizers by extending the ",(0,l.jsx)(o.B,{fn:"mlflow.genai.judges.base.AlignmentOptimizer",children:"AlignmentOptimizer"})," base class:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"from mlflow.genai.judges.base import AlignmentOptimizer\n\n\nclass CustomOptimizer(AlignmentOptimizer):\n    def align(self, judge, traces):\n        # Your custom alignment logic\n        return improved_judge\n\n\n# Use your custom optimizer\naligned_judge = quality_judge.align(traces, CustomOptimizer())\n"})}),"\n",(0,l.jsx)(n.h2,{id:"integration-with-mlflow-evaluation",children:"Integration with MLflow Evaluation"}),"\n",(0,l.jsxs)(n.p,{children:["Scorers are the building blocks of MLflow's evaluation framework. They integrate seamlessly with ",(0,l.jsx)(n.code,{children:"mlflow.genai.evaluate()"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\n\n# Your test data\ntest_data = pd.DataFrame(\n    [\n        {\n            "inputs": {"question": "What is MLflow?"},\n            "outputs": {\n                "response": "MLflow is an open-source platform for ML lifecycle management."\n            },\n            "expectations": {\n                "ground_truth": "MLflow is an open-source platform for managing the ML lifecycle"\n            },\n        },\n        {\n            "inputs": {"question": "How do I track experiments?"},\n            "outputs": {\n                "response": "Use mlflow.start_run() to track experiments in MLflow."\n            },\n            "expectations": {\n                "ground_truth": "Use mlflow.start_run() to track experiments"\n            },\n        },\n    ]\n)\n\n\n# Your application (optional if data already has outputs)\ndef my_app(inputs):\n    # Your model logic here\n    return {"response": f"Answer to: {inputs[\'question\']}"}\n\n\n# Evaluate with multiple scorers\nresults = mlflow.genai.evaluate(\n    data=test_data,\n    # predict_fn is optional if data already has outputs\n    scorers=[\n        correctness_judge,  # LLM judge from above\n        reading_level,  # Custom scorer from above\n    ],\n)\n\n# Access evaluation metrics\nprint(f"Correctness: {results.metrics.get(\'correctness/mean\', \'N/A\')}")\nprint(f"Reading Level: {results.metrics.get(\'reading_level/mode\', \'N/A\')}")\n'})}),"\n",(0,l.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Choose the Right Scorer Type"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Use code-based scorers for objective, deterministic metrics"}),"\n",(0,l.jsx)(n.li,{children:"Use LLM judges for subjective qualities requiring understanding"}),"\n",(0,l.jsx)(n.li,{children:"Use Agent-as-a-Judge for evaluating complex multi-step processes"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Combine Multiple Scorers"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"No single metric captures all aspects of quality"}),"\n",(0,l.jsx)(n.li,{children:"Use a portfolio of scorers to get comprehensive evaluation"}),"\n",(0,l.jsx)(n.li,{children:"Balance efficiency (fast code-based) with depth (LLM and Agent judges)"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Align with Human Judgment"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Validate that your scorers correlate with human quality assessments"}),"\n",(0,l.jsx)(n.li,{children:"Use human feedback to improve LLM and Agent judge instructions"}),"\n",(0,l.jsx)(n.li,{children:"Consider using human-aligned judges for critical evaluations"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Monitor Scorer Performance"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Track scorer execution time and costs"}),"\n",(0,l.jsx)(n.li,{children:"Monitor for scorer failures and handle gracefully"}),"\n",(0,l.jsx)(n.li,{children:"Regularly review scorer outputs for consistency"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,l.jsxs)(m.A,{children:[(0,l.jsx)(h.A,{icon:u.A,iconSize:48,title:"LLM-based Scorers",description:"Learn about using LLMs as judges for evaluation",href:"/genai/eval-monitor/scorers/llm-judge/custom-judges/",linkText:"Explore LLM judges \u2192",containerHeight:64}),(0,l.jsx)(h.A,{icon:_.A,iconSize:48,title:"Judge Alignment",description:"Align judges with human feedback for domain expertise",href:"/genai/eval-monitor/scorers/llm-judge/alignment",linkText:"Learn alignment \u2192",containerHeight:64}),(0,l.jsx)(h.A,{icon:x.A,iconSize:48,title:"Code-based Scorers",description:"Create custom Python functions for evaluation",href:"/genai/eval-monitor/scorers/custom",linkText:"Build custom scorers \u2192",containerHeight:64}),(0,l.jsx)(h.A,{icon:j.A,iconSize:48,title:"Evaluation Guide",description:"Learn how to run comprehensive evaluations",href:"/genai/eval-monitor/quickstart",linkText:"Start evaluating \u2192",containerHeight:64})]})]})}function M(e={}){let{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(L,{...e})}):L(e)}},54725(e,n,t){t.d(n,{B:()=>a});var r=t(74848);t(96540);var l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),i=t(66497);function a({fn:e,children:n,hash:t}){let a=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(l[t])return t}return null})(e);if(!a)return(0,r.jsx)(r.Fragment,{children:n});let s=(0,i.default)(`/${l[a]}#${t??e}`);return(0,r.jsx)("a",{href:s,target:"_blank",children:n??(0,r.jsxs)("code",{children:[e,"()"]})})}},34742(e,n,t){t.d(n,{A:()=>l});var r=t(74848);t(96540);function l({concepts:e,title:n}){return(0,r.jsxs)("div",{className:"conceptOverview_x8T_",children:[n&&(0,r.jsx)("h3",{className:"overviewTitle_HyAI",children:n}),(0,r.jsx)("div",{className:"conceptGrid_uJNV",children:e.map((e,n)=>(0,r.jsxs)("div",{className:"conceptCard_oday",children:[(0,r.jsxs)("div",{className:"conceptHeader_HCk5",children:[e.icon&&(0,r.jsx)("div",{className:"conceptIcon_gejw",children:(0,r.jsx)(e.icon,{size:20})}),(0,r.jsx)("h4",{className:"conceptTitle_TGMM",children:e.title})]}),(0,r.jsx)("p",{className:"conceptDescription_ZyDn",children:e.description})]},n))})]})}},33508(e,n,t){t.d(n,{A:()=>l});var r=t(74848);t(96540);function l({features:e,col:n=2}){return(0,r.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${n}, 1fr)`},children:e.map((e,n)=>(0,r.jsxs)("div",{className:"highlightItem_XPnN",children:[e.icon&&(0,r.jsx)("div",{className:"highlightIcon_SUR8",children:(0,r.jsx)(e.icon,{size:24})}),(0,r.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,r.jsx)("h4",{children:e.title}),(0,r.jsx)("p",{children:e.description})]})]},n))})}},95986(e,n,t){t.d(n,{A:()=>l});var r=t(74848);t(96540);function l({children:e}){return(0,r.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,n,t){t.d(n,{A:()=>c});var r=t(74848);t(96540);var l=t(95310),i=t(34164);let a="tileImage_O4So";var s=t(66497),o=t(92802);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:d,iconSize:p=32,containerHeight:m,title:h,description:u,href:f,linkText:g="Learn more \u2192",className:y}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let w=m?{height:`${m}px`}:{},_={};return c&&(_.width=`${c}px`),d&&(_.height=`${d}px`),(0,r.jsxs)(l.A,{href:f,className:(0,i.A)("tileCard_NHsj",y),children:[(0,r.jsx)("div",{className:"tileIcon_pyoR",style:w,children:e?(0,r.jsx)(e,{size:p}):t?(0,r.jsx)(o.A,{sources:{light:(0,s.default)(n),dark:(0,s.default)(t)},alt:h,className:a,style:_}):(0,r.jsx)("img",{src:(0,s.default)(n),alt:h,className:a,style:_})}),(0,r.jsx)("h3",{children:h}),(0,r.jsx)("p",{children:u}),(0,r.jsx)("div",{className:"tileLink_iUbu",children:g})]})}},10440(e,n,t){t.d(n,{A:()=>i});var r=t(74848);t(96540);var l=t(34164);function i({children:e,className:n}){return(0,r.jsx)("div",{className:(0,l.A)("tilesGrid_hB9N",n),children:e})}}}]);