"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[1448],{9697:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>f,contentTitle:()=>h,default:()=>w,frontMatter:()=>m,metadata:()=>l,toc:()=>u});const l=JSON.parse('{"id":"tracing/integrations/listing/fireworksai","title":"Tracing FireworksAI","description":"FireworksAI is an inference and customization engine for open source AI. It provides day zero access to the latest SOTA OSS models and allows developers to build lightning AI applications.","source":"@site/docs/genai/tracing/integrations/listing/fireworksai.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/fireworksai","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/fireworksai","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"sidebar_label":"FireworksAI"},"sidebar":"genAISidebar","previous":{"title":"DeepSeek","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/deepseek"},"next":{"title":"Gemini","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/gemini"}}');var o=t(74848),a=t(28453),r=t(49374),i=t(11470),s=t(19365),c=t(66927),p=t(47020);const m={sidebar_position:12,sidebar_label:"FireworksAI"},h="Tracing FireworksAI",f={},u=[{value:"Supported APIs",id:"supported-apis",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Chat Completion API Examples",id:"chat-completion-api-examples",level:2},{value:"Token Usage",id:"token-usage",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"tracing-fireworksai",children:"Tracing FireworksAI"})}),"\n",(0,o.jsx)(c.A,{src:"/images/llms/tracing/fireworks-ai-tracing.png",alt:"FireworksAI Tracing via autolog"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://fireworks.ai",children:"FireworksAI"})," is an inference and customization engine for open source AI. It provides day zero access to the latest SOTA OSS models and allows developers to build lightning AI applications."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/genai/tracing",children:"MLflow Tracing"})," provides automatic tracing capability for FireworksAI through the OpenAI SDK compatibility. FireworksAI is ",(0,o.jsx)(n.a,{href:"https://fireworks.ai/docs/tools-sdks/openai-compatibility#openai-compatibility",children:"OpenAI SDK compatible"}),", you can use the ",(0,o.jsx)(r.B,{fn:"mlflow.openai.autolog"})," function to enable automatic tracing. MLflow will capture traces for LLM invocations and log them to the active MLflow Experiment."]}),"\n",(0,o.jsx)(n.p,{children:"MLflow automatically captures the following information about FireworksAI calls:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Prompts and completion responses"}),"\n",(0,o.jsx)(n.li,{children:"Latencies"}),"\n",(0,o.jsx)(n.li,{children:"Model name"}),"\n",(0,o.jsxs)(n.li,{children:["Additional metadata such as ",(0,o.jsx)(n.code,{children:"temperature"}),", ",(0,o.jsx)(n.code,{children:"max_completion_tokens"}),", if specified"]}),"\n",(0,o.jsx)(n.li,{children:"Tool Use if returned in the response"}),"\n",(0,o.jsx)(n.li,{children:"Any exception if raised"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"supported-apis",children:"Supported APIs"}),"\n",(0,o.jsxs)(n.p,{children:["Since FireworksAI is OpenAI SDK compatible, all APIs supported by MLflow's OpenAI integration work seamlessly with FireworksAI. See ",(0,o.jsx)(n.a,{href:"https://fireworks.ai/models",children:"the model library"})," for a list of available models on FireworksAI."]}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Normal"}),(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Tool Use"}),(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Structured Outputs"}),(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Streaming"}),(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Async"})]})}),(0,o.jsx)(n.tbody,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]})})]}),"\n",(0,o.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\nimport os\n\n# Enable auto-tracing\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("FireworksAI")\n\n# Create an OpenAI client configured for FireworksAI\nopenai_client = openai.OpenAI(\n    base_url="https://api.fireworks.ai/inference/v1",\n    api_key=os.getenv("FIREWORKS_API_KEY"),\n)\n\n# Use the client as usual - traces will be automatically captured\nresponse = openai_client.chat.completions.create(\n    model="accounts/fireworks/models/deepseek-v3-0324",  # For other models see: https://fireworks.ai/models\n    messages=[\n        {"role": "user", "content": "Why is open source better than closed source?"}\n    ],\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"chat-completion-api-examples",children:"Chat Completion API Examples"}),"\n",(0,o.jsx)(p.A,{children:(0,o.jsxs)(i.A,{children:[(0,o.jsx)(s.A,{value:"basic",label:"Basic Example",default:!0,children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport mlflow\nimport os\n\n# Enable auto-tracing\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\n# If running locally you can start a server with:  `mlflow server --host 127.0.0.1 --port 5000`\nmlflow.set_tracking_uri("http://127.0.0.1:5000")\nmlflow.set_experiment("FireworksAI")\n\n# Configure OpenAI client for FireworksAI\nopenai_client = openai.OpenAI(\n    base_url="https://api.fireworks.ai/inference/v1",\n    api_key=os.getenv("FIREWORKS_API_KEY"),\n)\n\nmessages = [\n    {\n        "role": "user",\n        "content": "What is the capital of France?",\n    }\n]\n\n# To use different models check out the model library at: https://fireworks.ai/models\nresponse = openai_client.chat.completions.create(\n    model="accounts/fireworks/models/deepseek-v3-0324",\n    messages=messages,\n    max_completion_tokens=100,\n)\n'})})}),(0,o.jsxs)(s.A,{value:"streaming",label:"Streaming",children:[(0,o.jsxs)(n.p,{children:["MLflow Tracing supports streaming API outputs of FireworksAI endpoints through the OpenAI SDK. With the same setup of auto tracing, MLflow automatically traces the streaming response and renders the concatenated output in the span UI. The actual chunks in the response stream can be found in the ",(0,o.jsx)(n.code,{children:"Event"})," tab as well."]}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport mlflow\nimport os\n\n# Enable trace logging\nmlflow.openai.autolog()\n\nclient = openai.OpenAI(\n    base_url="https://api.fireworks.ai/inference/v1",\n    api_key=os.getenv("FIREWORKS_API_KEY"),\n)\n\nstream = client.chat.completions.create(\n    model="accounts/fireworks/models/deepseek-v3-0324",\n    messages=[\n        {"role": "user", "content": "How fast would a glass of water freeze on Titan?"}\n    ],\n    stream=True,  # Enable streaming response\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or "", end="")\n'})})]}),(0,o.jsxs)(s.A,{value:"async",label:"Async",children:[(0,o.jsx)(n.p,{children:"MLflow Tracing supports asynchronous API returns of FireworksAI through the OpenAI SDK. The usage is the same as the synchronous API."}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport mlflow\nimport os\n\n# Enable trace logging\nmlflow.openai.autolog()\n\nclient = openai.AsyncOpenAI(\n    base_url="https://api.fireworks.ai/inference/v1",\n    api_key=os.getenv("FIREWORKS_API_KEY"),\n)\n\nresponse = await client.chat.completions.create(\n    model="accounts/fireworks/models/deepseek-v3-0324",\n    messages=[{"role": "user", "content": "What is the best open source LLM?"}],\n    # Async streaming is also supported\n    # stream=True\n)\n'})})]}),(0,o.jsxs)(s.A,{value:"tool-use",label:"Tool Use",children:[(0,o.jsxs)(n.p,{children:["MLflow Tracing automatically captures tool use responses from FireworksAI models. The function instruction in the response will be highlighted in the trace UI. Moreover, you can annotate the tool function with the ",(0,o.jsx)(n.code,{children:"@mlflow.trace"})," decorator to create a span for the tool execution."]}),(0,o.jsx)(n.p,{children:"The following example implements a simple tool use agent using FireworksAI and MLflow Tracing:"}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import json\nfrom openai import OpenAI\nimport mlflow\nfrom mlflow.entities import SpanType\nimport os\n\nclient = OpenAI(\n    base_url="https://api.fireworks.ai/inference/v1",\n    api_key=os.getenv("FIREWORKS_API_KEY"),\n)\n\n\n# Define the tool function. Decorate it with `@mlflow.trace` to create a span for its execution.\n@mlflow.trace(span_type=SpanType.TOOL)\ndef get_weather(city: str) -> str:\n    if city == "Tokyo":\n        return "sunny"\n    elif city == "Paris":\n        return "rainy"\n    return "unknown"\n\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "parameters": {\n                "type": "object",\n                "properties": {"city": {"type": "string"}},\n            },\n        },\n    }\n]\n\n_tool_functions = {"get_weather": get_weather}\n\n\n# Define a simple tool calling agent\n@mlflow.trace(span_type=SpanType.AGENT)\ndef run_tool_agent(question: str):\n    messages = [{"role": "user", "content": question}]\n\n    # Invoke the model with the given question and available tools\n    response = client.chat.completions.create(\n        model="accounts/fireworks/models/gpt-oss-20b",\n        messages=messages,\n        tools=tools,\n    )\n    ai_msg = response.choices[0].message\n    messages.append(ai_msg)\n\n    # If the model requests tool call(s), invoke the function with the specified arguments\n    if tool_calls := ai_msg.tool_calls:\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            if tool_func := _tool_functions.get(function_name):\n                args = json.loads(tool_call.function.arguments)\n                tool_result = tool_func(**args)\n            else:\n                raise RuntimeError("An invalid tool is returned from the assistant!")\n\n            messages.append(\n                {\n                    "role": "tool",\n                    "tool_call_id": tool_call.id,\n                    "content": tool_result,\n                }\n            )\n\n        # Send the tool results to the model and get a new response\n        response = client.chat.completions.create(\n            model="accounts/fireworks/models/llama-v3p1-8b-instruct", messages=messages\n        )\n\n    return response.choices[0].message.content\n\n\n# Run the tool calling agent\nquestion = "What\'s the weather like in Paris today?"\nanswer = run_tool_agent(question)\n'})})]})]})}),"\n",(0,o.jsx)(n.h2,{id:"token-usage",children:"Token Usage"}),"\n",(0,o.jsxs)(n.p,{children:["MLflow supports token usage tracking for FireworksAI. The token usage for each LLM call will be logged in the ",(0,o.jsx)(n.code,{children:"mlflow.chat.tokenUsage"})," attribute. The total token usage throughout the trace will be available in the ",(0,o.jsx)(n.code,{children:"token_usage"})," field of the trace info object."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import json\nimport mlflow\n\nmlflow.openai.autolog()\n\n# Run the tool calling agent defined in the previous section\nquestion = "What\'s the weather like in Paris today?"\nanswer = run_tool_agent(question)\n\n# Get the trace object just created\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Detailed usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 20\n  Output tokens: 283\n  Total tokens: 303\n\n== Detailed usage for each LLM call: ==\nCompletions:\n  Input tokens: 20\n  Output tokens: 283\n  Total tokens: 303\n"})}),"\n",(0,o.jsx)(n.h2,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,o.jsxs)(n.p,{children:["Auto tracing for FireworksAI can be disabled globally by calling ",(0,o.jsx)(n.code,{children:"mlflow.openai.autolog(disable=True)"})," or ",(0,o.jsx)(n.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function w(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},11470:(e,n,t)=>{t.d(n,{A:()=>v});var l=t(96540),o=t(34164),a=t(17559),r=t(23104),i=t(56347),s=t(205),c=t(57485),p=t(31682),m=t(70679);function h(e){return l.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function f(e){const{values:n,children:t}=e;return(0,l.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:t,default:l}})=>({value:e,label:n,attributes:t,default:l}))}(t);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function u({value:e,tabValues:n}){return n.some(n=>n.value===e)}function d({queryString:e=!1,groupId:n}){const t=(0,i.W6)(),o=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(o),(0,l.useCallback)(e=>{if(!o)return;const n=new URLSearchParams(t.location.search);n.set(o,e),t.replace({...t.location,search:n.toString()})},[o,t])]}function w(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,a=f(e),[r,i]=(0,l.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[c,p]=d({queryString:t,groupId:o}),[h,w]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,o]=(0,m.Dv)(n);return[t,(0,l.useCallback)(e=>{n&&o.set(e)},[n,o])]}({groupId:o}),g=(()=>{const e=c??h;return u({value:e,tabValues:a})?e:null})();(0,s.A)(()=>{g&&i(g)},[g]);return{selectedValue:r,selectValue:(0,l.useCallback)(e=>{if(!u({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),p(e),w(e)},[p,w,a]),tabValues:a}}var g=t(92303);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function b({className:e,block:n,selectedValue:t,selectValue:l,tabValues:a}){const i=[],{blockElementScrollPositionUntilNextRender:s}=(0,r.a_)(),c=e=>{const n=e.currentTarget,o=i.indexOf(n),r=a[o].value;r!==t&&(s(n),l(r))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:l})=>(0,y.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:p,onClick:c,...l,className:(0,o.A)("tabs__item",_.tabItem,l?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function k({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,l.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,l.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function x(e){const n=w(e);return(0,y.jsxs)("div",{className:(0,o.A)(a.G.tabs.container,"tabs-container",_.tabList),children:[(0,y.jsx)(b,{...n,...e}),(0,y.jsx)(k,{...n,...e})]})}function v(e){const n=(0,g.A)();return(0,y.jsx)(x,{...e,children:h(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var l=t(34164);const o={tabItem:"tabItem_Ymn6"};var a=t(74848);function r({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,l.A)(o.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var l=t(96540);const o={},a=l.createContext(o);function r(e){const n=l.useContext(a);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),l.createElement(a.Provider,{value:n},e.children)}},47020:(e,n,t)=>{t.d(n,{A:()=>a});t(96540);const l={wrapper:"wrapper_sf5q"};var o=t(74848);function a({children:e}){return(0,o.jsx)("div",{className:l.wrapper,children:e})}},49374:(e,n,t)=>{t.d(n,{B:()=>i});t(96540);const l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var o=t(86025),a=t(74848);const r=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(l[e])return e}return null};function i({fn:e,children:n,hash:t}){const i=r(e);if(!i)return(0,a.jsx)(a.Fragment,{children:n});const s=(0,o.default)(`/${l[i]}#${t??e}`);return(0,a.jsx)("a",{href:s,target:"_blank",children:n??(0,a.jsxs)("code",{children:[e,"()"]})})}},66927:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);const l={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var o=t(86025),a=t(74848);function r({src:e,alt:n,width:t,caption:r,className:i}){return(0,a.jsxs)("div",{className:`${l.container} ${i||""}`,children:[(0,a.jsx)("div",{className:l.imageWrapper,style:t?{width:t}:{},children:(0,a.jsx)("img",{src:(0,o.default)(e),alt:n,className:l.image})}),r&&(0,a.jsx)("p",{className:l.caption,children:r})]})}}}]);