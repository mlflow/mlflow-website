"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["5787"],{79897(e,r,n){n.r(r),n.d(r,{metadata:()=>t,default:()=>v,frontMatter:()=>x,contentTitle:()=>j,toc:()=>w,assets:()=>g});var t=JSON.parse('{"id":"eval-monitor/scorers/custom/index","title":"Create custom code-based scorers","description":"Custom code-based scorers offer the ultimate flexibility to define precisely how your GenAI application\'s quality is measured. You can define evaluation metrics tailored to your specific business use case, whether based on simple heuristics, advanced logic, or programmatic evaluations.","source":"@site/docs/genai/eval-monitor/scorers/custom/index.mdx","sourceDirName":"eval-monitor/scorers/custom","slug":"/eval-monitor/scorers/custom/","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Create a Custom Judge","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge"},"next":{"title":"Code-based Scorer Examples","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom/code-examples"}}'),s=n(74848),a=n(28453),l=n(54725),o=n(46077),i=n(78010),c=n(57250),d=n(95986),h=n(77541),p=n(10440),m=n(93164),u=n(7043),f=n(42640);let x={},j="Create custom code-based scorers",g={},w=[{value:"How custom scorers work",id:"how-custom-scorers-work",level:2},{value:"Define scorers with the <code>@scorer</code> decorator",id:"define-scorers-with-the-scorer-decorator",level:2},{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Simple values",id:"simple-values",level:3},{value:"Rich feedback",id:"rich-feedback",level:3},{value:"Metric naming behavior",id:"metric-naming-behavior",level:2},{value:"Parsing Traces for Scoring",id:"parsing-traces-for-scoring",level:2},{value:"Example 1: Evaluating Retrieved Documents Recall",id:"example-1-evaluating-retrieved-documents-recall",level:3},{value:"Example 2: Evaluating Tool Call Trajectory",id:"example-2-evaluating-tool-call-trajectory",level:3},{value:"Example 3: Evaluating Sub-Agents Routing",id:"example-3-evaluating-sub-agents-routing",level:3},{value:"Error handling",id:"error-handling",level:2},{value:"Let exceptions propagate (recommended)",id:"let-exceptions-propagate-recommended",level:3},{value:"Handle exceptions explicitly",id:"handle-exceptions-explicitly",level:3},{value:"Define scorers with the Scorer class",id:"define-scorers-with-the-scorer-class",level:2},{value:"State management",id:"state-management",level:3},{value:"Next steps",id:"next-steps",level:2}];function _(e){let r={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"create-custom-code-based-scorers",children:"Create custom code-based scorers"})}),"\n",(0,s.jsxs)(r.p,{children:["Custom code-based ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers",children:"scorers"})," offer the ultimate flexibility to define precisely how your GenAI application's quality is measured. You can define evaluation metrics tailored to your specific business use case, whether based on simple heuristics, advanced logic, or programmatic evaluations."]}),"\n",(0,s.jsx)(r.p,{children:"Use custom scorers for the following scenarios:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Defining a custom heuristic or code-based evaluation metric."}),"\n",(0,s.jsxs)(r.li,{children:["Customizing how the data from your app's trace is mapped to MLflow's ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges",children:"research-backed LLM judges"}),"."]}),"\n",(0,s.jsx)(r.li,{children:"Using your own LLM for evaluation."}),"\n",(0,s.jsxs)(r.li,{children:["Any other use cases where you need more flexibility and control than provided by ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges",children:"custom LLM judges"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(r.p,{children:["For a tutorial with many examples, see ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/custom/code-examples",children:"Code-based scorer examples"}),"."]}),"\n",(0,s.jsx)(r.h2,{id:"how-custom-scorers-work",children:"How custom scorers work"}),"\n",(0,s.jsxs)(r.p,{children:["Custom scorers are written in Python and give you full control to evaluate any data from your app's traces. After you define a custom scorer, you can use it exactly like a ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges",children:"built-in LLM Judge"}),"."]}),"\n",(0,s.jsxs)(r.p,{children:["For example, suppose you want a scorer that checks if the LLM's response exactly matches the ",(0,s.jsx)(r.code,{children:"expected_response"})," and is short enough. The image of the MLflow UI below shows traces scored by these custom metrics."]}),"\n",(0,s.jsx)(o.A,{src:"/images/mlflow-3/eval-monitor/scorers/code-scorers-results.png",alt:"Code-based Scorers"}),"\n",(0,s.jsxs)(r.p,{children:["The code snippet below defines these custom scorers and uses it with ",(0,s.jsx)(l.B,{fn:"mlflow.genai.evaluate",children:(0,s.jsx)(r.code,{children:"mlflow.genai.evaluate()"})}),":"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.genai import scorer\nfrom mlflow.entities import Feedback\n\n\n@scorer\ndef exact_match(outputs: dict, expectations: dict) -> bool:\n    return outputs == expectations["expected_response"]\n\n\n@scorer\ndef is_short(outputs: dict) -> Feedback:\n    score = len(outputs.split()) <= 5\n    rationale = (\n        "The response is short enough."\n        if score\n        else f"The response is not short enough because it has ({len(outputs.split())} words)."\n    )\n    return Feedback(value=score, rationale=rationale)\n\n\neval_dataset = [\n    {\n        "inputs": {"question": "How many countries are there in the world?"},\n        "outputs": "195",\n        "expectations": {"expected_response": "195"},\n    },\n    {\n        "inputs": {"question": "What is the capital of France?"},\n        "outputs": "The capital of France is Paris.",\n        "expectations": {"expected_response": "Paris"},\n    },\n]\n\nmlflow.genai.evaluate(\n    data=eval_dataset,\n    scorers=[exact_match, is_short],\n)\n'})}),"\n",(0,s.jsx)(r.p,{children:"The example above illustrates a common pattern for code-based scorers:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsxs)(r.a,{href:"/genai/eval-monitor/scorers/custom/#define-scorers-with-the-scorer-decorator",children:["The ",(0,s.jsx)(r.code,{children:"@scorer"})," decorator"]})," is used to define the scorer."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/custom/#inputs",children:"The input"})," to this scorer is the full trace, giving it access to the AI app's inputs, intermediate spans, and outputs."]}),"\n",(0,s.jsx)(r.li,{children:"Scorer logic can be fully custom. You can call LLMs or other scorers."}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/custom/#outputs",children:"The output"})," of this scorer is a rich ",(0,s.jsx)(r.code,{children:"Feedback"})," object with values and explanations."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/custom/#metric-naming-behavior",children:"The metric name"})," is ",(0,s.jsx)(r.code,{children:"llm_response_time_good"}),", matching the scorer function name."]}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"This pattern is just one possibility for code-based scorers. The rest of this article explains options for defining custom scorers."}),"\n",(0,s.jsxs)(r.h2,{id:"define-scorers-with-the-scorer-decorator",children:["Define scorers with the ",(0,s.jsx)(r.code,{children:"@scorer"})," decorator"]}),"\n",(0,s.jsxs)(r.p,{children:["Most code-based scorers should be defined using the ",(0,s.jsxs)(l.B,{fn:"mlflow.genai.scorers.scorer",children:[(0,s.jsx)(r.code,{children:"@scorer"})," decorator"]}),". Below is the signature for such scorers, illustrating possible inputs and outputs."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"from mlflow.genai.scorers import scorer\nfrom typing import Optional, Any\nfrom mlflow.entities import Feedback\n\n\n@scorer\ndef my_custom_scorer(\n    *,  # All arguments are keyword-only\n    inputs: Optional[\n        dict[str, Any]\n    ],  # App's raw input, a dictionary of input argument names and values\n    outputs: Optional[Any],  # App's raw output\n    expectations: Optional[\n        dict[str, Any]\n    ],  # Ground truth, a dictionary of label names and values\n    trace: Optional[mlflow.entities.Trace]  # Complete trace with all spans and metadata\n) -> Union[int, float, bool, str, Feedback, List[Feedback]]:\n    # Your evaluation logic here\n    ...\n"})}),"\n",(0,s.jsxs)(r.p,{children:["For more flexibility than the ",(0,s.jsx)(r.code,{children:"@scorer"})," decorator allows, you can instead define scorers using the ",(0,s.jsxs)(r.a,{href:"#define-scorers-with-the-scorer-class",children:[(0,s.jsx)(r.code,{children:"Scorer"})," class"]}),"."]}),"\n",(0,s.jsx)(r.h2,{id:"inputs",children:"Inputs"}),"\n",(0,s.jsxs)(r.p,{children:["Scorers receive the complete ",(0,s.jsx)(r.a,{href:"/genai/concepts/trace",children:"MLflow trace"})," containing all spans, attributes, and outputs. As a convenience, MLflow also extracts commonly needed data and passes it as named arguments. All input arguments are optional, so declare only what your scorer needs:"]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"inputs"}),": The request sent to your app (e.g., user query, context)."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"outputs"}),": The response from your app (e.g., generated text, tool calls)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"expectations"}),": Ground truth or labels (e.g., expected response, guidelines, etc.)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"trace"}),": The complete ",(0,s.jsx)(r.a,{href:"/genai/concepts/trace",children:"MLflow trace"})," with all spans, allowing analysis of intermediate steps, latency, tool usage, and more. The trace is passed to the custom scorer as an instantiated ",(0,s.jsxs)(l.B,{fn:"mlflow.entities.Trace",children:[(0,s.jsx)(r.code,{children:"mlflow.entities.Trace"})," class"]})," class."]}),"\n"]}),"\n",(0,s.jsxs)(r.p,{children:["When running ",(0,s.jsx)(l.B,{fn:"mlflow.genai.evaluate",children:"mlflow.genai.evaluate()"}),", the ",(0,s.jsx)(r.code,{children:"inputs"}),", ",(0,s.jsx)(r.code,{children:"outputs"}),", and ",(0,s.jsx)(r.code,{children:"expectations"})," parameters can be specified in the ",(0,s.jsx)(r.code,{children:"data"})," argument, or parsed from the trace."]}),"\n",(0,s.jsx)(r.h2,{id:"outputs",children:"Outputs"}),"\n",(0,s.jsxs)(r.p,{children:["Scorers can return different types of ",(0,s.jsx)(r.a,{href:"#simple-values",children:"simple values"})," or ",(0,s.jsx)(r.a,{href:"#rich-feedback",children:"rich Feedback objects"})," depending on your evaluation needs."]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Return Type"}),(0,s.jsx)(r.th,{children:"MLflow UI Display"}),(0,s.jsx)(r.th,{children:"Use Case"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:'"yes"'}),"/",(0,s.jsx)(r.code,{children:'"no"'})]}),(0,s.jsx)(r.td,{children:"Pass/Fail"}),(0,s.jsx)(r.td,{children:"Binary evaluation"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"True"}),"/",(0,s.jsx)(r.code,{children:"False"})]}),(0,s.jsx)(r.td,{children:"True/False"}),(0,s.jsx)(r.td,{children:"Boolean checks"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"int"}),"/",(0,s.jsx)(r.code,{children:"float"})]}),(0,s.jsx)(r.td,{children:"Numeric value"}),(0,s.jsx)(r.td,{children:"Scores, counts"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(l.B,{fn:"mlflow.entities.Feedback",children:(0,s.jsx)(r.code,{children:"Feedback"})})}),(0,s.jsx)(r.td,{children:"Value + rationale"}),(0,s.jsx)(r.td,{children:"Detailed assessment"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:["List[",(0,s.jsx)(l.B,{fn:"mlflow.entities.Feedback",children:"Feedback"}),"]"]}),(0,s.jsx)(r.td,{children:"Multiple metrics"}),(0,s.jsx)(r.td,{children:"Multi-aspect evaluation"})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"simple-values",children:"Simple values"}),"\n",(0,s.jsx)(r.p,{children:"Output primitive values for straightforward pass/fail or numeric assessments. Below are simple scorers for an AI app that returns a string as a response."}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'@scorer\ndef response_length(outputs: str) -> int:\n    # Return a numeric metric\n    return len(outputs.split())\n\n\n@scorer\ndef contains_citation(outputs: str) -> str:\n    # Return pass/fail string\n    return "yes" if "[source]" in outputs else "no"\n'})}),"\n",(0,s.jsx)(r.h3,{id:"rich-feedback",children:"Rich feedback"}),"\n",(0,s.jsxs)(r.p,{children:["Return a ",(0,s.jsx)(l.B,{fn:"mlflow.entities.Feedback",children:(0,s.jsx)(r.code,{children:"Feedback"})})," object or list of ",(0,s.jsx)(r.code,{children:"Feedback"})," objects for detailed assessments with scores, rationales, and metadata."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.entities import Feedback, AssessmentSource\n\n\n@scorer\ndef content_quality(outputs):\n    return Feedback(\n        value=0.85,  # Can be numeric, boolean, string, or other types\n        rationale="Clear and accurate, minor grammar issues",\n        # Optional: source of the assessment. Several source types are supported,\n        # such as "HUMAN", "CODE", "LLM_JUDGE".\n        source=AssessmentSource(source_type="HUMAN", source_id="grammar_checker_v1"),\n        # Optional: additional metadata about the assessment.\n        metadata={\n            "annotator": "me@example.com",\n        },\n    )\n'})}),"\n",(0,s.jsxs)(r.p,{children:["Multiple feedback objects can be returned as a list. Each feedback should have the ",(0,s.jsx)(r.code,{children:"name"})," field specified, and those names will be displayed as separate metrics in the evaluation results."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'@scorer\ndef comprehensive_check(inputs, outputs):\n    return [\n        Feedback(name="relevance", value=True, rationale="Directly addresses query"),\n        Feedback(\n            name="tone", value="professional", rationale="Appropriate for audience"\n        ),\n        Feedback(name="length", value=150, rationale="Word count within limits"),\n    ]\n'})}),"\n",(0,s.jsx)(r.h2,{id:"metric-naming-behavior",children:"Metric naming behavior"}),"\n",(0,s.jsxs)(r.p,{children:["As you define scorers, use clear, consistent names that indicate the scorer's purpose. These names will appear as metric names in your evaluation and dashboards. Follow MLflow naming conventions such as ",(0,s.jsx)(r.code,{children:"safety_check"})," or ",(0,s.jsx)(r.code,{children:"relevance_monitor"}),"."]}),"\n",(0,s.jsxs)(r.p,{children:["When you define scorers using either the ",(0,s.jsx)(l.B,{fn:"mlflow.genai.scorers.scorer",children:(0,s.jsx)(r.code,{children:"@scorer"})})," decorator or the ",(0,s.jsxs)(r.a,{href:"#define-scorers-with-the-scorer-class",children:[(0,s.jsx)(r.code,{children:"Scorer"})," class"]}),", the metric names in the ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/#running-an-evaluation",children:"evaluation runs"})," created by evaluation and monitoring follow simple rules:"]}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:["If the scorer returns one or more ",(0,s.jsx)(r.code,{children:"Feedback"})," objects, then ",(0,s.jsx)(r.code,{children:"Feedback.name"})," fields take precedence, if specified."]}),"\n",(0,s.jsxs)(r.li,{children:["For primitive return values or unnamed ",(0,s.jsx)(r.code,{children:"Feedback"}),"s, the function name (for the ",(0,s.jsx)(r.code,{children:"@scorer"})," decorator) or the ",(0,s.jsx)(r.code,{children:"Scorer.name"})," field (for the ",(0,s.jsx)(r.code,{children:"Scorer"})," class) are used."]}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"Expanding these rules to all possibilities gives the following table for metric naming behavior:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Return value"}),(0,s.jsxs)(r.th,{children:[(0,s.jsx)(r.code,{children:"@scorer"})," decorator behavior"]}),(0,s.jsxs)(r.th,{children:[(0,s.jsx)(r.code,{children:"Scorer"})," class behavior"]})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:["Primitive value (",(0,s.jsx)(r.code,{children:"int"}),", ",(0,s.jsx)(r.code,{children:"float"}),", ",(0,s.jsx)(r.code,{children:"str"}),")"]}),(0,s.jsx)(r.td,{children:"Function name"}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"name"})," field"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Feedback without name"}),(0,s.jsx)(r.td,{children:"Function name"}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"name"})," field"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Feedback with name"}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"Feedback"})," name"]}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"Feedback"})," name"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"List[Feedback]"})," with names"]}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"Feedback"})," names"]}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"Feedback"})," names"]})]})]})]}),"\n",(0,s.jsxs)(r.p,{children:["For evaluation, it is important that all metrics have distinct names. If a scorer returns ",(0,s.jsx)(r.code,{children:"List[Feedback]"}),", then each ",(0,s.jsx)(r.code,{children:"Feedback"})," in the ",(0,s.jsx)(r.code,{children:"List"})," must have a distinct name."]}),"\n",(0,s.jsxs)(r.p,{children:["See ",(0,s.jsx)(r.a,{href:"/genai/eval-monitor/scorers/custom/code-examples",children:"examples of naming behavior"})," in the tutorial."]}),"\n",(0,s.jsx)(r.h2,{id:"parsing-traces-for-scoring",children:"Parsing Traces for Scoring"}),"\n",(0,s.jsxs)(r.admonition,{title:"Important: Agent-as-a-Judge Scorers Require Active Traces",type:"warning",children:[(0,s.jsxs)(r.p,{children:["Scorers that accept a ",(0,s.jsx)(r.code,{children:"trace"})," parameter ",(0,s.jsx)(r.strong,{children:"cannot be used with pandas DataFrames"}),". They require actual execution traces from your application."]}),(0,s.jsxs)(r.p,{children:["If you need to evaluate static data (e.g., a CSV file with pre-generated responses), use field-based scorers that work with ",(0,s.jsx)(r.code,{children:"inputs"}),", ",(0,s.jsx)(r.code,{children:"outputs"}),", and ",(0,s.jsx)(r.code,{children:"expectations"})," parameters only."]})]}),"\n",(0,s.jsxs)(r.p,{children:["Scorers have access to the complete MLflow traces, including spans, attributes, and outputs, allowing you to evaluate the agent's behavior precisely, not just the final output.\nThe ",(0,s.jsx)(l.B,{fn:"mlflow.entities.Trace.search_spans",children:(0,s.jsx)(r.code,{children:"Trace.search_spans"})})," API is a powerful way to retrieve such intermediate information from the trace."]}),"\n",(0,s.jsx)(r.p,{children:"Open the tabs below to see examples of custom scorers that evaluate the detailed behavior of agents by parsing the trace."}),"\n",(0,s.jsx)(d.A,{children:(0,s.jsxs)(i.A,{children:[(0,s.jsxs)(c.A,{value:"retrieved_document_recall",label:"Retrieved Document Recall",children:[(0,s.jsx)(r.h3,{id:"example-1-evaluating-retrieved-documents-recall",children:"Example 1: Evaluating Retrieved Documents Recall"}),(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.entities import SpanType, Trace\nfrom mlflow.genai import scorer\n\n\n@scorer\ndef retrieved_document_recall(trace: Trace, expectations: dict) -> Feedback:\n    # Search for retriever spans in the trace\n    retriever_spans = trace.search_spans(span_type=SpanType.RETRIEVER)\n\n    # If there are no retriever spans\n    if not retriever_spans:\n        return Feedback(\n            value=0,\n            rationale="No retriever span found in the trace.",\n        )\n\n    # Gather all retrieved document URLs from the retriever spans\n    all_document_urls = []\n    for span in retriever_spans:\n        all_document_urls.extend([document["doc_uri"] for document in span.outputs])\n\n    # Compute the recall\n    true_positives = len(\n        set(all_document_urls) & set(expectations["relevant_document_urls"])\n    )\n    expected_positives = len(expectations["relevant_document_urls"])\n    recall = true_positives / expected_positives\n    return Feedback(\n        value=recall,\n        rationale=f"Retrieved {true_positives} relevant documents out of {expected_positives} expected.",\n    )\n'})})]}),(0,s.jsxs)(c.A,{value:"tool_call_trajectory",label:"Tool Call Trajectory",children:[(0,s.jsx)(r.h3,{id:"example-2-evaluating-tool-call-trajectory",children:"Example 2: Evaluating Tool Call Trajectory"}),(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.entities import SpanType, Trace\nfrom mlflow.genai import scorer\n\n\n@scorer\ndef tool_call_trajectory(trace: Trace, expectations: dict) -> Feedback:\n    # Search for tool call spans in the trace\n    tool_call_spans = trace.search_spans(span_type=SpanType.TOOL)\n\n    # Compare the tool trajectory with expectations\n    actual_trajectory = [span.name for span in tool_call_spans]\n    expected_trajectory = expectations["tool_call_trajectory"]\n\n    if actual_trajectory == expected_trajectory:\n        return Feedback(value=1, rationale="The tool call trajectory is correct.")\n    else:\n        return Feedback(\n            value=0,\n            rationale=(\n                "The tool call trajectory is incorrect.\\n"\n                f"Expected: {expected_trajectory}.\\n"\n                f"Actual: {actual_trajectory}."\n            ),\n        )\n'})})]}),(0,s.jsxs)(c.A,{value:"sub_agents_routing",label:"Sub-Agents Routing",children:[(0,s.jsx)(r.h3,{id:"example-3-evaluating-sub-agents-routing",children:"Example 3: Evaluating Sub-Agents Routing"}),(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.entities import SpanType, Trace\nfrom mlflow.genai import scorer\n\n\n@scorer\ndef is_routing_correct(trace: Trace, expectations: dict) -> Feedback:\n    # Search for sub-agent spans in the trace\n    sub_agent_spans = trace.search_spans(span_type=SpanType.AGENT)\n\n    invoked_agents = [span.name for span in sub_agent_spans]\n    expected_agents = expectations["expected_agents"]\n\n    if invoked_agents == expected_agents:\n        return Feedback(value=True, rationale="The sub-agents routing is correct.")\n    else:\n        return Feedback(\n            value=False,\n            rationale=(\n                "The sub-agents routing is incorrect.\\n"\n                f"Expected: {expected_agents}.\\n"\n                f"Actual: {invoked_agents}."\n            ),\n        )\n'})})]})]})}),"\n",(0,s.jsx)(r.h2,{id:"error-handling",children:"Error handling"}),"\n",(0,s.jsx)(r.p,{children:"When a scorer encounters an error for a trace, MLflow can capture error details for that trace and then continue executing gracefully. For capturing error details, MLflow provides two approaches:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Let exceptions propagate (recommended) so that MLflow can capture error messages for you."}),"\n",(0,s.jsx)(r.li,{children:"Handle exceptions explicitly."}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"let-exceptions-propagate-recommended",children:"Let exceptions propagate (recommended)"}),"\n",(0,s.jsxs)(r.p,{children:["The simplest approach is to let exceptions throw naturally. MLflow automatically captures the exception and creates a ",(0,s.jsx)(r.a,{href:"https://mlflow.org/docs/latest/api_reference/python_api/mlflow.entities.html#mlflow.entities.Feedback",children:(0,s.jsx)(r.code,{children:"Feedback"})})," object with the following error details:"]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"value"}),": ",(0,s.jsx)(r.code,{children:"None"})]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"error"}),": The exception details, such as exception object, error message, and stack trace"]}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"The error information is displayed in the evaluation results. Open the corresponding row to see the error details."}),"\n",(0,s.jsx)(r.h3,{id:"handle-exceptions-explicitly",children:"Handle exceptions explicitly"}),"\n",(0,s.jsxs)(r.p,{children:["For custom error handling or to provide specific error messages, catch exceptions and return a ",(0,s.jsx)(l.B,{fn:"mlflow.entities.Feedback",children:(0,s.jsx)(r.code,{children:"Feedback"})})," with ",(0,s.jsx)(r.code,{children:"None"})," value and error details:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import json\nfrom mlflow.entities import AssessmentError, Feedback\n\n\n@scorer\ndef is_valid_response(outputs):\n    try:\n        data = json.loads(outputs)\n        required_fields = ["summary", "confidence", "sources"]\n        missing = [f for f in required_fields if f not in data]\n\n        if missing:\n            return Feedback(\n                error=AssessmentError(\n                    error_code="MISSING_REQUIRED_FIELDS",\n                    error_message=f"Missing required fields: {missing}",\n                ),\n            )\n\n        return Feedback(value=True, rationale="Valid JSON with all required fields")\n\n    except json.JSONDecodeError as e:\n        return Feedback(\n            error=e\n        )  # Can pass exception object directly to the error parameter\n'})}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"error"})," parameter accepts:"]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Python Exception"}),": Pass the exception object directly"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(l.B,{fn:"mlflow.entities.Feedback",children:(0,s.jsx)(r.code,{children:"AssessmentError()"})}),": For structured error reporting with error codes"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"define-scorers-with-the-scorer-class",children:"Define scorers with the Scorer class"}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsxs)(r.a,{href:"#define-scorers-with-the-scorer-decorator",children:[(0,s.jsx)(r.code,{children:"@scorer"})," decorator"]})," described above is simple and generally recommended, but when it is insufficient, you can instead use the ",(0,s.jsx)(l.B,{fn:"mlflow.genai.Scorer",children:(0,s.jsx)(r.code,{children:"Scorer"})})," base class. Class-based definitions allow for more complex scorers, especially scorers that require state. The ",(0,s.jsx)(l.B,{fn:"mlflow.genai.Scorer",children:(0,s.jsx)(r.code,{children:"Scorer"})})," class is a ",(0,s.jsx)(r.a,{href:"https://docs.pydantic.dev/latest/concepts/models/",children:"Pydantic object"}),", so you can define additional fields and use them in the ",(0,s.jsx)(r.code,{children:"__call__"})," method."]}),"\n",(0,s.jsxs)(r.p,{children:["You must define the ",(0,s.jsx)(r.code,{children:"name"})," field to set the metric name. If you return a list of ",(0,s.jsx)(r.code,{children:"Feedback"})," objects, then you must set the ",(0,s.jsx)(r.code,{children:"name"})," field in each ",(0,s.jsx)(r.code,{children:"Feedback"})," to avoid naming conflicts."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.genai.scorers import Scorer\nfrom mlflow.entities import Feedback\nfrom typing import Optional\n\n\n# Scorer class is a Pydantic object\nclass CustomScorer(Scorer):\n    # The `name` field is mandatory\n    name: str = "response_quality"\n    # Define additional fields\n    my_custom_field_1: int = 50\n    my_custom_field_2: Optional[list[str]] = None\n\n    # Override the __call__ method to implement the scorer logic\n    def __call__(self, outputs: str) -> Feedback:\n        # Your logic here\n        return Feedback(value=True, rationale="Response meets all quality criteria")\n'})}),"\n",(0,s.jsx)(r.h3,{id:"state-management",children:"State management"}),"\n",(0,s.jsxs)(r.p,{children:["When writing scorers using the ",(0,s.jsx)(r.code,{children:"Scorer"})," class, be aware of rules for managing state with Python classes. In particular, be sure to use instance attributes, not mutable class attributes. The example below illustrates mistakenly sharing state across scorer instances."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from mlflow.genai.scorers import Scorer\nfrom mlflow.entities import Feedback\n\n\n# WRONG: Don\'t use mutable class attributes\nclass BadScorer(Scorer):\n    results = []  # Shared across all instances!\n\n    name: str = "bad_scorer"\n\n    def __call__(self, outputs, **kwargs):\n        self.results.append(outputs)  # Causes issues\n        return Feedback(value=True)\n\n\n# CORRECT: Use instance attributes\nclass GoodScorer(Scorer):\n    results: list[str] = None\n\n    name: str = "good_scorer"\n\n    def __init__(self):\n        self.results = []  # Per-instance state\n\n    def __call__(self, outputs, **kwargs):\n        self.results.append(outputs)  # Safe\n        return Feedback(value=True)\n'})}),"\n",(0,s.jsx)(r.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,s.jsxs)(p.A,{children:[(0,s.jsx)(h.A,{icon:m.A,title:"Code-based scorer examples",description:"See many examples of code-based scorers",href:"/genai/eval-monitor/scorers/custom/code-examples"}),(0,s.jsx)(h.A,{icon:u.A,title:"Develop code-based scorers",description:"Step through the development workflow for custom scorers",href:"/genai/eval-monitor/scorers/custom/tutorial"}),(0,s.jsx)(h.A,{icon:f.A,title:"Evaluate GenAI during development",description:"Understand how mlflow.genai.evaluate() uses your scorers",href:"/genai/eval-monitor/"})]})]})}function v(e={}){let{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(_,{...e})}):_(e)}},75689(e,r,n){n.d(r,{A:()=>i});var t=n(96540);let s=e=>{let r=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,r,n)=>n?n.toUpperCase():r.toLowerCase());return r.charAt(0).toUpperCase()+r.slice(1)},a=(...e)=>e.filter((e,r,n)=>!!e&&""!==e.trim()&&n.indexOf(e)===r).join(" ").trim();var l={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let o=(0,t.forwardRef)(({color:e="currentColor",size:r=24,strokeWidth:n=2,absoluteStrokeWidth:s,className:o="",children:i,iconNode:c,...d},h)=>(0,t.createElement)("svg",{ref:h,...l,width:r,height:r,stroke:e,strokeWidth:s?24*Number(n)/Number(r):n,className:a("lucide",o),...!i&&!(e=>{for(let r in e)if(r.startsWith("aria-")||"role"===r||"title"===r)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,r])=>(0,t.createElement)(e,r)),...Array.isArray(i)?i:[i]])),i=(e,r)=>{let n=(0,t.forwardRef)(({className:n,...l},i)=>(0,t.createElement)(o,{ref:i,iconNode:r,className:a(`lucide-${s(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,n),...l}));return n.displayName=s(e),n}},42640(e,r,n){n.d(r,{A:()=>t});let t=(0,n(75689).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},93164(e,r,n){n.d(r,{A:()=>t});let t=(0,n(75689).A)("code",[["path",{d:"m16 18 6-6-6-6",key:"eg8j8"}],["path",{d:"m8 6-6 6 6 6",key:"ppft3o"}]])},7043(e,r,n){n.d(r,{A:()=>t});let t=(0,n(75689).A)("hammer",[["path",{d:"m15 12-8.373 8.373a1 1 0 1 1-3-3L12 9",key:"eefl8a"}],["path",{d:"m18 15 4-4",key:"16gjal"}],["path",{d:"m21.5 11.5-1.914-1.914A2 2 0 0 1 19 8.172V7l-2.26-2.26a6 6 0 0 0-4.202-1.756L9 2.96l.92.82A6.18 6.18 0 0 1 12 8.4V10l2 2h1.172a2 2 0 0 1 1.414.586L18.5 14.5",key:"b7pghm"}]])},57250(e,r,n){n.d(r,{A:()=>a});var t=n(74848);n(96540);var s=n(34164);function a({children:e,hidden:r,className:n}){return(0,t.jsx)("div",{role:"tabpanel",className:(0,s.A)("tabItem_Ymn6",n),hidden:r,children:e})}},78010(e,r,n){n.d(r,{A:()=>w});var t=n(74848),s=n(96540),a=n(34164),l=n(88287),o=n(28584),i=n(56347),c=n(99989),d=n(96629),h=n(80618),p=n(41367);function m(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){let{props:r}=e;return!!r&&"object"==typeof r&&"value"in r}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u({value:e,tabValues:r}){return r.some(r=>r.value===e)}var f=n(19863);function x({className:e,block:r,selectedValue:n,selectValue:s,tabValues:l}){let i=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{let r=e.currentTarget,t=l[i.indexOf(r)].value;t!==n&&(c(r),s(t))},h=e=>{let r=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{let n=i.indexOf(e.currentTarget)+1;r=i[n]??i[0];break}case"ArrowLeft":{let n=i.indexOf(e.currentTarget)-1;r=i[n]??i[i.length-1]}}r?.focus()};return(0,t.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":r},e),children:l.map(({value:e,label:r,attributes:s})=>(0,t.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{i.push(e)},onKeyDown:h,onClick:d,...s,className:(0,a.A)("tabs__item","tabItem_LNqP",s?.className,{"tabs__item--active":n===e}),children:r??e},e))})}function j({lazy:e,children:r,selectedValue:n}){let l=(Array.isArray(r)?r:[r]).filter(Boolean);if(e){let e=l.find(e=>e.props.value===n);return e?(0,s.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,t.jsx)("div",{className:"margin-top--md",children:l.map((e,r)=>(0,s.cloneElement)(e,{key:r,hidden:e.props.value!==n}))})}function g(e){let r=function(e){let r,{defaultValue:n,queryString:t=!1,groupId:a}=e,l=function(e){let{values:r,children:n}=e;return(0,s.useMemo)(()=>{let e=r??m(n).map(({props:{value:e,label:r,attributes:n,default:t}})=>({value:e,label:r,attributes:n,default:t})),t=(0,h.XI)(e,(e,r)=>e.value===r.value);if(t.length>0)throw Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[r,n])}(e),[o,f]=(0,s.useState)(()=>(function({defaultValue:e,tabValues:r}){if(0===r.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:r}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${r.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let n=r.find(e=>e.default)??r[0];if(!n)throw Error("Unexpected error: 0 tabValues");return n.value})({defaultValue:n,tabValues:l})),[x,j]=function({queryString:e=!1,groupId:r}){let n=(0,i.W6)(),t=function({queryString:e=!1,groupId:r}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!r)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return r??null}({queryString:e,groupId:r});return[(0,d.aZ)(t),(0,s.useCallback)(e=>{if(!t)return;let r=new URLSearchParams(n.location.search);r.set(t,e),n.replace({...n.location,search:r.toString()})},[t,n])]}({queryString:t,groupId:a}),[g,w]=function({groupId:e}){let r=e?`docusaurus.tab.${e}`:null,[n,t]=(0,p.Dv)(r);return[n,(0,s.useCallback)(e=>{r&&t.set(e)},[r,t])]}({groupId:a}),_=u({value:r=x??g,tabValues:l})?r:null;return(0,c.A)(()=>{_&&f(_)},[_]),{selectedValue:o,selectValue:(0,s.useCallback)(e=>{if(!u({value:e,tabValues:l}))throw Error(`Can't select invalid tab value=${e}`);f(e),j(e),w(e)},[j,w,l]),tabValues:l}}(e);return(0,t.jsxs)("div",{className:(0,a.A)(l.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,t.jsx)(x,{...r,...e}),(0,t.jsx)(j,{...r,...e})]})}function w(e){let r=(0,f.A)();return(0,t.jsx)(g,{...e,children:m(e.children)},String(r))}},54725(e,r,n){n.d(r,{B:()=>l});var t=n(74848);n(96540);var s=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),a=n(66497);function l({fn:e,children:r,hash:n}){let l=(e=>{let r=e.split(".");for(let e=r.length;e>0;e--){let n=r.slice(0,e).join(".");if(s[n])return n}return null})(e);if(!l)return(0,t.jsx)(t.Fragment,{children:r});let o=(0,a.default)(`/${s[l]}#${n??e}`);return(0,t.jsx)("a",{href:o,target:"_blank",children:r??(0,t.jsxs)("code",{children:[e,"()"]})})}},46077(e,r,n){n.d(r,{A:()=>a});var t=n(74848);n(96540);var s=n(66497);function a({src:e,alt:r,width:n,caption:a,className:l}){return(0,t.jsxs)("div",{className:`container_JwLF ${l||""}`,children:[(0,t.jsx)("div",{className:"imageWrapper_RfGN",style:n?{width:n}:{},children:(0,t.jsx)("img",{src:(0,s.default)(e),alt:r,className:"image_bwOA"})}),a&&(0,t.jsx)("p",{className:"caption_jo2G",children:a})]})}},95986(e,r,n){n.d(r,{A:()=>s});var t=n(74848);n(96540);function s({children:e}){return(0,t.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,r,n){n.d(r,{A:()=>c});var t=n(74848);n(96540);var s=n(95310),a=n(34164);let l="tileImage_O4So";var o=n(66497),i=n(92802);function c({icon:e,image:r,imageDark:n,imageWidth:c,imageHeight:d,iconSize:h=32,containerHeight:p,title:m,description:u,href:f,linkText:x="Learn more \u2192",className:j}){if(!e&&!r)throw Error("TileCard requires either an icon or image prop");let g=p?{height:`${p}px`}:{},w={};return c&&(w.width=`${c}px`),d&&(w.height=`${d}px`),(0,t.jsxs)(s.A,{href:f,className:(0,a.A)("tileCard_NHsj",j),children:[(0,t.jsx)("div",{className:"tileIcon_pyoR",style:g,children:e?(0,t.jsx)(e,{size:h}):n?(0,t.jsx)(i.A,{sources:{light:(0,o.default)(r),dark:(0,o.default)(n)},alt:m,className:l,style:w}):(0,t.jsx)("img",{src:(0,o.default)(r),alt:m,className:l,style:w})}),(0,t.jsx)("h3",{children:m}),(0,t.jsx)("p",{children:u}),(0,t.jsx)("div",{className:"tileLink_iUbu",children:x})]})}},10440(e,r,n){n.d(r,{A:()=>a});var t=n(74848);n(96540);var s=n(34164);function a({children:e,className:r}){return(0,t.jsx)("div",{className:(0,s.A)("tilesGrid_hB9N",r),children:e})}},28453(e,r,n){n.d(r,{R:()=>l,x:()=>o});var t=n(96540);let s={},a=t.createContext(s);function l(e){let r=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(a.Provider,{value:r},e.children)}}}]);