"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["9054"],{36218(e,n,l){l.r(n),l.d(n,{metadata:()=>t,default:()=>_,frontMatter:()=>f,contentTitle:()=>u,toc:()=>w,assets:()=>g});var t=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/rag/index","title":"RAG Evaluation with Built-in Judges","description":"Retrieval-Augmented Generation (RAG) systems combine retrieval and generation to provide contextually relevant responses. Evaluating RAG applications requires assessing both the retrieval quality (are the right documents retrieved?) and the generation quality (is the response grounded in those documents?).","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/rag/index.mdx","sourceDirName":"eval-monitor/scorers/llm-judge/rag","slug":"/eval-monitor/scorers/llm-judge/rag/","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Guidelines","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/guidelines"},"next":{"title":"Answer and Context Relevance","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/relevance"}}'),r=l(74848),i=l(28453),o=l(54725),a=l(77541),s=l(10440),c=l(51004),m=l(60665),p=l(47792),d=l(83884),h=l(55935);let f={},u="RAG Evaluation with Built-in Judges",g={},w=[{value:"Available RAG Judges",id:"available-rag-judges",level:2},...d.RM,{value:"Complete RAG Example",id:"complete-rag-example",level:2},{value:"Understanding the Results",id:"understanding-the-results",level:3},...h.RM,{value:"Next Steps",id:"next-steps",level:2}];function y(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"rag-evaluation-with-built-in-judges",children:"RAG Evaluation with Built-in Judges"})}),"\n",(0,r.jsx)(n.p,{children:"Retrieval-Augmented Generation (RAG) systems combine retrieval and generation to provide contextually relevant responses. Evaluating RAG applications requires assessing both the retrieval quality (are the right documents retrieved?) and the generation quality (is the response grounded in those documents?)."}),"\n",(0,r.jsx)(n.p,{children:"MLflow provides built-in judges designed specifically for evaluating RAG applications. These judges help you identify common issues in your RAG pipeline:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Poor retrieval"}),": Documents aren't relevant to the query"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Insufficient context"}),": Retrieved documents don't contain all necessary information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hallucinations"}),": Generated responses include information not present in the retrieved context"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"available-rag-judges",children:"Available RAG Judges"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Judge"}),(0,r.jsx)(n.th,{children:"What does it evaluate?"}),(0,r.jsx)(n.th,{children:"Requires ground-truth?"}),(0,r.jsx)(n.th,{children:"Requires traces?"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/rag/relevance/#retrievalrelevance-judge",children:"RetrievalRelevance"})}),(0,r.jsx)(n.td,{children:"Are retrieved documents relevant to the user's request?"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsxs)(n.td,{children:["\u26A0\uFE0F ",(0,r.jsx)(n.strong,{children:"Trace Required"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/rag/groundedness",children:"RetrievalGroundedness"})}),(0,r.jsx)(n.td,{children:"Is the app's response grounded in retrieved information?"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsxs)(n.td,{children:["\u26A0\uFE0F ",(0,r.jsx)(n.strong,{children:"Trace Required"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency",children:"RetrievalSufficiency"})}),(0,r.jsx)(n.td,{children:"Do retrieved documents contain all necessary information?"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsxs)(n.td,{children:["\u26A0\uFE0F ",(0,r.jsx)(n.strong,{children:"Trace Required"})]})]})]})]}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["All RAG judges require MLflow Traces with at least one span marked as ",(0,r.jsx)(n.code,{children:'span_type="RETRIEVER"'}),". Use the ",(0,r.jsx)(o.B,{fn:"mlflow.trace",children:"@mlflow.trace"})," decorator with ",(0,r.jsx)(n.code,{children:'span_type="RETRIEVER"'})," on your retrieval functions."]})}),"\n",(0,r.jsx)(d.Ay,{}),"\n",(0,r.jsx)(n.h2,{id:"complete-rag-example",children:"Complete RAG Example"}),"\n",(0,r.jsx)(n.p,{children:"Here's a complete example showing how to build a RAG application and evaluate it with the judges:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\nfrom mlflow.genai.scorers import (\n    RetrievalRelevance,\n    RetrievalGroundedness,\n    RetrievalSufficiency,\n)\nfrom mlflow.entities import Document\nfrom typing import List\n\nmlflow.openai.autolog()\nclient = openai.OpenAI()\n\n# Sample knowledge base - in practice, this would be a vector database\nKNOWLEDGE_BASE = {\n    "mlflow": [\n        Document(\n            id="doc_1",\n            page_content="MLflow is an open-source platform for managing the ML lifecycle. It has four main components: Tracking, Projects, Models, and Registry.",\n            metadata={"source": "mlflow_overview.txt"},\n        ),\n    ],\n}\n\n\n# Define a retriever function with proper span type\n@mlflow.trace(span_type="RETRIEVER")\ndef retrieve_docs(query: str) -> List[Document]:\n    # Simulated retrieval - in practice, this would query a vector database\n    # using embeddings to find semantically similar documents\n    query_lower = query.lower()\n\n    if "mlflow" in query_lower:\n        return KNOWLEDGE_BASE["mlflow"]\n    else:\n        return []  # No relevant documents found\n\n\n# Define your RAG agent\n@mlflow.trace\ndef rag_agent(query: str):\n    docs = retrieve_docs(query)\n    if not docs:\n        return {"response": "I don\'t have information to answer that question."}\n    context = "\\n\\n".join([doc.page_content for doc in docs])\n\n    # Pass retrieved context to your LLM\n    messages = [\n        {\n            "role": "system",\n            "content": f"Answer the user\'s question based only on the following context:\\n\\n{context}",\n        },\n        {"role": "user", "content": query},\n    ]\n\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n    )\n\n    return {"response": response.choices[0].message.content}\n\n\n# Create evaluation dataset with ground truth expectations\neval_dataset = [\n    {\n        "inputs": {"query": "What are the main components of MLflow?"},\n        "expectations": {\n            "expected_facts": [\n                "MLflow has four main components",\n                "The components are Tracking, Projects, Models, and Registry",\n            ]\n        },\n    }\n]\n\n# Run evaluation with multiple RAG judges\neval_results = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=rag_agent,\n    scorers=[\n        RetrievalRelevance(model="openai:/gpt-4o-mini"),\n        RetrievalGroundedness(model="openai:/gpt-4o-mini"),\n        RetrievalSufficiency(model="openai:/gpt-4o-mini"),\n    ],\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"understanding-the-results",children:"Understanding the Results"}),"\n",(0,r.jsx)(n.p,{children:"Each RAG judge evaluates retriever spans separately:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RetrievalRelevance"}),": Returns feedback for each retrieved document, indicating whether it's relevant to the query"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RetrievalGroundedness"}),": Checks if the generated response is factually supported by the retrieved context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RetrievalSufficiency"}),": Verifies that the retrieved documents contain all information needed to answer based on the ",(0,r.jsx)(n.code,{children:"expected facts"})]}),"\n"]}),"\n",(0,r.jsx)(h.Ay,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsx)(a.A,{icon:c.A,title:"RetrievalRelevance",description:"Evaluate if retrieved documents are relevant to queries",href:"/genai/eval-monitor/scorers/llm-judge/rag/relevance/#retrievalrelevance-judge"}),(0,r.jsx)(a.A,{icon:m.A,title:"RetrievalGroundedness",description:"Check if responses are grounded in retrieved context",href:"/genai/eval-monitor/scorers/llm-judge/rag/groundedness"}),(0,r.jsx)(a.A,{icon:p.A,title:"RetrievalSufficiency",description:"Verify retrieved context contains all necessary information",href:"/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency"})]})]})}function _(e={}){let{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(y,{...e})}):y(e)}},83884(e,n,l){l.d(n,{Ay:()=>a,RM:()=>i});var t=l(74848),r=l(28453);let i=[{value:"Prerequisites for running the examples",id:"prerequisites-for-running-the-examples",level:2}];function o(e){let n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"prerequisites-for-running-the-examples",children:"Prerequisites for running the examples"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Install MLflow and required packages"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install --upgrade mlflow\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Create an MLflow experiment by following the ",(0,t.jsx)(n.a,{href:"/genai/getting-started/connect-environment/",children:"setup your environment quickstart"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["(Optional, if using OpenAI models) Use the native OpenAI SDK to connect to OpenAI-hosted models. Select a model from the ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/models",children:"available OpenAI models"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\nimport openai\n\n# Ensure your OPENAI_API_KEY is set in your environment\n# os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>" # Uncomment and set if not globally configured\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Create an OpenAI client\nclient = openai.OpenAI()\n\n# Select an LLM\nmodel_name = "gpt-4o-mini"\n'})}),"\n"]}),"\n"]})]})}function a(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},55935(e,n,l){l.d(n,{Ay:()=>a,RM:()=>i});var t=l(74848),r=l(28453);let i=[{value:"Select the LLM that powers the judge",id:"select-the-llm-that-powers-the-judge",level:2}];function o(e){let n={a:"a",code:"code",h2:"h2",p:"p",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"select-the-llm-that-powers-the-judge",children:"Select the LLM that powers the judge"}),"\n",(0,t.jsxs)(n.p,{children:["You can change the judge model by using the ",(0,t.jsx)(n.code,{children:"model"})," argument in the judge definition. The model must be specified in the format ",(0,t.jsx)(n.code,{children:"<provider>:/<model-name>"}),", where ",(0,t.jsx)(n.code,{children:"<provider>"})," is a LiteLLM-compatible model provider."]}),"\n",(0,t.jsxs)(n.p,{children:["For a list of supported models, see ",(0,t.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges/#selecting-judge-models",children:"selecting judge models"}),"."]})]})}function a(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},75689(e,n,l){l.d(n,{A:()=>s});var t=l(96540);let r=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,l)=>l?l.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},i=(...e)=>e.filter((e,n,l)=>!!e&&""!==e.trim()&&l.indexOf(e)===n).join(" ").trim();var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let a=(0,t.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:l=2,absoluteStrokeWidth:r,className:a="",children:s,iconNode:c,...m},p)=>(0,t.createElement)("svg",{ref:p,...o,width:n,height:n,stroke:e,strokeWidth:r?24*Number(l)/Number(n):l,className:i("lucide",a),...!s&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(m)&&{"aria-hidden":"true"},...m},[...c.map(([e,n])=>(0,t.createElement)(e,n)),...Array.isArray(s)?s:[s]])),s=(e,n)=>{let l=(0,t.forwardRef)(({className:l,...o},s)=>(0,t.createElement)(a,{ref:s,iconNode:n,className:i(`lucide-${r(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,l),...o}));return l.displayName=r(e),l}},60665(e,n,l){l.d(n,{A:()=>t});let t=(0,l(75689).A)("book-open",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]])},51004(e,n,l){l.d(n,{A:()=>t});let t=(0,l(75689).A)("database",[["ellipse",{cx:"12",cy:"5",rx:"9",ry:"3",key:"msslwz"}],["path",{d:"M3 5V19A9 3 0 0 0 21 19V5",key:"1wlel7"}],["path",{d:"M3 12A9 3 0 0 0 21 12",key:"mv7ke4"}]])},47792(e,n,l){l.d(n,{A:()=>t});let t=(0,l(75689).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},54725(e,n,l){l.d(n,{B:()=>o});var t=l(74848);l(96540);var r=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),i=l(66497);function o({fn:e,children:n,hash:l}){let o=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let l=n.slice(0,e).join(".");if(r[l])return l}return null})(e);if(!o)return(0,t.jsx)(t.Fragment,{children:n});let a=(0,i.default)(`/${r[o]}#${l??e}`);return(0,t.jsx)("a",{href:a,target:"_blank",children:n??(0,t.jsxs)("code",{children:[e,"()"]})})}},77541(e,n,l){l.d(n,{A:()=>c});var t=l(74848);l(96540);var r=l(95310),i=l(34164);let o="tileImage_O4So";var a=l(66497),s=l(92802);function c({icon:e,image:n,imageDark:l,imageWidth:c,imageHeight:m,iconSize:p=32,containerHeight:d,title:h,description:f,href:u,linkText:g="Learn more \u2192",className:w}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let y=d?{height:`${d}px`}:{},_={};return c&&(_.width=`${c}px`),m&&(_.height=`${m}px`),(0,t.jsxs)(r.A,{href:u,className:(0,i.A)("tileCard_NHsj",w),children:[(0,t.jsx)("div",{className:"tileIcon_pyoR",style:y,children:e?(0,t.jsx)(e,{size:p}):l?(0,t.jsx)(s.A,{sources:{light:(0,a.default)(n),dark:(0,a.default)(l)},alt:h,className:o,style:_}):(0,t.jsx)("img",{src:(0,a.default)(n),alt:h,className:o,style:_})}),(0,t.jsx)("h3",{children:h}),(0,t.jsx)("p",{children:f}),(0,t.jsx)("div",{className:"tileLink_iUbu",children:g})]})}},10440(e,n,l){l.d(n,{A:()=>i});var t=l(74848);l(96540);var r=l(34164);function i({children:e,className:n}){return(0,t.jsx)("div",{className:(0,r.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,l){l.d(n,{R:()=>o,x:()=>a});var t=l(96540);let r={},i=t.createContext(r);function o(e){let n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);