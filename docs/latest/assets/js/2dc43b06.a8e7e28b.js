"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["1106"],{77180(e,n,s){s.r(n),s.d(n,{metadata:()=>i,default:()=>u,frontMatter:()=>l,contentTitle:()=>t,toc:()=>a,assets:()=>d});var i=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/custom-judges/supported-models","title":"Supported Models","description":"AI Gateway Endpoints","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/custom-judges/supported-models.mdx","sourceDirName":"eval-monitor/scorers/llm-judge/custom-judges","slug":"/eval-monitor/scorers/llm-judge/custom-judges/supported-models","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges/supported-models","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Custom Judges","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges/"},"next":{"title":"Create a Custom Judge","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge"}}'),o=s(74848),r=s(28453);let l={},t="Supported Models",d={},a=[{value:"AI Gateway Endpoints",id:"ai-gateway-endpoints",level:2},{value:"Direct Model Providers",id:"direct-model-providers",level:2},{value:"Choosing the Right LLM for Your Judge",id:"choosing-the-right-llm-for-your-judge",level:2},{value:"Early Development Stage (Inner Loop)",id:"early-development-stage-inner-loop",level:3},{value:"Production &amp; Scaling Stage",id:"production--scaling-stage",level:3}];function c(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"supported-models",children:"Supported Models"})}),"\n",(0,o.jsx)(n.h2,{id:"ai-gateway-endpoints",children:"AI Gateway Endpoints"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/genai/governance/ai-gateway/",children:"AI Gateway"})," endpoints are the recommended way to configure judge models, especially when creating judges from the UI. Benefits include:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Run judges directly from the UI"})," - Test and execute judges without leaving the browser"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Centralized API key management"})," - No need to configure API keys locally"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Traffic routing and fallbacks"})," - Configure load balancing and provider fallbacks"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["To use AI Gateway endpoints, select the endpoint from the UI dropdown or specify the endpoint name from the SDK with the ",(0,o.jsx)(n.code,{children:"gateway:/"})," prefix, e.g., ",(0,o.jsx)(n.code,{children:"gateway:/my-chat-endpoint"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"direct-model-providers",children:"Direct Model Providers"}),"\n",(0,o.jsx)(n.p,{children:"MLflow also supports calling model providers directly:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"OpenAI / Azure OpenAI"}),"\n",(0,o.jsx)(n.li,{children:"Anthropic"}),"\n",(0,o.jsx)(n.li,{children:"Amazon Bedrock"}),"\n",(0,o.jsx)(n.li,{children:"Cohere"}),"\n",(0,o.jsx)(n.li,{children:"Together AI"}),"\n",(0,o.jsxs)(n.li,{children:["Any other providers supported by ",(0,o.jsx)(n.a,{href:"https://docs.litellm.ai/docs/providers",children:"LiteLLM"}),", such as Google Gemini, xAI, Mistral, and more."]}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsxs)(n.p,{children:["Judges configured with direct model providers require API keys to be set locally (e.g., ",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"}),") and ",(0,o.jsx)(n.strong,{children:"cannot be run from the UI"}),". Use AI Gateway endpoints if you want to run the judges from the UI."]})}),"\n",(0,o.jsxs)(n.p,{children:["To use LiteLLM integrated models, install LiteLLM by running ",(0,o.jsx)(n.code,{children:"pip install litellm"})," and specify the provider and model name in the same format as natively supported providers, e.g., ",(0,o.jsx)(n.code,{children:"gemini:/gemini-2.0-flash"}),"."]}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["In Databricks, the default model is set to ",(0,o.jsx)("ins",{children:(0,o.jsx)(n.a,{href:"https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/judges/",children:"Databricks's research-backed LLM judges"})}),"."]})}),"\n",(0,o.jsx)(n.h2,{id:"choosing-the-right-llm-for-your-judge",children:"Choosing the Right LLM for Your Judge"}),"\n",(0,o.jsx)(n.p,{children:"The choice of LLM model significantly impacts judge performance and cost. Here's guidance based on your development stage and use case:"}),"\n",(0,o.jsx)(n.h3,{id:"early-development-stage-inner-loop",children:"Early Development Stage (Inner Loop)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Recommended"}),": Start with powerful models like GPT-4o or Claude Opus"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Why"}),": When you're beginning your agent development journey, you typically lack:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use-case-specific grading criteria"}),"\n",(0,o.jsx)(n.li,{children:"Labeled data for optimization"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Benefits"}),": More intelligent models can deeply explore traces, identify patterns, and help you understand common issues in your system"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Trade-off"}),": Higher cost, but lower evaluation volume during development makes this acceptable"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"production--scaling-stage",children:"Production & Scaling Stage"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Recommended"}),": Transition to smaller models (GPT-4o-mini, Claude Haiku) with smarter optimizers"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Why"}),": As you move toward production:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"You've collected labeled data and established grading criteria"}),"\n",(0,o.jsx)(n.li,{children:"Cost becomes a critical factor at scale"}),"\n",(0,o.jsx)(n.li,{children:"You can align smaller judges using more powerful optimizers"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Approach"}),": Use a smaller judge model paired with a powerful optimizer model (e.g., GPT-4o-mini judge aligned using Claude Opus optimizer)"]}),"\n"]})]})}function u(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},28453(e,n,s){s.d(n,{R:()=>l,x:()=>t});var i=s(96540);let o={},r=i.createContext(o);function l(e){let n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);