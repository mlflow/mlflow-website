/*! For license information please see 8a897e48.3713e9b7.js.LICENSE.txt */
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6087],{190:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>P,contentTitle:()=>D,default:()=>W,frontMatter:()=>U,metadata:()=>t,toc:()=>O});const t=JSON.parse('{"id":"assessments/feedback","title":"Feedback Collection","description":"MLflow Feedback provides a comprehensive system for capturing quality evaluations from multiple sources - whether automated AI judges, programmatic rules, or human reviewers. This systematic approach to feedback collection enables you to understand and improve your GenAI application\'s performance at scale.","source":"@site/docs/genai/assessments/feedback.mdx","sourceDirName":"assessments","slug":"/assessments/feedback","permalink":"/mlflow-website/docs/latest/genai/assessments/feedback","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"sidebar_label":"Feedback Collection"},"sidebar":"genAISidebar","previous":{"title":"Evaluate & Monitor","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/"},"next":{"title":"Ground Truth Expectations","permalink":"/mlflow-website/docs/latest/genai/assessments/expectations"}}');var i=n(74848),s=n(28453),r=(n(10493),n(82238)),l=n(79206),o=n(96869),c=n(65592),d=n(6789),p=(n(66927),n(49374)),m=n(42640),h=n(93164),u=n(93893),f=n(76316),_=n(22492),g=n(3160),v=n(51004),x=n(46534),y=n(80697),w=n(72216),b=n(3549),k=n(80964),j=n(57906),A=n(47504),C=n(47792),N=n(98445),T=n(84722);const S=(0,T.A)("ellipsis-vertical",[["circle",{cx:"12",cy:"12",r:"1",key:"41hilf"}],["circle",{cx:"12",cy:"5",r:"1",key:"gxeob9"}],["circle",{cx:"12",cy:"19",r:"1",key:"lyex9k"}]]),F=(0,T.A)("square-pen",[["path",{d:"M12 3H5a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2v-7",key:"1m0v6g"}],["path",{d:"M18.375 2.625a1 1 0 0 1 3 3l-9.013 9.014a2 2 0 0 1-.853.505l-2.873.84a.5.5 0 0 1-.62-.62l.84-2.873a2 2 0 0 1 .506-.852z",key:"ohrbg2"}]]),I=(0,T.A)("save",[["path",{d:"M15.2 3a2 2 0 0 1 1.4.6l3.8 3.8a2 2 0 0 1 .6 1.4V19a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2z",key:"1c8476"}],["path",{d:"M17 21v-7a1 1 0 0 0-1-1H8a1 1 0 0 0-1 1v7",key:"1ydtos"}],["path",{d:"M7 3v4a1 1 0 0 0 1 1h7",key:"t51u73"}]]);var L=n(45244),M=n(22864),E=n(11470),H=n(19365),q=n(82309);const R=n.p+"assets/images/edit_feedback_ui-7746f8c3cb4f56d003de48c376e52d0c.png";var G=n(77655);const U={sidebar_label:"Feedback Collection"},D="Feedback Collection",P={},O=[{value:"What is Feedback?",id:"what-is-feedback",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Sources of Feedback",id:"sources-of-feedback",level:2},{value:"Why Collect Feedback?",id:"why-collect-feedback",level:2},{value:"How Feedback Works",id:"how-feedback-works",level:2},{value:"Via API",id:"via-api",level:3},{value:"Step-by-Step Guides",id:"step-by-step-guides",level:2},{value:"Add Human Evaluation via UI",id:"add-human-evaluation-via-ui",level:3},{value:"Adding New Feedback",id:"adding-new-feedback",level:4},{value:"Editing Existing Feedback",id:"editing-existing-feedback",level:4},{value:"Adding Additional Feedback to Existing Entries",id:"adding-additional-feedback-to-existing-entries",level:4},{value:"Log Automated Assessment via API",id:"log-automated-assessment-via-api",level:3},{value:"Managing Feedback",id:"managing-feedback",level:2},{value:"Retrieving Feedback",id:"retrieving-feedback",level:3},{value:"Updating Feedback",id:"updating-feedback",level:3},{value:"Deleting Feedback",id:"deleting-feedback",level:3},{value:"Overriding Automated Feedback",id:"overriding-automated-feedback",level:2},{value:"When to Override vs Update",id:"when-to-override-vs-update",level:3},{value:"Override Example",id:"override-example",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Consistent Naming Conventions",id:"consistent-naming-conventions",level:3},{value:"Traceable Source Attribution",id:"traceable-source-attribution",level:3},{value:"Rich Metadata",id:"rich-metadata",level:3},{value:"Next Steps",id:"next-steps",level:2}];function B(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.header,{children:(0,i.jsx)(a.h1,{id:"feedback-collection",children:"Feedback Collection"})}),"\n",(0,i.jsx)(a.p,{children:"MLflow Feedback provides a comprehensive system for capturing quality evaluations from multiple sources - whether automated AI judges, programmatic rules, or human reviewers. This systematic approach to feedback collection enables you to understand and improve your GenAI application's performance at scale."}),"\n",(0,i.jsxs)(a.p,{children:["For complete API documentation and implementation details, see the ",(0,i.jsx)(p.B,{fn:"mlflow.log_feedback"})," reference."]}),"\n",(0,i.jsx)(a.h2,{id:"what-is-feedback",children:"What is Feedback?"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.a,{href:"/genai/tracing/concepts/feedback",children:"Feedback"})," captures evaluations of how well your AI performed. It measures the actual quality of what your AI produced across various dimensions like accuracy, relevance, safety, and helpfulness. Unlike ",(0,i.jsx)(a.a,{href:"/genai/assessments/expectations",children:"expectations"})," that define what should happen, feedback tells you what actually happened and how well it met your quality standards."]}),"\n",(0,i.jsx)(a.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(a.p,{children:"Before using feedback collection in MLflow, ensure you have:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"MLflow 3.2.0 or later installed"}),"\n",(0,i.jsx)(a.li,{children:"An active MLflow tracking server or local tracking setup"}),"\n",(0,i.jsx)(a.li,{children:"Traces that have been logged from your GenAI application to an MLflow Experiment"}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"sources-of-feedback",children:"Sources of Feedback"}),"\n",(0,i.jsx)(a.p,{children:"MLflow supports three types of feedback sources, each with unique strengths. You can use a single source or combine multiple sources for comprehensive quality coverage."}),"\n",(0,i.jsx)(l.A,{concepts:[{icon:m.A,title:"LLM Judge Evaluation",description:"AI-powered evaluation at scale. LLM judges provide consistent quality assessments for nuanced dimensions like relevance, tone, and safety without human intervention."},{icon:h.A,title:"Programmatic Code Checks",description:"Deterministic rule-based evaluation. Perfect for format validation, compliance checks, and business logic rules that need instant, cost-effective assessment."},{icon:u.A,title:"Human Expert Review",description:"Domain expert evaluation for high-stakes content. Human feedback captures nuanced insights that automated systems miss and serves as the gold standard."}]}),"\n",(0,i.jsx)(a.p,{children:"Using the feedback sources within the Python APIs is done as follows:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'from mlflow.entities import AssessmentSource, AssessmentSourceType\n\n# Human expert providing evaluation\nhuman_source = AssessmentSource(\n    source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"\n)\n\n# Automated rule-based evaluation\ncode_source = AssessmentSource(\n    source_type=AssessmentSourceType.CODE, source_id="accuracy_checker_v1"\n)\n\n# AI-powered evaluation at scale\nllm_judge_source = AssessmentSource(\n    source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"\n)\n'})}),"\n",(0,i.jsx)(a.h2,{id:"why-collect-feedback",children:"Why Collect Feedback?"}),"\n",(0,i.jsx)(a.p,{children:"Collecting feedback on the quality of GenAI applications is critical to a continuous improvement process, ensuring that your application remains effective and is enhanced over time."}),"\n",(0,i.jsx)(r.A,{features:[{icon:f.A,title:"Enable Continuous Improvement",description:"Create data-driven improvement cycles by systematically collecting quality signals to identify patterns, fix issues, and enhance AI performance over time."},{icon:_.A,title:"Scale Quality Assurance",description:"Monitor quality at production scale by evaluating every trace instead of small samples, catching issues before they impact users."},{icon:g.A,title:"Build Trust Through Transparency",description:"Show stakeholders exactly how quality is measured and by whom, building confidence in your AI system's reliability through clear attribution."},{icon:v.A,title:"Create Training Data",description:"Generate high-quality training datasets from feedback, especially human corrections, to improve both AI applications and evaluation systems."}]}),"\n",(0,i.jsx)(a.h2,{id:"how-feedback-works",children:"How Feedback Works"}),"\n",(0,i.jsx)(a.h3,{id:"via-api",children:"Via API"}),"\n",(0,i.jsxs)(a.p,{children:["Use the programmatic ",(0,i.jsx)(p.B,{fn:"mlflow.log_feedback"})," API when you need to automate feedback collection at scale, integrate with existing systems, or build custom evaluation workflows. The API enables you to collect feedback from all three sources programmatically."]}),"\n",(0,i.jsx)(a.h2,{id:"step-by-step-guides",children:"Step-by-Step Guides"}),"\n",(0,i.jsx)(a.h3,{id:"add-human-evaluation-via-ui",children:"Add Human Evaluation via UI"}),"\n",(0,i.jsx)(a.p,{children:"The MLflow UI provides an intuitive way to add, edit, and manage feedback directly on traces. This approach is ideal for manual review, collaborative evaluation, and situations where domain experts need to provide feedback without writing code."}),"\n",(0,i.jsx)(a.h4,{id:"adding-new-feedback",children:"Adding New Feedback"}),"\n",(0,i.jsx)(o.A,{steps:[{icon:x.A,title:"Navigate to your experiment",description:"Select the trace containing the AI response you want to evaluate"},{icon:y.A,title:'Click "Add Assessment" button',description:"Access the feedback creation form on the trace detail page"},{icon:w.A,title:'Select "Feedback" from the Assessment Type dropdown',description:"Choose Assessment Type to evaluate quality rather than define expectations"},{icon:b.A,title:"Enter a descriptive name",description:'Use clear names like "response_helpfulness", "accuracy_rating", or "content_safety"'},{icon:k.A,title:"Choose the appropriate data type",description:"Select Boolean for pass/fail, Number for ratings, String for categories"},{icon:j.A,title:"Enter your evaluation value",description:"Provide rating from 1-5, True/False, or descriptive assessment text"},{icon:A.A,title:"Add rationale explaining your evaluation reasoning",description:"Document why you gave this assessment for future reference and improvement"},{icon:C.A,title:'Click "Create" to record your feedback',description:"Save your evaluation to build quality insights over time"}],screenshot:{src:q.A,alt:"Add Feedback"}}),"\n",(0,i.jsx)(a.p,{children:"The feedback will be immediately attached to the trace with your user information as the source."}),"\n",(0,i.jsx)(a.h4,{id:"editing-existing-feedback",children:"Editing Existing Feedback"}),"\n",(0,i.jsx)(a.p,{children:"To refine evaluations or correct mistakes:"}),"\n",(0,i.jsx)(o.A,{steps:[{icon:N.A,title:"Locate the feedback entry you want to modify",description:"Find the specific feedback on the trace detail page that needs updating"},{icon:S,title:"Click the hamburger menu (\u22ee) next to the feedback entry",description:"Access the feedback options dropdown menu"},{icon:F,title:'Select "Edit" from the dropdown menu',description:"Choose the edit option to access the feedback modification form"},{icon:k.A,title:"Modify the value, rationale, or other fields",description:"Update rating, explanation, or other feedback details as needed"},{icon:I,title:'Click "Save" to update the feedback',description:'Save your changes to preserve the updated evaluation, or click "Cancel" to discard changes'}],screenshot:{src:R,alt:"Edit Feedback"}}),"\n",(0,i.jsx)(a.h4,{id:"adding-additional-feedback-to-existing-entries",children:"Adding Additional Feedback to Existing Entries"}),"\n",(0,i.jsx)(a.p,{children:"When multiple reviewers want to provide feedback on the same aspect, or when you want to add corrections to automated evaluations:"}),"\n",(0,i.jsx)(o.A,{steps:[{icon:N.A,title:"Find the existing feedback name you want to add to",description:"Locate the feedback category where you want to add another perspective"},{icon:y.A,title:"Click the plus (+) icon next to the feedback name",description:"Access the additional feedback form for the same quality aspect"},{icon:j.A,title:"Enter your additional evaluation with appropriate rationale",description:"Provide your assessment and explanation from your perspective"},{icon:I,title:'Click "Create" to add your feedback alongside existing entries',description:'Your evaluation will be added to the collection of perspectives, or click "Cancel" to discard'}],screenshot:{src:G.A,alt:"Additional Feedback"}}),"\n",(0,i.jsx)(a.p,{children:"This collaborative approach enables multiple perspectives on the same trace aspect, creating richer evaluation datasets and helping identify cases where evaluators disagree."}),"\n",(0,i.jsx)(a.h3,{id:"log-automated-assessment-via-api",children:"Log Automated Assessment via API"}),"\n",(0,i.jsxs)(E.A,{children:[(0,i.jsxs)(H.A,{value:"llm_judge",label:"LLM Judge",default:!0,children:[(0,i.jsx)(a.p,{children:"Implement automated LLM-based evaluation with these steps:"}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"1. Set up your evaluation environment:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'import json\nimport mlflow\nfrom mlflow.entities import AssessmentSource, AssessmentError\nfrom mlflow.entities.assessment_source import AssessmentSourceType\nimport openai  # or your preferred LLM client\n\n# Configure your LLM client\nclient = openai.OpenAI(api_key="your-api-key")\n'})}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"2. Create your evaluation prompt:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'def create_evaluation_prompt(user_input, ai_response):\n    return f"""\n    Evaluate the AI response for helpfulness and accuracy.\n\n    User Input: {user_input}\n    AI Response: {ai_response}\n\n    Rate the response on a scale of 0.0 to 1.0 for:\n    1. Helpfulness: How well does it address the user\'s needs?\n    2. Accuracy: Is the information factually correct?\n\n    Respond with only a JSON object:\n    {{"helpfulness": 0.0-1.0, "accuracy": 0.0-1.0, "rationale": "explanation"}}\n    """\n'})}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"3. Implement the evaluation function:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'def evaluate_with_llm_judge(trace_id, user_input, ai_response):\n    try:\n        # Get LLM evaluation\n        response = client.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {\n                    "role": "user",\n                    "content": create_evaluation_prompt(user_input, ai_response),\n                }\n            ],\n            temperature=0.0,\n        )\n\n        # Parse the evaluation\n\n        evaluation = json.loads(response.choices[0].message.content)\n\n        # Log feedback to MLflow\n        mlflow.log_feedback(\n            trace_id=trace_id,\n            name="llm_judge_evaluation",\n            value=evaluation,\n            rationale=evaluation.get("rationale", ""),\n            source=AssessmentSource(\n                source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"\n            ),\n        )\n\n    except Exception as e:\n        # Log evaluation failure\n        mlflow.log_feedback(\n            trace_id=trace_id,\n            name="llm_judge_evaluation",\n            error=AssessmentError(error_code="EVALUATION_FAILED", error_message=str(e)),\n            source=AssessmentSource(\n                source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"\n            ),\n        )\n'})}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"4. Use the evaluation function:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Example usage\ntrace_id = "your-trace-id"\nuser_question = "What is the capital of France?"\nai_answer = "The capital of France is Paris."\n\nevaluate_with_llm_judge(trace_id, user_question, ai_answer)\n'})})]}),(0,i.jsxs)(H.A,{value:"code_based",label:"Heuristics Metrics",children:[(0,i.jsx)(a.p,{children:"Implement programmatic rule-based evaluation:"}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"1. Define your evaluation rules:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'def evaluate_response_compliance(response_text):\n    """Evaluate response against business rules."""\n    results = {\n        "has_disclaimer": False,\n        "appropriate_length": False,\n        "contains_prohibited_terms": False,\n        "rationale": [],\n    }\n\n    # Check for required disclaimer\n    if "This is not financial advice" in response_text:\n        results["has_disclaimer"] = True\n    else:\n        results["rationale"].append("Missing required disclaimer")\n\n    # Check response length\n    if 50 <= len(response_text) <= 500:\n        results["appropriate_length"] = True\n    else:\n        results["rationale"].append(\n            f"Response length {len(response_text)} outside acceptable range"\n        )\n\n    # Check for prohibited terms\n    prohibited_terms = ["guaranteed returns", "risk-free", "get rich quick"]\n    found_terms = [\n        term for term in prohibited_terms if term.lower() in response_text.lower()\n    ]\n    if found_terms:\n        results["contains_prohibited_terms"] = True\n        results["rationale"].append(f"Contains prohibited terms: {found_terms}")\n\n    return results\n'})}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"2. Implement the logging function:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'def log_compliance_check(trace_id, response_text):\n    # Run compliance evaluation\n    evaluation = evaluate_response_compliance(response_text)\n\n    # Calculate overall compliance score\n    compliance_score = (\n        sum(\n            [\n                evaluation["has_disclaimer"],\n                evaluation["appropriate_length"],\n                not evaluation["contains_prohibited_terms"],\n            ]\n        )\n        / 3\n    )\n\n    # Log the feedback\n    mlflow.log_feedback(\n        trace_id=trace_id,\n        name="compliance_check",\n        value={"overall_score": compliance_score, "details": evaluation},\n        rationale="; ".join(evaluation["rationale"]) or "All compliance checks passed",\n        source=AssessmentSource(\n            source_type=AssessmentSourceType.CODE, source_id="compliance_validator_v2.1"\n        ),\n    )\n'})}),(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"3. Use in your application:"})}),(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Example usage after your AI generates a response\nwith mlflow.start_span(name="financial_advice") as span:\n    ai_response = your_ai_model.generate(user_question)\n    trace_id = span.trace_id\n\n    # Run automated compliance check\n    log_compliance_check(trace_id, ai_response)\n'})})]})]}),"\n",(0,i.jsx)(a.h2,{id:"managing-feedback",children:"Managing Feedback"}),"\n",(0,i.jsx)(a.p,{children:"Once you've collected feedback on your traces, you'll need to retrieve, update, and sometimes delete it. These operations are essential for maintaining accurate evaluation data."}),"\n",(0,i.jsx)(a.h3,{id:"retrieving-feedback",children:"Retrieving Feedback"}),"\n",(0,i.jsx)(a.p,{children:"Retrieve specific feedback to analyze evaluation results:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Get a specific feedback by ID\nfeedback = mlflow.get_assessment(\n    trace_id="tr-1234567890abcdef", assessment_id="a-0987654321abcdef"\n)\n\n# Access feedback details\nname = feedback.name\nvalue = feedback.value\nsource_type = feedback.source.source_type\nrationale = feedback.rationale if hasattr(feedback, "rationale") else None\n'})}),"\n",(0,i.jsx)(a.h3,{id:"updating-feedback",children:"Updating Feedback"}),"\n",(0,i.jsx)(a.p,{children:"Update existing feedback when you need to correct or refine evaluations:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'from mlflow.entities import Feedback\n\n# Update feedback with new information\nupdated_feedback = Feedback(\n    name="response_quality",\n    value=0.9,\n    rationale="Updated after additional review - response is more comprehensive than initially evaluated",\n)\n\nmlflow.update_assessment(\n    trace_id="tr-1234567890abcdef",\n    assessment_id="a-0987654321abcdef",\n    assessment=updated_feedback,\n)\n'})}),"\n",(0,i.jsx)(a.h3,{id:"deleting-feedback",children:"Deleting Feedback"}),"\n",(0,i.jsx)(a.p,{children:"Remove feedback that was logged incorrectly:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Delete specific feedback\nmlflow.delete_assessment(\n    trace_id="tr-1234567890abcdef", assessment_id="a-5555666677778888"\n)\n'})}),"\n",(0,i.jsx)(a.admonition,{type:"note",children:(0,i.jsxs)(a.p,{children:["If deleting feedback that has been marked as a replacement using the ",(0,i.jsx)(a.code,{children:"override_feedback"})," API, the original feedback will return to a valid state."]})}),"\n",(0,i.jsx)(a.h2,{id:"overriding-automated-feedback",children:"Overriding Automated Feedback"}),"\n",(0,i.jsxs)(a.p,{children:["The ",(0,i.jsx)(a.code,{children:"override_feedback"})," function allows human experts to correct automated evaluations while preserving the original for audit trails and learning."]}),"\n",(0,i.jsx)(a.h3,{id:"when-to-override-vs-update",children:"When to Override vs Update"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Override"}),": Use when correcting automated feedback - preserves original for analysis"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Update"}),": Use when fixing mistakes in existing feedback - modifies in place"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"override-example",children:"Override Example"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Step 1: Original automated feedback (logged earlier)\nllm_feedback = mlflow.log_feedback(\n    trace_id="tr-1234567890abcdef",\n    name="relevance",\n    value=0.6,\n    rationale="Response partially addresses the question",\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.LLM_JUDGE, source_id="gpt-4-evaluator"\n    ),\n)\n\n# Step 2: Human expert reviews and disagrees\ncorrected_feedback = mlflow.override_feedback(\n    trace_id="tr-1234567890abcdef",\n    assessment_id=llm_feedback.assessment_id,\n    value=0.9,\n    rationale="Response fully addresses the question with comprehensive examples",\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.HUMAN, source_id="expert_reviewer@company.com"\n    ),\n    metadata={"override_reason": "LLM underestimated relevance", "confidence": "high"},\n)\n'})}),"\n",(0,i.jsx)(a.p,{children:"The override process marks the original feedback as invalid but preserves it for historical analysis and model improvement."}),"\n",(0,i.jsx)(a.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(a.h3,{id:"consistent-naming-conventions",children:"Consistent Naming Conventions"}),"\n",(0,i.jsx)(a.p,{children:"Use clear, descriptive names that make feedback data easy to analyze:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Good: Descriptive, specific names\nmlflow.log_feedback(trace_id=trace_id, name="response_accuracy", value=0.95)\nmlflow.log_feedback(trace_id=trace_id, name="sql_syntax_valid", value=True)\nmlflow.log_feedback(trace_id=trace_id, name="execution_time_ms", value=245)\n\n# Poor: Vague, inconsistent names\nmlflow.log_feedback(trace_id=trace_id, name="good", value=True)\nmlflow.log_feedback(trace_id=trace_id, name="score", value=0.95)\n'})}),"\n",(0,i.jsx)(a.h3,{id:"traceable-source-attribution",children:"Traceable Source Attribution"}),"\n",(0,i.jsx)(a.p,{children:"Provide specific source information for audit trails:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Excellent: Version-specific, environment-aware\nsource = AssessmentSource(\n    source_type=AssessmentSourceType.CODE, source_id="response_validator_v2.1_prod"\n)\n\n# Good: Individual attribution\nsource = AssessmentSource(\n    source_type=AssessmentSourceType.HUMAN, source_id="expert@company.com"\n)\n\n# Poor: Generic, untraceable\nsource = AssessmentSource(source_type=AssessmentSourceType.CODE, source_id="validator")\n'})}),"\n",(0,i.jsx)(a.h3,{id:"rich-metadata",children:"Rich Metadata"}),"\n",(0,i.jsx)(a.p,{children:"Include context that helps with analysis:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'mlflow.log_feedback(\n    trace_id=trace_id,\n    name="response_quality",\n    value=0.85,\n    source=human_source,\n    metadata={\n        "reviewer_expertise": "domain_expert",\n        "review_duration_seconds": 45,\n        "confidence": "high",\n        "criteria_version": "v2.3",\n        "evaluation_context": "production_review",\n    },\n)\n'})}),"\n",(0,i.jsx)(a.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(c.A,{children:[(0,i.jsx)(d.A,{icon:L.A,iconSize:48,title:"Feedback Concepts",description:"Deep dive into feedback architecture and schema",href:"/genai/tracing/concepts/feedback",linkText:"Learn concepts \u2192",containerHeight:64}),(0,i.jsx)(d.A,{icon:C.A,iconSize:48,title:"Ground Truth Expectations",description:"Learn how to define expected outputs for evaluation",href:"/genai/assessments/expectations",linkText:"Start annotating \u2192",containerHeight:64}),(0,i.jsx)(d.A,{icon:M.A,iconSize:48,title:"LLM Evaluation",description:"Learn how to systematically evaluate and improve your GenAI applications",href:"/genai/eval-monitor/llm-evaluation",linkText:"Start evaluating \u2192",containerHeight:64})]})]})}function W(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(B,{...e})}):B(e)}},3160:(e,a,n)=>{n.d(a,{A:()=>t});const t=(0,n(84722).A)("eye",[["path",{d:"M2.062 12.348a1 1 0 0 1 0-.696 10.75 10.75 0 0 1 19.876 0 1 1 0 0 1 0 .696 10.75 10.75 0 0 1-19.876 0",key:"1nclc0"}],["circle",{cx:"12",cy:"12",r:"3",key:"1v7zrd"}]])},6789:(e,a,n)=>{n.d(a,{A:()=>o});n(96540);var t=n(28774),i=n(34164);const s={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var r=n(86025),l=n(74848);function o({icon:e,image:a,iconSize:n=32,containerHeight:o,title:c,description:d,href:p,linkText:m="Learn more \u2192",className:h}){if(!e&&!a)throw new Error("TileCard requires either an icon or image prop");const u=o?{height:`${o}px`}:{};return(0,l.jsxs)(t.A,{href:p,className:(0,i.A)(s.tileCard,h),children:[(0,l.jsx)("div",{className:s.tileIcon,style:u,children:e?(0,l.jsx)(e,{size:n}):(0,l.jsx)("img",{src:(0,r.Ay)(a),alt:c,className:s.tileImage})}),(0,l.jsx)("h3",{children:c}),(0,l.jsx)("p",{children:d}),(0,l.jsx)("div",{className:s.tileLink,children:m})]})}},10493:(e,a,n)=>{n.d(a,{Zp:()=>o,AC:()=>l,WO:()=>d,_C:()=>c,$3:()=>p,jK:()=>m});var t=n(34164);const i={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardRounded:"SmallLogoCardRounded_X50_",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var s=n(28774),r=n(74848);const l=({children:e,isSmall:a,cols:n})=>(0,r.jsx)("div",{className:(0,t.A)(i.CardGroup,a?i.AutofillColumns:n?i[`Cols${n}`]:i.MaxThreeColumns),children:e}),o=({children:e,link:a=""})=>a?(0,r.jsx)(s.A,{className:(0,t.A)(i.Link,i.Card,i.CardBordered),to:a,children:e}):(0,r.jsx)("div",{className:(0,t.A)(i.Card,i.CardBordered),children:e}),c=({headerText:e,link:a,text:n})=>(0,r.jsx)(o,{link:a,children:(0,r.jsxs)("span",{children:[(0,r.jsx)("div",{className:(0,t.A)(i.CardTitle,i.BoxRoot,i.PaddingBottom4),style:{pointerEvents:"none"},children:(0,r.jsx)("div",{className:(0,t.A)(i.BoxRoot,i.FlexFlex,i.FlexAlignItemsCenter,i.FlexDirectionRow,i.FlexJustifyContentFlexStart,i.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,r.jsx)("div",{className:(0,t.A)(i.BoxRoot,i.BoxHideIfEmpty,i.MarginTop4,i.MarginLeft4),style:{pointerEvents:"auto"},children:(0,r.jsx)("span",{className:"",children:e})})})}),(0,r.jsx)("span",{className:(0,t.A)(i.TextColor,i.CardBody),children:(0,r.jsx)("p",{children:n})})]})}),d=({description:e,children:a,link:n})=>(0,r.jsx)(o,{link:n,children:(0,r.jsxs)("div",{className:i.LogoCardContent,children:[(0,r.jsx)("div",{className:i.LogoCardImage,children:a}),(0,r.jsx)("p",{className:i.TextColor,children:e})]})}),p=({children:e,link:a})=>(0,r.jsx)("div",{className:(0,t.A)(i.Card,i.CardBordered,i.SmallLogoCardRounded),children:a?(0,r.jsx)(s.A,{className:(0,t.A)(i.Link),to:a,children:(0,r.jsx)("div",{className:i.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,t.A)("max-height-img-container",i.SmallLogoCardImage),children:e})})}):(0,r.jsx)("div",{className:i.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,t.A)("max-height-img-container",i.SmallLogoCardImage),children:e})})}),m=({title:e,description:a,link:n=""})=>(0,r.jsx)(o,{link:n,children:(0,r.jsxs)("div",{className:i.TitleCardContent,children:[(0,r.jsx)("div",{className:(0,t.A)(i.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,r.jsx)("hr",{className:(0,t.A)(i.TitleCardSeparator),style:{margin:"12px 0"}}),(0,r.jsx)("p",{className:(0,t.A)(i.TextColor),children:a})]})})},42640:(e,a,n)=>{n.d(a,{A:()=>t});const t=(0,n(84722).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},49374:(e,a,n)=>{n.d(a,{B:()=>l});n(96540);const t=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var i=n(86025),s=n(74848);const r=e=>{const a=e.split(".");for(let n=a.length;n>0;n--){const e=a.slice(0,n).join(".");if(t[e])return e}return null};function l({fn:e,children:a,hash:n}){const l=r(e);if(!l)return(0,s.jsx)(s.Fragment,{children:a});const o=(0,i.Ay)(`/${t[l]}#${n??e}`);return(0,s.jsx)("a",{href:o,target:"_blank",children:a??(0,s.jsxs)("code",{children:[e,"()"]})})}},65592:(e,a,n)=>{n.d(a,{A:()=>r});n(96540);var t=n(34164);const i={tilesGrid:"tilesGrid_hB9N"};var s=n(74848);function r({children:e,className:a}){return(0,s.jsx)("div",{className:(0,t.A)(i.tilesGrid,a),children:e})}},66927:(e,a,n)=>{n.d(a,{A:()=>r});n(96540);const t={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var i=n(86025),s=n(74848);function r({src:e,alt:a,width:n,caption:r,className:l}){return(0,s.jsxs)("div",{className:`${t.container} ${l||""}`,children:[(0,s.jsx)("div",{className:t.imageWrapper,style:n?{width:n}:{},children:(0,s.jsx)("img",{src:(0,i.Ay)(e),alt:a,className:t.image})}),r&&(0,s.jsx)("p",{className:t.caption,children:r})]})}},77655:(e,a,n)=>{n.d(a,{A:()=>t});const t=n.p+"assets/images/additional_feedback_ui-55514604c28b292d5b976cb56a30773a.png"},79206:(e,a,n)=>{n.d(a,{A:()=>s});n(96540);const t={conceptOverview:"conceptOverview_x8T_",overviewTitle:"overviewTitle_HyAI",conceptGrid:"conceptGrid_uJNV",conceptCard:"conceptCard_oday",conceptHeader:"conceptHeader_HCk5",conceptIcon:"conceptIcon_gejw",conceptTitle:"conceptTitle_TGMM",conceptDescription:"conceptDescription_ZyDn"};var i=n(74848);function s({concepts:e,title:a}){return(0,i.jsxs)("div",{className:t.conceptOverview,children:[a&&(0,i.jsx)("h3",{className:t.overviewTitle,children:a}),(0,i.jsx)("div",{className:t.conceptGrid,children:e.map(((e,a)=>(0,i.jsxs)("div",{className:t.conceptCard,children:[(0,i.jsxs)("div",{className:t.conceptHeader,children:[e.icon&&(0,i.jsx)("div",{className:t.conceptIcon,children:(0,i.jsx)(e.icon,{size:20})}),(0,i.jsx)("h4",{className:t.conceptTitle,children:e.title})]}),(0,i.jsx)("p",{className:t.conceptDescription,children:e.description})]},a)))})]})}},82238:(e,a,n)=>{n.d(a,{A:()=>s});n(96540);const t={featureHighlights:"featureHighlights_Ardf",highlightItem:"highlightItem_XPnN",highlightIcon:"highlightIcon_SUR8",highlightContent:"highlightContent_d0XP"};var i=n(74848);function s({features:e}){return(0,i.jsx)("div",{className:t.featureHighlights,children:e.map(((e,a)=>(0,i.jsxs)("div",{className:t.highlightItem,children:[e.icon&&(0,i.jsx)("div",{className:t.highlightIcon,children:(0,i.jsx)(e.icon,{size:24})}),(0,i.jsxs)("div",{className:t.highlightContent,children:[(0,i.jsx)("h4",{children:e.title}),(0,i.jsx)("p",{children:e.description})]})]},a)))})}},82309:(e,a,n)=>{n.d(a,{A:()=>t});const t=n.p+"assets/images/add_feedback_ui-d41b6141727bf92f5d87568ae5522ae1.png"},93164:(e,a,n)=>{n.d(a,{A:()=>t});const t=(0,n(84722).A)("code",[["path",{d:"m16 18 6-6-6-6",key:"eg8j8"}],["path",{d:"m8 6-6 6 6 6",key:"ppft3o"}]])},96869:(e,a,n)=>{n.d(a,{A:()=>k});var t=n(96540),i=n(72102),s=n(75107);const r="workflowContainer__N1v",l="workflowTitle_QrAr",o="stepsContainer_IGeu",c="screenshotContainer_OwzZ",d="stepsHeader_nfiw",p="toggleButton_bJGw",m="toggleText_gT1o",h="stepItem_GyHJ",u="stepIndicator_U2Wb",f="stepNumber_vINc",_="stepNumberText_eLd7",g="stepConnector_Si86",v="stepContent_D0CA",x="stepTitle_wujx",y="stepDescription_PIaE";var w=n(66927),b=n(74848);const k=({steps:e,title:a,screenshot:n,defaultExpanded:k=!1})=>{const[j,A]=(0,t.useState)(k);return(0,b.jsxs)("div",{className:r,children:[a&&(0,b.jsx)("h3",{className:l,children:a}),n&&(0,b.jsx)("div",{className:c,children:(0,b.jsx)(w.A,{src:n.src,alt:n.alt,width:n.width||"90%"})}),(0,b.jsx)("div",{className:d,children:(0,b.jsxs)("button",{className:p,onClick:()=>A(!j),"aria-expanded":j,children:[(0,b.jsxs)("span",{className:m,children:[j?"Hide":"Show"," Step-by-Step Instructions (",e.length," steps)"]}),j?(0,b.jsx)(i.A,{size:20}):(0,b.jsx)(s.A,{size:20})]})}),j&&(0,b.jsx)("div",{className:o,children:e.map(((a,n)=>(0,b.jsxs)("div",{className:h,children:[(0,b.jsxs)("div",{className:u,children:[(0,b.jsx)("div",{className:f,children:a.icon?(0,b.jsx)(a.icon,{size:16}):(0,b.jsx)("span",{className:_,children:n+1})}),n<e.length-1&&(0,b.jsx)("div",{className:g})]}),(0,b.jsxs)("div",{className:v,children:[(0,b.jsx)("h4",{className:x,children:a.title}),(0,b.jsx)("p",{className:y,children:a.description})]})]},n)))})]})}},98445:(e,a,n)=>{n.d(a,{A:()=>t});const t=(0,n(84722).A)("search",[["path",{d:"m21 21-4.34-4.34",key:"14j7rj"}],["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}]])}}]);