"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[369],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var l=t(96540);const r={},o=l.createContext(r);function i(e){const n=l.useContext(o);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),l.createElement(o.Provider,{value:n},e.children)}},36866:(e,n,t)=>{t.d(n,{A:()=>l});const l=t.p+"assets/images/hyper-parameter-tuning-ui-88db40c21c63fb307e84629607985a73.png"},65537:(e,n,t)=>{t.d(n,{A:()=>b});var l=t(96540),r=t(34164),o=t(65627),i=t(56347),a=t(50372),s=t(30604),c=t(11861),d=t(78749);function h(e){return l.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,l.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:l,default:r}}=e;return{value:n,label:t,attributes:l,default:r}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:t}=e;const r=(0,i.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s.aZ)(o),(0,l.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(r.location.search);n.set(o,e),r.replace({...r.location,search:n.toString()})}),[o,r])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,o=m(e),[i,s]=(0,l.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const l=t.find((e=>e.default))??t[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:o}))),[c,h]=u({queryString:t,groupId:r}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,o]=(0,d.Dv)(t);return[r,(0,l.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:r}),w=(()=>{const e=c??f;return p({value:e,tabValues:o})?e:null})();(0,a.A)((()=>{w&&s(w)}),[w]);return{selectedValue:i,selectValue:(0,l.useCallback)((e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),h(e),g(e)}),[h,g,o]),tabValues:o}}var g=t(9136);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function v(e){let{className:n,block:t,selectedValue:l,selectValue:i,tabValues:a}=e;const s=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{const n=e.currentTarget,t=s.indexOf(n),r=a[t].value;r!==l&&(c(n),i(r))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},n),children:a.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:l===n?0:-1,"aria-selected":l===n,ref:e=>{s.push(e)},onKeyDown:h,onClick:d,...o,className:(0,r.A)("tabs__item",w.tabItem,o?.className,{"tabs__item--active":l===n}),children:t??n},n)}))})}function x(e){let{lazy:n,children:t,selectedValue:o}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===o));return e?(0,l.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,l.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function j(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,r.A)("tabs-container",w.tabList),children:[(0,y.jsx)(v,{...n,...e}),(0,y.jsx)(x,{...n,...e})]})}function b(e){const n=(0,g.A)();return(0,y.jsx)(j,{...e,children:h(e.children)},String(n))}},67756:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const l=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=t(29030),o=t(56289),i=t(74848);const a=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(l[e])return e}return null};function s(e){let{fn:n,children:t}=e;const s=a(n);if(!s)return(0,i.jsx)(i.Fragment,{children:t});const c=(0,r.Ay)(`/${l[s]}#${n}`);return(0,i.jsx)(o.A,{to:c,target:"_blank",children:t??(0,i.jsxs)("code",{children:[n,"()"]})})}},79329:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var l=t(34164);const r={tabItem:"tabItem_Ymn6"};var o=t(74848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,l.A)(r.tabItem,i),hidden:t,children:n})}},83775:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>h,default:()=>f,frontMatter:()=>d,metadata:()=>l,toc:()=>p});const l=JSON.parse('{"id":"deployment/deploy-model-to-kubernetes/tutorial/index","title":"Develop ML model with MLflow and deploy to Kubernetes","description":"This tutorial assumes that you have access to a Kubernetes cluster. However, you can also complete this tutorial on your local machine","source":"@site/docs/deployment/deploy-model-to-kubernetes/tutorial/index.mdx","sourceDirName":"deployment/deploy-model-to-kubernetes/tutorial","slug":"/deployment/deploy-model-to-kubernetes/tutorial/","permalink":"/docs/latest/deployment/deploy-model-to-kubernetes/tutorial/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Deploy MLflow Model to Kubernetes","permalink":"/docs/latest/deployment/deploy-model-to-kubernetes/"},"next":{"title":"Overview","permalink":"/docs/latest/llms/deployments/"}}');var r=t(74848),o=t(28453),i=t(56289),a=t(65537),s=t(79329),c=t(67756);const d={},h="Develop ML model with MLflow and deploy to Kubernetes",m={},p=[{value:"Introduction: Scalable Model Serving with KServe and MLServer",id:"introduction-scalable-model-serving-with-kserve-and-mlserver",level:2},{value:"What is KServe?",id:"what-is-kserve",level:3},{value:"Benefits of using MLflow with KServe",id:"benefits-of-using-mlflow-with-kserve",level:3},{value:"Step 1: Installing MLflow and Additional Dependencies",id:"step-1-installing-mlflow-and-additional-dependencies",level:2},{value:"Step 2: Setting Up a Kubernetes Cluster",id:"step-2-setting-up-a-kubernetes-cluster",level:2},{value:"Step 3: Training the Model",id:"step-3-training-the-model",level:2},{value:"Step 4: Running Hyperparameter Tuning",id:"step-4-running-hyperparameter-tuning",level:2},{value:"Step 5: Packaging the Model and Dependencies",id:"step-5-packaging-the-model-and-dependencies",level:2},{value:"Step 6: Testing Model Serving Locally",id:"step-6-test-model-serving-locally",level:2},{value:"Step 7: Deploying the Model to KServe",id:"step-7-deploying-the-model-to-kserve",level:2},{value:"Create Namespace",id:"create-namespace",level:3},{value:"Create Deployment Configuration",id:"create-deployment-configuration",level:3},{value:"Register Docker Account",id:"register-docker-account",level:4},{value:"Build a Docker Image",id:"build-a-docker-image",level:4},{value:"Push the Docker Image",id:"push-the-docker-image",level:4},{value:"Write Deployment Configuration",id:"write-deployment-configuration",level:4},{value:"Get Remote Model URI",id:"get-remote-model-uri",level:4},{value:"Create Deployment Configuration",id:"create-deployment-configuration-1",level:4},{value:"Deploy Inference Service",id:"deploy-inference-service",level:3},{value:"Test the Deployment",id:"test-the-deployment",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"develop-ml-model-with-mlflow-and-deploy-to-kubernetes",children:"Develop ML model with MLflow and deploy to Kubernetes"})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["This tutorial assumes that you have access to a Kubernetes cluster. However, you can also complete this tutorial on your local machine\nby using local cluster emulation tools such as ",(0,r.jsx)(n.a,{href:"https://kind.sigs.k8s.io/docs/user/quick-start",children:"Kind"})," or ",(0,r.jsx)(n.a,{href:"https://minikube.sigs.k8s.io/docs/start",children:"Minikube"}),"."]})}),"\n",(0,r.jsx)(n.p,{children:"This guide demonstrates how to use MLflow end-to-end for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Training a linear regression model with ",(0,r.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Conducting hyper-parameter tuning to find the best model."}),"\n",(0,r.jsxs)(n.li,{children:["Packaging the model weights and dependencies as an ",(0,r.jsx)(n.a,{href:"/model",children:"MLflow Model"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Testing model serving locally with ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest/",children:"mlserver"})," using the ",(0,r.jsx)(i.A,{to:"/api_reference/cli.html#mlflow-models-serve",target:"_blank",children:"mlflow models serve"})," command."]}),"\n",(0,r.jsxs)(n.li,{children:["Deploying the model to a Kubernetes cluster using ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/",children:"KServe"})," with MLflow."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["We will cover an end-to-end model development process including model training and testing within this tutorial.\nIf you already have a model and simply want to learn how to deploy it to Kubernetes, you can skip to ",(0,r.jsx)(n.a,{href:"#step-6-test-model-serving-locally",children:"Step 6 - Test Model Serving Locally"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-scalable-model-serving-with-kserve-and-mlserver",children:"Introduction: Scalable Model Serving with KServe and MLServer"}),"\n",(0,r.jsxs)(n.p,{children:["MLflow provides an easy-to-use interface for deploying models within a Flask-based inference server. You can deploy the same inference\nserver to a Kubernetes cluster by containerizing it using the ",(0,r.jsx)(n.code,{children:"mlflow models build-docker"})," command. However, this approach may not be scalable\nand could be unsuitable for production use cases. Flask is not designed for high performance and scale (",(0,r.jsx)(n.a,{href:"/deployment/deploy-model-locally#serving-frameworks",children:"why?"}),"), and also\nmanually managing multiple instances of inference servers is backbreaking."]}),"\n",(0,r.jsxs)(n.p,{children:["Fortunately, MLflow offers a solution for this. MLflow provides an alternative inference engine that is better suited for larger-scale inference deployments with its support for ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest",children:"MLServer"}),",\nwhich enables one-step deployment to popular serverless model serving frameworks on Kubernetes, such as ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website",children:"KServe"}),", and\n",(0,r.jsx)(n.a,{href:"https://docs.seldon.io/projects/seldon-core/en/latest",children:"Seldon Core"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"what-is-kserve",children:"What is KServe?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://kserve.github.io/website",children:"KServe"}),", formally known as KFServing, provides performant, scalable, and highly-abstracted interfaces for common machine learning frameworks like Tensorflow, XGBoost, scikit-learn, and Pytorch.\nIt offers advanced features that aid in operating large-scale machine learning systems, such as ",(0,r.jsx)(n.strong,{children:"autoscaling"}),", ",(0,r.jsx)(n.strong,{children:"canary rollout"}),", ",(0,r.jsx)(n.strong,{children:"A/B testing"}),", ",(0,r.jsx)(n.strong,{children:"monitoring"}),",\n",(0,r.jsx)(n.strong,{children:"explainability"}),", and more, leveraging the Kubernetes ecosystem, including ",(0,r.jsx)(n.a,{href:"https://knative.dev",children:"KNative"})," and ",(0,r.jsx)(n.a,{href:"https://istio.io",children:"Istio"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"benefits-of-using-mlflow-with-kserve",children:"Benefits of using MLflow with KServe"}),"\n",(0,r.jsxs)(n.p,{children:["While KServe enables highly scalable and production-ready model serving, deploying your model there might require some effort.\nMLflow simplifies the process of deploying models to a Kubernetes cluster with KServe and MLServer. Additionally, it offers seamless ",(0,r.jsx)(n.strong,{children:"end-to-end model management"}),"\nas a single place to manage the entire ML lifecycle. This includes ",(0,r.jsx)(n.a,{href:"/tracking",children:"experiment tracking"}),", ",(0,r.jsx)(n.a,{href:"/model",children:"model packaging"}),",\n",(0,r.jsx)(n.a,{href:"/model-registry",children:"versioning"}),", ",(0,r.jsx)(n.a,{href:"/model-evaluation",children:"evaluation"}),", and ",(0,r.jsx)(n.a,{href:"/deployment",children:"deployment"}),", which we will cover in this tutorial."]}),"\n",(0,r.jsx)(n.h2,{id:"step-1-installing-mlflow-and-additional-dependencies",children:"Step 1: Installing MLflow and Additional Dependencies"}),"\n",(0,r.jsx)(n.p,{children:"First, please install mlflow to your local machine using the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install mlflow[mlserver]\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"[extras]"})," will install additional dependencies required for this tutorial including ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest",children:"mlserver"})," and\n",(0,r.jsx)(n.a,{href:"https://scikit-learn.org/stable",children:"scikit-learn"}),". Note that scikit-learn is not required for deployment, just for training the example model used in this tutorial."]}),"\n",(0,r.jsx)(n.p,{children:"You can check if MLflow is installed correctly by running:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow --version\n"})}),"\n",(0,r.jsx)(n.h2,{id:"step-2-setting-up-a-kubernetes-cluster",children:"Step 2: Setting Up a Kubernetes Cluster"}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsx)(s.A,{default:!0,label:"Kubernetes Cluster",value:"kubernetes-cluster",children:(0,r.jsxs)(n.p,{children:["If you already have access to a Kubernetes cluster, you can install KServe to your cluster by following ",(0,r.jsx)(n.a,{href:"https://github.com/kserve/kserve#hammer_and_wrench-installation",children:"the official instructions"}),"."]})}),(0,r.jsx)(s.A,{label:"Local Machine Emulation",value:"local-machine-emulation",children:(0,r.jsxs)(n.p,{children:["You can follow ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/latest/get_started",children:"KServe QuickStart"})," to set up a local cluster with ",(0,r.jsx)(n.a,{href:"https://kind.sigs.k8s.io/docs/user/quick-start",children:"Kind"})," and install KServe on it."]})})]}),"\n",(0,r.jsx)(n.p,{children:"Now that you have a Kubernetes cluster running as a deployment target, let's move on to creating the MLflow Model to deploy."}),"\n",(0,r.jsx)(n.h2,{id:"step-3-training-the-model",children:"Step 3: Training the Model"}),"\n",(0,r.jsx)(n.p,{children:"In this tutorial, we will train and deploy a simple regression model that predicts the quality of wine."}),"\n",(0,r.jsx)(n.p,{children:"Let's start from training a model with the default hyperparameters. Execute the following code in a notebook or as a Python script."}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["For the sake of convenience, we use the ",(0,r.jsx)(c.B,{fn:"mlflow.sklearn.autolog"})," function.\nThis function allows MLflow to automatically log the appropriate set of model parameters and metrics during training. To learn more about the auto-logging feature\nor how to log manually instead, see the ",(0,r.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking documentation"}),"."]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nimport numpy as np\nfrom sklearn import datasets, metrics\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import train_test_split\n\n\ndef eval_metrics(pred, actual):\n    rmse = np.sqrt(metrics.mean_squared_error(actual, pred))\n    mae = metrics.mean_absolute_error(actual, pred)\n    r2 = metrics.r2_score(actual, pred)\n    return rmse, mae, r2\n\n\n# Set th experiment name\nmlflow.set_experiment("wine-quality")\n\n# Enable auto-logging to MLflow\nmlflow.sklearn.autolog()\n\n# Load wine quality dataset\nX, y = datasets.load_wine(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n# Start a run and train a model\nwith mlflow.start_run(run_name="default-params"):\n    lr = ElasticNet()\n    lr.fit(X_train, y_train)\n\n    y_pred = lr.predict(X_test)\n    metrics = eval_metrics(y_pred, y_test)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Now you have trained a model, let's check if the parameters and metrics are logged correctly, via the MLflow UI.\nYou can start the MLflow UI by running the following command in your terminal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow ui --port 5000\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then visit ",(0,r.jsx)(n.a,{href:"http://localhost:5000",children:"http://localhost:5000"})," to open the UI."]}),"\n",(0,r.jsx)("div",{className:"center-div",style:{width:"80%"},children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:t(98437).A+"",width:"1131",height:"627"})})}),"\n",(0,r.jsxs)(n.p,{children:['Please open the experient named "wine-quality" on the left, then click the run named "default-params" in the table.\nFor this case, you should see parameters including ',(0,r.jsx)(n.code,{children:"alpha"})," and ",(0,r.jsx)(n.code,{children:"l1_ratio"})," and metrics like ",(0,r.jsx)(n.code,{children:"training_score"})," and ",(0,r.jsx)(n.code,{children:"mean_absolute_error_X_test"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-running-hyperparameter-tuning",children:"Step 4: Running Hyperparameter Tuning"}),"\n",(0,r.jsxs)(n.p,{children:["Now that we have established a baseline model, let's attempt to improve its performance by tuning the hyperparameters.\nWe will conduct a random search to identify the optimal combination of ",(0,r.jsx)(n.code,{children:"alpha"})," and ",(0,r.jsx)(n.code,{children:"l1_ratio"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from scipy.stats import uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nlr = ElasticNet()\n\n# Define distribution to pick parameter values from\ndistributions = dict(\n    alpha=uniform(loc=0, scale=10),  # sample alpha uniformly from [-5.0, 5.0]\n    l1_ratio=uniform(),  # sample l1_ratio uniformlyfrom [0, 1.0]\n)\n\n# Initialize random search instance\nclf = RandomizedSearchCV(\n    estimator=lr,\n    param_distributions=distributions,\n    # Optimize for mean absolute error\n    scoring="neg_mean_absolute_error",\n    # Use 5-fold cross validation\n    cv=5,\n    # Try 100 samples. Note that MLflow only logs the top 5 runs.\n    n_iter=100,\n)\n\n# Start a parent run\nwith mlflow.start_run(run_name="hyperparameter-tuning"):\n    search = clf.fit(X_train, y_train)\n\n    # Evaluate the best model on test dataset\n    y_pred = clf.best_estimator_.predict(X_test)\n    rmse, mae, r2 = eval_metrics(clf.best_estimator_, y_pred, y_test)\n    mlflow.log_metrics(\n        {\n            "mean_squared_error_X_test": rmse,\n            "mean_absolute_error_X_test": mae,\n            "r2_score_X_test": r2,\n        }\n    )\n'})}),"\n",(0,r.jsxs)(n.p,{children:['When you reopen the MLflow UI, you should notice that the run "hyperparameter-tuning" contains 5 child runs. MLflow utilizes parent-child relationship, which is particularly\nuseful for grouping a set of runs, such as those in hyper parameter tuning. Here the auto-logging is enabled and MLflow automatically create child runs for the top 5 runs\nbased on the ',(0,r.jsx)(n.code,{children:"scoring"})," metric, which is negative mean absolute error in this example. You can also create parent and child runs manually, please refer to ",(0,r.jsx)(n.a,{href:"/tracking/tracking-api/#child_runs",children:"Create Child Runs"}),"\nfor more details."]}),"\n",(0,r.jsx)("div",{className:"center-div",style:{width:"80%"},children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:t(36866).A+"",width:"2348",height:"1006"})})}),"\n",(0,r.jsx)(n.p,{children:"To compare the results and identify the best model, you can utilize the visualization feature in the MLflow UI."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'Select the first job ("default-params") and the parent job for hyperparameter tuning ("hyperparameter-turning").'}),"\n",(0,r.jsx)(n.li,{children:'Click on the "Chart" tab to visualize the metrics in a chart.'}),"\n",(0,r.jsx)(n.li,{children:"By default, a few bar charts for a predefined set of metrics are displayed."}),"\n",(0,r.jsx)(n.li,{children:"You can add different chart, such as a scatter plot, to compare multiple metrics. For example, we can see the best model from hyperparameter tuning outperforms the default parameter model, in the mean squared error on the test dataset:"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:['You can check the best combination of hyperparameters by looking at the parent run "hyperparameter-tuning".\nIn this example, the best model was ',(0,r.jsx)(n.code,{children:"alpha=0.11714084185001972"})," and ",(0,r.jsx)(n.code,{children:"l1_ratio=0.3599780644783639"})," (you may see different results)."]}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["To learn more about hyperparameter tuning with MLflow, please refer to ",(0,r.jsx)(n.a,{href:"/traditional-ml/hyperparameter-tuning-with-child-runs",children:"Hyperparameter Tuning with MLflow and Optuna"}),"."]})}),"\n",(0,r.jsx)(n.h2,{id:"step-5-packaging-the-model-and-dependencies",children:"Step 5: Packaging the Model and Dependencies"}),"\n",(0,r.jsxs)(n.p,{children:["Since we are using autologging, MLflow automatically logs the ",(0,r.jsx)(n.a,{href:"/model",children:"Model"})," for each run. This process conveniently packages the model weight\nand dependencies in a ready-to-deploy format."]}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["In practice, it is also recommended to use ",(0,r.jsx)(n.a,{href:"/model-registry",children:"MLflow Model Registry"})," for registering and managing your models."]})}),"\n",(0,r.jsxs)(n.p,{children:["Let's take a brief look at how this format appears. You can view the logged model through the ",(0,r.jsx)(n.code,{children:"Artifacts"})," tab on the Run detail page."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"model\n\u251c\u2500\u2500 MLmodel\n\u251c\u2500\u2500 model.pkl\n\u251c\u2500\u2500 conda.yaml\n\u251c\u2500\u2500 python_env.yaml\n\u2514\u2500\u2500 requirements.txt\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"model.pkl"})," is the file containing the serialized model weight. ",(0,r.jsx)(n.code,{children:"MLmodel"})," includes general metadata that instructs MLflow on how to load the model.\nThe other files specify the dependencies required to run the model."]}),"\n",(0,r.jsxs)(n.admonition,{type:"note",children:[(0,r.jsxs)(n.p,{children:["If you opt for manual logging, you will need to log the model explicitly using the ",(0,r.jsx)(c.B,{fn:"mlflow.sklearn.log_model",children:(0,r.jsx)(n.code,{children:"mlflow.sklearn.log_model"})}),"\nfunction, as shown below:"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'mlflow.sklearn.log_model(lr, name="model")\n'})})]}),"\n",(0,r.jsx)(n.h2,{id:"step-6-test-model-serving-locally",children:"Step 6: Testing Model Serving Locally"}),"\n",(0,r.jsxs)(n.p,{children:["Before deploying the model, let's first test that the model can be served locally. As outlined in the\n",(0,r.jsx)(n.a,{href:"/deployment/deploy-model-locally",children:"Deploy MLflow Model Locally"}),", you can run a local inference server with just a single command.\nRemember to use the ",(0,r.jsx)(n.code,{children:"enable-mlserver"})," flag, which instructs MLflow to use MLServer as the inference server. This ensures the model runs in the\nsame manner as it would in Kubernetes."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow models serve -m runs:/<run_id_for_your_best_run>/model -p 1234 --enable-mlserver\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This command starts a local server listening on port 1234. You can send a request to the server using ",(0,r.jsx)(n.code,{children:"curl"})," command:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'$ curl -X POST -H "Content-Type:application/json" --data \'{"inputs": [[14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0]]}\' http://127.0.0.1:1234/invocations\n\n{"predictions": [-0.03416275504140387]}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["For more information about the request format and response formats, refer to ",(0,r.jsx)(n.a,{href:"/deployment/deploy-model-locally/#local-inference-server-spec",children:"Inference Server Specification"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"step-7-deploying-the-model-to-kserve",children:"Step 7: Deploying the Model to KServe"}),"\n",(0,r.jsx)(n.p,{children:"Finally, we are all set to deploy the model to the Kubernetes cluster."}),"\n",(0,r.jsx)(n.h3,{id:"create-namespace",children:"Create Namespace"}),"\n",(0,r.jsx)(n.p,{children:"First, create a test namespace for deploying KServe resources and your model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl create namespace mlflow-kserve-test\n"})}),"\n",(0,r.jsx)(n.h3,{id:"create-deployment-configuration",children:"Create Deployment Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Create a YAML file describing the model deployment to KServe."}),"\n",(0,r.jsx)(n.p,{children:"There are two ways to specify the model for deployment in KServe configuration file:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Build a Docker image with the model and specify the image URI."}),"\n",(0,r.jsx)(n.li,{children:"Specify the model URI directly (this only works if your model is stored in remote storage)."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Please open the tabs below for details on each approach."}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsxs)(s.A,{default:!0,label:"Using Docker Image",value:"using-docker-image",children:[(0,r.jsx)(n.h4,{id:"register-docker-account",children:"Register Docker Account"}),(0,r.jsxs)(n.p,{children:["Since KServe cannot resolve a locally built Docker image, you need to push the image to a Docker registry.\nFor this tutorial, we'll push the image to ",(0,r.jsx)(n.a,{href:"https://hub.docker.com",children:"Docker Hub"}),", but you can use any other Docker registry,\nsuch as ",(0,r.jsx)(n.a,{href:"https://aws.amazon.com/ecr",children:"Amazon ECR"})," or a private registry."]}),(0,r.jsxs)(n.p,{children:["If you don't have a Docker Hub account yet, create one at ",(0,r.jsx)(n.a,{href:"https://hub.docker.com/signup",children:"https://hub.docker.com/signup"}),"."]}),(0,r.jsx)(n.h4,{id:"build-a-docker-image",children:"Build a Docker Image"}),(0,r.jsxs)(n.p,{children:["Build a ready-to-deploy Docker image with the ",(0,r.jsx)(n.code,{children:"mlflow models build-docker"})," command:"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow models build-docker -m runs:/<run_id_for_your_best_run>/model -n <your_dockerhub_user_name>/mlflow-wine-classifier --enable-mlserver\n"})}),(0,r.jsxs)(n.p,{children:["This command builds a Docker image with the model and dependencies, tagging it as ",(0,r.jsx)(n.code,{children:"mlflow-wine-classifier:latest"}),"."]}),(0,r.jsx)(n.h4,{id:"push-the-docker-image",children:"Push the Docker Image"}),(0,r.jsx)(n.p,{children:"After building the image, push it to Docker Hub (or to another registry using the appropriate command):"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker push <your_dockerhub_user_name>/mlflow-wine-classifier\n"})}),(0,r.jsx)(n.h4,{id:"write-deployment-configuration",children:"Write Deployment Configuration"}),(0,r.jsx)(n.p,{children:"Then create a YAML file like this:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: "serving.kserve.io/v1beta1"\nkind: "InferenceService"\nmetadata:\n  name: "mlflow-wine-classifier"\n  namespace: "mlflow-kserve-test"\nspec:\n  predictor:\n    containers:\n      - name: "mlflow-wine-classifier"\n        image: "<your_docker_user_name>/mlflow-wine-classifier"\n        ports:\n          - containerPort: 8080\n            protocol: TCP\n        env:\n          - name: PROTOCOL\n            value: "v2"\n'})})]}),(0,r.jsxs)(s.A,{label:"Using Model URI",value:"using-model-uri",children:[(0,r.jsx)(n.h4,{id:"get-remote-model-uri",children:"Get Remote Model URI"}),(0,r.jsxs)(n.p,{children:["KServe configuration allows direct specification of the model URI. However, it doesn't resolve MLflow-specific URI schemas like ",(0,r.jsx)(n.code,{children:"runs:/"})," and ",(0,r.jsx)(n.code,{children:"model:/"}),",\nnor local file URIs like ",(0,r.jsx)(n.code,{children:"file:///"}),". We need to specify the model URI in a remote storage URI format e.g. ",(0,r.jsx)(n.code,{children:"s3://xxx"})," or ",(0,r.jsx)(n.code,{children:"gs://xxx"}),".\nBy default, MLflow stores the model in the local file system, so you need to configure MLflow to store the model in remote storage.\nPlease refer to ",(0,r.jsx)(n.a,{href:"/tracking#artifact-stores",children:"Artifact Store"})," for setup instructions."]}),(0,r.jsx)(n.p,{children:"After configuring the artifact store, load and re-log the best model to the new artifact store, or repeat the model training steps."}),(0,r.jsx)(n.h4,{id:"create-deployment-configuration-1",children:"Create Deployment Configuration"}),(0,r.jsx)(n.p,{children:"With the remote model URI, create a YAML file:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: "serving.kserve.io/v1beta1"\nkind: "InferenceService"\nmetadata:\n  name: "mlflow-wine-classifier"\n  namespace: "mlflow-kserve-test"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: "<your_model_uri>"\n'})})]})]}),"\n",(0,r.jsx)(n.h3,{id:"deploy-inference-service",children:"Deploy Inference Service"}),"\n",(0,r.jsxs)(n.p,{children:["Run the following ",(0,r.jsx)(n.code,{children:"kubectl"})," command to deploy a new ",(0,r.jsx)(n.code,{children:"InferenceService"})," to your Kubernetes cluster:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ kubectl apply -f YOUR_CONFIG_FILE.yaml\n\ninferenceservice.serving.kserve.io/mlflow-wine-classifier created\n"})}),"\n",(0,r.jsx)(n.p,{children:"You can check the status of the deployment by running:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ kubectl get inferenceservice mlflow-wine-classifier\n\nNAME                     URL                                                     READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION\nmlflow-wine-classifier   http://mlflow-wine-classifier.mlflow-kserve-test.local   True             100                    mlflow-wine-classifier-100\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["It may take a few minutes for the deployment status to be ready. For detailed deployment status and logs,\nrun ",(0,r.jsx)(n.code,{children:"kubectl get inferenceservice mlflow-wine-classifier -oyaml"}),"."]})}),"\n",(0,r.jsx)(n.h3,{id:"test-the-deployment",children:"Test the Deployment"}),"\n",(0,r.jsx)(n.p,{children:"Once the deployment is ready, you can send a test request to the server."}),"\n",(0,r.jsxs)(n.p,{children:["First, create a JSON file with test data and save it as ",(0,r.jsx)(n.code,{children:"test-input.json"}),". Ensure the request data is formatted for the ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/latest/modelserving/inference_api/#inference-request-json-object",children:"V2 Inference Protocol"}),",\nbecause we created the model with ",(0,r.jsx)(n.code,{children:"protocolVersion: v2"}),". The request should look like this:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "inputs": [\n    {\n      "name": "input",\n      "shape": [13],\n      "datatype": "FP32",\n      "data": [14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0]\n    }\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"Then send the request to your inference service:"}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsxs)(s.A,{default:!0,label:"Kubernetes Cluster",value:"kubernetes-custer",children:[(0,r.jsxs)(n.p,{children:["Assuming your cluster is exposed via LoadBalancer, follow ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/0.10/get_started/first_isvc/#4-determine-the-ingress-ip-and-ports",children:"these instructions"})," to find the Ingress IP and port.\nThen send a test request using ",(0,r.jsx)(n.code,{children:"curl"})," command:"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'$ SERVICE_HOSTNAME=$(kubectl get inferenceservice mlflow-wine-classifier -n mlflow-kserve-test -o jsonpath=\'{.status.url}\' | cut -d "/" -f 3)\n$ curl -v \\\n  -H "Host: ${SERVICE_HOSTNAME}" \\\n  -H "Content-Type: application/json" \\\n  -d @./test-input.json \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/mlflow-wine-classifier/infer\n'})})]}),(0,r.jsxs)(s.A,{label:"Local Machine Emulation",value:"local-machine-emulation",children:[(0,r.jsxs)(n.p,{children:["Typically, Kubernetes clusters expose services via LoadBalancer, but a local cluster created by ",(0,r.jsx)(n.code,{children:"kind"})," doesn't have one.\nIn this case, you can access the inference service via port-forwarding."]}),(0,r.jsx)(n.p,{children:"Open a new terminal and run the following command to forward the port:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ INGRESS_GATEWAY_SERVICE=$(kubectl get svc -n istio-system --selector=\"app=istio-ingressgateway\" -o jsonpath='{.items[0].metadata.name}')\n$ kubectl port-forward -n istio-system svc/${INGRESS_GATEWAY_SERVICE} 8080:80\n\nForwarding from 127.0.0.1:8080 -> 8080\nForwarding from [::1]:8080 -> 8080\n"})}),(0,r.jsx)(n.p,{children:"Then, in the original terminal, send a test request to the server:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'$ SERVICE_HOSTNAME=$(kubectl get inferenceservice mlflow-wine-classifier -n mlflow-kserve-test -o jsonpath=\'{.status.url}\' | cut -d "/" -f 3)\n$ curl -v \\\n  -H "Host: ${SERVICE_HOSTNAME}" \\\n  -H "Content-Type: application/json" \\\n  -d @./test-input.json \\\n  http://localhost:8080/v2/models/mlflow-wine-classifier/infer\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsxs)(n.p,{children:["If you have any trouble during deployment, please consult with the ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website",children:"KServe official documentation"}),"\nand their ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/0.10/modelserving/v1beta1/mlflow/v2",children:"MLflow Deployment Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Congratulations on completing the guide! In this tutorial, you have learned how to use MLflow for training a model, running hyperparameter tuning,\nand deploying the model to Kubernetes cluster."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Further readings"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking"})," - Explore more about MLflow Tracking and various ways to manage experiments and models, such as team collaboration."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/model-registry",children:"MLflow Model Registry"})," - Discover more about MLflow Model Registry for managing model versions and stages in a centralized model store."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/deployment",children:"MLflow Deployment"})," - Learn more about MLflow deployment and different deployment targets."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/",children:"KServe official documentation"})," - Dive deeper into KServe and its advanced features, including autoscaling, canary rollout, A/B testing, monitoring, explainability, etc."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://docs.seldon.io/projects/seldon-core/en/latest/",children:"Seldon Core official documentation"})," - Learn about Seldon Core, an alternative serverless model serving framework we support for Kubernetes."]}),"\n"]})]})}function f(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},98437:(e,n,t)=>{t.d(n,{A:()=>l});const l=t.p+"assets/images/tracking-ui-default-620cdc67da31f91f653210afe78de3b8.png"}}]);