"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["7822"],{74487(e,n,t){t.r(n),t.d(n,{metadata:()=>l,default:()=>v,frontMatter:()=>f,contentTitle:()=>h,toc:()=>x,assets:()=>_});var l=JSON.parse('{"id":"evaluation/index","title":"Model Evaluation","description":"This documentation covers MLflow\'s classic evaluation system (mlflow.models.evaluate) which uses EvaluationMetric and make_metric for custom metrics.","source":"@site/docs/classic-ml/evaluation/index.mdx","sourceDirName":"evaluation","slug":"/evaluation/","permalink":"/mlflow-website/docs/latest/ml/evaluation/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Model Evaluation"},"sidebar":"classicMLSidebar","previous":{"title":"MLflow Datasets","permalink":"/mlflow-website/docs/latest/ml/dataset/"},"next":{"title":"Overview","permalink":"/mlflow-website/docs/latest/ml/model-registry/"}}'),a=t(74848),i=t(28453),r=t(33508),s=t(46858),o=t(55300),c=t(46816),d=t(93893),m=t(54725),p=t(78010),u=t(57250);let f={title:"Model Evaluation"},h="Model Evaluation",_={},x=[{value:"Introduction",id:"introduction",level:2},{value:"Model Evaluation",id:"model-evaluation-1",level:2},{value:"Quick Start",id:"quick-start",level:3},{value:"Model Types",id:"model-types",level:3},{value:"Evaluator Configuration",id:"evaluator-configuration",level:3},{value:"Evaluation Results",id:"evaluation-results",level:3},{value:"Model Validation",id:"model-validation",level:3},{value:"Dataset Evaluation",id:"dataset-evaluation",level:2},{value:"Usage",id:"usage",level:3},{value:"Parameters",id:"parameters",level:3},{value:"Function Evaluation",id:"function-evaluation",level:2},{value:"Usage",id:"usage-1",level:3},{value:"Function Requirements",id:"function-requirements",level:3},{value:"Custom Metrics &amp; Visualizations",id:"custom-metrics--visualizations",level:2},{value:"Custom Metrics",id:"custom-metrics",level:3},{value:"Custom Visualizations",id:"custom-visualizations",level:3},{value:"SHAP Integration",id:"shap-integration",level:2},{value:"Usage",id:"usage-2",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Using Saved Explainers",id:"using-saved-explainers",level:3},{value:"Plugin Evaluators",id:"plugin-evaluators",level:2},{value:"Giskard Plugin",id:"giskard-plugin",level:3},{value:"Trubrics Plugin",id:"trubrics-plugin",level:3},{value:"API Reference",id:"api-reference",level:2}];function g(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"model-evaluation",children:"Model Evaluation"})}),"\n",(0,a.jsxs)(n.admonition,{title:"Classic ML Evaluation System",type:"warning",children:[(0,a.jsxs)(n.p,{children:["This documentation covers MLflow's ",(0,a.jsx)(n.strong,{children:"classic evaluation system"})," (",(0,a.jsx)(n.code,{children:"mlflow.models.evaluate"}),") which uses ",(0,a.jsx)(n.code,{children:"EvaluationMetric"})," and ",(0,a.jsx)(n.code,{children:"make_metric"})," for custom metrics."]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"For GenAI/LLM evaluation"}),", please use the system at ",(0,a.jsx)(n.a,{href:"/genai/eval-monitor",children:"GenAI Evaluation"})," which uses:"]}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"mlflow.genai.evaluate()"})," instead of ",(0,a.jsx)(n.code,{children:"mlflow.models.evaluate()"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Scorer"})," objects instead of ",(0,a.jsx)(n.code,{children:"EvaluationMetric"})]}),"\n",(0,a.jsx)(n.li,{children:"Built-in LLM judges and scorers"}),"\n"]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Important"}),": These two systems are ",(0,a.jsx)(n.strong,{children:"not interoperable"}),". ",(0,a.jsx)(n.code,{children:"EvaluationMetric"})," objects cannot be used with ",(0,a.jsx)(n.code,{children:"mlflow.genai.evaluate()"}),", and ",(0,a.jsx)(n.code,{children:"Scorer"})," objects cannot be used with ",(0,a.jsx)(n.code,{children:"mlflow.models.evaluate()"}),"."]})]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"MLflow's evaluation framework provides automated model assessment for classification and regression tasks. It generates performance metrics, visualizations, and diagnostic information through a unified API."}),"\n",(0,a.jsx)(r.A,{features:[{icon:s.A,title:"Unified Evaluation API",description:"Evaluate models, Python functions, or static datasets with mlflow.models.evaluate() using a consistent interface across different evaluation modes."},{icon:o.A,title:"Automated Metrics & Visualizations",description:"Generate task-specific metrics and plots automatically, including confusion matrices, ROC curves, and feature importance with SHAP integration."},{icon:c.A,title:"Custom Metrics",description:"Define domain-specific evaluation criteria with make_metric() for business-specific performance measures beyond standard ML metrics."},{icon:d.A,title:"Plugin Architecture",description:"Extend evaluation with specialized frameworks like Giskard and Trubrics for advanced validation and vulnerability scanning."}]}),"\n",(0,a.jsx)(n.h2,{id:"model-evaluation-1",children:"Model Evaluation"}),"\n",(0,a.jsx)(n.p,{children:"Evaluate classification and regression models with automated metrics and visualizations."}),"\n",(0,a.jsx)(n.h3,{id:"quick-start",children:"Quick Start"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom mlflow.models import infer_signature\n\n# Load dataset\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train model\nmodel = xgb.XGBClassifier().fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["label"] = y_test\n\nwith mlflow.start_run():\n    # Log model\n    signature = infer_signature(X_test, model.predict(X_test))\n    model_info = mlflow.sklearn.log_model(model, name="model", signature=signature)\n\n    # Evaluate\n    result = mlflow.models.evaluate(\n        model_info.model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n    )\n\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\n    print(f"ROC AUC: {result.metrics[\'roc_auc\']:.3f}")\n'})}),"\n",(0,a.jsx)(n.p,{children:"This automatically generates performance metrics (accuracy, precision, recall, F1-score, ROC-AUC), visualizations (confusion matrix, ROC curve, precision-recall curve), and saves all artifacts to MLflow."}),"\n",(0,a.jsx)(n.h3,{id:"model-types",children:"Model Types"}),"\n",(0,a.jsxs)(p.A,{children:[(0,a.jsxs)(u.A,{value:"classification",label:"Classification",default:!0,children:[(0,a.jsx)(n.p,{children:"For classification tasks:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'result = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n)\n\n# Access metrics\nprint(f"Precision: {result.metrics[\'precision_score\']:.3f}")\nprint(f"Recall: {result.metrics[\'recall_score\']:.3f}")\nprint(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\nprint(f"ROC AUC: {result.metrics[\'roc_auc\']:.3f}")\n'})}),(0,a.jsx)(n.p,{children:"Automatically generates: accuracy, precision, recall, F1-score, ROC-AUC, precision-recall AUC, log loss, brier score, confusion matrix, and classification report."})]}),(0,a.jsxs)(u.A,{value:"regression",label:"Regression",children:[(0,a.jsx)(n.p,{children:"For regression tasks:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\n\n# Load regression dataset\nhousing = fetch_california_housing(as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    housing.data, housing.target, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = LinearRegression().fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["target"] = y_test\n\nwith mlflow.start_run():\n    signature = infer_signature(X_train, model.predict(X_train))\n    model_info = mlflow.sklearn.log_model(model, name="model", signature=signature)\n\n    result = mlflow.models.evaluate(\n        model_info.model_uri,\n        eval_data,\n        targets="target",\n        model_type="regressor",\n    )\n\n    print(f"MAE: {result.metrics[\'mean_absolute_error\']:.3f}")\n    print(f"RMSE: {result.metrics[\'root_mean_squared_error\']:.3f}")\n    print(f"R\xb2 Score: {result.metrics[\'r2_score\']:.3f}")\n'})}),(0,a.jsx)(n.p,{children:"Automatically generates: MAE, MSE, RMSE, R\xb2 score, adjusted R\xb2, MAPE, residual plots, and distribution analysis."})]})]}),"\n",(0,a.jsx)(n.h3,{id:"evaluator-configuration",children:"Evaluator Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["Control evaluator behavior with the ",(0,a.jsx)(n.code,{children:"evaluator_config"})," parameter:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Include SHAP explainer for feature importance\nresult = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    evaluator_config={\n        "log_explainer": True,\n        "explainer_type": "exact",\n    },\n)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Common options: ",(0,a.jsx)(n.code,{children:"log_explainer"})," (log SHAP explainer), ",(0,a.jsx)(n.code,{children:"explainer_type"}),' (SHAP type: "exact", "permutation", "partition"), ',(0,a.jsx)(n.code,{children:"pos_label"})," (positive class label for binary classification), ",(0,a.jsx)(n.code,{children:"average"}),' (averaging strategy for multiclass: "macro", "micro", "weighted").']}),"\n",(0,a.jsx)(n.h3,{id:"evaluation-results",children:"Evaluation Results"}),"\n",(0,a.jsx)(n.p,{children:"Access metrics, artifacts, and evaluation data:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Run evaluation\nresult = mlflow.models.evaluate(\n    model_uri, eval_data, targets="label", model_type="classifier"\n)\n\n# Access metrics\nfor metric_name, value in result.metrics.items():\n    print(f"{metric_name}: {value}")\n\n# Access artifacts (plots, tables)\nfor artifact_name, path in result.artifacts.items():\n    print(f"{artifact_name}: {path}")\n\n# Access evaluation table\neval_table = result.tables["eval_results_table"]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"model-validation",children:"Model Validation"}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsxs)(n.p,{children:["MLflow 2.18.0 moved model validation from ",(0,a.jsx)(m.B,{fn:"mlflow.models.evaluate"})," to ",(0,a.jsx)(m.B,{fn:"mlflow.validate_evaluation_results"}),"."]})}),"\n",(0,a.jsx)(n.p,{children:"Validate evaluation metrics against thresholds:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.models import MetricThreshold\n\n# Evaluate model\nresult = mlflow.models.evaluate(\n    model_uri, eval_data, targets="label", model_type="classifier"\n)\n\n# Define thresholds\nthresholds = {\n    "accuracy_score": MetricThreshold(threshold=0.85, greater_is_better=True),\n    "precision_score": MetricThreshold(threshold=0.80, greater_is_better=True),\n}\n\n# Validate\ntry:\n    mlflow.validate_evaluation_results(\n        candidate_result=result,\n        validation_thresholds=thresholds,\n    )\n    print("Model meets all thresholds")\nexcept mlflow.exceptions.ModelValidationFailedException as e:\n    print(f"Validation failed: {e}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"dataset-evaluation",children:"Dataset Evaluation"}),"\n",(0,a.jsx)(n.p,{children:"Evaluate pre-computed predictions without re-running the model."}),"\n",(0,a.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Generate sample data and train a model\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Generate predictions\npredictions = model.predict(X_test)\nprediction_probabilities = model.predict_proba(X_test)[:, 1]\n\n# Create evaluation dataset with predictions\neval_dataset = pd.DataFrame(\n    {\n        "prediction": predictions,\n        "target": y_test,\n    }\n)\n\nwith mlflow.start_run():\n    result = mlflow.models.evaluate(\n        data=eval_dataset,\n        predictions="prediction",\n        targets="target",\n        model_type="classifier",\n    )\n\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"parameters",children:"Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"data"}),": DataFrame containing predictions and targets"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"predictions"}),": Column name containing model predictions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"targets"}),": Column name containing ground truth labels"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"model_type"}),": Task type (",(0,a.jsx)(n.code,{children:'"classifier"'})," or ",(0,a.jsx)(n.code,{children:'"regressor"'}),")"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"When evaluating classification models with probability scores, include a column with probabilities for metrics like ROC-AUC:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'eval_dataset = pd.DataFrame(\n    {\n        "prediction": predictions,\n        "prediction_proba": prediction_probabilities,  # For ROC-AUC\n        "target": y_test,\n    }\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"function-evaluation",children:"Function Evaluation"}),"\n",(0,a.jsx)(n.p,{children:"Evaluate Python functions directly without logging models to MLflow."}),"\n",(0,a.jsx)(n.h3,{id:"usage-1",children:"Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Generate sample data\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train a model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n\n# Define a prediction function\ndef predict_function(input_data):\n    return model.predict(input_data)\n\n\n# Create evaluation dataset\neval_data = pd.DataFrame(X_test)\neval_data["target"] = y_test\n\nwith mlflow.start_run():\n    result = mlflow.models.evaluate(\n        predict_function,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n    )\n\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"function-requirements",children:"Function Requirements"}),"\n",(0,a.jsx)(n.p,{children:"The function must:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Accept input data as its first parameter (DataFrame, numpy array, or compatible format)"}),"\n",(0,a.jsxs)(n.li,{children:["Return predictions in a format compatible with the specified ",(0,a.jsx)(n.code,{children:"model_type"})]}),"\n",(0,a.jsx)(n.li,{children:"Be callable without additional arguments beyond the input data"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For classification tasks, the function should return class predictions. For regression tasks, it should return continuous values."}),"\n",(0,a.jsx)(n.h2,{id:"custom-metrics--visualizations",children:"Custom Metrics & Visualizations"}),"\n",(0,a.jsx)(n.p,{children:"Define custom evaluation metrics and create specialized visualizations."}),"\n",(0,a.jsx)(n.h3,{id:"custom-metrics",children:"Custom Metrics"}),"\n",(0,a.jsxs)(n.admonition,{title:"Classic System Only",type:"note",children:[(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"make_metric"})," function is part of MLflow's classic evaluation system."]}),(0,a.jsxs)(n.p,{children:["For GenAI/LLM custom metrics, use the ",(0,a.jsx)(n.a,{href:"/genai/eval-monitor/scorers/custom",children:"@scorer decorator"})," instead."]})]}),"\n",(0,a.jsxs)(n.p,{children:["Create custom metrics with ",(0,a.jsx)(n.code,{children:"make_metric"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nfrom mlflow.models import make_metric\nfrom mlflow.metrics.base import MetricValue\n\n\n# Define custom metric\ndef custom_metric_fn(predictions, targets, metrics):\n    """Custom metric function."""\n    tp = np.sum((predictions == 1) & (targets == 1))\n    fp = np.sum((predictions == 1) & (targets == 0))\n\n    # Calculate custom value\n    custom_value = (tp * 100) - (fp * 20)\n\n    return MetricValue(\n        aggregate_results={\n            "custom_value": custom_value,\n            "value_per_prediction": custom_value / len(predictions),\n        },\n    )\n\n\n# Create metric\ncustom_metric = make_metric(\n    eval_fn=custom_metric_fn, greater_is_better=True, name="custom_metric"\n)\n\nwith mlflow.start_run():\n    result = mlflow.models.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n        extra_metrics=[custom_metric],\n    )\n\n    print(f"Custom Value: {result.metrics[\'custom_metric/custom_value\']:.2f}")\n'})}),"\n",(0,a.jsx)(n.p,{children:"Custom metric functions receive three parameters:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"predictions"}),": Model predictions (numpy array)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"targets"}),": Ground truth labels (numpy array)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"metrics"}),": Dictionary of built-in metrics already computed"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Return a ",(0,a.jsx)(n.code,{children:"MetricValue"})," object with ",(0,a.jsx)(n.code,{children:"aggregate_results"})," dict containing your custom metric values."]}),"\n",(0,a.jsx)(n.h3,{id:"custom-visualizations",children:"Custom Visualizations"}),"\n",(0,a.jsx)(n.p,{children:"Create custom visualization artifacts:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import matplotlib.pyplot as plt\nimport os\n\n\ndef create_custom_plot(eval_df, builtin_metrics, artifacts_dir):\n    """Create custom visualization."""\n    plt.figure(figsize=(10, 6))\n    plt.scatter(eval_df["prediction"], eval_df["target"], alpha=0.5)\n    plt.xlabel("Predictions")\n    plt.ylabel("Targets")\n    plt.title("Custom Prediction Analysis")\n\n    # Save plot\n    plot_path = os.path.join(artifacts_dir, "custom_plot.png")\n    plt.savefig(plot_path)\n    plt.close()\n\n    return {"custom_plot": plot_path}\n\n\n# Use custom artifact\nwith mlflow.start_run():\n    result = mlflow.models.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n        custom_artifacts=[create_custom_plot],\n    )\n'})}),"\n",(0,a.jsx)(n.p,{children:"Custom artifact functions receive three parameters:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"eval_df"}),": DataFrame with predictions, targets, and input features"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"builtin_metrics"}),": Dictionary of computed metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"artifacts_dir"}),": Directory path to save artifact files"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Return a dictionary mapping artifact names to file paths."}),"\n",(0,a.jsx)(n.h2,{id:"shap-integration",children:"SHAP Integration"}),"\n",(0,a.jsx)(n.p,{children:"MLflow's built-in SHAP integration provides automatic model explanations and feature importance analysis."}),"\n",(0,a.jsx)(n.h3,{id:"usage-2",children:"Usage"}),"\n",(0,a.jsxs)(n.p,{children:["Enable SHAP explanations by setting ",(0,a.jsx)(n.code,{children:"log_explainer: True"})," in the evaluator config:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom mlflow.models import infer_signature\n\n# Load dataset\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train model\nmodel = xgb.XGBClassifier().fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["label"] = y_test\n\nwith mlflow.start_run():\n    signature = infer_signature(X_test, model.predict(X_test))\n    model_info = mlflow.sklearn.log_model(model, name="model", signature=signature)\n\n    # Evaluate with SHAP enabled\n    result = mlflow.models.evaluate(\n        model_info.model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n        evaluator_config={"log_explainer": True},\n    )\n\n    # Check generated SHAP artifacts\n    for artifact_name in result.artifacts:\n        if "shap" in artifact_name.lower():\n            print(f"Generated: {artifact_name}")\n'})}),"\n",(0,a.jsx)(n.p,{children:"This generates feature importance plots, SHAP summary plots, and saves a SHAP explainer model."}),"\n",(0,a.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Control SHAP behavior with evaluator config options:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'result = mlflow.models.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    evaluator_config={\n        "log_explainer": True,\n        "explainer_type": "exact",\n        "max_error_examples": 100,\n        "log_model_explanations": True,\n    },\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Configuration Options:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"log_explainer"}),": Whether to save the SHAP explainer as a model (default: False)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"explainer_type"}),': SHAP algorithm type - "exact", "permutation", or "partition"']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_error_examples"}),": Number of misclassified examples to explain in detail"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"log_model_explanations"}),": Whether to log individual prediction explanations"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"using-saved-explainers",children:"Using Saved Explainers"}),"\n",(0,a.jsx)(n.p,{children:"Load and use saved SHAP explainers on new data:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Load the saved explainer\nexplainer_uri = f"runs:/{run_id}/explainer"\nexplainer = mlflow.pyfunc.load_model(explainer_uri)\n\n# Generate explanations for new data\nnew_data = X_test[:10]\nexplanations = explainer.predict(new_data)\n\n# explanations contains SHAP values for each feature and prediction\nprint(f"Explanations shape: {explanations.shape}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"plugin-evaluators",children:"Plugin Evaluators"}),"\n",(0,a.jsx)(n.p,{children:"MLflow's evaluation framework supports plugin evaluators that extend evaluation with specialized validation capabilities."}),"\n",(0,a.jsx)(n.h3,{id:"giskard-plugin",children:"Giskard Plugin"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/index.html",children:"Giskard"})," plugin scans models for vulnerabilities including performance bias, robustness issues, overconfidence, underconfidence, ethical bias, data leakage, stochasticity, and spurious correlations."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Examples:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-tabular-example.html",children:"Tabular ML Models"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-llm-example.html",children:"Text ML Models (LLMs)"})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Documentation:"})," ",(0,a.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/index.html",children:"Giskard-MLflow integration docs"})]}),"\n",(0,a.jsx)(n.h3,{id:"trubrics-plugin",children:"Trubrics Plugin"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://github.com/trubrics/trubrics-sdk",children:"Trubrics"})," plugin provides a validation framework with pre-built validation checks and support for custom Python validation functions."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Example:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/trubrics/trubrics-python/blob/v1.3.4/examples/mlflow/mlflow-trubrics.ipynb",children:"Official example notebook"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Documentation:"})," ",(0,a.jsx)(n.a,{href:"https://trubrics.github.io/trubrics-sdk/mlflow/",children:"Trubrics-MLflow integration docs"})," "]}),"\n",(0,a.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(m.B,{fn:"mlflow.models.evaluate"})," - Main evaluation API"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(m.B,{fn:"mlflow.validate_evaluation_results"})," - Validate evaluation results"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(m.B,{fn:"mlflow.models.make_metric"})," - Create custom metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(m.B,{fn:"mlflow.metrics.base.MetricValue"})," - Metric return value"]}),"\n"]})]})}function v(e={}){let{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}},54725(e,n,t){t.d(n,{B:()=>r});var l=t(74848);t(96540);var a=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),i=t(66497);function r({fn:e,children:n,hash:t}){let r=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(a[t])return t}return null})(e);if(!r)return(0,l.jsx)(l.Fragment,{children:n});let s=(0,i.default)(`/${a[r]}#${t??e}`);return(0,l.jsx)("a",{href:s,target:"_blank",children:n??(0,l.jsxs)("code",{children:[e,"()"]})})}},33508(e,n,t){t.d(n,{A:()=>a});var l=t(74848);t(96540);function a({features:e,col:n=2}){return(0,l.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${n}, 1fr)`},children:e.map((e,n)=>(0,l.jsxs)("div",{className:"highlightItem_XPnN",children:[e.icon&&(0,l.jsx)("div",{className:"highlightIcon_SUR8",children:(0,l.jsx)(e.icon,{size:24})}),(0,l.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,l.jsx)("h4",{children:e.title}),(0,l.jsx)("p",{children:e.description})]})]},n))})}}}]);