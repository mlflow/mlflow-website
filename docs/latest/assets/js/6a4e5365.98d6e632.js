"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3498],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var s=t(96540);const o={},r=s.createContext(o);function i(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(r.Provider,{value:n},e.children)}},97175:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"llms/deployments/guides/step2-query-deployments/index","title":"Querying Endpoints in the MLflow Deployment Server","description":"Now that the deployment server is operational, it\'s time to send it some data. You can interact with the","source":"@site/docs/llms/deployments/guides/step2-query-deployments/index.mdx","sourceDirName":"llms/deployments/guides/step2-query-deployments","slug":"/llms/deployments/guides/step2-query-deployments/","permalink":"/docs/latest/llms/deployments/guides/step2-query-deployments/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Configuring and Starting the Gateway Server","permalink":"/docs/latest/llms/deployments/guides/step1-create-deployments/"},"next":{"title":"Unity Catalog Integration","permalink":"/docs/latest/llms/deployments/uc_integration/"}}');var o=t(74848),r=t(28453);const i={},a="Querying Endpoints in the MLflow Deployment Server",l={},d=[{value:"Example 1: Completions",id:"example-1-completions",level:2},{value:"Example 2: Chat",id:"example-2-chat",level:2},{value:"Example 3: Embeddings",id:"example-3-embeddings",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"querying-endpoints-in-the-mlflow-deployment-server",children:"Querying Endpoints in the MLflow Deployment Server"})}),"\n",(0,o.jsx)(n.p,{children:"Now that the deployment server is operational, it's time to send it some data. You can interact with the\ngateway server using the deployments APIs or REST APIs. In this instance, we'll utilize the deployments APIs for simplicity."}),"\n",(0,o.jsx)(n.p,{children:"Let's elaborate on the three types of supported models:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Completions"}),': This type of model is used to generate predictions or suggestions based on the\ninput provided, helping to "complete" a sequence or pattern.']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Chat"}),": These models facilitate interactive conversations, capable of understanding and responding\nto user inputs in a conversational manner."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Embeddings"}),": Embedding models transform input data (like text or images) into a numerical vector\nspace, where similar items are positioned closely in the space, facilitating various machine learning tasks."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"In the following steps, we will explore how to query the gateway server using these model types."}),"\n",(0,o.jsx)(n.h2,{id:"example-1-completions",children:"Example 1: Completions"}),"\n",(0,o.jsx)(n.p,{children:"Completion models are designed to finish sentences or respond to prompts."}),"\n",(0,o.jsxs)(n.p,{children:["To query these models via the MLflow AI Gateway, you need to provide a ",(0,o.jsx)(n.code,{children:"prompt"})," parameter,\nwhich is the string the Language Model (LLM) will respond to. The gateway server also accommodates\nvarious other parameters. For detailed information, please refer to the documentation."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client("http://localhost:5000")\nname = "completions"\ndata = dict(\n    prompt="Name three potions or spells in harry potter that sound like an insult. Only show the names.",\n    n=2,\n    temperature=0.2,\n    max_tokens=1000,\n)\n\nresponse = client.predict(endpoint=name, inputs=data)\nprint(response)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-2-chat",children:"Example 2: Chat"}),"\n",(0,o.jsx)(n.p,{children:"Chat models facilitate interactive conversations with users, gradually accumulating context over time."}),"\n",(0,o.jsxs)(n.p,{children:["Creating a chat payload is slightly more complex compared to the other model types since it accommodates an\nunlimited number of messages from three distinct personas: ",(0,o.jsx)(n.code,{children:"system"}),", ",(0,o.jsx)(n.code,{children:"user"}),", and ",(0,o.jsx)(n.code,{children:"assistant"}),". To set up\na chat payload through the MLflow AI Gateway, you'll need to specify a ",(0,o.jsx)(n.code,{children:"messages"})," parameter. This parameter\ntakes a list of dictionaries formatted as follows:"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.code,{children:'{"role": "system/user/assistant", "content": "user-specified content"}'})}),"\n",(0,o.jsx)(n.p,{children:"For further details, please consult the documentation."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client("http://localhost:5000")\nname = "chat_3.5"\ndata = dict(\n    messages=[\n        {"role": "system", "content": "You are the sorting hat from harry potter."},\n        {\n            "role": "user",\n            "content": "I am brave, hard-working, wise, and backstabbing.",\n        },\n        {\n            "role": "user",\n            "content": "Which harry potter house am I most likely to belong to?",\n        },\n    ],\n    n=3,\n    temperature=0.5,\n)\n\nresponse = client.predict(endpoint=name, inputs=data)\nprint(response)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-3-embeddings",children:"Example 3: Embeddings"}),"\n",(0,o.jsx)(n.p,{children:"Embedding models transform tokens into numerical vectors."}),"\n",(0,o.jsxs)(n.p,{children:["To use embedding models via the MLflow AI Gateway, supply a ",(0,o.jsx)(n.code,{children:"text"})," parameter, which can be a\nstring or a list of strings. The gateway server then processes these strings and returns their\nrespective numerical vectors. Let's proceed with an example..."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client("http://localhost:5000")\nname = "embeddings"\ndata = dict(\n    input=[\n        "Gryffindor: Values bravery, courage, and leadership.",\n        "Hufflepuff: Known for loyalty, a strong work ethic, and a grounded nature.",\n        "Ravenclaw: A house for individuals who value wisdom, intellect, and curiosity.",\n        "Slytherin: Appreciates ambition, cunning, and resourcefulness.",\n    ],\n)\n\nresponse = client.predict(endpoint=name, inputs=data)\nprint(response)\n'})}),"\n",(0,o.jsx)(n.p,{children:"And there you have it! You've successfully set up your first gateway server and served three OpenAI models."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);