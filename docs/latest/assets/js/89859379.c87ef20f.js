"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2827],{65:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/models_from_code-3299c0ebefae1e6e936610a593c27c31.png"},804:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/artifact-view-ui-1a8cb0896e974d99fa85146d5ea83d05.png"},3203:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/model_evaluation_feature_importance-e3b91eb353333c150ec7b4e91f67f5f4.png"},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var o=t(96540);const l={},a=o.createContext(l);function i(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),o.createElement(a.Provider,{value:n},e.children)}},51830:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>m,default:()=>u,frontMatter:()=>c,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"model/index","title":"MLflow Models","description":"An MLflow Model is a standard format for packaging machine learning models that can be used in a","source":"@site/docs/model/index.mdx","sourceDirName":"model","slug":"/model/","permalink":"/docs/latest/model/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_label":"Overview","sidebar_position":0,"toc_max_heading_level":2},"sidebar":"docsSidebar","previous":{"title":"System Metrics","permalink":"/docs/latest/system-metrics/"},"next":{"title":"Model Signature","permalink":"/docs/latest/model/signatures/"}}');var l=t(74848),a=t(28453),i=t(61096),s=t(67756),r=t(56289),d=t(29030);const c={sidebar_label:"Overview",sidebar_position:0,toc_max_heading_level:2},m="MLflow Models",h={},p=[{value:"Storage Format",id:"storage-format",level:2},{value:"MLmodel file",id:"mlmodel-file",level:3},{value:"Additional Logged Files",id:"additional-logged-files",level:3},{value:"Environment variables file",id:"environment-variables-file",level:3},{value:"Managing Model Dependencies",id:"managing-model-dependencies",level:2},{value:"Model Signatures And Input Examples",id:"model-signatures-and-input-examples",level:2},{value:"Model API",id:"model-api",level:2},{value:"Models From Code",id:"models-from-code",level:2},{value:"Built-In Model Flavors",id:"models_built-in-model-flavors",level:2},{value:"Python Function (<code>python_function</code>)",id:"pyfunc-model-flavor",level:3},{value:"How To Save Model As Python Function",id:"how-to-save-model-as-python-function",level:4},{value:"How To Load And Score Python Function Models",id:"how-to-load-and-score-python-function-models",level:4},{value:"Loading Models",id:"loading-models",level:5},{value:"Scoring Models",id:"scoring-models",level:5},{value:"Demonstrating <code>predict_stream()</code>",id:"demonstrating-predict_stream",level:5},{value:"Python Function Model Interfaces",id:"python-function-model-interfaces",level:4},{value:"R Function (<code>crate</code>)",id:"r-function-crate",level:3},{value:"<code>crate</code> usage",id:"crate-usage",level:4},{value:"H<sub>2</sub>O (<code>h2o</code>)",id:"h2o-h2o",level:3},{value:"h2o pyfunc usage",id:"h2o-pyfunc-usage",level:4},{value:"Keras (<code>keras</code>)",id:"tf-keras-example",level:3},{value:"Keras pyfunc usage",id:"keras-pyfunc-usage",level:4},{value:"PyTorch (<code>pytorch</code>)",id:"pytorch-pytorch",level:3},{value:"PyTorch pyfunc usage",id:"pytorch-pyfunc-usage",level:4},{value:"Scikit-learn (<code>sklearn</code>)",id:"scikit-learn-sklearn",level:3},{value:"Scikit-learn pyfunc usage",id:"scikit-learn-pyfunc-usage",level:4},{value:"Spark MLlib (<code>spark</code>)",id:"spark-mllib-spark",level:3},{value:"Spark MLlib pyfunc usage",id:"spark-mllib-pyfunc-usage",level:4},{value:"TensorFlow (<code>tensorflow</code>)",id:"tensorflow-tensorflow",level:3},{value:"ONNX (<code>onnx</code>)",id:"onnx-onnx",level:3},{value:"ONNX pyfunc usage example",id:"onnx-pyfunc-usage-example",level:4},{value:"XGBoost (<code>xgboost</code>)",id:"xgboost-xgboost",level:3},{value:"<code>XGBoost</code> pyfunc usage",id:"xgboost-pyfunc-usage",level:4},{value:"LightGBM (<code>lightgbm</code>)",id:"lightgbm-lightgbm",level:3},{value:"<code>LightGBM</code> pyfunc usage",id:"lightgbm-pyfunc-usage",level:4},{value:"CatBoost (<code>catboost</code>)",id:"catboost-catboost",level:3},{value:"<code>CatBoost</code> pyfunc usage",id:"catboost-pyfunc-usage",level:4},{value:"Spacy(<code>spaCy</code>)",id:"spacyspacy",level:3},{value:"<code>Spacy</code> pyfunc usage",id:"spacy-pyfunc-usage",level:4},{value:"Statsmodels (<code>statsmodels</code>)",id:"statsmodels-statsmodels",level:3},{value:"Statsmodels pyfunc usage",id:"statsmodels-pyfunc-usage",level:4},{value:"Prophet (<code>prophet</code>)",id:"prophet-prophet",level:3},{value:"Prophet pyfunc usage",id:"prophet-pyfunc-usage",level:4},{value:"Pmdarima (<code>pmdarima</code>)",id:"pmdarima-flavor",level:3},{value:"OpenAI (<code>openai</code>) (Experimental)",id:"openai-openai-experimental",level:3},{value:"LangChain (<code>langchain</code>) (Experimental)",id:"langchain-langchain-experimental",level:3},{value:"John Snow Labs (<code>johnsnowlabs</code>)",id:"john-snow-labs-johnsnowlabs",level:3},{value:"To deploy the John Snow Labs model as a container",id:"to-deploy-the-john-snow-labs-model-as-a-container",level:4},{value:"To deploy the John Snow Labs model without a container",id:"to-deploy-the-john-snow-labs-model-without-a-container",level:4},{value:"Diviner (<code>diviner</code>)",id:"diviner-diviner",level:3},{value:"Diviner Types",id:"diviner-types",level:4},{value:"Metrics and Parameters logging for Diviner",id:"metrics-and-parameters-logging-for-diviner",level:4},{value:"Diviner pyfunc usage",id:"diviner-pyfunc-usage",level:4},{value:"Transformers (<code>transformers</code>)",id:"transformers-transformers",level:3},{value:"SentenceTransformers (<code>sentence_transformers</code>)",id:"sentencetransformers-sentence_transformers",level:3},{value:"Promptflow (<code>promptflow</code>)",id:"promptflow-promptflow",level:3},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Evaluating with LLMs",id:"model-evaluation-llms",level:3},{value:"Evaluating with Extra Metrics",id:"evaluating-with-extra-metrics",level:3},{value:"Evaluating with a Function",id:"evaluating-with-a-function",level:3},{value:"Evaluating with a Static Dataset",id:"evaluating-with-a-static-dataset",level:3},{value:"Performing Model Validation",id:"performing-model-validation",level:3},{value:"Validating a Model Against a Baseline Model",id:"validating-a-model-against-a-baseline-model",level:4},{value:"Validating a Model Against Static Thresholds",id:"validating-a-model-against-static-thresholds",level:4},{value:"Reusing Baseline Result For Multiple Validations",id:"reusing-baseline-result-for-multiple-validations",level:4},{value:"Model Validation with Giskard&#39;s plugin",id:"giskard_plugin",level:3},{value:"Model Validation with Trubrics&#39; plugin",id:"trubrics_plugin",level:3},{value:"Model Customization",id:"model-customization",level:2},{value:"Custom Python Models",id:"custom-python-models",level:3},{value:"Example: Creating a model with type hints",id:"example-creating-a-model-with-type-hints",level:4},{value:"Example: Creating a custom \u201cadd n\u201d model",id:"example-creating-a-custom-add-n-model",level:4},{value:"Example: Saving an XGBoost model in MLflow format",id:"example-saving-an-xgboost-model-in-mlflow-format",level:4},{value:"Example: Logging a transformers model with hf:/ schema to avoid copying large files",id:"example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files",level:4},{value:"Custom Flavors",id:"custom-flavors",level:3},{value:"Example: Creating a custom &quot;sktime&quot; flavor",id:"example-creating-a-custom-sktime-flavor",level:4},{value:"Example: Using the custom &quot;sktime&quot; flavor",id:"example-using-the-custom-sktime-flavor",level:4},{value:"Automatic Model Dependency Locking (Experimental)",id:"automatic-model-dependency-locking-experimental",level:2},{value:"How to Enable Dependency Locking",id:"how-to-enable-dependency-locking",level:3},{value:"Installing <code>uv</code>",id:"installing-uv",level:3},{value:"Validate Models before Deployment",id:"validate-models-before-deployment",level:2},{value:"Environment managers",id:"environment-managers",level:3},{value:"Built-In Deployment Tools",id:"built-in-deployment",level:2},{value:"Export a <code>python_function</code> model as an Apache Spark UDF",id:"export-a-python_function-model-as-an-apache-spark-udf",level:2},{value:"Deployment to Custom Targets",id:"deployment_plugin",level:2},{value:"Commands",id:"commands",level:3},{value:"Community Model Flavors",id:"community-model-flavors",level:2}];function f(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"mlflow-models",children:"MLflow Models"})}),"\n",(0,l.jsx)(n.p,{children:'An MLflow Model is a standard format for packaging machine learning models that can be used in a\nvariety of downstream tools---for example, real-time serving through a REST API or batch inference\non Apache Spark. The format defines a convention that lets you save a model in different "flavors"\nthat can be understood by different downstream tools.'}),"\n",(0,l.jsx)(n.h2,{id:"storage-format",children:"Storage Format"}),"\n",(0,l.jsxs)(n.p,{children:["Each MLflow Model is a directory containing arbitrary files, together with an ",(0,l.jsx)(n.code,{children:"MLmodel"}),"\nfile in the root of the directory that can define multiple ",(0,l.jsx)(n.em,{children:"flavors"})," that the model can be viewed\nin."]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.strong,{children:"model"})," aspect of the MLflow Model can either be a serialized object (e.g., a pickled ",(0,l.jsx)(n.code,{children:"scikit-learn"})," model)\nor a Python script (or notebook, if running in Databricks) that contains the model instance that has been defined\nwith the ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API."]}),"\n",(0,l.jsxs)(n.p,{children:['Flavors are the key concept that makes MLflow Models powerful: they are a convention that deployment\ntools can use to understand the model, which makes it possible to write tools that work with models\nfrom any ML library without having to integrate each tool with each library. MLflow defines\nseveral "standard" flavors that all of its built-in deployment tools support, such as a "Python\nfunction" flavor that describes how to run the model as a Python function. However, libraries can\nalso define and use other flavors. For example, MLflow\'s ',(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:(0,l.jsx)(n.code,{children:"mlflow.sklearn"})}),"\nlibrary allows loading models back as a scikit-learn ",(0,l.jsx)(n.code,{children:"Pipeline"})," object for use in code that is aware of\nscikit-learn, or as a generic Python function for use in tools that just need to apply the model\n(for example, the ",(0,l.jsx)(n.code,{children:"mlflow deployments"})," tool with the option ",(0,l.jsx)(n.code,{children:"-t sagemaker"})," for deploying models\nto Amazon SageMaker)."]}),"\n",(0,l.jsx)(n.h3,{id:"mlmodel-file",children:"MLmodel file"}),"\n",(0,l.jsxs)(n.p,{children:["All of the flavors that a particular model supports are defined in its ",(0,l.jsx)(n.code,{children:"MLmodel"})," file in YAML\nformat. For example, running ",(0,l.jsx)(n.code,{children:"python examples/sklearn_logistic_regression/train.py"})," from\n",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/sklearn_logistic_regression/train.py",children:"MLflow repo"}),"\nwill create the following files under the ",(0,l.jsx)(n.code,{children:"model"})," directory:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'# Directory written by mlflow.sklearn.save_model(model, "model", input_example=...)\nmodel/\n\u251c\u2500\u2500 MLmodel\n\u251c\u2500\u2500 model.pkl\n\u251c\u2500\u2500 conda.yaml\n\u251c\u2500\u2500 python_env.yaml\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 input_example.json (optional, only logged when input example is provided and valid during model logging)\n\u251c\u2500\u2500 serving_input_example.json (optional, only logged when input example is provided and valid during model logging)\n\u2514\u2500\u2500 environment_variables.txt (optional, only logged when environment variables are used during model inference)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["And its ",(0,l.jsx)(n.code,{children:"MLmodel"})," file describes two flavors:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:"time_created: 2018-05-25T17:28:53.35\n\nflavors:\n  sklearn:\n    sklearn_version: 0.19.1\n    pickled_model: model.pkl\n  python_function:\n    loader_module: mlflow.sklearn\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Apart from a ",(0,l.jsx)(n.strong,{children:"flavors"})," field listing the model flavors, the MLmodel YAML format can contain\nthe following fields:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"time_created"}),": Date and time when the model was created, in UTC ISO 8601 format."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"run_id"}),": ID of the run that created the model, if the model was saved using ",(0,l.jsx)(n.a,{href:"/tracking",children:"tracking"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"signature"}),": ",(0,l.jsx)(n.a,{href:"/model/signatures#model-signature",children:"model signature"})," in JSON format."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"input_example"}),": reference to an artifact with ",(0,l.jsx)(n.a,{href:"/model/signatures#input-example",children:"input example"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"databricks_runtime"}),": Databricks runtime version and type, if the model was trained in a Databricks notebook or job."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"mlflow_version"}),": The version of MLflow that was used to log the model."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"additional-logged-files",children:"Additional Logged Files"}),"\n",(0,l.jsxs)(n.p,{children:["For environment recreation, we automatically log ",(0,l.jsx)(n.code,{children:"conda.yaml"}),", ",(0,l.jsx)(n.code,{children:"python_env.yaml"}),", and ",(0,l.jsx)(n.code,{children:"requirements.txt"})," files whenever a model is logged.\nThese files can then be used to reinstall dependencies using ",(0,l.jsx)(n.code,{children:"conda"})," or ",(0,l.jsx)(n.code,{children:"virtualenv"})," with ",(0,l.jsx)(n.code,{children:"pip"}),". Please see\n",(0,l.jsx)(n.a,{href:"/model/dependencies#how-mlflow-records-dependencies",children:"How MLflow Model Records Dependencies"})," for more details about these files."]}),"\n",(0,l.jsxs)(n.p,{children:["If a model input example is provided when logging the model, two additional files ",(0,l.jsx)(n.code,{children:"input_example.json"})," and ",(0,l.jsx)(n.code,{children:"serving_input_example.json"})," are logged.\nSee ",(0,l.jsx)(n.a,{href:"/model/signatures#input-example",children:"Model Input Example"})," for more details."]}),"\n",(0,l.jsxs)(n.p,{children:["When logging a model, model metadata files (",(0,l.jsx)(n.code,{children:"MLmodel"}),", ",(0,l.jsx)(n.code,{children:"conda.yaml"}),", ",(0,l.jsx)(n.code,{children:"python_env.yaml"}),", ",(0,l.jsx)(n.code,{children:"requirements.txt"}),") are copied to a subdirectory named ",(0,l.jsx)(n.code,{children:"metadata"}),".\nFor wheeled models, ",(0,l.jsx)(n.code,{children:"original_requirements.txt"})," file is also copied."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When a model registered in the MLflow Model Registry is downloaded, a YAML file named\n",(0,l.jsx)(n.code,{children:"registered_model_meta"})," is added to the model directory on the downloader's side.\nThis file contains the name and version of the model referenced in the MLflow Model Registry,\nand will be used for deployment and other purposes."]})}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:["If you log a model within Databricks, MLflow also creates a ",(0,l.jsx)(n.code,{children:"metadata"})," subdirectory within\nthe model directory. This subdirectory contains the lightweight copy of aforementioned\nmetadata files for internal use."]})}),"\n",(0,l.jsx)(n.h3,{id:"environment-variables-file",children:"Environment variables file"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow records the environment variables that are used during model inference in ",(0,l.jsx)(n.code,{children:"environment_variables.txt"}),"\nfile when logging a model."]}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"environment_variables.txt"})," file ",(0,l.jsx)(n.strong,{children:"only contains names"})," of the environment variables that are used during\nmodel inference, ",(0,l.jsx)(n.strong,{children:"values are not stored"}),"."]})}),"\n",(0,l.jsx)(n.p,{children:"Currently MLflow only logs the environment variables whose name contains any of the following keywords:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'RECORD_ENV_VAR_ALLOWLIST = {\n    # api key related\n    "API_KEY",  # e.g. OPENAI_API_KEY\n    "API_TOKEN",\n    # databricks auth related\n    "DATABRICKS_HOST",\n    "DATABRICKS_USERNAME",\n    "DATABRICKS_PASSWORD",\n    "DATABRICKS_TOKEN",\n    "DATABRICKS_INSECURE",\n    "DATABRICKS_CLIENT_ID",\n    "DATABRICKS_CLIENT_SECRET",\n    "_DATABRICKS_WORKSPACE_HOST",\n    "_DATABRICKS_WORKSPACE_ID",\n}\n'})}),"\n",(0,l.jsx)(n.p,{children:"Example of a pyfunc model that uses environment variables:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nos.environ["TEST_API_KEY"] = "test_api_key"\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        if os.environ.get("TEST_API_KEY"):\n            return model_input\n        raise Exception("API key not found")\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="model", python_model=MyModel(), input_example="data"\n    )\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Environment variable ",(0,l.jsx)(n.code,{children:"TEST_API_KEY"})," is logged in the environment_variables.txt file like below"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"# This file records environment variable names that are used during model inference.\n# They might need to be set when creating a serving endpoint from this model.\n# Note: it is not guaranteed that all environment variables listed here are required\nTEST_API_KEY\n"})}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:["Before you deploy a model to a serving endpoint, ",(0,l.jsx)(n.strong,{children:"review the environment_variables.txt file"})," to ensure\nall necessary environment variables for model inference are set. Note that ",(0,l.jsx)(n.strong,{children:"not all environment variables\nlisted in the file are always required for model inference."})," For detailed instructions on setting\nenvironment variables on a databricks serving endpoint, refer to\n",(0,l.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/store-env-variable-model-serving.html#add-plain-text-environment-variables",children:"this guidance"}),"."]})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["To disable this feature, set the environment variable ",(0,l.jsx)(n.code,{children:"MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING"})," to ",(0,l.jsx)(n.code,{children:"false"}),"."]})}),"\n",(0,l.jsx)(n.h2,{id:"managing-model-dependencies",children:"Managing Model Dependencies"}),"\n",(0,l.jsxs)(n.p,{children:["An MLflow Model infers dependencies required for the model flavor and automatically logs them. However, it also allows\nyou to define extra dependencies or custom Python code, and offer a tool to validate them in a sandbox environment.\nPlease refer to ",(0,l.jsx)(n.a,{href:"/model/dependencies",children:"Managing Dependencies in MLflow Models"})," for more details."]}),"\n",(0,l.jsx)(n.h2,{id:"model-signatures-and-input-examples",children:"Model Signatures And Input Examples"}),"\n",(0,l.jsx)(n.p,{children:"In MLflow, understanding the intricacies of model signatures and input examples is crucial for\neffective model management and deployment."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Signature"}),": Defines the schema for model inputs, outputs, and additional inference parameters,\npromoting a standardized interface for model interaction."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Input Example"}),": Provides a concrete instance of valid model input, aiding in understanding and\ntesting model requirements. Additionally, if an input example is provided when logging a model, a model\nsignature will be automatically inferred and stored if not explicitly provided."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model Serving Payload Example"}),": Provides a json payload example for querying a deployed model endpoint.\nIf an input example is provided when logging a model, a serving payload example is automatically generated\nfrom the input example and saved as ",(0,l.jsx)(n.code,{children:"serving_input_example.json"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Our documentation delves into several key areas:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Supported Signature Types"}),": We cover the different data types that are supported, such as tabular data\nfor traditional machine learning models and tensors for deep learning models."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Signature Enforcement"}),": Discusses how MLflow enforces schema compliance, ensuring that the provided\ninputs match the model's expectations."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Logging Models with Signatures"}),": Guides on how to incorporate signatures when logging models, enhancing\nclarity and reliability in model operations."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["For a detailed exploration of these concepts, including examples and best practices, visit the\n",(0,l.jsx)(n.a,{href:"/model/signatures",children:"Model Signatures and Examples Guide"}),". If you would like to see signature enforcement in action,\nsee the ",(0,l.jsx)(n.a,{href:"/model/notebooks/signature_examples",children:"notebook tutorial on Model Signatures"})," to learn more."]}),"\n",(0,l.jsx)(n.h2,{id:"model-api",children:"Model API"}),"\n",(0,l.jsxs)(n.p,{children:["You can save and load MLflow Models in multiple ways. First, MLflow includes integrations with\nseveral common libraries. For example, ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:"mlflow.sklearn"}),"\ncontains ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.save_model",children:"save_model"}),", ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.log_model",children:"log_model"}),",\nand ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.load_model",children:"load_model"})," functions for scikit-learn models. Second,\nyou can use the ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model",children:"mlflow.models.Model"})," class to create and write models. This\nclass has four key functions:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.add_flavor",children:"add_flavor"})," to add a flavor to the model. Each flavor\nhas a string name and a dictionary of key-value attributes, where the values can be any object\nthat can be serialized to YAML."]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.save",children:"save"})," to save the model to a local directory."]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.log",children:"log"})," to log the model as an artifact in the\ncurrent run using MLflow Tracking."]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.load",children:"load"})," to load a model from a local directory or\nfrom an artifact in a previous run."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"models-from-code",children:"Models From Code"}),"\n",(0,l.jsxs)(n.p,{children:["To ",(0,l.jsx)(n.strong,{children:"learn more about the Models From Code feature"}),", please visit ",(0,l.jsx)(n.a,{href:"/model/models-from-code",children:"the deep dive guide"}),"\nfor more in-depth explanation and to see additional examples."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"The Models from Code feature is available in MLflow versions 2.12.2 and later. This feature is experimental and may change in future releases."})}),"\n",(0,l.jsxs)(n.p,{children:["The Models from Code feature allows you to define and log models directly from a stand-alone python script. This feature is particularly useful when you want to\nlog models that can be effectively stored as a code representation (models that do not need optimized weights through training) or applications\nthat rely on external services (e.g., LangChain chains). Another benefit is that this approach entirely bypasses the use of the ",(0,l.jsx)(n.code,{children:"pickle"})," or\n",(0,l.jsx)(n.code,{children:"cloudpickle"})," modules within Python, which can carry security risks when loading untrusted models."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["This feature is only supported for ",(0,l.jsx)(n.strong,{children:"LangChain"}),", ",(0,l.jsx)(n.strong,{children:"LlamaIndex"}),", and ",(0,l.jsx)(n.strong,{children:"PythonModel"})," models."]})}),"\n",(0,l.jsxs)(n.p,{children:["In order to log a model from code, you can leverage the ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API. This API allows you to define a model by specifying\nan instance of the model class directly within the file where the model is defined. When logging such a model, a\nfile path is specified (instead of an object) that points to the Python file containing both the model class definition and the usage of the\n",(0,l.jsx)(n.code,{children:"set_model"})," API applied on an instance of your custom model."]}),"\n",(0,l.jsx)(n.p,{children:"The figure below provides a comparison of the standard model logging process and the Models from Code feature for models that are eligible to be\nsaved using the Models from Code feature:"}),"\n",(0,l.jsx)("div",{class:"center-div",style:{width:"60%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"Models from Code",src:t(65).A+"",width:"1821",height:"1299"})})}),"\n",(0,l.jsxs)(n.p,{children:["For example, defining a model in a separate file named ",(0,l.jsx)(n.code,{children:"my_model.py"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\nfrom mlflow.models import set_model\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\n# Define the custom PythonModel instance that will be used for inference\nset_model(MyModel())\n"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["The Models from code feature does not support capturing import statements that are from external file references. If you have dependencies that\nare not captured via a ",(0,l.jsx)(n.code,{children:"pip"})," install, dependencies will need to be included and resolved via appropriate absolute path import references from\nusing the ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/model/dependencies.html#saving-extra-code-with-an-mlflow-model-manual-declaration",children:"code_paths feature"}),".\nFor simplicity's sake, it is recommended to encapsulate all of your required local dependencies for a model defined from code within the same\npython script file due to limitations around ",(0,l.jsx)(n.code,{children:"code_paths"})," dependency pathing resolution."]})}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:["When defining a model from code and using the ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API, the code that is defined in the script that is being logged\nwill be executed internally to ensure that it is valid code. If you have connections to external services within your script (e.g. you are connecting\nto a GenAI service within LangChain), be aware that you will incur a connection request to that service when the model is being logged."]})}),"\n",(0,l.jsx)(n.p,{children:"Then, logging the model from the file path in a different python script:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmodel_path = "my_model.py"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=model_path,  # Define the model as the path to the Python file\n        name="my_model",\n    )\n\n# Loading the model behaves exactly as if an instance of MyModel had been logged\nmy_model = mlflow.pyfunc.load_model(model_info.model_uri)\n'})}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.models.set_model"})," API is ",(0,l.jsx)(n.strong,{children:"not threadsafe"}),". Do not attempt to use this feature if you are logging models concurrently\nfrom multiple threads. This fluent API utilizes a global active model state that has no consistency guarantees. If you are interested in threadsafe\nlogging APIs, please use the ",(0,l.jsx)(s.B,{fn:"mlflow.client.MlflowClient",children:"mlflow.client.MlflowClient"})," APIs for logging models."]})}),"\n",(0,l.jsx)(n.h2,{id:"models_built-in-model-flavors",children:"Built-In Model Flavors"}),"\n",(0,l.jsx)(n.p,{children:"MLflow provides several standard flavors that might be useful in your applications. Specifically,\nmany of its deployment tools support these flavors, so you can export your own model in one of these\nflavors to benefit from all these tools:"}),"\n",(0,l.jsx)(i.A,{toc:p.slice(p.findIndex((e=>"models_built-in-model-flavors"===e.id))+1,p.findIndex((e=>"model-evaluation"===e.id)))}),"\n",(0,l.jsxs)(n.h3,{id:"pyfunc-model-flavor",children:["Python Function (",(0,l.jsx)(n.code,{children:"python_function"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"python_function"})," model flavor serves as a default model interface for MLflow Python models.\nAny MLflow Python model is expected to be loadable as a ",(0,l.jsx)(n.code,{children:"python_function"})," model. This enables\nother MLflow tools to work with any python model regardless of which persistence module or\nframework was used to produce the model. This interoperability is very powerful because it allows\nany Python model to be productionized in a variety of environments."]}),"\n",(0,l.jsxs)(n.p,{children:["In addition, the ",(0,l.jsx)(n.code,{children:"python_function"})," model flavor defines a generic\nfilesystem ",(0,l.jsx)(r.A,{to:"/api_reference/python_api/mlflow.pyfunc.html#pyfunc-filesystem-format",target:"_blank",children:"model format"}),"\nfor Python models and provides utilities for saving and loading models\nto and from this format. The format is self-contained in the sense that it includes all the\ninformation necessary to load and use a model. Dependencies are stored either directly with the\nmodel or referenced via conda environment. This model format allows other tools to integrate\ntheir models with MLflow."]}),"\n",(0,l.jsx)(n.h4,{id:"how-to-save-model-as-python-function",children:"How To Save Model As Python Function"}),"\n",(0,l.jsxs)(n.p,{children:["Most ",(0,l.jsx)(n.code,{children:"python_function"})," models are saved as part of other model flavors - for example, all mlflow\nbuilt-in flavors include the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor in the exported models. In addition,\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:"mlflow.pyfunc"})," module defines functions for creating ",(0,l.jsx)(n.code,{children:"python_function"})," models explicitly.\nThis module also includes utilities for creating custom Python models, which is a convenient way of\nadding custom python code to ML models. For more information, see the ",(0,l.jsx)(n.a,{href:"#custom-python-models",children:"custom Python models\ndocumentation"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["For information on how to store a custom model from a python script (models from code functionality),\nsee the ",(0,l.jsx)(n.a,{href:"/model/models-from-code",children:"guide to models from code"})," for the recommended approaches."]}),"\n",(0,l.jsx)(n.h4,{id:"how-to-load-and-score-python-function-models",children:"How To Load And Score Python Function Models"}),"\n",(0,l.jsx)(n.h5,{id:"loading-models",children:"Loading Models"}),"\n",(0,l.jsxs)(n.p,{children:["You can load ",(0,l.jsx)(n.code,{children:"python_function"})," models in Python by using the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"})," function. It is important\nto note that ",(0,l.jsx)(n.code,{children:"load_model"})," assumes all dependencies are already available and ",(0,l.jsx)(n.em,{children:"will not"})," perform any checks or installations\nof dependencies. For deployment options that handle dependencies, refer to the ",(0,l.jsx)(n.a,{href:"#built-in-deployment",children:"model deployment section"}),"."]}),"\n",(0,l.jsx)(n.h5,{id:"scoring-models",children:"Scoring Models"}),"\n",(0,l.jsx)(n.p,{children:"Once a model is loaded, it can be scored in two primary ways:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Synchronous Scoring"}),"\nThe standard method for scoring is using the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.PyFuncModel.predict",children:(0,l.jsx)(n.code,{children:"predict"})})," method, which supports various\ninput types and returns a scalar or collection based on the input data. The method signature is:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"predict(data: Union[pandas.Series, pandas.DataFrame, numpy.ndarray, csc_matrix, csr_matrix, List[Any], Dict[str, Any], str],\n        params: Optional[Dict[str, Any]] = None) \u2192 Union[pandas.Series, pandas.DataFrame, numpy.ndarray, list, str]\n"})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Synchronous Streaming Scoring"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"predict_stream"})," is a new interface that was added to MLflow in the 2.12.2 release. Previous versions of MLflow will not support this interface.\nIn order to utilize ",(0,l.jsx)(n.code,{children:"predict_stream"})," in a custom Python Function Model, you must implement the ",(0,l.jsx)(n.code,{children:"predict_stream"})," method in your model class and\nreturn a generator type."]})}),"\n",(0,l.jsxs)(n.p,{children:["For models that support streaming data processing, ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.PyFuncModel.predict_stream",children:"predict_stream"}),"\nmethod is available. This method returns a",(0,l.jsx)(n.code,{children:"generator"}),", which yields a stream of responses, allowing for efficient processing of\nlarge datasets or continuous data streams. Note that the ",(0,l.jsx)(n.code,{children:"predict_stream"})," method is not available for all model types.\nThe usage involves iterating over the generator to consume the responses:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"predict_stream(data: Any, params: Optional[Dict[str, Any]] = None) \u2192 GeneratorType\n"})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h5,{id:"demonstrating-predict_stream",children:["Demonstrating ",(0,l.jsx)(n.code,{children:"predict_stream()"})]}),"\n",(0,l.jsxs)(n.p,{children:["Below is an example demonstrating how to define, save, load, and use a streamable model with the ",(0,l.jsx)(n.code,{children:"predict_stream()"})," method:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\n\n# Define a custom model that supports streaming\nclass StreamableModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        # Regular predict method implementation (optional for this demo)\n        return "regular-predict-output"\n\n    def predict_stream(self, context, model_input, params=None):\n        # Yielding elements one at a time\n        for element in ["a", "b", "c", "d", "e"]:\n            yield element\n\n\n# Save the model to a directory\ntmp_path = "/tmp/test_model"\npyfunc_model_path = os.path.join(tmp_path, "pyfunc_model")\npython_model = StreamableModel()\nmlflow.pyfunc.save_model(path=pyfunc_model_path, python_model=python_model)\n\n# Load the model\nloaded_pyfunc_model = mlflow.pyfunc.load_model(model_uri=pyfunc_model_path)\n\n# Use predict_stream to get a generator\nstream_output = loaded_pyfunc_model.predict_stream("single-input")\n\n# Consuming the generator using next\nprint(next(stream_output))  # Output: \'a\'\nprint(next(stream_output))  # Output: \'b\'\n\n# Alternatively, consuming the generator using a for-loop\nfor response in stream_output:\n    print(response)  # This will print \'c\', \'d\', \'e\'\n'})}),"\n",(0,l.jsx)(n.h4,{id:"python-function-model-interfaces",children:"Python Function Model Interfaces"}),"\n",(0,l.jsxs)(n.p,{children:["All PyFunc models will support ",(0,l.jsx)(n.code,{children:"pandas.DataFrame"})," as an input. In addition to ",(0,l.jsx)(n.code,{children:"pandas.DataFrame"}),",\nDL PyFunc models will also support tensor inputs in the form of ",(0,l.jsx)(n.code,{children:"numpy.ndarrays"}),". To verify\nwhether a model flavor supports tensor inputs, please check the flavor's documentation."]}),"\n",(0,l.jsxs)(n.p,{children:["For models with a column-based schema, inputs are typically provided in the form of a ",(0,l.jsx)(n.code,{children:"pandas.DataFrame"}),".\nIf a dictionary mapping column name to values is provided as input for schemas with named columns or if a\npython ",(0,l.jsx)(n.code,{children:"List"})," or a ",(0,l.jsx)(n.code,{children:"numpy.ndarray"})," is provided as input for schemas with unnamed columns, MLflow will cast the\ninput to a DataFrame. Schema enforcement and casting with respect to the expected data types is performed against\nthe DataFrame."]}),"\n",(0,l.jsxs)(n.p,{children:["For models with a tensor-based schema, inputs are typically provided in the form of a ",(0,l.jsx)(n.code,{children:"numpy.ndarray"})," or a\ndictionary mapping the tensor name to its np.ndarray value. Schema enforcement will check the provided input's\nshape and type against the shape and type specified in the model's schema and throw an error if they do not match."]}),"\n",(0,l.jsx)(n.p,{children:"For models where no schema is defined, no changes to the model inputs and outputs are made. MLflow will\npropagate any errors raised by the model if the model does not accept the provided input type."}),"\n",(0,l.jsxs)(n.p,{children:["The python environment that a PyFunc model is loaded into for prediction or inference may differ from the environment\nin which it was trained. In the case of an environment mismatch, a warning message will be printed when\ncalling ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),". This warning statement will identify the packages that have a version mismatch\nbetween those used during training and the current environment. In order to get the full dependencies of the\nenvironment in which the model was trained, you can call ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.get_model_dependencies"}),".\nFurthermore, if you want to run model inference in the same environment used in model training, you can\ncall ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"})," with the ",(0,l.jsx)(n.code,{children:"env_manager"}),' argument set as "conda". This will generate the environment\nfrom the ',(0,l.jsx)(n.code,{children:"conda.yaml"})," file, ensuring that the python UDF will execute with the exact package versions that were used\nduring training."]}),"\n",(0,l.jsx)(n.p,{children:"Some PyFunc models may accept model load configuration, which controls how the model is loaded and predictions\ncomputed. You can learn which configuration the model supports by inspecting the model's flavor metadata:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"model_info = mlflow.models.get_model_info(model_uri)\nmodel_info.flavors[mlflow.pyfunc.FLAVOR_NAME][mlflow.pyfunc.MODEL_CONFIG]\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Alternatively, you can load the PyFunc model and inspect the ",(0,l.jsx)(n.code,{children:"model_config"})," property:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"pyfunc_model = mlflow.pyfunc.load_model(model_uri)\npyfunc_model.model_config\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Model configuration can be changed at loading time by indicating ",(0,l.jsx)(n.code,{children:"model_config"})," parameter in\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"})," method:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"pyfunc_model = mlflow.pyfunc.load_model(model_uri, model_config=dict(temperature=0.93))\n"})}),"\n",(0,l.jsx)(n.p,{children:"When a model configuration value is changed, those values the configuration the model was saved with. Indicating an\ninvalid model configuration key for a model results in that configuration being ignored. A warning is displayed mentioning\nthe ignored entries."}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Model configuration vs parameters with default values in signatures:"})," Use model configuration when you need to provide\nmodel publishers for a way to change how the model is loaded into memory and how predictions are computed for all the\nsamples. For instance, a key like ",(0,l.jsx)(n.code,{children:"user_gpu"}),". Model consumers are not able to change those values at predict time. Use\nparameters with default values in the signature to provide a users the ability to change how predictions are computed on\neach data sample."]})}),"\n",(0,l.jsxs)(n.h3,{id:"r-function-crate",children:["R Function (",(0,l.jsx)(n.code,{children:"crate"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"crate"})," model flavor defines a generic model format for representing an arbitrary R prediction\nfunction as an MLflow model using the ",(0,l.jsx)(n.code,{children:"crate"})," function from the\n",(0,l.jsx)(n.a,{href:"https://github.com/r-lib/carrier",children:"carrier"})," package. The prediction function is expected to take a dataframe as input and\nproduce a dataframe, a vector or a list with the predictions as output."]}),"\n",(0,l.jsx)(n.p,{children:"This flavor requires R to be installed in order to be used."}),"\n",(0,l.jsxs)(n.h4,{id:"crate-usage",children:[(0,l.jsx)(n.code,{children:"crate"})," usage"]}),"\n",(0,l.jsx)(n.p,{children:"For a minimal crate model, an example configuration for the predict function is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-r",children:'library(mlflow)\nlibrary(carrier)\n# Load iris dataset\ndata("iris")\n\n# Learn simple linear regression model\nmodel <- lm(Sepal.Width~Sepal.Length, data = iris)\n\n# Define a crate model\n# call package functions with an explicit :: namespace.\ncrate_model <- crate(\n  function(new_obs)  stats::predict(model, data.frame("Sepal.Length" = new_obs)),\n  model = model\n)\n\n# log the model\nmodel_path <- mlflow_log_model(model = crate_model, artifact_path = "iris_prediction")\n\n# load the logged model and make a prediction\nmodel_uri <- paste0(mlflow_get_run()$artifact_uri, "/iris_prediction")\nmlflow_model <- mlflow_load_model(model_uri = model_uri,\n                                  flavor = NULL,\n                                  client = mlflow_client())\n\nprediction <- mlflow_predict(model = mlflow_model, data = 5)\nprint(prediction)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"h2o-h2o",children:["H",(0,l.jsx)("sub",{children:"2"}),"O (",(0,l.jsx)(n.code,{children:"h2o"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"h2o"})," model flavor enables logging and loading H2O models."]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.h2o",children:"mlflow.h2o"})," module defines ",(0,l.jsx)(s.B,{fn:"mlflow.h2o.save_model",children:"save_model()"}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.h2o.log_model",children:"log_model()"})," methods in python,\nand ",(0,l.jsx)(r.A,{to:(0,d.Ay)("/api_reference/R-api.html#mlflow-save-model-h2o"),target:"_blank",children:"mlflow_save_model"}),"\nand ",(0,l.jsx)(r.A,{to:(0,d.Ay)("/api_reference/R-api.html#mlflow-log-model"),target:"_blank",children:"mlflow_log_model"})," in R for saving H2O\nmodels in MLflow Model format. These methods produce MLflow Models with the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor, allowing you to load them\nas generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can be scored with only DataFrame input. When you load\nMLflow Models with the ",(0,l.jsx)(n.code,{children:"h2o"})," flavor using ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),",\nthe ",(0,l.jsx)(n.a,{href:"http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.init",children:"h2o.init()"})," method is\ncalled. Therefore, the correct version of ",(0,l.jsx)(n.code,{children:"h2o(-py)"})," must be installed in the loader's\nenvironment. You can customize the arguments given to\n",(0,l.jsx)(n.a,{href:"http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.init",children:"h2o.init()"})," by modifying the\n",(0,l.jsx)(n.code,{children:"init"})," entry of the persisted H2O model's YAML configuration file: ",(0,l.jsx)(n.code,{children:"model.h2o/h2o.yaml"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["Finally, you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.h2o.load_model"})," method to load MLflow Models with the\n",(0,l.jsx)(n.code,{children:"h2o"})," flavor as H2O model objects."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.h2o",children:"mlflow.h2o"}),"."]}),"\n",(0,l.jsx)(n.h4,{id:"h2o-pyfunc-usage",children:"h2o pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"For a minimal h2o model, here is an example of the pyfunc predict() method in a classification scenario:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport h2o\n\nh2o.init()\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n# import the prostate data\ndf = h2o.import_file(\n    "http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv.zip"\n)\n\n# convert the columns to factors\ndf["CAPSULE"] = df["CAPSULE"].asfactor()\ndf["RACE"] = df["RACE"].asfactor()\ndf["DCAPS"] = df["DCAPS"].asfactor()\ndf["DPROS"] = df["DPROS"].asfactor()\n\n# split the data\ntrain, test, valid = df.split_frame(ratios=[0.7, 0.15])\n\n# generate a GLM model\nglm_classifier = H2OGeneralizedLinearEstimator(\n    family="binomial", lambda_=0, alpha=0.5, nfolds=5, compute_p_values=True\n)\n\nwith mlflow.start_run():\n    glm_classifier.train(\n        y="CAPSULE", x=["AGE", "RACE", "VOL", "GLEASON"], training_frame=train\n    )\n    metrics = glm_classifier.model_performance()\n    metrics_to_track = ["MSE", "RMSE", "r2", "logloss"]\n    metrics_to_log = {\n        key: value\n        for key, value in metrics._metric_json.items()\n        if key in metrics_to_track\n    }\n    params = glm_classifier.params\n    mlflow.log_params(params)\n    mlflow.log_metrics(metrics_to_log)\n    model_info = mlflow.h2o.log_model(glm_classifier, name="h2o_model_info")\n\n# load h2o model and make a prediction\nh2o_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\ntest_df = test.as_data_frame()\npredictions = h2o_pyfunc.predict(test_df)\nprint(predictions)\n\n# it is also possible to load the model and predict using h2o methods on the h2o frame\n\n# h2o_model = mlflow.h2o.load_model(model_info.model_uri)\n# predictions = h2o_model.predict(test)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"tf-keras-example",children:["Keras (",(0,l.jsx)(n.code,{children:"keras"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"keras"})," model flavor enables logging and loading Keras models. It is available in both Python\nand R clients. In R, you can save or log the model using\n",(0,l.jsx)(n.code,{children:"mlflow_save_model"})," and ",(0,l.jsx)(n.code,{children:"mlflow_log_model"}),".\nThese functions serialize Keras models as HDF5 files using the Keras library's built-in\nmodel persistence functions. You can use\n",(0,l.jsx)(n.code,{children:"mlflow_load_model"})," function in R to load MLflow Models\nwith the ",(0,l.jsx)(n.code,{children:"keras"})," flavor as ",(0,l.jsx)(n.a,{href:"https://keras.io/models/about-keras-models/",children:"Keras Model objects"}),"."]}),"\n",(0,l.jsx)(n.h4,{id:"keras-pyfunc-usage",children:"Keras pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"For a minimal Sequential model, an example configuration for the pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nimport pathlib\nimport shutil\nfrom tensorflow import keras\n\nmlflow.tensorflow.autolog()\n\nX = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\ny = np.array([0, 0, 1, 1, 1, 0])\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=(1,)),\n        keras.layers.Dense(1, activation="sigmoid"),\n    ]\n)\nmodel.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])\nmodel.fit(X, y, batch_size=3, epochs=5, validation_split=0.2)\n\nlocal_artifact_dir = "/tmp/mlflow/keras_model"\npathlib.Path(local_artifact_dir).mkdir(parents=True, exist_ok=True)\n\nmodel_uri = f"runs:/{mlflow.last_active_run().info.run_id}/model"\nkeras_pyfunc = mlflow.pyfunc.load_model(\n    model_uri=model_uri, dst_path=local_artifact_dir\n)\n\ndata = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1)\npredictions = keras_pyfunc.predict(data)\n\nshutil.rmtree(local_artifact_dir)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"pytorch-pytorch",children:["PyTorch (",(0,l.jsx)(n.code,{children:"pytorch"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"pytorch"})," model flavor enables logging and loading PyTorch models."]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch",children:"mlflow.pytorch"})," module defines utilities for saving and\nloading MLflow Models with the ",(0,l.jsx)(n.code,{children:"pytorch"})," flavor. You can use the ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch.save_model"}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch.log_model"})," methods to save PyTorch models in MLflow format; both of these\nfunctions use the ",(0,l.jsx)(n.a,{href:"https://pytorch.org/docs/stable/torch.html#torch.save",children:"torch.save()"})," method to\nserialize PyTorch models. Additionally, you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch.load_model"}),"\nmethod to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"pytorch"})," flavor as PyTorch model objects. This loaded\nPyFunc model can be scored with both DataFrame input and numpy array input. Finally, models\nproduced by ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch.log_model"})," contain\nthe ",(0,l.jsx)(n.code,{children:"python_function"})," flavor, allowing you to load them as generic Python functions for inference\nvia ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),"."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When using the PyTorch flavor, if a GPU is available at prediction time, the default GPU will be used to run\ninference. To disable this behavior, users can use\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.environment_variables.MLFLOW_DEFAULT_PREDICTION_DEVICE",children:"MLFLOW_DEFAULT_PREDICTION_DEVICE"}),"\nor pass in a device with the ",(0,l.jsx)(n.code,{children:"device"})," parameter for the ",(0,l.jsx)(n.code,{children:"predict"})," function."]})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"In case of multi gpu training, ensure to save the model only with global rank 0 gpu. This avoids\nlogging multiple copies of the same model."})}),"\n",(0,l.jsx)(n.h4,{id:"pytorch-pyfunc-usage",children:"PyTorch pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"For a minimal PyTorch model, an example configuration for the pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport mlflow\nfrom mlflow.models import infer_signature\nimport torch\nfrom torch import nn\n\n\nnet = nn.Linear(6, 1)\nloss_function = nn.L1Loss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n\nX = torch.randn(6)\ny = torch.randn(1)\n\nepochs = 5\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(X)\n\n    loss = loss_function(outputs, y)\n    loss.backward()\n\n    optimizer.step()\n\nwith mlflow.start_run() as run:\n    signature = infer_signature(X.numpy(), net(X).detach().numpy())\n    model_info = mlflow.pytorch.log_model(net, name="model", signature=signature)\n\npytorch_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\npredictions = pytorch_pyfunc.predict(torch.randn(6).numpy())\nprint(predictions)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.pytorch",children:"mlflow.pytorch"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"scikit-learn-sklearn",children:["Scikit-learn (",(0,l.jsx)(n.code,{children:"sklearn"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"sklearn"})," model flavor provides an easy-to-use interface for saving and loading scikit-learn\nmodels. The ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:"mlflow.sklearn"})," module defines ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.save_model",children:(0,l.jsx)(n.code,{children:"save_model()"})}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.log_model",children:(0,l.jsx)(n.code,{children:"log_model()"})})," functions that save scikit-learn models in\nMLflow format, using either Python's pickle module (Pickle) or CloudPickle for model serialization.\nThese functions produce MLflow Models with the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor, allowing them to\nbe loaded as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with DataFrame input. Finally, you can use\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"sklearn"})," flavor as\nscikit-learn model objects."]}),"\n",(0,l.jsx)(n.h4,{id:"scikit-learn-pyfunc-usage",children:"Scikit-learn pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"For a Scikit-learn LogisticRegression model, an example configuration for the pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import infer_signature\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nwith mlflow.start_run():\n    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n    y = np.array([0, 0, 1, 1, 1, 0])\n    lr = LogisticRegression()\n    lr.fit(X, y)\n    signature = infer_signature(X, lr.predict(X))\n\n    model_info = mlflow.sklearn.log_model(\n        sk_model=lr, name="model", signature=signature\n    )\n\nsklearn_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\ndata = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1)\n\npredictions = sklearn_pyfunc.predict(data)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn",children:"mlflow.sklearn"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"spark-mllib-spark",children:["Spark MLlib (",(0,l.jsx)(n.code,{children:"spark"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"spark"})," model flavor enables exporting Spark MLlib models as MLflow Models."]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.spark"})," module defines"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(s.B,{fn:"mlflow.spark.save_model",children:(0,l.jsx)(n.code,{children:"save_model()"})})," to save a Spark MLlib model to a DBFS path."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(s.B,{fn:"mlflow.spark.log_model",children:(0,l.jsx)(n.code,{children:"log_model()"})})," to upload a Spark MLlib model to the tracking server."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(s.B,{fn:"mlflow.spark.load_model",children:(0,l.jsx)(n.code,{children:"load_model()"})})," to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"spark"})," flavor as Spark MLlib pipelines."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["MLflow Models produced by these functions contain the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor,\nallowing you to load them as generic Python functions via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with DataFrame input.\nWhen a model with the ",(0,l.jsx)(n.code,{children:"spark"})," flavor is loaded as a Python function via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),"\n, a new ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext",children:"SparkContext"}),"\nis created for model inference; additionally, the function converts all Pandas DataFrame inputs to\nSpark DataFrames before scoring. While this initialization overhead and format translation latency\nis not ideal for high-performance use cases, it enables you to easily deploy any\n",(0,l.jsx)(n.a,{href:"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=pipelinemodel#pyspark.ml.Pipeline",children:"MLlib PipelineModel"})," to any production environment supported by MLflow\n(SageMaker, AzureML, etc)."]}),"\n",(0,l.jsx)(n.h4,{id:"spark-mllib-pyfunc-usage",children:"Spark MLlib pyfunc usage"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql import SparkSession\nimport mlflow\n\n# Prepare training data from a list of (label, features) tuples.\nspark = SparkSession.builder.appName("LogisticRegressionExample").getOrCreate()\ntraining = spark.createDataFrame(\n    [\n        (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n        (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n        (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n        (1.0, Vectors.dense([0.0, 1.2, -0.5])),\n    ],\n    ["label", "features"],\n)\n\n# Create and fit a LogisticRegression instance\nlr = LogisticRegression(maxIter=10, regParam=0.01)\nlr_model = lr.fit(training)\n\n# Serialize the Model\nwith mlflow.start_run():\n    model_info = mlflow.spark.log_model(lr_model, artifact_path="spark-model")\n\n# Load saved model\nlr_model_saved = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Make predictions on test data.\n# The DataFrame used in the predict method must be a Pandas DataFrame\ntest = spark.createDataFrame(\n    [\n        (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n        (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n        (1.0, Vectors.dense([0.0, 2.2, -1.5])),\n    ],\n    ["label", "features"],\n).toPandas()\n\nprediction = lr_model_saved.predict(test)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"tensorflow-tensorflow",children:["TensorFlow (",(0,l.jsx)(n.code,{children:"tensorflow"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The simple example below shows how to log params and metrics in mlflow for a custom training loop\nusing low-level TensorFlow API. See ",(0,l.jsx)(n.a,{href:"#tf-keras-example",children:"tf-keras-example"}),". for an example of mlflow and ",(0,l.jsx)(n.code,{children:"tf.keras"})," models."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport tensorflow as tf\n\nimport mlflow\n\nx = np.linspace(-4, 4, num=512)\ny = 3 * x + 10\n\n# estimate w and b where y = w * x + b\nlearning_rate = 0.1\nx_train = tf.Variable(x, trainable=False, dtype=tf.float32)\ny_train = tf.Variable(y, trainable=False, dtype=tf.float32)\n\n# initial values\nw = tf.Variable(1.0)\nb = tf.Variable(1.0)\n\nwith mlflow.start_run():\n    mlflow.log_param("learning_rate", learning_rate)\n\n    for i in range(1000):\n        with tf.GradientTape(persistent=True) as tape:\n            # calculate MSE = 0.5 * (y_predict - y_train)^2\n            y_predict = w * x_train + b\n            loss = 0.5 * tf.reduce_mean(tf.square(y_predict - y_train))\n            mlflow.log_metric("loss", value=loss.numpy(), step=i)\n\n        # Update the trainable variables\n        # w = w - learning_rate * gradient of loss function w.r.t. w\n        # b = b - learning_rate * gradient of loss function w.r.t. b\n        w.assign_sub(learning_rate * tape.gradient(loss, w))\n        b.assign_sub(learning_rate * tape.gradient(loss, b))\n\nprint(f"W = {w.numpy():.2f}, b = {b.numpy():.2f}")\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"onnx-onnx",children:["ONNX (",(0,l.jsx)(n.code,{children:"onnx"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"onnx"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"http://onnx.ai/",children:"ONNX models"})," in MLflow format via\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.log_model"})," methods. These\nmethods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can be scored with both DataFrame input and numpy array input. The ",(0,l.jsx)(n.code,{children:"python_function"}),"\nrepresentation of an MLflow ONNX model uses the ",(0,l.jsx)(n.a,{href:"https://github.com/microsoft/onnxruntime",children:"ONNX Runtime execution engine"}),"\nfor evaluation. Finally, you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.load_model"})," method to load MLflow\nModels with the ",(0,l.jsx)(n.code,{children:"onnx"})," flavor in native ONNX format."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.onnx",children:(0,l.jsx)(n.code,{children:"mlflow.onnx"})})," and ",(0,l.jsx)(n.a,{href:"http://onnx.ai/",children:"http://onnx.ai/"}),"."]}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["The default behavior for saving ONNX files is to use the ONNX save option ",(0,l.jsx)(n.code,{children:"save_as_external_data=True"}),"\nin order to support model files that are ",(0,l.jsx)(n.strong,{children:"in excess of 2GB"}),". For edge deployments of small model files, this\nmay create issues. If you need to save a small model as a single file for such deployment considerations,\nyou can set the parameter ",(0,l.jsx)(n.code,{children:"save_as_external_data=False"})," in either ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.save_model"}),"\nor ",(0,l.jsx)(s.B,{fn:"mlflow.onnx.log_model"})," to force the serialization of the model as a small file. Note that if the\nmodel is in excess of 2GB, ",(0,l.jsx)(n.strong,{children:"saving as a single file will not work"}),"."]})}),"\n",(0,l.jsx)(n.h4,{id:"onnx-pyfunc-usage-example",children:"ONNX pyfunc usage example"}),"\n",(0,l.jsx)(n.p,{children:"For an ONNX model, an example configuration that uses pytorch to train a dummy model,\nconverts it to ONNX, logs to mlflow and makes a prediction using pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport mlflow\nfrom mlflow.models import infer_signature\nimport onnx\nimport torch\nfrom torch import nn\n\n# define a torch model\nnet = nn.Linear(6, 1)\nloss_function = nn.L1Loss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n\nX = torch.randn(6)\ny = torch.randn(1)\n\n# run model training\nepochs = 5\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(X)\n\n    loss = loss_function(outputs, y)\n    loss.backward()\n\n    optimizer.step()\n\n# convert model to ONNX and load it\ntorch.onnx.export(net, X, "model.onnx")\nonnx_model = onnx.load_model("model.onnx")\n\n# log the model into a mlflow run\nwith mlflow.start_run():\n    signature = infer_signature(X.numpy(), net(X).detach().numpy())\n    model_info = mlflow.onnx.log_model(onnx_model, name="model", signature=signature)\n\n# load the logged model and make a prediction\nonnx_pyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\n\npredictions = onnx_pyfunc.predict(X.numpy())\nprint(predictions)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"xgboost-xgboost",children:["XGBoost (",(0,l.jsx)(n.code,{children:"xgboost"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"xgboost"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster",children:"XGBoost models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.xgboost.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.xgboost.log_model"}),"\nmethods in python and ",(0,l.jsx)(r.A,{to:(0,d.Ay)("/api_reference/R-api.html#mlflow-save-model-crate"),target:"_blank",children:(0,l.jsx)(n.code,{children:"mlflow_save_model"})}),"\nand ",(0,l.jsx)(r.A,{to:(0,d.Ay)("/api_reference/R-api.html#mlflow-log-model"),target:"_blank",children:(0,l.jsx)(n.code,{children:"mlflow_log_model"})})," in R respectively.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with DataFrame input. You can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.xgboost.load_model"}),"\nmethod to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"xgboost"})," model flavor in native XGBoost format."]}),"\n",(0,l.jsxs)(n.p,{children:["Note that the ",(0,l.jsx)(n.code,{children:"xgboost"})," model flavor only supports an instance of ",(0,l.jsx)(n.a,{href:"https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster",children:"xgboost.Booster"}),",\nnot models that implement the ",(0,l.jsx)(n.a,{href:"https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn",children:"scikit-learn API"}),"."]}),"\n",(0,l.jsxs)(n.h4,{id:"xgboost-pyfunc-usage",children:[(0,l.jsx)(n.code,{children:"XGBoost"})," pyfunc usage"]}),"\n",(0,l.jsx)(n.p,{children:"The example below"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Loads the IRIS dataset from ",(0,l.jsx)(n.code,{children:"scikit-learn"})]}),"\n",(0,l.jsx)(n.li,{children:"Trains an XGBoost Classifier"}),"\n",(0,l.jsxs)(n.li,{children:["Logs the model and params using ",(0,l.jsx)(n.code,{children:"mlflow"})]}),"\n",(0,l.jsx)(n.li,{children:"Loads the logged model and makes predictions"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nimport mlflow\nfrom mlflow.models import infer_signature\n\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    data["data"], data["target"], test_size=0.2\n)\n\nxgb_classifier = XGBClassifier(\n    n_estimators=10,\n    max_depth=3,\n    learning_rate=1,\n    objective="binary:logistic",\n    random_state=123,\n)\n\n# log fitted model and XGBClassifier parameters\nwith mlflow.start_run():\n    xgb_classifier.fit(X_train, y_train)\n    clf_params = xgb_classifier.get_xgb_params()\n    mlflow.log_params(clf_params)\n    signature = infer_signature(X_train, xgb_classifier.predict(X_train))\n    model_info = mlflow.xgboost.log_model(\n        xgb_classifier, name="iris-classifier", signature=signature\n    )\n\n# Load saved model and make predictions\nxgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri)\ny_pred = xgb_classifier_saved.predict(X_test)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.xgboost",children:"mlflow.xgboost"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"lightgbm-lightgbm",children:["LightGBM (",(0,l.jsx)(n.code,{children:"lightgbm"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"lightgbm"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm-booster",children:"LightGBM models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"lightgbm"}),"\nmodel flavor in native LightGBM format."]}),"\n",(0,l.jsxs)(n.p,{children:["Note that the scikit-learn API for LightGBM is now supported. For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.lightgbm",children:(0,l.jsx)(n.code,{children:"mlflow.lightgbm"})}),"."]}),"\n",(0,l.jsxs)(n.h4,{id:"lightgbm-pyfunc-usage",children:[(0,l.jsx)(n.code,{children:"LightGBM"})," pyfunc usage"]}),"\n",(0,l.jsx)(n.p,{children:"The example below"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Loads the IRIS dataset from ",(0,l.jsx)(n.code,{children:"scikit-learn"})]}),"\n",(0,l.jsxs)(n.li,{children:["Trains a LightGBM ",(0,l.jsx)(n.code,{children:"LGBMClassifier"})]}),"\n",(0,l.jsxs)(n.li,{children:["Logs the model and feature importance's using ",(0,l.jsx)(n.code,{children:"mlflow"})]}),"\n",(0,l.jsx)(n.li,{children:"Loads the logged model and makes predictions"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from lightgbm import LGBMClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport mlflow\nfrom mlflow.models import infer_signature\n\ndata = load_iris()\n\n# Remove special characters from feature names to be able to use them as keys for mlflow metrics\nfeature_names = [\n    name.replace(" ", "_").replace("(", "").replace(")", "")\n    for name in data["feature_names"]\n]\nX_train, X_test, y_train, y_test = train_test_split(\n    data["data"], data["target"], test_size=0.2\n)\n# create model instance\nlgb_classifier = LGBMClassifier(\n    n_estimators=10,\n    max_depth=3,\n    learning_rate=1,\n    objective="binary:logistic",\n    random_state=123,\n)\n\n# Fit and save model and LGBMClassifier feature importances as mlflow metrics\nwith mlflow.start_run():\n    lgb_classifier.fit(X_train, y_train)\n    feature_importances = dict(zip(feature_names, lgb_classifier.feature_importances_))\n    feature_importance_metrics = {\n        f"feature_importance_{feature_name}": imp_value\n        for feature_name, imp_value in feature_importances.items()\n    }\n    mlflow.log_metrics(feature_importance_metrics)\n    signature = infer_signature(X_train, lgb_classifier.predict(X_train))\n    model_info = mlflow.lightgbm.log_model(\n        lgb_classifier, name="iris-classifier", signature=signature\n    )\n\n# Load saved model and make predictions\nlgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri)\ny_pred = lgb_classifier_saved.predict(X_test)\nprint(y_pred)\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"catboost-catboost",children:["CatBoost (",(0,l.jsx)(n.code,{children:"catboost"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"catboost"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://catboost.ai/docs/concepts/python-reference_catboost.html",children:"CatBoost models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.catboost.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.catboost.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.catboost.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"catboost"}),"\nmodel flavor in native CatBoost format."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.catboost",children:(0,l.jsx)(n.code,{children:"mlflow.catboost"})}),"."]}),"\n",(0,l.jsxs)(n.h4,{id:"catboost-pyfunc-usage",children:[(0,l.jsx)(n.code,{children:"CatBoost"})," pyfunc usage"]}),"\n",(0,l.jsx)(n.p,{children:"For a CatBoost Classifier model, an example configuration for the pyfunc predict() method is:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import infer_signature\nfrom catboost import CatBoostClassifier\nfrom sklearn import datasets\n\n# prepare data\nX, y = datasets.load_wine(as_frame=False, return_X_y=True)\n\n# train the model\nmodel = CatBoostClassifier(\n    iterations=5,\n    loss_function="MultiClass",\n    allow_writing_files=False,\n)\nmodel.fit(X, y)\n\n# create model signature\npredictions = model.predict(X)\nsignature = infer_signature(X, predictions)\n\n# log the model into a mlflow run\nwith mlflow.start_run():\n    model_info = mlflow.catboost.log_model(model, name="model", signature=signature)\n\n# load the logged model and make a prediction\ncatboost_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\nprint(catboost_pyfunc.predict(X[:5]))\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"spacyspacy",children:["Spacy(",(0,l.jsx)(n.code,{children:"spaCy"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"spaCy"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://spacy.io/models",children:"spaCy models"})," in MLflow format via\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.spacy.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.spacy.log_model"})," methods. Additionally, these\nmethods add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the models to be\ninterpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with DataFrame input. You can\nalso use the ",(0,l.jsx)(s.B,{fn:"mlflow.spacy.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"spacy"})," model flavor\nin native spaCy format."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.spacy",children:(0,l.jsx)(n.code,{children:"mlflow.spacy"})}),"."]}),"\n",(0,l.jsxs)(n.h4,{id:"spacy-pyfunc-usage",children:[(0,l.jsx)(n.code,{children:"Spacy"})," pyfunc usage"]}),"\n",(0,l.jsxs)(n.p,{children:["The example below shows how to train a ",(0,l.jsx)(n.code,{children:"Spacy"})," ",(0,l.jsx)(n.code,{children:"TextCategorizer"})," model, log the model artifact and metrics to the\nmlflow tracking server and then load the saved model to make predictions. For this example, we will be using the\n",(0,l.jsx)(n.code,{children:"Polarity 2.0"})," dataset available in the ",(0,l.jsx)(n.code,{children:"nltk"})," package. This dataset consists of 10000 positive and 10000 negative\nshort movie reviews."]}),"\n",(0,l.jsxs)(n.p,{children:['First we convert the texts and sentiment labels ("pos" or "neg") from NLTK native format to ',(0,l.jsx)(n.code,{children:"Spacy"}),"'s ",(0,l.jsx)(n.code,{children:"DocBin"})," format:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import pandas as pd\nimport spacy\nfrom nltk.corpus import movie_reviews\nfrom spacy import Language\nfrom spacy.tokens import DocBin\n\nnltk.download("movie_reviews")\n\n\ndef get_sentences(sentiment_type: str) -> pd.DataFrame:\n    """Reconstruct the sentences from the word lists for each review record for a specific ``sentiment_type``\n    as a pandas DataFrame with two columns: \'sentence\' and \'sentiment\'.\n    """\n    file_ids = movie_reviews.fileids(sentiment_type)\n    sent_df = []\n    for file_id in file_ids:\n        sentence = " ".join(movie_reviews.words(file_id))\n        sent_df.append({"sentence": sentence, "sentiment": sentiment_type})\n    return pd.DataFrame(sent_df)\n\n\ndef convert(data_df: pd.DataFrame, target_file: str):\n    """Convert a DataFrame with \'sentence\' and \'sentiment\' columns to a\n    spacy DocBin object and save it to \'target_file\'.\n    """\n    nlp = spacy.blank("en")\n    sentiment_labels = data_df.sentiment.unique()\n    spacy_doc = DocBin()\n\n    for _, row in data_df.iterrows():\n        sent_tokens = nlp.make_doc(row["sentence"])\n        # To train a Spacy TextCategorizer model, the label must be attached to the "cats" dictionary of the "Doc"\n        # object, e.g. {"pos": 1.0, "neg": 0.0} for a "pos" label.\n        for label in sentiment_labels:\n            sent_tokens.cats[label] = 1.0 if label == row["sentiment"] else 0.0\n        spacy_doc.add(sent_tokens)\n\n    spacy_doc.to_disk(target_file)\n\n\n# Build a single DataFrame with both positive and negative reviews, one row per review\nreview_data = [get_sentences(sentiment_type) for sentiment_type in ("pos", "neg")]\nreview_data = pd.concat(review_data, axis=0)\n\n# Split the DataFrame into a train and a dev set\ntrain_df = review_data.groupby("sentiment", group_keys=False).apply(\n    lambda x: x.sample(frac=0.7, random_state=100)\n)\ndev_df = review_data.loc[review_data.index.difference(train_df.index), :]\n\n# Save the train and dev data files to the current directory as "corpora.train" and "corpora.dev", respectively\nconvert(train_df, "corpora.train")\nconvert(dev_df, "corpora.dev")\n'})}),"\n",(0,l.jsxs)(n.p,{children:["To set up the training job, we first need to generate a configuration file as described in the\n",(0,l.jsx)(n.a,{href:"https://spacy.io/usage/training#config",children:"Spacy Documentation"})," For simplicity, we will only use\na ",(0,l.jsx)(n.code,{children:"TextCategorizer"})," in the pipeline."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"python -m spacy init config --pipeline textcat --lang en mlflow-textcat.cfg\n"})}),"\n",(0,l.jsx)(n.p,{children:"Change the default train and dev paths in the config file to the current directory:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-diff",children:'  [paths]\n- train = null\n- dev = null\n+ train = "."\n+ dev = "."\n'})}),"\n",(0,l.jsxs)(n.p,{children:["In ",(0,l.jsx)(n.code,{children:"Spacy"}),', the training loop is defined internally in Spacy\'s code. Spacy provides a "logging" extension point where\nwe can use ',(0,l.jsx)(n.code,{children:"mlflow"}),". To do this,"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["We have to define a function to write metrics / model input to ",(0,l.jsx)(n.code,{children:"mlfow"})]}),"\n",(0,l.jsxs)(n.li,{children:["Register it as a logger in ",(0,l.jsx)(n.code,{children:"Spacy"}),"'s component registry"]}),"\n",(0,l.jsxs)(n.li,{children:["Change the default console logger in the ",(0,l.jsx)(n.code,{children:"Spacy"}),"'s configuration file (",(0,l.jsx)(n.code,{children:"mlflow-textcat.cfg"}),")"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from typing import IO, Callable, Tuple, Dict, Any, Optional\nimport spacy\nfrom spacy import Language\nimport mlflow\n\n\n@spacy.registry.loggers("mlflow_logger.v1")\ndef mlflow_logger():\n    """Returns a function, ``setup_logger`` that returns two functions:\n\n    * ``log_step`` is called internally by Spacy for every evaluation step. We can log the intermediate train and\n    validation scores to the mlflow tracking server here.\n    * ``finalize``: is called internally by Spacy after training is complete. We can log the model artifact to the\n    mlflow tracking server here.\n    """\n\n    def setup_logger(\n        nlp: Language,\n        stdout: IO = sys.stdout,\n        stderr: IO = sys.stderr,\n    ) -> Tuple[Callable, Callable]:\n        def log_step(info: Optional[Dict[str, Any]]):\n            if info:\n                step = info["step"]\n                score = info["score"]\n                metrics = {}\n\n                for pipe_name in nlp.pipe_names:\n                    loss = info["losses"][pipe_name]\n                    metrics[f"{pipe_name}_loss"] = loss\n                    metrics[f"{pipe_name}_score"] = score\n                mlflow.log_metrics(metrics, step=step)\n\n        def finalize():\n            uri = mlflow.spacy.log_model(nlp, name="mlflow_textcat_example")\n            mlflow.end_run()\n\n        return log_step, finalize\n\n    return setup_logger\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Check the ",(0,l.jsx)(n.a,{href:"https://pypi.org/project/spacy-loggers/",children:"spacy-loggers library"})," for a more complete implementation."]}),"\n",(0,l.jsxs)(n.p,{children:["Point to our mlflow logger in ",(0,l.jsx)(n.code,{children:"Spacy"})," configuration file. For this example, we will lower the number of training steps\nand eval frequency:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-diff",children:'  [training.logger]\n- @loggers = "spacy.ConsoleLogger.v1"\n- dev = null\n+ @loggers = "mlflow_logger.v1"\n\n  [training]\n- max_steps = 20000\n- eval_frequency = 100\n+ max_steps = 100\n+ eval_frequency = 10\n'})}),"\n",(0,l.jsx)(n.p,{children:"Train our model:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from spacy.cli.train import train as spacy_train\n\nspacy_train("mlflow-textcat.cfg")\n'})}),"\n",(0,l.jsx)(n.p,{children:"To make predictions, we load the saved model from the last run:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow import MlflowClient\n\n# look up the last run info from mlflow\nclient = MlflowClient()\nlast_run = client.search_runs(experiment_ids=["0"], max_results=1)[0]\n\n# We need to append the spacy model directory name to the artifact uri\nspacy_model = mlflow.pyfunc.load_model(\n    f"{last_run.info.artifact_uri}/mlflow_textcat_example"\n)\npredictions_in = dev_df.loc[:, ["sentence"]]\npredictions_out = spacy_model.predict(predictions_in).squeeze().tolist()\npredicted_labels = [\n    "pos" if row["pos"] > row["neg"] else "neg" for row in predictions_out\n]\nprint(dev_df.assign(predicted_sentiment=predicted_labels))\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"statsmodels-statsmodels",children:["Statsmodels (",(0,l.jsx)(n.code,{children:"statsmodels"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"statsmodels"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://www.statsmodels.org/stable/api.html",children:"Statsmodels models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels.save_model"}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels.log_model"})," methods. These methods also add the ",(0,l.jsx)(n.code,{children:"python_function"}),"\nflavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python\nfunctions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),". This loaded PyFunc model can only\nbe scored with DataFrame input. You can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels.load_model"}),"\nmethod to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"statsmodels"})," model flavor in native statsmodels format."]}),"\n",(0,l.jsxs)(n.p,{children:["As for now, automatic logging is restricted to parameters, metrics and models generated by a call to ",(0,l.jsx)(n.code,{children:"fit"}),"\non a ",(0,l.jsx)(n.code,{children:"statsmodels"})," model."]}),"\n",(0,l.jsx)(n.h4,{id:"statsmodels-pyfunc-usage",children:"Statsmodels pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"The following 2 examples illustrate usage of a basic regression model (OLS) and an ARIMA time series model\nfrom the following statsmodels apis : statsmodels.formula.api and statsmodels.tsa.api"}),"\n",(0,l.jsx)(n.p,{children:"For a minimal statsmodels regression model, here is an example of the pyfunc predict() method:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nimport statsmodels.formula.api as smf\n\n# load the diabetes dataset from sklearn\ndiabetes = load_diabetes()\n\n# create X and y dataframes for the features and target\nX = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(data=diabetes.target, columns=["target"])\n\n# concatenate X and y dataframes\ndf = pd.concat([X, y], axis=1)\n\n# create the linear regression model (ordinary least squares)\nmodel = smf.ols(\n    formula="target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6", data=df\n)\n\nmlflow.statsmodels.autolog(\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=False,\n    silent=False,\n    registered_model_name=None,\n)\n\nwith mlflow.start_run():\n    res = model.fit(method="pinv", use_t=True)\n    model_info = mlflow.statsmodels.log_model(res, name="OLS_model")\n\n# load the pyfunc model\nstatsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# generate predictions\npredictions = statsmodels_pyfunc.predict(X)\nprint(predictions)\n'})}),"\n",(0,l.jsx)(n.p,{children:"For a minimal time series ARIMA model, here is an example of the pyfunc predict() method :"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# create a time series dataset with seasonality\nnp.random.seed(0)\n\n# generate a time index with a daily frequency\ndates = pd.date_range(start="2022-12-01", end="2023-12-01", freq="D")\n\n# generate the seasonal component (weekly)\nseasonality = np.sin(np.arange(len(dates)) * (2 * np.pi / 365.25) * 7)\n\n# generate the trend component\ntrend = np.linspace(-5, 5, len(dates)) + 2 * np.sin(\n    np.arange(len(dates)) * (2 * np.pi / 365.25) * 0.1\n)\n\n# generate the residual component\nresiduals = np.random.normal(0, 1, len(dates))\n\n# generate the final time series by adding the components\ntime_series = seasonality + trend + residuals\n\n# create a dataframe from the time series\ndata = pd.DataFrame({"date": dates, "value": time_series})\ndata.set_index("date", inplace=True)\n\norder = (1, 0, 0)\n# create the ARIMA model\nmodel = ARIMA(data, order=order)\n\nmlflow.statsmodels.autolog(\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=False,\n    silent=False,\n    registered_model_name=None,\n)\n\nwith mlflow.start_run():\n    res = model.fit()\n    mlflow.log_params(\n        {\n            "order": order,\n            "trend": model.trend,\n            "seasonal_order": model.seasonal_order,\n        }\n    )\n    mlflow.log_params(res.params)\n    mlflow.log_metric("aic", res.aic)\n    mlflow.log_metric("bic", res.bic)\n    model_info = mlflow.statsmodels.log_model(res, name="ARIMA_model")\n\n# load the pyfunc model\nstatsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# prediction dataframes for a TimeSeriesModel must have exactly one row and include columns called start and end\nstart = pd.to_datetime("2024-01-01")\nend = pd.to_datetime("2024-01-07")\n\n# generate predictions\nprediction_data = pd.DataFrame({"start": start, "end": end}, index=[0])\npredictions = statsmodels_pyfunc.predict(prediction_data)\nprint(predictions)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.statsmodels",children:(0,l.jsx)(n.code,{children:"mlflow.statsmodels"})}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"prophet-prophet",children:["Prophet (",(0,l.jsx)(n.code,{children:"prophet"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"prophet"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://facebook.github.io/prophet/",children:"Prophet models"})," in MLflow format via\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.prophet.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.prophet.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodels to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with DataFrame input. You can also use\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.prophet.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"prophet"})," model\nflavor in native prophet format."]}),"\n",(0,l.jsx)(n.h4,{id:"prophet-pyfunc-usage",children:"Prophet pyfunc usage"}),"\n",(0,l.jsx)(n.p,{children:"This example uses a time series dataset from Prophet's GitHub repository, containing log number of daily views to\nPeyton Manning\u2019s Wikipedia page for several years. A sample of the dataset is as follows:"}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"ds"}),(0,l.jsx)("th",{children:"y"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2007-12-10"}),(0,l.jsx)("td",{children:"9.59076113897809"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2007-12-11"}),(0,l.jsx)("td",{children:"8.51959031601596"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2007-12-12"}),(0,l.jsx)("td",{children:"8.18367658262066"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2007-12-13"}),(0,l.jsx)("td",{children:"8.07246736935477"})]})]})]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport pandas as pd\nfrom prophet import Prophet, serialize\nfrom prophet.diagnostics import cross_validation, performance_metrics\n\nimport mlflow\nfrom mlflow.models import infer_signature\n\n# URL to the dataset\nSOURCE_DATA = "https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv"\n\nnp.random.seed(12345)\n\n\ndef extract_params(pr_model):\n    params = {attr: getattr(pr_model, attr) for attr in serialize.SIMPLE_ATTRIBUTES}\n    return {k: v for k, v in params.items() if isinstance(v, (int, float, str, bool))}\n\n\n# Load the training data\ntrain_df = pd.read_csv(SOURCE_DATA)\n\n# Create a "test" DataFrame with the "ds" column containing 10 days after the end date in train_df\ntest_dates = pd.date_range(start="2016-01-21", end="2016-01-31", freq="D")\ntest_df = pd.DataFrame({"ds": test_dates})\n\n# Initialize Prophet model with specific parameters\nprophet_model = Prophet(changepoint_prior_scale=0.5, uncertainty_samples=7)\n\nwith mlflow.start_run():\n    # Fit the model on the training data\n    prophet_model.fit(train_df)\n\n    # Extract and log model parameters\n    params = extract_params(prophet_model)\n    mlflow.log_params(params)\n\n    # Perform cross-validation\n    cv_results = cross_validation(\n        prophet_model,\n        initial="900 days",\n        period="30 days",\n        horizon="30 days",\n        parallel="threads",\n        disable_tqdm=True,\n    )\n\n    # Calculate and log performance metrics\n    cv_metrics = performance_metrics(cv_results, metrics=["mse", "rmse", "mape"])\n    average_metrics = cv_metrics.drop(columns=["horizon"]).mean(axis=0).to_dict()\n    mlflow.log_metrics(average_metrics)\n\n    # Generate predictions and infer model signature\n    train = prophet_model.history\n\n    # Log the Prophet model with MLflow\n    model_info = mlflow.prophet.log_model(\n        prophet_model,\n        name="prophet_model",\n        input_example=train[["ds"]].head(10),\n    )\n\n# Load the saved model as a pyfunc\nprophet_model_saved = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Generate predictions for the test set\npredictions = prophet_model_saved.predict(test_df)\n\n# Truncate and display the forecast if needed\nforecast = predictions[["ds", "yhat"]]\n\nprint(f"forecast:\\n{forecast.head(5)}")\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Output (",(0,l.jsx)(n.code,{children:"Pandas DataFrame"}),"):"]}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Index"}),(0,l.jsx)("th",{children:"ds"}),(0,l.jsx)("th",{children:"yhat"}),(0,l.jsx)("th",{children:"yhat_upper"}),(0,l.jsx)("th",{children:"yhat_lower"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"0"}),(0,l.jsx)("td",{children:"2016-01-21"}),(0,l.jsx)("td",{children:"8.526513"}),(0,l.jsx)("td",{children:"8.827397"}),(0,l.jsx)("td",{children:"8.328563"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"1"}),(0,l.jsx)("td",{children:"2016-01-22"}),(0,l.jsx)("td",{children:"8.541355"}),(0,l.jsx)("td",{children:"9.434994"}),(0,l.jsx)("td",{children:"8.112758"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2"}),(0,l.jsx)("td",{children:"2016-01-23"}),(0,l.jsx)("td",{children:"8.308332"}),(0,l.jsx)("td",{children:"8.633746"}),(0,l.jsx)("td",{children:"8.201323"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"3"}),(0,l.jsx)("td",{children:"2016-01-24"}),(0,l.jsx)("td",{children:"8.676326"}),(0,l.jsx)("td",{children:"9.534593"}),(0,l.jsx)("td",{children:"8.020874"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"4"}),(0,l.jsx)("td",{children:"2016-01-25"}),(0,l.jsx)("td",{children:"8.983457"}),(0,l.jsx)("td",{children:"9.430136"}),(0,l.jsx)("td",{children:"8.121798"})]})]})]}),"\n",(0,l.jsxs)(n.p,{children:["For more information, see ",(0,l.jsx)(s.B,{fn:"mlflow.prophet",children:(0,l.jsx)(n.code,{children:"mlflow.prophet"})}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"pmdarima-flavor",children:["Pmdarima (",(0,l.jsx)(n.code,{children:"pmdarima"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"pmdarima"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"http://alkaline-ml.com/pmdarima/",children:"pmdarima models"})," in MLflow\nformat via the ",(0,l.jsx)(s.B,{fn:"mlflow.pmdarima.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.pmdarima.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the\nmodel to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with a DataFrame input.\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.pmdarima.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"pmdarima"}),"\nmodel flavor in native pmdarima formats."]}),"\n",(0,l.jsxs)(n.p,{children:["The interface for utilizing a ",(0,l.jsx)(n.code,{children:"pmdarima"})," model loaded as a ",(0,l.jsx)(n.code,{children:"pyfunc"})," type for generating forecast predictions uses\na ",(0,l.jsx)(n.em,{children:"single-row"})," ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," configuration argument. The following columns in this configuration\n",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," are supported:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"n_periods"})," (required) - specifies the number of future periods to generate starting from the last datetime value\nof the training dataset, utilizing the frequency of the input training series when the model was trained\n(for example, if the training data series elements represent one value per hour, in order to forecast 3 days of\nfuture data, set the column ",(0,l.jsx)(n.code,{children:"n_periods"})," to ",(0,l.jsx)(n.code,{children:"72"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"X"})," (optional) - exogenous regressor values (",(0,l.jsx)(n.em,{children:"only supported in pmdarima version >= 1.8.0"}),") a 2D array of values for\nfuture time period events. For more information, read the underlying library\n",(0,l.jsx)(n.a,{href:"https://www.statsmodels.org/stable/endog_exog.html",children:"explanation"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"return_conf_int"})," (optional) - a boolean (Default: ",(0,l.jsx)(n.code,{children:"False"}),") for whether to return confidence interval values.\nSee above note."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"alpha"})," (optional) - the significance value for calculating confidence intervals. (Default: ",(0,l.jsx)(n.code,{children:"0.05"}),")"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["An example configuration for the ",(0,l.jsx)(n.code,{children:"pyfunc"})," predict of a ",(0,l.jsx)(n.code,{children:"pmdarima"})," model is shown below, with a future period\nprediction count of 100, a confidence interval calculation generation, no exogenous regressor elements, and a default\nalpha of ",(0,l.jsx)(n.code,{children:"0.05"}),":"]}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Index"}),(0,l.jsx)("th",{children:"n-periods"}),(0,l.jsx)("th",{children:"return_conf_int"})]})}),(0,l.jsx)("tbody",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"0"}),(0,l.jsx)("td",{children:"100"}),(0,l.jsx)("td",{children:"True"})]})})]}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," passed to a ",(0,l.jsx)(n.code,{children:"pmdarima"})," ",(0,l.jsx)(n.code,{children:"pyfunc"})," flavor must only contain 1 row."]})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When predicting a ",(0,l.jsx)(n.code,{children:"pmdarima"})," flavor, the ",(0,l.jsx)(n.code,{children:"predict"})," method's ",(0,l.jsx)(n.code,{children:"DataFrame"})," configuration column\n",(0,l.jsx)(n.code,{children:"return_conf_int"}),"'s value controls the output format. When the column's value is set to ",(0,l.jsx)(n.code,{children:"False"})," or ",(0,l.jsx)(n.code,{children:"None"}),"\n(which is the default if this column is not supplied in the configuration ",(0,l.jsx)(n.code,{children:"DataFrame"}),"), the schema of the\nreturned ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," is a single column: ",(0,l.jsx)(n.code,{children:'["yhat"]'}),". When set to ",(0,l.jsx)(n.code,{children:"True"}),", the schema of the returned\n",(0,l.jsx)(n.code,{children:"DataFrame"})," is: ",(0,l.jsx)(n.code,{children:'["yhat", "yhat_lower", "yhat_upper"]'})," with the respective lower (",(0,l.jsx)(n.code,{children:"yhat_lower"}),") and\nupper (",(0,l.jsx)(n.code,{children:"yhat_upper"}),") confidence intervals added to the forecast predictions (",(0,l.jsx)(n.code,{children:"yhat"}),")."]})}),"\n",(0,l.jsx)(n.p,{children:"Example usage of pmdarima artifact loaded as a pyfunc with confidence intervals calculated:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import pmdarima\nimport mlflow\nimport pandas as pd\n\ndata = pmdarima.datasets.load_airpassengers()\n\nwith mlflow.start_run():\n    model = pmdarima.auto_arima(data, seasonal=True)\n    mlflow.pmdarima.save_model(model, "/tmp/model.pmd")\n\nloaded_pyfunc = mlflow.pyfunc.load_model("/tmp/model.pmd")\n\nprediction_conf = pd.DataFrame(\n    [{"n_periods": 4, "return_conf_int": True, "alpha": 0.1}]\n)\n\npredictions = loaded_pyfunc.predict(prediction_conf)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Output (",(0,l.jsx)(n.code,{children:"Pandas DataFrame"}),"):"]}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Index"}),(0,l.jsx)("th",{children:"yhat"}),(0,l.jsx)("th",{children:"yhat_lower"}),(0,l.jsx)("th",{children:"yhat_upper"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"0"}),(0,l.jsx)("td",{children:"467.573731"}),(0,l.jsx)("td",{children:"423.30995"}),(0,l.jsx)("td",{children:"511.83751"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"1"}),(0,l.jsx)("td",{children:"490.494467"}),(0,l.jsx)("td",{children:"416.17449"}),(0,l.jsx)("td",{children:"564.81444"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"2"}),(0,l.jsx)("td",{children:"509.138684"}),(0,l.jsx)("td",{children:"420.56255"}),(0,l.jsx)("td",{children:"597.71117"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"3"}),(0,l.jsx)("td",{children:"492.554714"}),(0,l.jsx)("td",{children:"397.30634"}),(0,l.jsx)("td",{children:"587.80309"})]})]})]}),"\n",(0,l.jsx)(n.admonition,{type:"warning",children:(0,l.jsxs)(n.p,{children:["Signature logging for ",(0,l.jsx)(n.code,{children:"pmdarima"})," will not function correctly if ",(0,l.jsx)(n.code,{children:"return_conf_int"})," is set to ",(0,l.jsx)(n.code,{children:"True"})," from\na non-pyfunc artifact. The output of the native ",(0,l.jsx)(n.code,{children:"ARIMA.predict()"})," when returning confidence intervals is not\na recognized signature type."]})}),"\n",(0,l.jsxs)(n.h3,{id:"openai-openai-experimental",children:["OpenAI (",(0,l.jsx)(n.code,{children:"openai"}),") (Experimental)"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide, including tutorials and detailed documentation for using\nthe ",(0,l.jsx)(n.code,{children:"openai"})," flavor ",(0,l.jsx)(n.a,{href:"/llms/openai",children:"can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"langchain-langchain-experimental",children:["LangChain (",(0,l.jsx)(n.code,{children:"langchain"}),") (Experimental)"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide, including tutorials and detailed documentation for using\nthe ",(0,l.jsx)(n.a,{href:"/llms/langchain",children:"langchain flavor can be viewed here"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"john-snow-labs-johnsnowlabs",children:["John Snow Labs (",(0,l.jsx)(n.code,{children:"johnsnowlabs"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"johnsnowlabs"})," model flavor gives you access to\n",(0,l.jsx)(n.a,{href:"https://nlp.johnsnowlabs.com/models",children:"20.000+ state-of-the-art enterprise NLP models in 200+ languages"}),"\nfor medical, finance, legal and many more domains."]}),"\n",(0,l.jsxs)(n.p,{children:["You can use ",(0,l.jsx)(s.B,{fn:"mlflow.johnsnowlabs.log_model"})," to log and export your model as"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.PyFuncModel",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc.PyFuncModel"})}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["This enables you to integrate ",(0,l.jsx)(n.a,{href:"https://nlp.johnsnowlabs.com/models",children:"any John Snow Labs model"}),"\ninto the MLflow framework. You can easily deploy your models for inference with MLflows serve functionalities.\nModels are interpreted as a generic Python function for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.johnsnowlabs.load_model"})," function to load a saved or logged MLflow\nModel with the ",(0,l.jsx)(n.code,{children:"johnsnowlabs"})," flavor from an stored artifact."]}),"\n",(0,l.jsxs)(n.p,{children:["Features include: LLM's, Text Summarization, Question Answering, Named Entity Recognition, Relation\nExtraction, Sentiment Analysis, Spell Checking, Image Classification, Automatic Speech Recognition and much more,\npowered by the latest Transformer Architectures. The models are provided by ",(0,l.jsx)(n.a,{href:"https://www.johnsnowlabs.com/",children:"John Snow Labs"}),"\nand requires a ",(0,l.jsx)(n.a,{href:"https://www.johnsnowlabs.com/",children:"John Snow Labs"})," Enterprise NLP License.\n",(0,l.jsx)(n.a,{href:"https://www.johnsnowlabs.com/schedule-a-demo/",children:"You can reach out to us"})," for a research or industry license."]}),"\n",(0,l.jsx)(n.p,{children:"Example: Export a John Snow Labs to MLflow format"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import json\nimport os\n\nimport pandas as pd\nfrom johnsnowlabs import nlp\n\nimport mlflow\nfrom mlflow.pyfunc import spark_udf\n\n# 1) Write your raw license.json string into the \'JOHNSNOWLABS_LICENSE_JSON\' env variable for MLflow\ncreds = {\n    "AWS_ACCESS_KEY_ID": "...",\n    "AWS_SECRET_ACCESS_KEY": "...",\n    "SPARK_NLP_LICENSE": "...",\n    "SECRET": "...",\n}\nos.environ["JOHNSNOWLABS_LICENSE_JSON"] = json.dumps(creds)\n\n# 2) Install enterprise libraries\nnlp.install()\n# 3) Start a Spark session with enterprise libraries\nspark = nlp.start()\n\n# 4) Load a model and test it\nnlu_model = "en.classify.bert_sequence.covid_sentiment"\nmodel_save_path = "my_model"\njohnsnowlabs_model = nlp.load(nlu_model)\njohnsnowlabs_model.predict(["I hate COVID,", "I love COVID"])\n\n# 5) Export model with pyfunc and johnsnowlabs flavors\nwith mlflow.start_run():\n    model_info = mlflow.johnsnowlabs.log_model(johnsnowlabs_model, name=model_save_path)\n\n# 6) Load model with johnsnowlabs flavor\nmlflow.johnsnowlabs.load_model(model_info.model_uri)\n\n# 7) Load model with pyfunc flavor\nmlflow.pyfunc.load_model(model_save_path)\n\npandas_df = pd.DataFrame({"text": ["Hello World"]})\nspark_df = spark.createDataFrame(pandas_df).coalesce(1)\npyfunc_udf = spark_udf(\n    spark=spark,\n    model_uri=model_save_path,\n    env_manager="virtualenv",\n    result_type="string",\n)\nnew_df = spark_df.withColumn("prediction", pyfunc_udf(*pandas_df.columns))\n\n# 9) You can now use the mlflow models serve command to serve the model see next section\n\n# 10)  You can also use x command to deploy model inside of a container see next section\n'})}),"\n",(0,l.jsx)(n.h4,{id:"to-deploy-the-john-snow-labs-model-as-a-container",children:"To deploy the John Snow Labs model as a container"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Start the Docker Container"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'docker run -p 5001:8080 -e JOHNSNOWLABS_LICENSE_JSON=your_json_string "mlflow-pyfunc"\n'})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Query server"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:5001/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_split": {\n      "columns": ["text"],\n      "data": [["I hate covid"], ["I love covid"]]\n  }\n}\'\n'})}),"\n",(0,l.jsx)(n.h4,{id:"to-deploy-the-john-snow-labs-model-without-a-container",children:"To deploy the John Snow Labs model without a container"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Export env variable and start server"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"export JOHNSNOWLABS_LICENSE_JSON=your_json_string\nmlflow models serve -m <model_uri>\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Query server"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:5000/invocations -H \'Content-Type: application/json\' -d \'{\n  "dataframe_split": {\n      "columns": ["text"],\n      "data": [["I hate covid"], ["I love covid"]]\n  }\n}\'\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"diviner-diviner",children:["Diviner (",(0,l.jsx)(n.code,{children:"diviner"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"diviner"})," model flavor enables logging of ",(0,l.jsx)(n.a,{href:"https://databricks-diviner.readthedocs.io/en/latest/index.html",children:"diviner models"}),"\nin MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.diviner.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.diviner.log_model"})," methods.\nThese methods also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the model to be\ninterpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nThis loaded PyFunc model can only be scored with a DataFrame input.\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.diviner.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"diviner"}),"\nmodel flavor in native diviner formats."]}),"\n",(0,l.jsx)(n.h4,{id:"diviner-types",children:"Diviner Types"}),"\n",(0,l.jsxs)(n.p,{children:["Diviner is a library that provides an orchestration framework for performing time series forecasting on groups of\nrelated series. Forecasting in ",(0,l.jsx)(n.code,{children:"diviner"})," is accomplished through wrapping popular open source libraries such as\n",(0,l.jsx)(n.a,{href:"https://facebook.github.io/prophet/",children:"prophet"})," and ",(0,l.jsx)(n.a,{href:"http://alkaline-ml.com/pmdarima/",children:"pmdarima"}),". The ",(0,l.jsx)(n.code,{children:"diviner"}),"\nlibrary offers a simplified set of APIs to simultaneously generate distinct time series forecasts for multiple data\ngroupings using a single input DataFrame and a unified high-level API."]}),"\n",(0,l.jsx)(n.h4,{id:"metrics-and-parameters-logging-for-diviner",children:"Metrics and Parameters logging for Diviner"}),"\n",(0,l.jsxs)(n.p,{children:["Unlike other flavors that are supported in MLflow, Diviner has the concept of grouped models. As a collection of many\n(perhaps thousands) of individual forecasting models, the burden to the tracking server to log individual metrics\nand parameters for each of these models is significant. For this reason, metrics and parameters are exposed for\nretrieval from Diviner's APIs as ",(0,l.jsx)(n.code,{children:"Pandas"})," ",(0,l.jsx)(n.code,{children:"DataFrames"}),", rather than discrete primitive values."]}),"\n",(0,l.jsx)(n.p,{children:"To illustrate, let us assume we are forecasting hourly electricity consumption from major cities around the world.\nA sample of our input data looks like this:"}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"country"}),(0,l.jsx)("th",{children:"city"}),(0,l.jsx)("th",{children:"datetime"}),(0,l.jsx)("th",{children:"watts"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"NewYork"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"23568.9"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"NewYork"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"22331.7"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"Boston"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"14220.1"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"Boston"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"14183.4"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"CA"}),(0,l.jsx)("td",{children:"Toronto"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"18562.2"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"CA"}),(0,l.jsx)("td",{children:"Toronto"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"17681.6"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"MX"}),(0,l.jsx)("td",{children:"MexicoCity"}),(0,l.jsx)("td",{children:"2022-03-01 00:01:00"}),(0,l.jsx)("td",{children:"19946.8"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"MX"}),(0,l.jsx)("td",{children:"MexicoCity"}),(0,l.jsx)("td",{children:"2022-03-01 00:02:00"}),(0,l.jsx)("td",{children:"19444.0"})]})]})]}),"\n",(0,l.jsxs)(n.p,{children:["If we were to ",(0,l.jsx)(n.code,{children:"fit"})," a model on this data, supplying the grouping keys as:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'grouping_keys = ["country", "city"]\n'})}),"\n",(0,l.jsx)(n.p,{children:"We will have a model generated for each of the grouping keys that have been supplied:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'[("US", "NewYork"), ("US", "Boston"), ("CA", "Toronto"), ("MX", "MexicoCity")]\n'})}),"\n",(0,l.jsx)(n.p,{children:"With a model constructed for each of these, entering each of their metrics and parameters wouldn't be an issue for the\nMLflow tracking server. What would become a problem, however, is if we modeled each major city on the planet and ran\nthis forecasting scenario every day. If we were to adhere to the conditions of the World Bank, that would mean just\nover 10,000 models as of 2022. After a mere few weeks of running this forecasting every day we would have a very large\nmetrics table."}),"\n",(0,l.jsxs)(n.p,{children:["To eliminate this issue for large-scale forecasting, the metrics and parameters for ",(0,l.jsx)(n.code,{children:"diviner"})," are extracted as a\ngrouping key indexed ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"}),", as shown below for example (float values truncated for visibility):"]}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"grouping_key_columns"}),(0,l.jsx)("th",{children:"country"}),(0,l.jsx)("th",{children:"city"}),(0,l.jsx)("th",{children:"mse"}),(0,l.jsx)("th",{children:"rmse"}),(0,l.jsx)("th",{children:"mae"}),(0,l.jsx)("th",{children:"mape"}),(0,l.jsx)("th",{children:"mdape"}),(0,l.jsx)("th",{children:"smape"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"CA"}),(0,l.jsx)("td",{children:"Toronto"}),(0,l.jsx)("td",{children:"8276851.6"}),(0,l.jsx)("td",{children:"2801.7"}),(0,l.jsx)("td",{children:"2417.7"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.159"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"MX"}),(0,l.jsx)("td",{children:"MexicoCity"}),(0,l.jsx)("td",{children:"3548872.4"}),(0,l.jsx)("td",{children:"1833.8"}),(0,l.jsx)("td",{children:"1584.5"}),(0,l.jsx)("td",{children:"0.15"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.159"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"NewYork"}),(0,l.jsx)("td",{children:"3167846.4"}),(0,l.jsx)("td",{children:"1732.4"}),(0,l.jsx)("td",{children:"1498.2"}),(0,l.jsx)("td",{children:"0.15"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.158"})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:'("country", "city")'}),(0,l.jsx)("td",{children:"US"}),(0,l.jsx)("td",{children:"Boston"}),(0,l.jsx)("td",{children:"14082666.4"}),(0,l.jsx)("td",{children:"3653.2"}),(0,l.jsx)("td",{children:"3156.2"}),(0,l.jsx)("td",{children:"0.15"}),(0,l.jsx)("td",{children:"0.16"}),(0,l.jsx)("td",{children:"0.159"})]})]})]}),"\n",(0,l.jsxs)(n.p,{children:["There are two recommended means of logging the metrics and parameters from a ",(0,l.jsx)(n.code,{children:"diviner"})," model :"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Writing the DataFrames to local storage and using ",(0,l.jsx)(s.B,{fn:"mlflow.log_artifacts"})]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\nimport mlflow\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    params = model.extract_model_params()\n    metrics = model.cross_validate_and_score(\n        horizon="72 hours",\n        period="240 hours",\n        initial="480 hours",\n        parallel="threads",\n        rolling_window=0.1,\n        monthly=False,\n    )\n    params.to_csv(f"{tmpdir}/params.csv", index=False, header=True)\n    metrics.to_csv(f"{tmpdir}/metrics.csv", index=False, header=True)\n\n    mlflow.log_artifacts(tmpdir, artifact_path="data")\n'})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Writing directly as a JSON artifact using ",(0,l.jsx)(s.B,{fn:"mlflow.log_dict"})]}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["The parameters extract from ",(0,l.jsx)(n.code,{children:"diviner"})," models ",(0,l.jsx)(n.em,{children:"may require"})," casting (or dropping of columns) if using the\n",(0,l.jsx)(n.code,{children:"pd.DataFrame.to_dict()"})," approach due to the inability of this method to serialize objects."]})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nparams = model.extract_model_params()\nmetrics = model.cross_validate_and_score(\n    horizon="72 hours",\n    period="240 hours",\n    initial="480 hours",\n    parallel="threads",\n    rolling_window=0.1,\n    monthly=False,\n)\nparams["t_scale"] = params["t_scale"].astype(str)\nparams["start"] = params["start"].astype(str)\nparams = params.drop("stan_backend", axis=1)\n\nmlflow.log_dict(params.to_dict(), "params.json")\nmlflow.log_dict(metrics.to_dict(), "metrics.json")\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Logging of the model artifact is shown in the ",(0,l.jsx)(n.code,{children:"pyfunc"})," example below."]}),"\n",(0,l.jsx)(n.h4,{id:"diviner-pyfunc-usage",children:"Diviner pyfunc usage"}),"\n",(0,l.jsxs)(n.p,{children:["The MLflow Diviner flavor includes an implementation of the ",(0,l.jsx)(n.code,{children:"pyfunc"})," interface for Diviner models. To control\nprediction behavior, you can specify configuration arguments in the first row of a Pandas DataFrame input."]}),"\n",(0,l.jsxs)(n.p,{children:["As this configuration is dependent upon the underlying model type (i.e., the ",(0,l.jsx)(n.code,{children:"diviner.GroupedProphet.forecast()"}),"\nmethod has a different signature than does ",(0,l.jsx)(n.code,{children:"diviner.GroupedPmdarima.predict()"}),"), the Diviner pyfunc implementation\nattempts to coerce arguments to the types expected by the underlying model."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:['Diviner models support both "full group" and "partial group" forecasting. If a column named ',(0,l.jsx)(n.code,{children:'"groups"'})," is present\nin the configuration ",(0,l.jsx)(n.code,{children:"DataFrame"})," submitted to the ",(0,l.jsx)(n.code,{children:"pyfunc"})," flavor, the grouping key values in the first row\nwill be used to generate a subset of forecast predictions. This functionality removes the need to filter a subset\nfrom the full output of all groups forecasts if the results of only a few (or one) groups are needed."]})}),"\n",(0,l.jsxs)(n.p,{children:["For a ",(0,l.jsx)(n.code,{children:"GroupedPmdarima"})," model, an example configuration for the ",(0,l.jsx)(n.code,{children:"pyfunc"})," ",(0,l.jsx)(n.code,{children:"predict()"})," method is:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom pmdarima.arima.auto import AutoARIMA\nfrom diviner import GroupedPmdarima\n\nwith mlflow.start_run():\n    base_model = AutoARIMA(out_of_sample_size=96, maxiter=200)\n    model = GroupedPmdarima(model_template=base_model).fit(\n        df=df,\n        group_key_columns=["country", "city"],\n        y_col="watts",\n        datetime_col="datetime",\n        silence_warnings=True,\n    )\n\n    mlflow.diviner.save_model(diviner_model=model, path="/tmp/diviner_model")\n\ndiviner_pyfunc = mlflow.pyfunc.load_model(model_uri="/tmp/diviner_model")\n\npredict_conf = pd.DataFrame(\n    {\n        "n_periods": 120,\n        "groups": [\n            ("US", "NewYork"),\n            ("CA", "Toronto"),\n            ("MX", "MexicoCity"),\n        ],  # NB: List of tuples required.\n        "predict_col": "wattage_forecast",\n        "alpha": 0.1,\n        "return_conf_int": True,\n        "on_error": "warn",\n    },\n    index=[0],\n)\n\nsubset_forecasts = diviner_pyfunc.predict(predict_conf)\n'})}),"\n",(0,l.jsxs)(n.admonition,{type:"note",children:[(0,l.jsxs)(n.p,{children:["There are several instances in which a configuration ",(0,l.jsx)(n.code,{children:"DataFrame"})," submitted to the ",(0,l.jsx)(n.code,{children:"pyfunc"})," ",(0,l.jsx)(n.code,{children:"predict()"})," method\nwill cause an ",(0,l.jsx)(n.code,{children:"MlflowException"})," to be raised:"]}),(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["If neither ",(0,l.jsx)(n.code,{children:"horizon"})," or ",(0,l.jsx)(n.code,{children:"n_periods"})," are provided."]}),"\n",(0,l.jsxs)(n.li,{children:["The value of ",(0,l.jsx)(n.code,{children:"n_periods"})," or ",(0,l.jsx)(n.code,{children:"horizon"})," is not an integer."]}),"\n",(0,l.jsxs)(n.li,{children:["If the model is of type ",(0,l.jsx)(n.code,{children:"GroupedProphet"}),", ",(0,l.jsx)(n.code,{children:"frequency"})," as a string type must be provided."]}),"\n",(0,l.jsxs)(n.li,{children:["If both ",(0,l.jsx)(n.code,{children:"horizon"})," and ",(0,l.jsx)(n.code,{children:"n_periods"})," are provided with different values."]}),"\n"]})]}),"\n",(0,l.jsxs)(n.h3,{id:"transformers-transformers",children:["Transformers (",(0,l.jsx)(n.code,{children:"transformers"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The full guide, including tutorials and detailed documentation for using the ",(0,l.jsx)(n.code,{children:"transformers"})," flavor is available at ",(0,l.jsx)(n.a,{href:"/llms/transformers/",children:"this location"}),"."]}),"\n",(0,l.jsxs)(n.h3,{id:"sentencetransformers-sentence_transformers",children:["SentenceTransformers (",(0,l.jsx)(n.code,{children:"sentence_transformers"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"sentence_transformers"})," model flavor enables logging of\n",(0,l.jsx)(n.a,{href:"https://www.sbert.net/docs/pretrained_models.html",children:"sentence-transformers models"})," in MLflow format via\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.sentence_transformers.save_model"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.sentence_transformers.log_model"})," functions.\nUse of these functions also adds the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models that they produce, allowing the model to be\ninterpreted as a generic Python function for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.sentence_transformers.load_model"})," function to load a saved or logged MLflow\nModel with the ",(0,l.jsx)(n.code,{children:"sentence_transformers"})," flavor as a native ",(0,l.jsx)(n.code,{children:"sentence-transformers"})," model."]}),"\n",(0,l.jsx)(n.p,{children:"Example:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from sentence_transformers import SentenceTransformer\n\nimport mlflow\nimport mlflow.sentence_transformers\n\nmodel = SentenceTransformer("all-MiniLM-L6-v2")\n\nexample_sentences = ["This is a sentence.", "This is another sentence."]\n\n# Define the signature\nsignature = mlflow.models.infer_signature(\n    model_input=example_sentences,\n    model_output=model.encode(example_sentences),\n)\n\n# Log the model using mlflow\nwith mlflow.start_run():\n    logged_model = mlflow.sentence_transformers.log_model(\n        model=model,\n        name="sbert_model",\n        signature=signature,\n        input_example=example_sentences,\n    )\n\n# Load option 1: mlflow.pyfunc.load_model returns a PyFuncModel\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nembeddings1 = loaded_model.predict(["hello world", "i am mlflow"])\n\n# Load option 2: mlflow.sentence_transformers.load_model returns a SentenceTransformer\nloaded_model = mlflow.sentence_transformers.load_model(logged_model.model_uri)\nembeddings2 = loaded_model.encode(["hello world", "i am mlflow"])\n\nprint(embeddings1)\n\n"""\n>> [[-3.44772562e-02  3.10232025e-02  6.73496164e-03  2.61089969e-02\n  ...\n  2.37922110e-02 -2.28897743e-02  3.89375277e-02  3.02067865e-02]\n [ 4.81191138e-03 -9.33756605e-02  6.95968643e-02  8.09735525e-03\n  ...\n   6.57437667e-02 -2.72239652e-02  4.02687863e-02 -1.05599344e-01]]\n"""\n'})}),"\n",(0,l.jsxs)(n.h3,{id:"promptflow-promptflow",children:["Promptflow (",(0,l.jsx)(n.code,{children:"promptflow"}),")"]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"promptflow"})," model flavor is capable of packaging your flow in MLflow format via the ",(0,l.jsx)(s.B,{fn:"mlflow.promptflow.save_model"}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.promptflow.log_model"})," functions. Currently, a ",(0,l.jsx)(n.code,{children:"flow.dag.yaml"})," file is required to be\npresent in the flow's directory. These functions also add the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor to the MLflow Models,\nallowing the models to be interpreted as generic Python functions for inference via ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"}),".\nYou can also use the ",(0,l.jsx)(s.B,{fn:"mlflow.promptflow.load_model"})," method to load MLflow Models with the ",(0,l.jsx)(n.code,{children:"promptflow"}),"\nmodel flavor in native promptflow format."]}),"\n",(0,l.jsxs)(n.p,{children:["Please note that the ",(0,l.jsx)(n.code,{children:"signature"})," in ",(0,l.jsx)(n.code,{children:"MLmodel"})," file will NOT BE automatically inferred from the flow itself.\nTo save model with the signature, you can either pass the ",(0,l.jsx)(n.code,{children:"input_example"})," or specify the input signature manually."]}),"\n",(0,l.jsx)(n.p,{children:"Example:"}),"\n",(0,l.jsxs)(n.p,{children:["Reach the flow source at ",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/promptflow/basic",children:"example from the MLflow GitHub Repository"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\nfrom pathlib import Path\n\nfrom promptflow import load_flow\n\nimport mlflow\n\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\n\n\n# The example flow will write a simple code snippet that displays the greeting message with specific language.\nflow_folder = Path(__file__).parent / "basic"\nflow = load_flow(flow_folder)\n\nwith mlflow.start_run():\n    logged_model = mlflow.promptflow.log_model(flow, name="promptflow_model")\n\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nprint(loaded_model.predict({"text": "Python Hello World!"}))\n'})}),"\n",(0,l.jsx)(n.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,l.jsxs)(n.p,{children:["After building and training your MLflow Model, you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," API to\nevaluate its performance on one or more datasets of your choosing. ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"}),"\ncurrently supports evaluation of MLflow Models with the ",(0,l.jsx)(n.a,{href:"#pyfunc-model-flavor",children:"python_function (pyfunc) model flavor"}),"\nfor classification, regression, and numerous language modeling tasks (see ",(0,l.jsx)(n.a,{href:"#model-evaluation-llms",children:"Evaluating with LLMs"}),"),\ncomputing a variety of task-specific performance metrics, model performance plots, and\nmodel explanations. Evaluation results are logged to ",(0,l.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["The following ",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_on_binary_classifier.py",children:"example from the MLflow GitHub Repository"}),"\nuses ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," to evaluate the performance of a classifier\non the ",(0,l.jsx)(n.a,{href:"https://archive.ics.uci.edu/ml/datasets/adult",children:"UCI Adult Data Set"}),", logging a\ncomprehensive collection of MLflow Metrics and Artifacts that provide insight into model performance\nand behavior:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import xgboost\nimport shap\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom sklearn.model_selection import train_test_split\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Fit an XGBoost binary classifier on the training data split\nmodel = xgboost.XGBClassifier().fit(X_train, y_train)\n\n# Create a model signature\nsignature = infer_signature(X_test, model.predict(X_test))\n\n# Build the Evaluation Dataset from the test set\neval_data = X_test\neval_data["label"] = y_test\n\nwith mlflow.start_run() as run:\n    # Log the baseline model to MLflow\n    mlflow.sklearn.log_model(model, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n\n    # Evaluate the logged model\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n        evaluators=["default"],\n        # Uncomment the following line to log the SHAP explainer used to compute the feature\n        # importance as a model\n        # evaluator_config={"log_explainer": True},\n    )\n'})}),"\n",(0,l.jsxs)("div",{style:{display:"flex"},children:[(0,l.jsx)("span",{style:{width:"15%"},children:(0,l.jsx)(n.img,{src:t(92317).A+"",width:"266",height:"613"})}),(0,l.jsx)("span",{style:{width:"84%"},children:(0,l.jsx)(n.img,{src:t(3203).A+"",width:"1835",height:"735"})})]}),"\n",(0,l.jsx)(n.h3,{id:"model-evaluation-llms",children:"Evaluating with LLMs"}),"\n",(0,l.jsxs)(n.p,{children:["As of MLflow 2.4.0, ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," has built-in support for a variety of tasks with\nLLMs, including text summarization, text classification, question answering, and text generation.\nThe following example uses ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," to evaluate a model that answers\nquestions about MLflow (note that you must have the ",(0,l.jsx)(n.code,{children:"OPENAI_API_TOKEN"})," environment variable set\nin your current system environment in order to run the example):"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\nimport pandas as pd\n\nimport mlflow\nimport openai\n\n# Create a question answering model using prompt engineering with OpenAI. Log the\n# prompt and the model to MLflow Tracking\nmlflow.start_run()\nsystem_prompt = (\n    "Your job is to answer questions about MLflow. When you are asked a question about MLflow,"\n    " respond to it. Make sure to include code examples. If the question is not related to"\n    " MLflow, refuse to answer and say that the question is unrelated."\n)\nmlflow.log_param("system_prompt", system_prompt)\nlogged_model = mlflow.openai.log_model(\n    model="gpt-4o-mini",\n    task=openai.chat.completions,\n    name="model",\n    messages=[\n        {"role": "system", "content": system_prompt},\n        {"role": "user", "content": "{question}"},\n    ],\n)\n\n# Evaluate the model on some example questions\nquestions = pd.DataFrame(\n    {\n        "question": [\n            "How do you create a run with MLflow?",\n            "How do you log a model with MLflow?",\n            "What is the capital of France?",\n        ]\n    }\n)\nmlflow.evaluate(\n    model=logged_model.model_uri,\n    model_type="question-answering",\n    data=questions,\n)\n\n# Load and inspect the evaluation results\nresults: pd.DataFrame = mlflow.load_table(\n    "eval_results_table.json", extra_columns=["run_id", "params.system_prompt"]\n)\nprint("Evaluation results:")\nprint(results)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["MLflow also provides an Artifact View UI for comparing inputs and outputs across multiple models\nbuilt with LLMs. For example, after evaluating multiple prompts for question answering\n(see the ",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/tree/master/examples/llms/question_answering/question_answering.py",children:"MLflow OpenAI question answering full example"}),"),\nyou can navigate to the Artifact View to view the questions and compare the answers for\neach model:"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(804).A+"",width:"3504",height:"2118"})}),"\n",(0,l.jsxs)(n.p,{children:["For additional examples demonstrating the use of ",(0,l.jsx)(n.code,{children:"mlflow.evaluate()"})," with LLMs, check out the\n",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/tree/master/examples/llms",children:"MLflow LLMs example repository"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"evaluating-with-extra-metrics",children:"Evaluating with Extra Metrics"}),"\n",(0,l.jsxs)(n.p,{children:["If the default set of metrics is insufficient, you can supply ",(0,l.jsx)(n.code,{children:"extra_metrics"})," and ",(0,l.jsx)(n.code,{children:"custom_artifacts"}),"\nto ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," to produce extra metrics and artifacts for the model(s) that you're evaluating."]}),"\n",(0,l.jsxs)(n.p,{children:["To define an extra metric, you should define an ",(0,l.jsx)(n.code,{children:"eval_fn"})," function that takes in ",(0,l.jsx)(n.code,{children:"predictions"})," and ",(0,l.jsx)(n.code,{children:"targets"})," as arguments\nand outputs a ",(0,l.jsx)(n.code,{children:"MetricValue"})," object. ",(0,l.jsx)(n.code,{children:"predictions"})," and ",(0,l.jsx)(n.code,{children:"targets"})," are ",(0,l.jsx)(n.code,{children:"pandas.Series"}),"\nobjects. If ",(0,l.jsx)(n.code,{children:"predictions"})," or ",(0,l.jsx)(n.code,{children:"targets"})," specified in ",(0,l.jsx)(n.code,{children:"mlflow.evaluate()"})," is either ",(0,l.jsx)(n.code,{children:"numpy.array"})," or ",(0,l.jsx)(n.code,{children:"List"}),",\nthey will be converted to ",(0,l.jsx)(n.code,{children:"pandas.Series"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["To use values from other metrics to compute your custom metric, include the name of the metric as an argument to ",(0,l.jsx)(n.code,{children:"eval_fn"}),".\nThis argument will contain a ",(0,l.jsx)(n.code,{children:"MetricValue"})," object which contains the values calculated from the specified metric and can be used to compute your custom metric."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'{\n    "accuracy_score": MetricValue(\n        scores=None, justifications=None, aggregate_results={"accuracy_score": 1.0}\n    )\n}\n'})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"MetricValue"})," class has three attributes:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"scores"}),": a list that contains per-row metrics."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"aggregate_results"}),": a dictionary that maps the aggregation method names to the corresponding aggregated values. This is intended to be used to aggregate ",(0,l.jsx)(n.code,{children:"scores"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"justifications"}),": a list that contains per-row justifications of the values in ",(0,l.jsx)(n.code,{children:"scores"}),". This is optional, and is usually used with genai metrics."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"The code block below demonstrates how to define a custom metric evaluation function:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.metrics import MetricValue\n\n\ndef my_metric_eval_fn(predictions, targets):\n    scores = np.abs(predictions - targets)\n    return MetricValue(\n        scores=list(scores),\n        aggregate_results={\n            "mean": np.mean(scores),\n            "variance": np.var(scores),\n            "median": np.median(scores),\n        },\n    )\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Once you have defined an ",(0,l.jsx)(n.code,{children:"eval_fn"}),", you then use ",(0,l.jsx)(n.code,{children:"make_metric()"})," to wrap this ",(0,l.jsx)(n.code,{children:"eval_fn"})," function into a metric.\nIn addition to ",(0,l.jsx)(n.code,{children:"eval_fn"}),", ",(0,l.jsx)(n.code,{children:"make_metric()"})," requires an additional parameter , ",(0,l.jsx)(n.code,{children:"greater_is_better"}),", for optimization purposes. This parameter\nindicates whether this is a metric we want to maximize or minimize."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"from mlflow.metrics import make_metric\n\nmymetric = make_metric(eval_fn=my_metric_eval_fn, greater_is_better=False)\n"})}),"\n",(0,l.jsx)(n.p,{children:"The extra metric allows you to either evaluate a model directly, or to evaluate an output dataframe."}),"\n",(0,l.jsxs)(n.p,{children:["To evaluate the model directly, you will have to provide ",(0,l.jsx)(n.code,{children:"mlflow.evaluate()"})," either a pyfunc model\ninstance, a URI referring to a pyfunc model, or a callable function that takes in the data as input\nand outputs the predictions."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def model(x):\n    return x["inputs"]\n\n\neval_dataset = pd.DataFrame(\n    {\n        "targets": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        "inputs": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nmlflow.evaluate(model, eval_dataset, targets="targets", extra_metrics=[mymetric])\n'})}),"\n",(0,l.jsxs)(n.p,{children:["To directly evaluate an output dataframe, you can ",(0,l.jsx)(n.strong,{children:"omit"})," the ",(0,l.jsx)(n.code,{children:"model"})," parameter. However, you will need\nto set the ",(0,l.jsx)(n.code,{children:"predictions"})," parameter in ",(0,l.jsx)(n.code,{children:"mlflow.evaluate()"})," in order to evaluate an inference output dataframe."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'eval_dataset = pd.DataFrame(\n    {\n        "targets": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        "predictions": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nresult = mlflow.evaluate(\n    data=eval_dataset,\n    predictions="predictions",\n    targets="targets",\n    extra_metrics=[mymetric],\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["When your model has multiple outputs, the model must return a pandas DataFrame with multiple columns. You must\nspecify one column among the model output columns as the predictions column using the ",(0,l.jsx)(n.code,{children:"predictions"})," parameter,\nand other output columns of the model will be accessible from the ",(0,l.jsx)(n.code,{children:"eval_fn"})," based on their column names. For example, if\nyour model has two outputs ",(0,l.jsx)(n.code,{children:"retrieved_context"})," and ",(0,l.jsx)(n.code,{children:"answer"}),", you can specify ",(0,l.jsx)(n.code,{children:"answer"})," as the predictions\ncolumn, and ",(0,l.jsx)(n.code,{children:"retrieved_context"})," column will be accessible as the ",(0,l.jsx)(n.code,{children:"context"})," parameter from ",(0,l.jsx)(n.code,{children:"eval_fn"})," via ",(0,l.jsx)(n.code,{children:"col_mapping"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def eval_fn(predictions, targets, context):\n    scores = (predictions == targets) + context\n    return MetricValue(\n        scores=list(scores),\n        aggregate_results={"mean": np.mean(scores), "sum": np.sum(scores)},\n    )\n\n\nmymetric = make_metric(eval_fn=eval_fn, greater_is_better=False, name="mymetric")\n\n\ndef model(x):\n    return pd.DataFrame({"retrieved_context": x["inputs"] + 1, "answer": x["inputs"]})\n\n\neval_dataset = pd.DataFrame(\n    {\n        "targets": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        "inputs": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nconfig = {"col_mapping": {"context": "retrieved_context"}}\n\nresult = mlflow.evaluate(\n    model,\n    eval_dataset,\n    predictions="answer",\n    targets="targets",\n    extra_metrics=[mymetric],\n    evaluator_config=config,\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["However, you can also avoid using ",(0,l.jsx)(n.code,{children:"col_mapping"})," if the parameter of ",(0,l.jsx)(n.code,{children:"eval_fn"})," is the same as the output column name of the model."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def eval_fn(predictions, targets, retrieved_context):\n    scores = (predictions == targets) + retrieved_context\n    return MetricValue(\n        scores=list(scores),\n        aggregate_results={"mean": np.mean(scores), "sum": np.sum(scores)},\n    )\n\n\nmymetric = make_metric(eval_fn=eval_fn, greater_is_better=False, name="mymetric")\n\n\ndef model(x):\n    return pd.DataFrame({"retrieved_context": x["inputs"] + 1, "answer": x["inputs"]})\n\n\neval_dataset = pd.DataFrame(\n    {\n        "targets": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        "inputs": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nresult = mlflow.evaluate(\n    model,\n    eval_dataset,\n    predictions="answer",\n    targets="targets",\n    extra_metrics=[mymetric],\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"col_mapping"})," also allows you to pass additional parameters to the extra metric function, in this case passing a value ",(0,l.jsx)(n.code,{children:"k"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def eval_fn(predictions, targets, k):\n    scores = k * (predictions == targets)\n    return MetricValue(scores=list(scores), aggregate_results={"mean": np.mean(scores)})\n\n\nweighted_mymetric = make_metric(eval_fn=eval_fn, greater_is_better=False)\n\n\ndef model(x):\n    return x["inputs"]\n\n\neval_dataset = pd.DataFrame(\n    {\n        "targets": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        "inputs": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nconfig = {"col_mapping": {"k": 5}}\nmlflow.evaluate(\n    model,\n    eval_dataset,\n    targets="targets",\n    extra_metrics=[weighted_mymetric],\n    evaluator_config=config,\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["You can also add the name of other metrics as an argument to the extra metric function, which will pass in the ",(0,l.jsx)(n.code,{children:"MetricValue"})," calculated for that metric."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def eval_fn(predictions, targets, retrieved_context):\n    scores = (predictions == targets) + retrieved_context\n    return MetricValue(\n        scores=list(scores),\n        aggregate_results={"mean": np.mean(scores), "sum": np.sum(scores)},\n    )\n\n\nmymetric = make_metric(eval_fn=eval_fn, greater_is_better=False, name="mymetric")\n\n\ndef eval_fn_2(predictions, targets, mymetric):\n    scores = ["true" if score else "false" for score in mymetric.scores]\n    return MetricValue(\n        scores=list(scores),\n    )\n\n\nmymetric2 = make_metric(eval_fn=eval_fn_2, greater_is_better=False, name="mymetric2")\n\n\ndef model(x):\n    return pd.DataFrame({"retrieved_context": x["inputs"] + 1, "answer": x["inputs"]})\n\n\neval_dataset = pd.DataFrame(\n    {\n        "targets": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        "inputs": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nresult = mlflow.evaluate(\n    model,\n    eval_dataset,\n    predictions="answer",\n    targets="targets",\n    extra_metrics=[mymetric, mymetric2],\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["The following ",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_with_custom_metrics.py",children:"short example from the MLflow GitHub Repository"}),"\nuses ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," with an extra metric function to evaluate the performance of a regressor on the\n",(0,l.jsx)(n.a,{href:"https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html",children:"California Housing Dataset"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nfrom mlflow.models import infer_signature, make_metric\n\n# loading the California housing dataset\ncali_housing = fetch_california_housing(as_frame=True)\n\n# split the dataset into train and test partitions\nX_train, X_test, y_train, y_test = train_test_split(\n    cali_housing.data, cali_housing.target, test_size=0.2, random_state=123\n)\n\n# train the model\nlin_reg = LinearRegression().fit(X_train, y_train)\n\n# Infer model signature\npredictions = lin_reg.predict(X_train)\nsignature = infer_signature(X_train, predictions)\n\n# creating the evaluation dataframe\neval_data = X_test.copy()\neval_data["target"] = y_test\n\n\ndef squared_diff_plus_one(eval_df, _builtin_metrics):\n    """\n    This example custom metric function creates a metric based on the ``prediction`` and\n    ``target`` columns in ``eval_df`.\n    """\n    return np.sum(np.abs(eval_df["prediction"] - eval_df["target"] + 1) ** 2)\n\n\ndef sum_on_target_divided_by_two(_eval_df, builtin_metrics):\n    """\n    This example custom metric function creates a metric derived from existing metrics in\n    ``builtin_metrics``.\n    """\n    return builtin_metrics["sum_on_target"] / 2\n\n\ndef prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):\n    """\n    This example custom artifact generates and saves a scatter plot to ``artifacts_dir`` that\n    visualizes the relationship between the predictions and targets for the given model to a\n    file as an image artifact.\n    """\n    plt.scatter(eval_df["prediction"], eval_df["target"])\n    plt.xlabel("Targets")\n    plt.ylabel("Predictions")\n    plt.title("Targets vs. Predictions")\n    plot_path = os.path.join(artifacts_dir, "example_scatter_plot.png")\n    plt.savefig(plot_path)\n    return {"example_scatter_plot_artifact": plot_path}\n\n\nwith mlflow.start_run() as run:\n    mlflow.sklearn.log_model(lin_reg, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n    result = mlflow.evaluate(\n        model=model_uri,\n        data=eval_data,\n        targets="target",\n        model_type="regressor",\n        evaluators=["default"],\n        extra_metrics=[\n            make_metric(\n                eval_fn=squared_diff_plus_one,\n                greater_is_better=False,\n            ),\n            make_metric(\n                eval_fn=sum_on_target_divided_by_two,\n                greater_is_better=True,\n            ),\n        ],\n        custom_artifacts=[prediction_target_scatter],\n    )\n\nprint(f"metrics:\\n{result.metrics}")\nprint(f"artifacts:\\n{result.artifacts}")\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For a more comprehensive extra metrics usage example, refer to\n",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_with_custom_metrics_comprehensive.py",children:"this example from the MLflow GitHub Repository"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"evaluating-with-a-function",children:"Evaluating with a Function"}),"\n",(0,l.jsxs)(n.p,{children:["As of MLflow 2.8.0, ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," supports evaluating a python function without requiring\nlogging the model to MLflow. This is useful when you don't want to log the model and just want to evaluate\nit. The requirements for the function's input and output are the same as the requirements for a model's input and\noutput."]}),"\n",(0,l.jsxs)(n.p,{children:["The following example uses ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," to evaluate a function:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import shap\nimport xgboost\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Fit an XGBoost binary classifier on the training data split\nmodel = xgboost.XGBClassifier().fit(X_train, y_train)\n\n# Build the Evaluation Dataset from the test set\neval_data = X_test\neval_data["label"] = y_test\n\n\n# Define a function that calls the model\'s predict method\ndef fn(X):\n    return model.predict(X)\n\n\nwith mlflow.start_run() as run:\n    # Evaluate the function without logging the model\n    result = mlflow.evaluate(\n        fn,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n        evaluators=["default"],\n    )\n\nprint(f"metrics:\\n{result.metrics}")\nprint(f"artifacts:\\n{result.artifacts}")\n'})}),"\n",(0,l.jsx)(n.h3,{id:"evaluating-with-a-static-dataset",children:"Evaluating with a Static Dataset"}),"\n",(0,l.jsxs)(n.p,{children:["As of MLflow 2.8.0, ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," supports evaluating a static dataset without specifying a model.\nThis is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and\nwant to evaluate the static dataset without re-running the model."]}),"\n",(0,l.jsxs)(n.p,{children:["If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the\ntop-level ",(0,l.jsx)(n.code,{children:"predictions"})," parameter in ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Assume that the model output is saved to the pandas_df["model_output"] column\nmlflow.evaluate(\n    ...,\n    data=pandas_df,\n    predictions="model_output",\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["If you are using an MLflow PandasDataset, you must specify the column name that contains the model output using\nthe ",(0,l.jsx)(n.code,{children:"predictions"})," parameter in ",(0,l.jsx)(s.B,{fn:"mlflow.data.from_pandas"}),", and specify ",(0,l.jsx)(n.code,{children:"None"})," for the\n",(0,l.jsx)(n.code,{children:"predictions"})," parameter in ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Assume that the model output is saved to the pandas_df["model_output"] column\ndataset = mlflow.data.from_pandas(pandas_df, predictions="model_output")\nmlflow.evaluate(\n    data=pandas_df,\n    predictions=None,\n    # other arguments\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:['When your model has multiple outputs, you must specify one column among the model output columns as the predictions\ncolumn. The other output columns of the model will be treated as "input" columns. For example, if your model\nhas two outputs named ',(0,l.jsx)(n.code,{children:"retrieved_context"})," and ",(0,l.jsx)(n.code,{children:"answer"}),", you can specify ",(0,l.jsx)(n.code,{children:"answer"})," as the predictions column. The\n",(0,l.jsx)(n.code,{children:"retrieved_context"}),' column will be treated as an "input" column when calculating the metrics.']}),"\n",(0,l.jsxs)(n.p,{children:["The following example uses ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," to evaluate a static dataset:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import shap\nimport xgboost\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Fit an XGBoost binary classifier on the training data split\nmodel = xgboost.XGBClassifier().fit(X_train, y_train)\n\n# Build the Evaluation Dataset from the test set\ny_test_pred = model.predict(X=X_test)\neval_data = X_test\neval_data["label"] = y_test\neval_data["predictions"] = y_test_pred\n\n\nwith mlflow.start_run() as run:\n    # Evaluate the static dataset without providing a model\n    result = mlflow.evaluate(\n        data=eval_data,\n        targets="label",\n        predictions="predictions",\n        model_type="classifier",\n    )\n\nprint(f"metrics:\\n{result.metrics}")\nprint(f"artifacts:\\n{result.artifacts}")\n'})}),"\n",(0,l.jsx)(n.h3,{id:"performing-model-validation",children:"Performing Model Validation"}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:["MLflow 2.18.0 has moved the model validation functionality from the ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," API\nto a dedicated ",(0,l.jsx)(s.B,{fn:"mlflow.validate_evaluation_results"})," API. The relevant parameters, such as baseline_model,\nare deprecated and will be removed from the older API in future versions."]})}),"\n",(0,l.jsxs)(n.p,{children:["With the ",(0,l.jsx)(s.B,{fn:"mlflow.validate_evaluation_results"})," API, you can validate metrics generated during model evaluation\nto assess the quality of your model against a baseline. To achieve this, first evaluate both the candidate and baseline models\nusing ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," (or load persisted evaluation results from local storage). Then, pass the results along\nwith a validation_thresholds dictionary that maps metric names to ",(0,l.jsx)(s.B,{fn:"mlflow.models.MetricThreshold",children:(0,l.jsx)(n.code,{children:"mlflow.models.MetricThreshold"})}),"\nobjects. If your model fails to meet the specified thresholds, ",(0,l.jsx)(s.B,{fn:"mlflow.validate_evaluation_results"})," will raise a\n",(0,l.jsx)(n.code,{children:"ModelValidationFailedException"})," with details about the validation failure."]}),"\n",(0,l.jsxs)(n.p,{children:["More information on model evaluation behavior and outputs can be found in the ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," API documentation."]}),"\n",(0,l.jsx)(n.h4,{id:"validating-a-model-against-a-baseline-model",children:"Validating a Model Against a Baseline Model"}),"\n",(0,l.jsx)(n.p,{children:"Below is an example of how to validate a candidate model's performance against a baseline using predefined thresholds."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import xgboost\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nimport mlflow\nfrom mlflow.models import MetricThreshold\n\n# load UCI Adult Data Set; segment it into training and test sets\nX, y = shap.datasets.adult()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# construct an evaluation dataset from the test set\neval_data = X_test\neval_data["label"] = y_test\n\n# Log and evaluate the candidate model\ncandidate_model = xgboost.XGBClassifier().fit(X_train, y_train)\n\nwith mlflow.start_run(run_name="candidate") as run:\n    candidate_model_uri = mlflow.sklearn.log_model(\n        candidate_model, name="candidate_model"\n    ).model_uri\n\n    candidate_result = mlflow.evaluate(\n        candidate_model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n    )\n\n# Log and evaluate the baseline model\nbaseline_model = DummyClassifier(strategy="uniform").fit(X_train, y_train)\n\nwith mlflow.start_run(run_name="baseline") as run:\n    baseline_model_uri = mlflow.sklearn.log_model(\n        baseline_model, name="baseline_model"\n    ).model_uri\n\n    baseline_result = mlflow.evaluate(\n        baseline_model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",\n    )\n\n# Define criteria for model to be validated against\nthresholds = {\n    "accuracy_score": MetricThreshold(\n        threshold=0.8,  # accuracy should be >=0.8\n        min_absolute_change=0.05,  # accuracy should be at least 0.05 greater than baseline model accuracy\n        min_relative_change=0.05,  # accuracy should be at least 5 percent greater than baseline model accuracy\n        greater_is_better=True,\n    ),\n}\n\n# Validate the candidate model against baseline\nmlflow.validate_evaluation_results(\n    candidate_result=candidate_result,\n    baseline_result=baseline_result,\n    validation_thresholds=thresholds,\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Refer to ",(0,l.jsx)(s.B,{fn:"mlflow.models.MetricThreshold",children:(0,l.jsx)(n.code,{children:"mlflow.models.MetricThreshold"})}),"\nto see details on how the thresholds are specified and checked."]}),"\n",(0,l.jsx)(n.p,{children:"The logged output within the MLflow UI for the comprehensive example is shown below."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(79783).A+"",width:"1832",height:"693"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"Model validation results are not included in the active MLflow run."})}),"\n",(0,l.jsx)(n.h4,{id:"validating-a-model-against-static-thresholds",children:"Validating a Model Against Static Thresholds"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.validate_evaluation_results"})," API can also be used to validate a candidate\nmodel against static thresholds, instead of comparing it to a baseline model, by passing ",(0,l.jsx)(n.code,{children:"None"})," to\nthe ",(0,l.jsx)(n.code,{children:"baseline_result"})," parameter."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import MetricThreshold\n\nthresholds = {\n    "accuracy_score": MetricThreshold(\n        threshold=0.8,  # accuracy should be >=0.8\n        greater_is_better=True,\n    ),\n}\n\n# Validate the candidate model against static threshold\nmlflow.validate_evaluation_results(\n    candidate_result=candidate_result,\n    baseline_result=None,\n    validation_thresholds=thresholds,\n)\n'})}),"\n",(0,l.jsx)(n.h4,{id:"reusing-baseline-result-for-multiple-validations",children:"Reusing Baseline Result For Multiple Validations"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.models.EvaluationResult",children:(0,l.jsx)(n.code,{children:"mlflow.models.EvaluationResult"})})," object\nreturned by ",(0,l.jsx)(s.B,{fn:"mlflow.evaluate"})," can be saved to and loaded from local storage.\nThis feature allows you to reuse the same baseline result across different candidate models,\nwhich is particularly useful for automating model quality monitoring."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models.evaluation import EvaluationResult\n\nbaseline_result = mlflow.evaluate(\n    baseline_model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n)\nbaseline_result.save("RESULT_PATH")\n\n# Load the evaluation result for validation\nbaseline_result = EvaluationResult.load("RESULT_PATH")\n'})}),"\n",(0,l.jsxs)(n.admonition,{type:"note",children:[(0,l.jsx)(n.p,{children:"There are plugins that support in-depth model validation with features that are not supported\ndirectly in MLflow. To learn more, see:"}),(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#giskard_plugin",children:"Model Validation with Giskard\u2019s plugin"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#trubrics_plugin",children:"Model Validation with Trubrics\u2019 plugin"})}),"\n"]})]}),"\n",(0,l.jsxs)(n.admonition,{type:"note",children:[(0,l.jsxs)(n.p,{children:["Differences in the computation of Area under Curve Precision Recall score (metric name\n",(0,l.jsx)(n.code,{children:"precision_recall_auc"}),") between multi and binary classifiers:"]}),(0,l.jsxs)(n.p,{children:["Multiclass classifier models, when evaluated, utilize the standard scoring metric from sklearn:\n",(0,l.jsx)(n.code,{children:"sklearn.metrics.roc_auc_score"})," to calculate the area under the precision recall curve. This\nalgorithm performs a linear interpolation calculation utilizing the trapezoidal rule to estimate\nthe area under the precision recall curve. It is well-suited for use in evaluating multi-class\nclassification models to provide a single numeric value of the quality of fit."]}),(0,l.jsxs)(n.p,{children:["Binary classifier models, on the other hand, use the ",(0,l.jsx)(n.code,{children:"sklearn.metrics.average_precision_score"})," to\navoid the shortcomings of the ",(0,l.jsx)(n.code,{children:"roc_auc_score"})," implementation when applied to heavily\nimbalanced classes in binary classification. Usage of the ",(0,l.jsx)(n.code,{children:"roc_auc_score"})," for imbalanced\ndatasets can give a misleading result (optimistically better than the model's actual ability\nto accurately predict the minority class membership)."]}),(0,l.jsxs)(n.p,{children:["For additional information on the topic of why different algorithms are employed for this, as\nwell as links to the papers that informed the implementation of these metrics within the\n",(0,l.jsx)(n.code,{children:"sklearn.metrics"})," module, refer to\n",(0,l.jsx)(n.a,{href:"https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics",children:"the documentation"}),"."]}),(0,l.jsxs)(n.p,{children:["For simplicity purposes, both methodologies evaluation metric results (whether for multi-class\nor binary classification) are unified in the single metric: ",(0,l.jsx)(n.code,{children:"precision_recall_auc"}),"."]})]}),"\n",(0,l.jsx)(n.h3,{id:"giskard_plugin",children:"Model Validation with Giskard's plugin"}),"\n",(0,l.jsxs)(n.p,{children:["To extend the validation capabilities of MLflow and anticipate issues before they go to production, a plugin has been built by\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/index.html",children:"Giskard"})," allowing users to:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["scan a model in order to detect hidden vulnerabilities such as\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/performance_bias/index.html",children:"Performance bias"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/robustness/index.html",children:"Unrobustness"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/overconfidence/index.html",children:"Overconfidence"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/underconfidence/index.html",children:"Underconfidence"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/ethics/index.html",children:"Ethical bias"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/data_leakage/index.html",children:"Data leakage"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/stochasticity/index.html",children:"Stochasticity"}),",\n",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/spurious/index.html",children:"Spurious correlation"}),", and others"]}),"\n",(0,l.jsx)(n.li,{children:"explore samples in the data that highlight the vulnerabilities found"}),"\n",(0,l.jsx)(n.li,{children:"log the vulnerabilities as well-defined and quantified metrics"}),"\n",(0,l.jsx)(n.li,{children:"compare the metrics across different models"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"See the following plugin example notebooks for a demo:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-tabular-example.html",children:"Tabular ML models"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-llm-example.html",children:"Text ML models (LLMs)"})}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["For more information on the plugin, see the ",(0,l.jsx)(n.a,{href:"https://docs.giskard.ai/en/latest/integrations/mlflow/index.html",children:"giskard-mlflow docs"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"trubrics_plugin",children:"Model Validation with Trubrics' plugin"}),"\n",(0,l.jsxs)(n.p,{children:["To extend the validation capabilities of MLflow, a plugin has been built by ",(0,l.jsx)(n.a,{href:"https://github.com/trubrics/trubrics-sdk",children:"Trubrics"})," allowing users:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"to use a large number of out-of-the-box validations"}),"\n",(0,l.jsx)(n.li,{children:"to validate a run with any custom python functions"}),"\n",(0,l.jsx)(n.li,{children:"to view all validation results in a .json file, for diagnosis of why an MLflow run could have failed"}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["See the ",(0,l.jsx)(n.a,{href:"https://github.com/trubrics/trubrics-sdk/blob/main/examples/mlflow/mlflow-trubrics.ipynb",children:"plugin example notebook"})," for a demo."]}),"\n",(0,l.jsxs)(n.p,{children:["For more information on the plugin, see the ",(0,l.jsx)(n.a,{href:"https://trubrics.github.io/trubrics-sdk/mlflow/",children:"trubrics-mlflow docs"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"model-customization",children:"Model Customization"}),"\n",(0,l.jsxs)(n.p,{children:["While MLflow's built-in model persistence utilities are convenient for packaging models from various\npopular ML libraries in MLflow Model format, they do not cover every use case. For example, you may\nwant to use a model from an ML library that is not explicitly supported by MLflow's built-in\nflavors. Alternatively, you may want to package custom inference code and data to create an\nMLflow Model. Fortunately, MLflow provides two solutions that can be used to accomplish these\ntasks: ",(0,l.jsx)(n.a,{href:"#custom-python-models",children:"Custom Python Models"})," and ",(0,l.jsx)(n.a,{href:"#custom-flavors",children:"Custom Flavors"}),"."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"In this section:"})}),"\n",(0,l.jsx)(i.A,{toc:p.slice(p.findIndex((e=>"custom-python-models"===e.id)),p.findIndex((e=>"built-in-deployment"===e.id))),minHeadingLevel:3,maxHeadingLevel:4}),"\n",(0,l.jsx)(n.h3,{id:"custom-python-models",children:"Custom Python Models"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc"})})," module provides ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.save_model",children:(0,l.jsx)(n.code,{children:"save_model()"})}),"\nand ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.log_model",children:(0,l.jsx)(n.code,{children:"log_model()"})})," utilities for creating MLflow Models with the\n",(0,l.jsx)(n.code,{children:"python_function"})," flavor that contain user-specified code and ",(0,l.jsx)(n.em,{children:"artifact"})," (file) dependencies.\nThese artifact dependencies may include serialized models produced by any Python ML library."]}),"\n",(0,l.jsxs)(n.p,{children:["Because these custom models contain the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor, they can be deployed\nto any of MLflow's supported production environments, such as SageMaker, AzureML, or local\nREST endpoints."]}),"\n",(0,l.jsxs)(n.p,{children:["The following examples demonstrate how you can use the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc"})}),"\nmodule to create custom Python models. For additional information about model customization with MLflow's\n",(0,l.jsx)(n.code,{children:"python_function"})," utilities, see the ",(0,l.jsx)(r.A,{target:"_blank",to:"/api_reference/python_api/mlflow.pyfunc.html#pyfunc-create-custom",children:"python_function custom models documentation"}),"."]}),"\n",(0,l.jsx)(n.h4,{id:"example-creating-a-model-with-type-hints",children:"Example: Creating a model with type hints"}),"\n",(0,l.jsxs)(n.p,{children:["This example demonstrates how to create a custom Python model with type hints, enabling MLflow to perform data\nvalidation based on the type hints specified for the model input. For additional information about\nPythonModel type hints support, see the ",(0,l.jsx)(n.a,{href:"../model/python_model#type-hint-usage-in-pythonmodel",children:"PythonModel Type Hints Guide"}),"."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsx)(n.p,{children:"PythonModel with type hints supports data validation starting from MLflow version 2.20.0."})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import pydantic\nimport mlflow\nfrom mlflow.pyfunc import PythonModel\n\n\n# Define the pydantic model input\nclass Message(pydantic.BaseModel):\n    role: str\n    content: str\n\n\nclass CustomModel(PythonModel):\n    # Define the model_input type hint\n    # NB: it must be list[...], check the python model type hints guide for more information\n    def predict(self, model_input: list[Message], params=None) -> list[str]:\n        return [m.content for m in model_input]\n\n\n# Construct the model and test\nmodel = CustomModel()\n\n# The input_example can be a list of Message objects as defined in the type hint\ninput_example = [\n    Message(role="system", content="Hello"),\n    Message(role="user", content="Hi"),\n]\nassert model.predict(input_example) == ["Hello", "Hi"]\n\n# The input example can also be a list of dictionaries that match the Message schema\ninput_example = [\n    {"role": "system", "content": "Hello"},\n    {"role": "user", "content": "Hi"},\n]\nassert model.predict(input_example) == ["Hello", "Hi"]\n\n# Log the model\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="model",\n        python_model=model,\n        input_example=input_example,\n    )\n\n# Load the model as pyfunc\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nassert pyfunc_model.predict(input_example) == ["Hello", "Hi"]\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-creating-a-custom-add-n-model",children:"Example: Creating a custom \u201cadd n\u201d model"}),"\n",(0,l.jsxs)(n.p,{children:["This example defines a class for a custom model that adds a specified numeric value, ",(0,l.jsx)(n.code,{children:"n"}),", to all\ncolumns of a Pandas DataFrame input. Then, it uses the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc",children:(0,l.jsx)(n.code,{children:"mlflow.pyfunc"})}),"\nAPIs to save an instance of this model with ",(0,l.jsx)(n.code,{children:"n = 5"})," in MLflow Model format. Finally, it loads the model in\n",(0,l.jsx)(n.code,{children:"python_function"})," format and uses it to evaluate a sample input."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow.pyfunc\n\n\n# Define the model class\nclass AddN(mlflow.pyfunc.PythonModel):\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input, params=None):\n        return model_input.apply(lambda column: column + self.n)\n\n\n# Construct and save the model\nmodel_path = "add_n_model"\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n\n# Evaluate the model\nimport pandas as pd\n\nmodel_input = pd.DataFrame([range(10)])\nmodel_output = loaded_model.predict(model_input)\nassert model_output.equals(pd.DataFrame([range(5, 15)]))\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-saving-an-xgboost-model-in-mlflow-format",children:"Example: Saving an XGBoost model in MLflow format"}),"\n",(0,l.jsxs)(n.p,{children:["This example begins by training and saving a gradient boosted tree model using the XGBoost\nlibrary. Next, it defines a wrapper class around the XGBoost model that conforms to MLflow's\n",(0,l.jsx)(n.code,{children:"python_function"})," ",(0,l.jsx)(r.A,{target:"_blank",to:"/api_reference/python_api/mlflow.pyfunc.html#pyfunc-inference-api",children:"inference API"}),".\nThen, it uses the wrapper class and the saved XGBoost model to construct an MLflow Model that performs inference using the gradient\nboosted tree. Finally, it loads the MLflow Model in ",(0,l.jsx)(n.code,{children:"python_function"})," format and uses it to\nevaluate test data."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Load training and test datasets\nfrom sys import version_info\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nPYTHON_VERSION = f"{version_info.major}.{version_info.minor}.{version_info.micro}"\niris = datasets.load_iris()\nx = iris.data[:, 2:]\ny = iris.target\nx_train, x_test, y_train, _ = train_test_split(x, y, test_size=0.2, random_state=42)\ndtrain = xgb.DMatrix(x_train, label=y_train)\n\n# Train and save an XGBoost model\nxgb_model = xgb.train(params={"max_depth": 10}, dtrain=dtrain, num_boost_round=10)\nxgb_model_path = "xgb_model.pth"\nxgb_model.save_model(xgb_model_path)\n\n# Create an `artifacts` dictionary that assigns a unique name to the saved XGBoost model file.\n# This dictionary will be passed to `mlflow.pyfunc.save_model`, which will copy the model file\n# into the new MLflow Model\'s directory.\nartifacts = {"xgb_model": xgb_model_path}\n\n# Define the model class\nimport mlflow.pyfunc\n\n\nclass XGBWrapper(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        import xgboost as xgb\n\n        self.xgb_model = xgb.Booster()\n        self.xgb_model.load_model(context.artifacts["xgb_model"])\n\n    def predict(self, context, model_input, params=None):\n        input_matrix = xgb.DMatrix(model_input.values)\n        return self.xgb_model.predict(input_matrix)\n\n\n# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.\nimport cloudpickle\n\nconda_env = {\n    "channels": ["defaults"],\n    "dependencies": [\n        f"python={PYTHON_VERSION}",\n        "pip",\n        {\n            "pip": [\n                f"mlflow=={mlflow.__version__}",\n                f"xgboost=={xgb.__version__}",\n                f"cloudpickle=={cloudpickle.__version__}",\n            ],\n        },\n    ],\n    "name": "xgb_env",\n}\n\n# Save the MLflow Model\nmlflow_pyfunc_model_path = "xgb_mlflow_pyfunc"\nmlflow.pyfunc.save_model(\n    path=mlflow_pyfunc_model_path,\n    python_model=XGBWrapper(),\n    artifacts=artifacts,\n    conda_env=conda_env,\n)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n\n# Evaluate the model\nimport pandas as pd\n\ntest_predictions = loaded_model.predict(pd.DataFrame(x_test))\nprint(test_predictions)\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files",children:"Example: Logging a transformers model with hf:/ schema to avoid copying large files"}),"\n",(0,l.jsxs)(n.p,{children:["This example shows how to use a special schema ",(0,l.jsx)(n.code,{children:"hf:/"})," to log a transformers model from huggingface\nhub directly. This is useful when the model is too large and especially when you want to serve the\nmodel directly, but it doesn't save extra space if you want to download and test the model locally."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models import infer_signature\nimport numpy as np\nimport transformers\n\n\n# Define a custom PythonModel\nclass QAModel(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """\n        This method initializes the tokenizer and language model\n        using the specified snapshot location from model context.\n        """\n        snapshot_location = context.artifacts["bert-tiny-model"]\n        # Initialize tokenizer and language model\n        tokenizer = transformers.AutoTokenizer.from_pretrained(snapshot_location)\n        model = transformers.BertForQuestionAnswering.from_pretrained(snapshot_location)\n        self.pipeline = transformers.pipeline(\n            task="question-answering", model=model, tokenizer=tokenizer\n        )\n\n    def predict(self, context, model_input, params=None):\n        question = model_input["question"][0]\n        if isinstance(question, np.ndarray):\n            question = question.item()\n        ctx = model_input["context"][0]\n        if isinstance(ctx, np.ndarray):\n            ctx = ctx.item()\n        return self.pipeline(question=question, context=ctx)\n\n\n# Log the model\ndata = {"question": "Who\'s house?", "context": "The house is owned by Run."}\npyfunc_artifact_path = "question_answering_model"\nwith mlflow.start_run() as run:\n    model_info = mlflow.pyfunc.log_model(\n        name=pyfunc_artifact_path,\n        python_model=QAModel(),\n        artifacts={"bert-tiny-model": "hf:/prajjwal1/bert-tiny"},\n        input_example=data,\n        signature=infer_signature(data, ["Run"]),\n        extra_pip_requirements=["torch", "accelerate", "transformers", "numpy"],\n    )\n'})}),"\n",(0,l.jsx)(n.h3,{id:"custom-flavors",children:"Custom Flavors"}),"\n",(0,l.jsxs)(n.p,{children:["You can also create custom MLflow Models by writing a custom ",(0,l.jsx)(n.em,{children:"flavor"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["As discussed in the ",(0,l.jsx)(n.a,{href:"#model-api",children:"Model API"})," and ",(0,l.jsx)(n.a,{href:"#storage-format",children:"Storage Format"})," sections, an MLflow Model\nis defined by a directory of files that contains an ",(0,l.jsx)(n.code,{children:"MLmodel"})," configuration file. This ",(0,l.jsx)(n.code,{children:"MLmodel"}),"\nfile describes various model attributes, including the flavors in which the model can be\ninterpreted. The ",(0,l.jsx)(n.code,{children:"MLmodel"})," file contains an entry for each flavor name; each entry is\na YAML-formatted collection of flavor-specific attributes."]}),"\n",(0,l.jsxs)(n.p,{children:["To create a new flavor to support a custom model, you define the set of flavor-specific attributes\nto include in the ",(0,l.jsx)(n.code,{children:"MLmodel"})," configuration file, as well as the code that can interpret the\ncontents of the model directory and the flavor's attributes. A detailed example of constructing a\ncustom model flavor and its usage is shown below. New custom flavors not considered for official\ninclusion into MLflow should be introduced as separate GitHub repositories with documentation\nprovided in the ",(0,l.jsx)(n.a,{href:"/community-model-flavors",children:"Community Model Flavors"})," page."]}),"\n",(0,l.jsx)(n.h4,{id:"example-creating-a-custom-sktime-flavor",children:'Example: Creating a custom "sktime" flavor'}),"\n",(0,l.jsxs)(n.p,{children:["This example illustrates the creation of a custom flavor for\n",(0,l.jsx)(n.a,{href:"https://github.com/sktime/sktime",children:"sktime"})," time series library. The library provides a unified\ninterface for multiple learning tasks including time series forecasting. While the custom flavor in\nthis example is specific in terms of the ",(0,l.jsx)(n.code,{children:"sktime"})," inference API and model serialization format,\nits interface design is similar to many of the existing built-in flavors. Particularly, the\ninterface for utilizing the custom model loaded as a ",(0,l.jsx)(n.code,{children:"python_function"})," flavor for generating\npredictions uses a ",(0,l.jsx)(n.em,{children:"single-row"})," ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," configuration argument to expose the parameters\nof the ",(0,l.jsx)(n.code,{children:"sktime"})," inference API. The complete code for this example is included in the\n",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/tree/master/examples/sktime/flavor.py",children:"flavor.py"})," module of the\n",(0,l.jsx)(n.code,{children:"sktime"})," example directory."]}),"\n",(0,l.jsxs)(n.p,{children:["Let's examine the custom flavor module in more detail. The first step is to import several modules\nincluding ",(0,l.jsx)(n.code,{children:"sktime"})," library, various MLflow utilities as well as the MLflow ",(0,l.jsx)(n.code,{children:"pyfunc"})," module which\nis required to add the ",(0,l.jsx)(n.code,{children:"pyfunc"})," specification to the MLflow model configuration. Note also the import\nof the ",(0,l.jsx)(n.code,{children:"flavor"})," module itself. This will be passed to the ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.log"}),"\nmethod to log the model as an artifact to the current MLflow run."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import logging\nimport os\nimport pickle\n\nimport flavor\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport sktime\nimport yaml\nfrom mlflow import pyfunc\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model\nfrom mlflow.models.model import MLMODEL_FILE_NAME\nfrom mlflow.models.utils import _save_example\nfrom mlflow.protos.databricks_pb2 import INTERNAL_ERROR, INVALID_PARAMETER_VALUE\nfrom mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS\nfrom mlflow.tracking.artifact_utils import _download_artifact_from_uri\nfrom mlflow.utils.environment import (\n    _CONDA_ENV_FILE_NAME,\n    _CONSTRAINTS_FILE_NAME,\n    _PYTHON_ENV_FILE_NAME,\n    _REQUIREMENTS_FILE_NAME,\n    _mlflow_conda_env,\n    _process_conda_env,\n    _process_pip_requirements,\n    _PythonEnv,\n    _validate_env_arguments,\n)\nfrom mlflow.utils.file_utils import write_to\nfrom mlflow.utils.model_utils import (\n    _add_code_from_conf_to_system_path,\n    _get_flavor_configuration,\n    _validate_and_copy_code_paths,\n    _validate_and_prepare_target_save_path,\n)\nfrom mlflow.utils.requirements_utils import _get_pinned_requirement\nfrom sktime.utils.multiindex import flatten_multiindex\n\n_logger = logging.getLogger(__name__)\n"})}),"\n",(0,l.jsx)(n.p,{children:"We continue by defining a set of important variables used throughout the code that follows."}),"\n",(0,l.jsxs)(n.p,{children:["The flavor name needs to be provided for every custom flavor and should reflect the name of the\nlibrary to be supported. It is saved as part of the flavor-specific attributes to the ",(0,l.jsx)(n.code,{children:"MLmodel"}),"\nconfiguration file. This example also defines some ",(0,l.jsx)(n.code,{children:"sktime"})," specific variables. For illustration\npurposes, only a subset of the available predict methods to be exposed via the\n",(0,l.jsx)(n.code,{children:"_SktimeModelWrapper"})," class is included when loading the model in its ",(0,l.jsx)(n.code,{children:"python_function"})," flavor\n(additional methods could be added in a similar fashion). Additionally, the model serialization\nformats, namely ",(0,l.jsx)(n.code,{children:"pickle"})," (default) and ",(0,l.jsx)(n.code,{children:"cloudpickle"}),", are defined. Note that both serialization\nmodules require using the same python environment (version) in whatever environment this model is\nused for inference to ensure that the model will load with the appropriate version of\npickle (cloudpickle)."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'FLAVOR_NAME = "sktime"\n\nSKTIME_PREDICT = "predict"\nSKTIME_PREDICT_INTERVAL = "predict_interval"\nSKTIME_PREDICT_QUANTILES = "predict_quantiles"\nSKTIME_PREDICT_VAR = "predict_var"\nSUPPORTED_SKTIME_PREDICT_METHODS = [\n    SKTIME_PREDICT,\n    SKTIME_PREDICT_INTERVAL,\n    SKTIME_PREDICT_QUANTILES,\n    SKTIME_PREDICT_VAR,\n]\n\nSERIALIZATION_FORMAT_PICKLE = "pickle"\nSERIALIZATION_FORMAT_CLOUDPICKLE = "cloudpickle"\nSUPPORTED_SERIALIZATION_FORMATS = [\n    SERIALIZATION_FORMAT_PICKLE,\n    SERIALIZATION_FORMAT_CLOUDPICKLE,\n]\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Similar to the MLflow built-in flavors, a custom flavor logs the model in MLflow format via the\n",(0,l.jsx)(n.code,{children:"save_model()"})," and ",(0,l.jsx)(n.code,{children:"log_model()"})," functions. In the ",(0,l.jsx)(n.code,{children:"save_model()"})," function, the ",(0,l.jsx)(n.code,{children:"sktime"}),"\nmodel is saved to a specified output directory. Additionally, ",(0,l.jsx)(n.code,{children:"save_model()"})," leverages\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.add_flavor"})," and ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.save"})," methods to\nproduce the ",(0,l.jsx)(n.code,{children:"MLmodel"})," configuration containing the ",(0,l.jsx)(n.code,{children:"sktime"})," and the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor.\nThe resulting configuration has several flavor-specific attributes, such as the flavor name and\n",(0,l.jsx)(n.code,{children:"sktime_version"}),", which denotes the version of the ",(0,l.jsx)(n.code,{children:"sktime"})," library that was used to train the\nmodel. An example of the output directory for the custom ",(0,l.jsx)(n.code,{children:"sktime"})," model is shown below:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'# Directory written by flavor.save_model(model, "my_model")\nmy_model/\n\u251c\u2500\u2500 MLmodel\n\u251c\u2500\u2500 conda.yaml\n\u251c\u2500\u2500 model.pkl\n\u251c\u2500\u2500 python_env.yaml\n\u2514\u2500\u2500 requirements.txt\n'})}),"\n",(0,l.jsxs)(n.p,{children:["And its YAML-formatted ",(0,l.jsx)(n.code,{children:"MLmodel"})," file describes the two flavors:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:"flavors:\n  python_function:\n    env:\n      conda: conda.yaml\n      virtualenv: python_env.yaml\n    loader_module: flavor\n    model_path: model.pkl\n    python_version: 3.8.15\n  sktime:\n    code: null\n    pickled_model: model.pkl\n    serialization_format: pickle\n    sktime_version: 0.16.0\n"})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"save_model()"})," function also provides flexibility to add additional parameters which can be\nadded as flavor-specific attributes to the model configuration. In this example there is only one\nflavor-specific parameter for specifying the model serialization format. All other parameters are\nnon-flavor specific (for a detailed description of these parameters take a look\nat ",(0,l.jsx)(s.B,{fn:"mlflow.sklearn.save_model",children:"mlflow.sklearn.save_model"}),").\nNote: When creating your own custom flavor, be sure rename the ",(0,l.jsx)(n.code,{children:"sktime_model"})," parameter in both the\n",(0,l.jsx)(n.code,{children:"save_model()"})," and ",(0,l.jsx)(n.code,{children:"log_model()"})," functions to reflect the name of your custom model flavor."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def save_model(\n    sktime_model,\n    path,\n    conda_env=None,\n    code_paths=None,\n    mlflow_model=None,\n    signature=None,\n    input_example=None,\n    pip_requirements=None,\n    extra_pip_requirements=None,\n    serialization_format=SERIALIZATION_FORMAT_PICKLE,\n):\n    _validate_env_arguments(conda_env, pip_requirements, extra_pip_requirements)\n\n    if serialization_format not in SUPPORTED_SERIALIZATION_FORMATS:\n        raise MlflowException(\n            message=(\n                f"Unrecognized serialization format: {serialization_format}. "\n                "Please specify one of the following supported formats: "\n                "{SUPPORTED_SERIALIZATION_FORMATS}."\n            ),\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    _validate_and_prepare_target_save_path(path)\n    code_dir_subpath = _validate_and_copy_code_paths(code_paths, path)\n\n    if mlflow_model is None:\n        mlflow_model = Model()\n    if signature is not None:\n        mlflow_model.signature = signature\n    if input_example is not None:\n        _save_example(mlflow_model, input_example, path)\n\n    model_data_subpath = "model.pkl"\n    model_data_path = os.path.join(path, model_data_subpath)\n    _save_model(\n        sktime_model, model_data_path, serialization_format=serialization_format\n    )\n\n    pyfunc.add_to_model(\n        mlflow_model,\n        loader_module="flavor",\n        model_path=model_data_subpath,\n        conda_env=_CONDA_ENV_FILE_NAME,\n        python_env=_PYTHON_ENV_FILE_NAME,\n        code=code_dir_subpath,\n    )\n\n    mlflow_model.add_flavor(\n        FLAVOR_NAME,\n        pickled_model=model_data_subpath,\n        sktime_version=sktime.__version__,\n        serialization_format=serialization_format,\n        code=code_dir_subpath,\n    )\n    mlflow_model.save(os.path.join(path, MLMODEL_FILE_NAME))\n\n    if conda_env is None:\n        if pip_requirements is None:\n            include_cloudpickle = (\n                serialization_format == SERIALIZATION_FORMAT_CLOUDPICKLE\n            )\n            default_reqs = get_default_pip_requirements(include_cloudpickle)\n            inferred_reqs = mlflow.models.infer_pip_requirements(\n                path, FLAVOR_NAME, fallback=default_reqs\n            )\n            default_reqs = sorted(set(inferred_reqs).union(default_reqs))\n        else:\n            default_reqs = None\n        conda_env, pip_requirements, pip_constraints = _process_pip_requirements(\n            default_reqs, pip_requirements, extra_pip_requirements\n        )\n    else:\n        conda_env, pip_requirements, pip_constraints = _process_conda_env(conda_env)\n\n    with open(os.path.join(path, _CONDA_ENV_FILE_NAME), "w") as f:\n        yaml.safe_dump(conda_env, stream=f, default_flow_style=False)\n\n    if pip_constraints:\n        write_to(os.path.join(path, _CONSTRAINTS_FILE_NAME), "\\n".join(pip_constraints))\n\n    write_to(os.path.join(path, _REQUIREMENTS_FILE_NAME), "\\n".join(pip_requirements))\n\n    _PythonEnv.current().to_yaml(os.path.join(path, _PYTHON_ENV_FILE_NAME))\n\n\ndef _save_model(model, path, serialization_format):\n    with open(path, "wb") as out:\n        if serialization_format == SERIALIZATION_FORMAT_PICKLE:\n            pickle.dump(model, out)\n        else:\n            import cloudpickle\n\n            cloudpickle.dump(model, out)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"save_model()"})," function also writes the model dependencies to a ",(0,l.jsx)(n.code,{children:"requirements.txt"})," and\n",(0,l.jsx)(n.code,{children:"conda.yaml"})," file in the model output directory. For this purpose the set of ",(0,l.jsx)(n.code,{children:"pip"})," dependencies\nproduced by this flavor need to be added to the ",(0,l.jsx)(n.code,{children:"get_default_pip_requirements()"})," function. In this\nexample only the minimum required dependencies are provided. In practice, additional requirements needed for\npreprocessing or post-processing steps could be included. Note that for any custom flavor,\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.models.infer_pip_requirements"})," method in the ",(0,l.jsx)(n.code,{children:"save_model()"})," function will\nreturn the default requirements defined in ",(0,l.jsx)(n.code,{children:"get_default_pip_requirements()"})," as package imports are\nonly inferred for built-in flavors."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def get_default_pip_requirements(include_cloudpickle=False):\n    pip_deps = [_get_pinned_requirement("sktime")]\n    if include_cloudpickle:\n        pip_deps += [_get_pinned_requirement("cloudpickle")]\n\n    return pip_deps\n\n\ndef get_default_conda_env(include_cloudpickle=False):\n    return _mlflow_conda_env(\n        additional_pip_deps=get_default_pip_requirements(include_cloudpickle)\n    )\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Next, we add the ",(0,l.jsx)(n.code,{children:"log_model()"})," function. This function is little more than a wrapper around\nthe ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.log"})," method to enable logging our custom model as an artifact to the\ncurrent MLflow run. Any flavor-specific parameters (e.g. ",(0,l.jsx)(n.code,{children:"serialization_format"}),") introduced in the\n",(0,l.jsx)(n.code,{children:"save_model()"})," function also need to be added in the ",(0,l.jsx)(n.code,{children:"log_model()"})," function. We also need to\npass the ",(0,l.jsx)(n.code,{children:"flavor"})," module to the ",(0,l.jsx)(s.B,{fn:"mlflow.models.Model.log"})," method which internally calls\nthe ",(0,l.jsx)(n.code,{children:"save_model()"})," function from above to persist the model."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"def log_model(\n    sktime_model,\n    artifact_path,\n    conda_env=None,\n    code_paths=None,\n    registered_model_name=None,\n    signature=None,\n    input_example=None,\n    await_registration_for=DEFAULT_AWAIT_MAX_SLEEP_SECONDS,\n    pip_requirements=None,\n    extra_pip_requirements=None,\n    serialization_format=SERIALIZATION_FORMAT_PICKLE,\n    **kwargs,\n):\n    return Model.log(\n        artifact_path=artifact_path,\n        flavor=flavor,\n        registered_model_name=registered_model_name,\n        sktime_model=sktime_model,\n        conda_env=conda_env,\n        code_paths=code_paths,\n        signature=signature,\n        input_example=input_example,\n        await_registration_for=await_registration_for,\n        pip_requirements=pip_requirements,\n        extra_pip_requirements=extra_pip_requirements,\n        serialization_format=serialization_format,\n        **kwargs,\n    )\n"})}),"\n",(0,l.jsxs)(n.p,{children:["To interpret model directories produced by ",(0,l.jsx)(n.code,{children:"save_model()"}),", the custom flavor must also define a\n",(0,l.jsx)(n.code,{children:"load_model()"})," function. The ",(0,l.jsx)(n.code,{children:"load_model()"})," function reads the ",(0,l.jsx)(n.code,{children:"MLmodel"})," configuration from\nthe specified model directory and uses the configuration attributes to load and return the\n",(0,l.jsx)(n.code,{children:"sktime"})," model from its serialized representation."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def load_model(model_uri, dst_path=None):\n    local_model_path = _download_artifact_from_uri(\n        artifact_uri=model_uri, output_path=dst_path\n    )\n    flavor_conf = _get_flavor_configuration(\n        model_path=local_model_path, flavor_name=FLAVOR_NAME\n    )\n    _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n    sktime_model_file_path = os.path.join(\n        local_model_path, flavor_conf["pickled_model"]\n    )\n    serialization_format = flavor_conf.get(\n        "serialization_format", SERIALIZATION_FORMAT_PICKLE\n    )\n    return _load_model(\n        path=sktime_model_file_path, serialization_format=serialization_format\n    )\n\n\ndef _load_model(path, serialization_format):\n    with open(path, "rb") as pickled_model:\n        if serialization_format == SERIALIZATION_FORMAT_PICKLE:\n            return pickle.load(pickled_model)\n        elif serialization_format == SERIALIZATION_FORMAT_CLOUDPICKLE:\n            import cloudpickle\n\n            return cloudpickle.load(pickled_model)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"_load_pyfunc()"})," function will be called by the ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"})," method\nto load the custom model flavor as a ",(0,l.jsx)(n.code,{children:"pyfunc"})," type. The MLmodel flavor configuration is used to\npass any flavor-specific attributes to the ",(0,l.jsx)(n.code,{children:"_load_model()"})," function (i.e., the path to the\n",(0,l.jsx)(n.code,{children:"python_function"})," flavor in the model directory and the model serialization format)."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'def _load_pyfunc(path):\n    try:\n        sktime_flavor_conf = _get_flavor_configuration(\n            model_path=path, flavor_name=FLAVOR_NAME\n        )\n        serialization_format = sktime_flavor_conf.get(\n            "serialization_format", SERIALIZATION_FORMAT_PICKLE\n        )\n    except MlflowException:\n        _logger.warning(\n            "Could not find sktime flavor configuration during model "\n            "loading process. Assuming \'pickle\' serialization format."\n        )\n        serialization_format = SERIALIZATION_FORMAT_PICKLE\n\n    pyfunc_flavor_conf = _get_flavor_configuration(\n        model_path=path, flavor_name=pyfunc.FLAVOR_NAME\n    )\n    path = os.path.join(path, pyfunc_flavor_conf["model_path"])\n\n    return _SktimeModelWrapper(\n        _load_model(path, serialization_format=serialization_format)\n    )\n'})}),"\n",(0,l.jsxs)(n.p,{children:["The final step is to create the model wrapper class defining the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor. The\ndesign of the wrapper class determines how the flavor's inference API is exposed when making\npredictions using the ",(0,l.jsx)(n.code,{children:"python_function"})," flavor. Just like the built-in flavors, the ",(0,l.jsx)(n.code,{children:"predict()"}),"\nmethod of the ",(0,l.jsx)(n.code,{children:"sktime"})," wrapper class accepts a ",(0,l.jsx)(n.em,{children:"single-row"})," ",(0,l.jsx)(n.code,{children:"Pandas DataFrame"})," configuration\nargument. For an example of how to construct this configuration DataFrame refer to the usage example\nin the next section. A detailed description of the supported parameters and input formats is\nprovided in the ",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/tree/master/examples/sktime/flavor.py",children:"flavor.py"})," module\ndocstrings."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'class _SktimeModelWrapper:\n    def __init__(self, sktime_model):\n        self.sktime_model = sktime_model\n\n    def predict(self, dataframe, params=None) -> pd.DataFrame:\n        df_schema = dataframe.columns.values.tolist()\n\n        if len(dataframe) > 1:\n            raise MlflowException(\n                f"The provided prediction pd.DataFrame contains {len(dataframe)} rows. "\n                "Only 1 row should be supplied.",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n        # Convert the configuration dataframe into a dictionary to simplify the\n        # extraction of parameters passed to the sktime predcition methods.\n        attrs = dataframe.to_dict(orient="index").get(0)\n        predict_method = attrs.get("predict_method")\n\n        if not predict_method:\n            raise MlflowException(\n                f"The provided prediction configuration pd.DataFrame columns ({df_schema}) do not "\n                "contain the required column `predict_method` for specifying the prediction method.",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n        if predict_method not in SUPPORTED_SKTIME_PREDICT_METHODS:\n            raise MlflowException(\n                "Invalid `predict_method` value."\n                f"The supported prediction methods are {SUPPORTED_SKTIME_PREDICT_METHODS}",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n        # For inference parameters \'fh\', \'X\', \'coverage\', \'alpha\', and \'cov\'\n        # the respective sktime default value is used if the value was not\n        # provided in the configuration dataframe.\n        fh = attrs.get("fh", None)\n\n        # Any model that is trained with exogenous regressor elements will need\n        # to provide `X` entries as a numpy ndarray to the predict method.\n        X = attrs.get("X", None)\n\n        # When the model is served via REST API the exogenous regressor must be\n        # provided as a list to the configuration DataFrame to be JSON serializable.\n        # Below we convert the list back to ndarray type as required by sktime\n        # predict methods.\n        if isinstance(X, list):\n            X = np.array(X)\n\n        # For illustration purposes only a subset of the available sktime prediction\n        # methods is exposed. Additional methods (e.g. predict_proba) could be added\n        # in a similar fashion.\n        if predict_method == SKTIME_PREDICT:\n            predictions = self.sktime_model.predict(fh=fh, X=X)\n\n        if predict_method == SKTIME_PREDICT_INTERVAL:\n            coverage = attrs.get("coverage", 0.9)\n            predictions = self.sktime_model.predict_interval(\n                fh=fh, X=X, coverage=coverage\n            )\n\n        if predict_method == SKTIME_PREDICT_QUANTILES:\n            alpha = attrs.get("alpha", None)\n            predictions = self.sktime_model.predict_quantiles(fh=fh, X=X, alpha=alpha)\n\n        if predict_method == SKTIME_PREDICT_VAR:\n            cov = attrs.get("cov", False)\n            predictions = self.sktime_model.predict_var(fh=fh, X=X, cov=cov)\n\n        # Methods predict_interval() and predict_quantiles() return a pandas\n        # MultiIndex column structure. As MLflow signature inference does not\n        # support MultiIndex column structure the columns must be flattened.\n        if predict_method in [SKTIME_PREDICT_INTERVAL, SKTIME_PREDICT_QUANTILES]:\n            predictions.columns = flatten_multiindex(predictions)\n\n        return predictions\n'})}),"\n",(0,l.jsx)(n.h4,{id:"example-using-the-custom-sktime-flavor",children:'Example: Using the custom "sktime" flavor'}),"\n",(0,l.jsxs)(n.p,{children:["This example trains a ",(0,l.jsx)(n.code,{children:"sktime"})," NaiveForecaster model using the Longley dataset for forecasting\nwith exogenous variables. It shows a custom model type implementation that logs the training\nhyper-parameters, evaluation metrics and the trained model as an artifact. The ",(0,l.jsx)(n.em,{children:"single-row"}),"\nconfiguration DataFrame for this example defines an interval forecast with nominal coverage values\n",(0,l.jsx)(n.code,{children:"[0.9,0.95]"}),", a future forecast horizon of four periods, and an exogenous regressor."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import json\n\nimport flavor\nimport pandas as pd\nfrom sktime.datasets import load_longley\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.performance_metrics.forecasting import (\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n)\n\nimport mlflow\n\nARTIFACT_PATH = "model"\n\nwith mlflow.start_run() as run:\n    y, X = load_longley()\n    y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n\n    forecaster = NaiveForecaster()\n    forecaster.fit(\n        y_train,\n        X=X_train,\n    )\n\n    # Extract parameters\n    parameters = forecaster.get_params()\n\n    # Evaluate model\n    y_pred = forecaster.predict(fh=[1, 2, 3, 4], X=X_test)\n    metrics = {\n        "mae": mean_absolute_error(y_test, y_pred),\n        "mape": mean_absolute_percentage_error(y_test, y_pred),\n    }\n\n    print(f"Parameters: \\n{json.dumps(parameters, indent=2)}")\n    print(f"Metrics: \\n{json.dumps(metrics, indent=2)}")\n\n    # Log parameters and metrics\n    mlflow.log_params(parameters)\n    mlflow.log_metrics(metrics)\n\n    # Log model using custom model flavor with pickle serialization (default).\n    flavor.log_model(\n        sktime_model=forecaster,\n        artifact_path=ARTIFACT_PATH,\n        serialization_format="pickle",\n    )\n    model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)\n\n# Load model in native sktime flavor and pyfunc flavor\nloaded_model = flavor.load_model(model_uri=model_uri)\nloaded_pyfunc = flavor.pyfunc.load_model(model_uri=model_uri)\n\n# Convert test data to 2D numpy array so it can be passed to pyfunc predict using\n# a single-row Pandas DataFrame configuration argument\nX_test_array = X_test.to_numpy()\n\n# Create configuration DataFrame\npredict_conf = pd.DataFrame(\n    [\n        {\n            "fh": [1, 2, 3, 4],\n            "predict_method": "predict_interval",\n            "coverage": [0.9, 0.95],\n            "X": X_test_array,\n        }\n    ]\n)\n\n# Generate interval forecasts with native sktime flavor and pyfunc flavor\nprint(\n    f"\\nNative sktime \'predict_interval\':\\n${loaded_model.predict_interval(fh=[1, 2, 3], X=X_test, coverage=[0.9, 0.95])}"\n)\nprint(f"\\nPyfunc \'predict_interval\':\\n${loaded_pyfunc.predict(predict_conf)}")\n\n# Print the run id which is used for serving the model to a local REST API endpoint\nprint(f"\\nMLflow run id:\\n{run.info.run_id}")\n'})}),"\n",(0,l.jsx)(n.p,{children:"When opening the MLflow runs detail page the serialized model artifact will show up, such as:"}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(53631).A+"",width:"1881",height:"747"})}),"\n",(0,l.jsxs)(n.p,{children:["To serve the model to a local REST API endpoint run the following MLflow CLI command substituting\nthe run id printed during execution of the previous block (for more details refer to the\n",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/models.html#deploy-mlflow-models",children:"Deploy MLflow models"})," section):"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n"})}),"\n",(0,l.jsxs)(n.p,{children:["An example of requesting a prediction from the served model is shown below. The exogenous regressor\nneeds to be provided as a list to be JSON serializable. The wrapper instance will convert the list\nback to ",(0,l.jsx)(n.code,{children:"numpy ndarray"})," type as required by ",(0,l.jsx)(n.code,{children:"sktime"})," inference API."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import pandas as pd\nimport requests\n\nfrom sktime.datasets import load_longley\nfrom sktime.forecasting.model_selection import temporal_train_test_split\n\ny, X = load_longley()\ny_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n\n# Define local host and endpoint url\nhost = "127.0.0.1"\nurl = f"http://{host}:5000/invocations"\n\n# Create configuration DataFrame\nX_test_list = X_test.to_numpy().tolist()\npredict_conf = pd.DataFrame(\n    [\n        {\n            "fh": [1, 2, 3, 4],\n            "predict_method": "predict_interval",\n            "coverage": [0.9, 0.95],\n            "X": X_test_list,\n        }\n    ]\n)\n\n# Create dictionary with pandas DataFrame in the split orientation\njson_data = {"dataframe_split": predict_conf.to_dict(orient="split")}\n\n# Score model\nresponse = requests.post(url, json=json_data)\nprint(f"\\nPyfunc \'predict_interval\':\\n${response.json()}")\n'})}),"\n",(0,l.jsx)(n.h2,{id:"automatic-model-dependency-locking-experimental",children:"Automatic Model Dependency Locking (Experimental)"}),"\n",(0,l.jsxs)(n.p,{children:["By default, MLflow only pins the direct dependencies of your model (for example, ",(0,l.jsx)(n.code,{children:"scikit-learn"})," for a ",(0,l.jsx)(n.code,{children:"scikit-learn"})," model), but does ",(0,l.jsx)(n.strong,{children:"not"})," lock transitive dependencies (such as ",(0,l.jsx)(n.code,{children:"numpy"})," installed by ",(0,l.jsx)(n.code,{children:"scikit-learn"}),"). This can sometimes lead to inconsistencies when deploying or sharing models, as transitive dependencies may change over time."]}),"\n",(0,l.jsxs)(n.p,{children:["To ensure fully reproducible environments, MLflow supports ",(0,l.jsx)(n.strong,{children:"automatic dependency locking"}),". When enabled, MLflow will lock both direct and transitive dependencies, generating a complete, deterministic set of package versions for your model. Here's an example of ",(0,l.jsx)(n.code,{children:"requirements.txt"})," generated with dependency locking enabled:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"# Original requirements\n# mlflow==3.0.0\n# cloudpickle==3.1.0\n#\n# Locked requirements\nalembic==1.16.1\nannotated-types==0.7.0\nanyio==4.9.0\nblinker==1.9.0\ncachetools==5.5.2\ncertifi==2025.4.26\n...\n"})}),"\n",(0,l.jsx)(n.h3,{id:"how-to-enable-dependency-locking",children:"How to Enable Dependency Locking"}),"\n",(0,l.jsxs)(n.p,{children:["Set the environment variable ",(0,l.jsx)(n.code,{children:"MLFLOW_LOCK_MODEL_DEPENDENCIES"})," to ",(0,l.jsx)(n.code,{children:"true"})," before logging your model:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-sh",children:"export MLFLOW_LOCK_MODEL_DEPENDENCIES=true\n"})}),"\n",(0,l.jsxs)(n.p,{children:["With this setting, MLflow will use ",(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/",children:"uv"})," to resolve and lock all dependencies when saving or logging a model."]}),"\n",(0,l.jsxs)(n.h3,{id:"installing-uv",children:["Installing ",(0,l.jsx)(n.code,{children:"uv"})]}),"\n",(0,l.jsxs)(n.p,{children:["If ",(0,l.jsx)(n.code,{children:"uv"})," is not already installed, you can add it with:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-sh",children:"pip install uv\n"})}),"\n",(0,l.jsxs)(n.p,{children:["For other installation options, see the ",(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/getting-started/installation",children:"uv installation guide"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"validate-models-before-deployment",children:"Validate Models before Deployment"}),"\n",(0,l.jsxs)(n.p,{children:["After logging your model with MLflow Tracking, it is highly recommended to validate the model locally before deploying it to production.\nThe ",(0,l.jsx)(s.B,{fn:"mlflow.models.predict"})," API provides a convenient way to test your model in a virtual environment, offering isolated execution and several advantages:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Model dependencies validation: The API helps ensure that the dependencies logged with the model are correct and sufficient by executing the model with an input example in a virtual environment.\nFor more details, refer to ",(0,l.jsx)(n.a,{href:"/model/dependencies/#validating-environment-for-prediction",children:"Validating Environment for Prediction"}),"."]}),"\n",(0,l.jsx)(n.li,{children:"Input data validation: The API can be used to validate the input data interacts with the model as expected by simulating the same data processing during model serving.\nEnsure that the input data is a valid example that aligns with the pyfunc model\u2019s predict function requirements."}),"\n",(0,l.jsxs)(n.li,{children:["Extra environment variables validation: By specifying the ",(0,l.jsx)(n.code,{children:"extra_envs"})," parameter, you can test whether additional environment variables are required for the model to run successfully.\nNote that all existing environment variables in ",(0,l.jsx)(n.code,{children:"os.environ"})," are automatically passed into the virtual environment."]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return model_input\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        name="model",\n        python_model=MyModel(),\n        input_example=["a", "b", "c"],\n    )\n\nmlflow.models.predict(\n    model_uri=model_info.model_uri,\n    input_data=["a", "b", "c"],\n    pip_requirements_override=["..."],\n    extra_envs={"MY_ENV_VAR": "my_value"},\n)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"environment-managers",children:"Environment managers"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(s.B,{fn:"mlflow.models.predict"})," API supports the following environment managers to create the virtual environment for prediction:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://virtualenv.pypa.io/en/latest/",children:"virtualenv"}),": The default environment manager."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/",children:"uv"}),": An ",(0,l.jsx)(n.strong,{children:"extremely fast"})," environment manager written in Rust. ",(0,l.jsx)(n.strong,{children:"This is an experimental feature since MLflow 2.20.0."})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://docs.conda.io/projects/conda/",children:"conda"}),": uses conda to create environment."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"local"}),": uses the current environment to run the model. Note that ",(0,l.jsx)(n.code,{children:"pip_requirements_override"})," is not supported in this mode."]}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:["Starting from MLflow 2.20.0, ",(0,l.jsx)(n.code,{children:"uv"})," is available, and ",(0,l.jsx)(n.strong,{children:"it is extremely fast"}),".\nRun ",(0,l.jsx)(n.code,{children:"pip install uv"})," to install uv, or refer to ",(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/getting-started/installation",children:"uv installation guidance"})," for other installation methods."]})}),"\n",(0,l.jsxs)(n.p,{children:["Example of using ",(0,l.jsx)(n.code,{children:"uv"})," to create a virtual environment for prediction:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.models.predict(\n    model_uri="runs:/<run_id>/<model_path>",\n    input_data="your_data",\n    env_manager="uv",\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"built-in-deployment",children:"Built-In Deployment Tools"}),"\n",(0,l.jsxs)(n.p,{children:["This information is moved to ",(0,l.jsx)(n.a,{href:"/deployment",children:"MLflow Deployment"})," page."]}),"\n",(0,l.jsxs)(n.h2,{id:"export-a-python_function-model-as-an-apache-spark-udf",children:["Export a ",(0,l.jsx)(n.code,{children:"python_function"})," model as an Apache Spark UDF"]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["If you are using a model that has a very long running inference latency (i.e., a\n",(0,l.jsx)(n.code,{children:"transformers"})," model) that could take longer than the default timeout of 60 seconds,\nyou can utilize the ",(0,l.jsx)(n.code,{children:"extra_env"})," argument when defining the ",(0,l.jsx)(n.code,{children:"spark_udf"})," instance for your\nMLflow model, specifying an override to the environment variable ",(0,l.jsx)(n.code,{children:"MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT"}),".\nFor further guidance, please see :py:func:",(0,l.jsx)(n.code,{children:"mlflow.pyfunc.spark_udf"}),"."]})}),"\n",(0,l.jsxs)(n.p,{children:["You can output a ",(0,l.jsx)(n.code,{children:"python_function"})," model as an Apache Spark UDF, which can be uploaded to a\nSpark cluster and used to score the model."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, "<path-to-model>")\ndf = spark_df.withColumn("prediction", pyfunc_udf(struct([...])))\n'})}),"\n",(0,l.jsx)(n.p,{children:"If a model contains a signature, the UDF can be called without specifying column name arguments.\nIn this case, the UDF will be called with column names from signature, so the evaluation\ndataframe's column names must match the model signature's column names."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, "<path-to-model-with-signature>")\ndf = spark_df.withColumn("prediction", pyfunc_udf())\n'})}),"\n",(0,l.jsx)(n.p,{children:"If a model contains a signature with tensor spec inputs,\nyou will need to pass a column of array type as a corresponding UDF argument.\nThe values in this column must be comprised of one-dimensional arrays. The\nUDF will reshape the array values to the required shape with 'C' order\n(i.e. read / write the elements using C-like index order) and cast the values\nas the required tensor spec type. For example, assuming a model\nrequires input 'a' of shape (-1, 2, 3) and input 'b' of shape (-1, 4, 5). In order to\nperform inference on this data, we need to prepare a Spark DataFrame with column 'a'\ncontaining arrays of length 6 and column 'b' containing arrays of length 20. We can then\ninvoke the UDF like following example code:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n# Assuming the model requires input 'a' of shape (-1, 2, 3) and input 'b' of shape (-1, 4, 5)\nmodel_path = \"<path-to-model-requiring-multidimensional-inputs>\"\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n# The `spark_df` has column 'a' containing arrays of length 6 and\n# column 'b' containing arrays of length 20\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf(struct(\"a\", \"b\")))\n"})}),"\n",(0,l.jsxs)(n.p,{children:["The resulting UDF is based on Spark's Pandas UDF and is currently limited to producing either a single\nvalue, an array of values, or a struct containing multiple field values\nof the same type per observation. By default, we return the first\nnumeric column as a double. You can control what result is returned by supplying ",(0,l.jsx)(n.code,{children:"result_type"}),"\nargument. The following values are supported:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'int'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.IntegerType.html#pyspark.sql.types.IntegerType",children:"IntegerType"}),":\nThe leftmost integer that can fit in ",(0,l.jsx)(n.code,{children:"int32"})," result is returned or an exception is raised if there are none."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'long'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html#pyspark.sql.types.LongType",children:"LongType"}),":\nThe leftmost long integer that can fit in ",(0,l.jsx)(n.code,{children:"int64"})," result is returned or an exception is raised if there are none."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType",children:"ArrayType"}),"\n(",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.IntegerType.html#pyspark.sql.types.IntegerType",children:"IntegerType"}),"\n| ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html#pyspark.sql.types.LongType",children:"LongType"}),"):\nReturn all integer columns that can fit into the requested size."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'float'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.FloatType.html#pyspark.sql.types.FloatType",children:"FloatType"}),":\nThe leftmost numeric result cast to ",(0,l.jsx)(n.code,{children:"float32"})," is returned or an exception is raised if there are no numeric columns."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'double'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.DoubleType.html#pyspark.sql.types.DoubleType",children:"DoubleType"}),":\nThe leftmost numeric result cast to ",(0,l.jsx)(n.code,{children:"double"})," is returned or an exception is raised if there are no numeric columns."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType",children:"ArrayType"}),"\n( ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.FloatType.html#pyspark.sql.types.FloatType",children:"FloatType"}),"\n| ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.DoubleType.html#pyspark.sql.types.DoubleType",children:"DoubleType"})," ):\nReturn all numeric columns cast to the requested type. An exception is raised if there are no numeric columns."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'string'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html#pyspark.sql.types.StringType",children:"StringType"}),":\nResult is the leftmost column cast as string."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType",children:"ArrayType"}),"\n( ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html#pyspark.sql.types.StringType",children:"StringType"})," ):\nReturn all columns cast as string."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'bool'"})," or ",(0,l.jsx)(n.code,{children:"'boolean'"})," or ",(0,l.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.BooleanType.html#pyspark.sql.types.BooleanType",children:"BooleanType"}),":\nThe leftmost column cast to ",(0,l.jsx)(n.code,{children:"bool"}),"\nis returned or an exception is raised if the values cannot be coerced."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"'field1 FIELD1_TYPE, field2 FIELD2_TYPE, ...'"}),": A struct type containing\nmultiple fields separated by comma, each field type must be one of types\nlisted above."]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n# Suppose the PyFunc model `predict` method returns a dict like:\n# `{'prediction': 1-dim_array, 'probability': 2-dim_array}`\n# You can supply result_type to be a struct type containing\n# 2 fields 'prediction' and 'probability' like following.\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, \"<path-to-model>\", result_type=\"prediction float, probability: array<float>\"\n)\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf())\n"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql.types import ArrayType, FloatType\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, "path/to/model", result_type=ArrayType(FloatType())\n)\n# The prediction column will contain all the numeric columns returned by the model as floats\ndf = spark_df.withColumn("prediction", pyfunc_udf(struct("name", "age")))\n'})}),"\n",(0,l.jsxs)(n.p,{children:["If you want to use conda to restore the python environment that was used to train the model,\nset the ",(0,l.jsx)(n.code,{children:"env_manager"})," argument when calling ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from pyspark.sql.types import ArrayType, FloatType\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark,\n    "path/to/model",\n    result_type=ArrayType(FloatType()),\n    env_manager="conda",  # Use conda to restore the environment used in training\n)\ndf = spark_df.withColumn("prediction", pyfunc_udf(struct("name", "age")))\n'})}),"\n",(0,l.jsxs)(n.p,{children:["If you want to call ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"})," through Databricks connect in remote client,\nyou need to build the model environment in Databricks runtime first."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from mlflow.pyfunc import build_model_env\n\n# Build the model env and save it as an archive file to the provided UC volume directory\n# and print the saved model env archive file path (like \'/Volumes/.../.../XXXXX.tar.gz\')\nprint(build_model_env(model_uri, "/Volumes/..."))\n\n# print the cluster id. Databricks Connect client needs to use the cluster id.\nprint(spark.conf.get("spark.databricks.clusterUsageTags.clusterId"))\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Once you have pre-built the model environment, you can run ",(0,l.jsx)(s.B,{fn:"mlflow.pyfunc.spark_udf"})," with 'prebuilt_model_env'\nparameter through Databricks connect in remote client,"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'from databricks.connect import DatabricksSession\n\nspark = DatabricksSession.builder.remote(\n    host=os.environ["DATABRICKS_HOST"],\n    token=os.environ["DATABRICKS_TOKEN"],\n    cluster_id="<cluster id>",  # get cluster id by spark.conf.get("spark.databricks.clusterUsageTags.clusterId")\n).getOrCreate()\n\n# The path generated by `build_model_env` in Databricks runtime.\nmodel_env_uc_uri = "dbfs:/Volumes/.../.../XXXXX.tar.gz"\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, model_uri, prebuilt_env_uri=model_env_uc_uri\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"deployment_plugin",children:"Deployment to Custom Targets"}),"\n",(0,l.jsxs)(n.p,{children:["In addition to the built-in deployment tools, MLflow provides a pluggable ",(0,l.jsx)(s.B,{fn:"mlflow.deployments"}),"\nand ",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments",target:"_blank",children:"mlflow deployments CLI"})," for deploying models to custom targets and environments.\nTo deploy to a custom target, you must first install an appropriate third-party Python plugin. See the list of\nknown community-maintained plugins ",(0,l.jsx)(n.a,{href:"/plugins#deployment-plugins",children:"here"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"commands",children:"Commands"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"mlflow deployments"})," CLI contains the following commands, which can also be invoked programmatically\nusing the ",(0,l.jsx)(s.B,{fn:"mlflow.deployments",children:"mlflow.deployments Python API"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-create",target:"_blank",children:"Create"}),": Deploy an MLflow model to a specified custom target"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-delete",target:"_blank",children:"Delete"}),": Delete a deployment"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-update",target:"_blank",children:"Update"}),": Update an existing deployment, for example to\ndeploy a new model version or change the deployment's configuration (e.g. increase replica count)"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-list",target:"_blank",children:"List"}),": List IDs of all deployments"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-get",target:"_blank",children:"Get"}),": Print a detailed description of a particular deployment"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-run-local",target:"_blank",children:"Run Local"}),": Deploy the model locally for testing"]}),"\n",(0,l.jsxs)(n.li,{children:["",(0,l.jsx)(r.A,{to:"/api_reference/cli.html#mlflow-deployments-help",target:"_blank",children:"Help"}),": Show the help string for the specified target"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"For more info, see:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow deployments --help\nmlflow deployments create --help\nmlflow deployments delete --help\nmlflow deployments update --help\nmlflow deployments list --help\nmlflow deployments get --help\nmlflow deployments run-local --help\nmlflow deployments help --help\n"})}),"\n",(0,l.jsx)(n.h2,{id:"community-model-flavors",children:"Community Model Flavors"}),"\n",(0,l.jsxs)(n.p,{children:["Go to the ",(0,l.jsx)(n.a,{href:"/community-model-flavors",children:"Community Model Flavors"}),"\npage to get an overview of other useful MLflow flavors, which are developed and\nmaintained by the MLflow community."]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(f,{...e})}):f(e)}},53631:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/tracking_artifact_ui_custom_flavor-94a45692f1737e1ff6eb1bf83ee0c794.png"},61096:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var o=t(71021);const l={tableOfContentsInline:"tableOfContentsInline_prmo"};var a=t(74848);function i(e){let{toc:n,minHeadingLevel:t,maxHeadingLevel:i}=e;return(0,a.jsx)("div",{className:l.tableOfContentsInline,children:(0,a.jsx)(o.A,{toc:n,minHeadingLevel:t,maxHeadingLevel:i,className:"table-of-contents",linkClassName:null})})}},67756:(e,n,t)=>{t.d(n,{B:()=>r});t(96540);const o=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=t(29030),a=t(56289),i=t(74848);const s=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(o[e])return e}return null};function r(e){let{fn:n,children:t}=e;const r=s(n);if(!r)return(0,i.jsx)(i.Fragment,{children:t});const d=(0,l.Ay)(`/${o[r]}#${n}`);return(0,i.jsx)(a.A,{to:d,target:"_blank",children:t??(0,i.jsxs)("code",{children:[n,"()"]})})}},71021:(e,n,t)=>{t.d(n,{A:()=>u});var o=t(96540),l=t(53115);function a(e){const n=e.map((e=>({...e,parentIndex:-1,children:[]}))),t=Array(7).fill(-1);n.forEach(((e,n)=>{const o=t.slice(2,e.level);e.parentIndex=Math.max(...o),t[e.level]=n}));const o=[];return n.forEach((e=>{const{parentIndex:t,...l}=e;t>=0?n[t].children.push(l):o.push(l)})),o}function i(e){let{toc:n,minHeadingLevel:t,maxHeadingLevel:o}=e;return n.flatMap((e=>{const n=i({toc:e.children,minHeadingLevel:t,maxHeadingLevel:o});return function(e){return e.level>=t&&e.level<=o}(e)?[{...e,children:n}]:n}))}function s(e){const n=e.getBoundingClientRect();return n.top===n.bottom?s(e.parentNode):n}function r(e,n){let{anchorTopOffset:t}=n;const o=e.find((e=>s(e).top>=t));if(o){return function(e){return e.top>0&&e.bottom<window.innerHeight/2}(s(o))?o:e[e.indexOf(o)-1]??null}return e[e.length-1]??null}function d(){const e=(0,o.useRef)(0),{navbar:{hideOnScroll:n}}=(0,l.p)();return(0,o.useEffect)((()=>{e.current=n?0:document.querySelector(".navbar").clientHeight}),[n]),e}function c(e){const n=(0,o.useRef)(void 0),t=d();(0,o.useEffect)((()=>{if(!e)return()=>{};const{linkClassName:o,linkActiveClassName:l,minHeadingLevel:a,maxHeadingLevel:i}=e;function s(){const e=function(e){return Array.from(document.getElementsByClassName(e))}(o),s=function(e){let{minHeadingLevel:n,maxHeadingLevel:t}=e;const o=[];for(let l=n;l<=t;l+=1)o.push(`h${l}.anchor`);return Array.from(document.querySelectorAll(o.join()))}({minHeadingLevel:a,maxHeadingLevel:i}),d=r(s,{anchorTopOffset:t.current}),c=e.find((e=>d&&d.id===function(e){return decodeURIComponent(e.href.substring(e.href.indexOf("#")+1))}(e)));e.forEach((e=>{!function(e,t){t?(n.current&&n.current!==e&&n.current.classList.remove(l),e.classList.add(l),n.current=e):e.classList.remove(l)}(e,e===c)}))}return document.addEventListener("scroll",s),document.addEventListener("resize",s),s(),()=>{document.removeEventListener("scroll",s),document.removeEventListener("resize",s)}}),[e,t])}var m=t(56289),h=t(74848);function p(e){let{toc:n,className:t,linkClassName:o,isChild:l}=e;return n.length?(0,h.jsx)("ul",{className:l?void 0:t,children:n.map((e=>(0,h.jsxs)("li",{children:[(0,h.jsx)(m.A,{to:`#${e.id}`,className:o??void 0,dangerouslySetInnerHTML:{__html:e.value}}),(0,h.jsx)(p,{isChild:!0,toc:e.children,className:t,linkClassName:o})]},e.id)))}):null}const f=o.memo(p);function u(e){let{toc:n,className:t="table-of-contents table-of-contents__left-border",linkClassName:s="table-of-contents__link",linkActiveClassName:r,minHeadingLevel:d,maxHeadingLevel:m,...p}=e;const u=(0,l.p)(),_=d??u.tableOfContents.minHeadingLevel,g=m??u.tableOfContents.maxHeadingLevel,x=function(e){let{toc:n,minHeadingLevel:t,maxHeadingLevel:l}=e;return(0,o.useMemo)((()=>i({toc:a(n),minHeadingLevel:t,maxHeadingLevel:l})),[n,t,l])}({toc:n,minHeadingLevel:_,maxHeadingLevel:g});return c((0,o.useMemo)((()=>{if(s&&r)return{linkClassName:s,linkActiveClassName:r,minHeadingLevel:_,maxHeadingLevel:g}}),[s,r,_,g])),(0,h.jsx)(f,{toc:x,className:t,linkClassName:s,...p})}},79783:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/model_evaluation_compare_feature_importance-99982d272aadd5d7aac0c8f3cfb39887.png"},92317:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/model_evaluation_metrics-be0d6d499945e467e96276cb545eff45.png"}}]);