"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2457],{211:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>m,contentTitle:()=>c,default:()=>f,frontMatter:()=>d,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"traditional-ml/sklearn/guide/index","title":"Scikit-learn with MLflow","description":"In this comprehensive guide, we\'ll walk you through how to use scikit-learn with MLflow for experiment tracking, model management, and production deployment. We\'ll cover both autologging and manual logging approaches, from basic usage to advanced production patterns.","source":"@site/docs/classic-ml/traditional-ml/sklearn/guide/index.mdx","sourceDirName":"traditional-ml/sklearn/guide","slug":"/traditional-ml/sklearn/guide/","permalink":"/docs/latest/ml/traditional-ml/sklearn/guide/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"Quickstart","permalink":"/docs/latest/ml/traditional-ml/sklearn/quickstart/quickstart-sklearn"},"next":{"title":"MLflow XGBoost Integration","permalink":"/docs/latest/ml/traditional-ml/xgboost/"}}');var a=r(74848),i=r(28453),o=r(49374),s=r(11470),l=r(19365);const d={},c="Scikit-learn with MLflow",m={},p=[{value:"Quick Start with Autologging",id:"quick-start-with-autologging",level:2},{value:"Understanding Autologging Behavior",id:"understanding-autologging-behavior",level:2},{value:"Logging Approaches",id:"logging-approaches",level:2},{value:"Hyperparameter Tuning",id:"hyperparameter-tuning",level:2},{value:"Model Evaluation with MLflow",id:"model-evaluation-with-mlflow",level:2},{value:"Model Comparison and Selection",id:"model-comparison-and-selection",level:2},{value:"Model Validation and Quality Gates",id:"model-validation-and-quality-gates",level:2},{value:"Model Management",id:"model-management",level:2},{value:"Production Deployment",id:"production-deployment",level:2},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"scikit-learn-with-mlflow",children:"Scikit-learn with MLflow"})}),"\n",(0,a.jsx)(n.p,{children:"In this comprehensive guide, we'll walk you through how to use scikit-learn with MLflow for experiment tracking, model management, and production deployment. We'll cover both autologging and manual logging approaches, from basic usage to advanced production patterns."}),"\n",(0,a.jsx)(n.h2,{id:"quick-start-with-autologging",children:"Quick Start with Autologging"}),"\n",(0,a.jsx)(n.p,{children:"The fastest way to get started is with MLflow's scikit-learn autologging. With just a single line of code, you can automatically track parameters, metrics, and models from your scikit-learn experiments. This approach requires no changes to your existing training code and captures everything you need for reproducible ML workflows."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# Enable autologging for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load sample data\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(\n    wine.data, wine.target, test_size=0.2, random_state=42\n)\n\n# Train your model - MLflow automatically logs everything\nwith mlflow.start_run():\n    model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n    model.fit(X_train, y_train)\n\n    # Evaluation metrics are automatically captured\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n\n    print(f"Training accuracy: {train_score:.3f}")\n    print(f"Test accuracy: {test_score:.3f}")\n'})}),"\n",(0,a.jsx)(n.p,{children:"This simple example automatically logs all model parameters, training metrics, the trained model with proper serialization, and model signatures for deployment\u2014all without any additional code."}),"\n",(0,a.jsx)(n.h2,{id:"understanding-autologging-behavior",children:"Understanding Autologging Behavior"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"what-logged",label:"What Gets Logged",default:!0,children:[(0,a.jsx)(n.p,{children:"MLflow's scikit-learn autologging captures comprehensive information about your training process automatically. Here's exactly what gets tracked every time you train a model:"}),(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Category"})}),(0,a.jsx)(n.th,{children:(0,a.jsx)(n.strong,{children:"Information Captured"})})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Parameters"})}),(0,a.jsxs)(n.td,{children:["All parameters from ",(0,a.jsx)(n.code,{children:"estimator.get_params(deep=True)"})]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Metrics"})}),(0,a.jsx)(n.td,{children:"Training score, classification/regression metrics"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Tags"})}),(0,a.jsx)(n.td,{children:"Estimator class name and fully qualified class name"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Artifacts"})}),(0,a.jsx)(n.td,{children:"Serialized model, model signature, metric information"})]})]})]}),(0,a.jsx)(n.p,{children:"The autologging system is designed to be comprehensive yet non-intrusive. It captures everything you need for reproducibility without requiring changes to your existing scikit-learn code."})]}),(0,a.jsxs)(l.A,{value:"supported",label:"Supported Estimators",children:[(0,a.jsx)(n.p,{children:"Autologging works seamlessly with virtually all scikit-learn estimators and workflows. The integration is designed to handle both simple models and complex meta-estimators:"}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Core Estimators:"})}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"All estimators"})," from ",(0,a.jsx)(n.code,{children:"sklearn.utils.all_estimators()"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pipeline"})," objects with preprocessing and modeling steps"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Meta-estimators"})," like ",(0,a.jsx)(n.code,{children:"GridSearchCV"})," and ",(0,a.jsx)(n.code,{children:"RandomizedSearchCV"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ensemble methods"})," including ",(0,a.jsx)(n.code,{children:"RandomForestClassifier"}),", ",(0,a.jsx)(n.code,{children:"GradientBoostingRegressor"})]}),"\n"]}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Special Handling:"})}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Meta-estimators automatically create child runs for parameter search results"}),"\n",(0,a.jsx)(n.li,{children:"Pipeline stages are logged with their individual parameters and transformations"}),"\n",(0,a.jsx)(n.li,{children:"Cross-validation results are captured and organized for easy comparison"}),"\n"]}),(0,a.jsx)(n.p,{children:"Most preprocessing estimators (like scalers and transformers) are excluded from individual logging to avoid clutter, but they're still tracked when used within Pipeline objects."})]})]}),"\n",(0,a.jsx)(n.h2,{id:"logging-approaches",children:"Logging Approaches"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"manual",label:"Manual Logging",default:!0,children:[(0,a.jsx)(n.p,{children:"For complete control over what gets logged, you can manually instrument your scikit-learn code. This approach is ideal when you need custom metrics, specific artifact logging, or want to organize experiments in a particular way:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom mlflow.models import infer_signature\n\n# Generate sample data\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Manual logging approach\nwith mlflow.start_run():\n    # Define hyperparameters\n    params = {"C": 1.0, "max_iter": 1000, "solver": "lbfgs", "random_state": 42}\n\n    # Log parameters\n    mlflow.log_params(params)\n\n    # Train model\n    model = LogisticRegression(**params)\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n\n    # Calculate and log metrics\n    metrics = {\n        "accuracy": accuracy_score(y_test, y_pred),\n        "precision": precision_score(y_test, y_pred, average="weighted"),\n        "recall": recall_score(y_test, y_pred, average="weighted"),\n        "f1_score": f1_score(y_test, y_pred, average="weighted"),\n    }\n    mlflow.log_metrics(metrics)\n\n    # Infer model signature\n    signature = infer_signature(X_train, model.predict(X_train))\n\n    # Log the model\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        name="model",\n        signature=signature,\n        input_example=X_train[:5],  # Sample input for documentation\n    )\n'})})]}),(0,a.jsxs)(l.A,{value:"post-training",label:"Post-Training Metrics",children:[(0,a.jsx)(n.p,{children:"One of MLflow's most powerful features is automatic capture of evaluation metrics after model training. This means any metrics you compute after training are automatically linked to your MLflow run, providing seamless tracking of model evaluation:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# Enable autologging with post-training metrics\nmlflow.sklearn.autolog(log_post_training_metrics=True)\n\n# Load data\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, test_size=0.2, random_state=42\n)\n\nwith mlflow.start_run():\n    # Train model\n    model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    # Make predictions - this links predictions to the MLflow run\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1]\n\n    # These metric calls are automatically logged to MLflow!\n    accuracy = accuracy_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_proba)\n\n    # Model scoring is also automatically captured\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n\n    print(f"Accuracy: {accuracy:.3f}")\n    print(f"AUC Score: {auc_score:.3f}")\n'})}),(0,a.jsx)(n.p,{children:"The post-training metrics feature intelligently detects when you're evaluating models and automatically logs those metrics with appropriate dataset context, making it easy to track performance across different evaluation datasets."})]})]}),"\n",(0,a.jsx)(n.h2,{id:"hyperparameter-tuning",children:"Hyperparameter Tuning"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"gridsearch",label:"GridSearchCV",default:!0,children:[(0,a.jsx)(n.p,{children:"MLflow provides exceptional support for scikit-learn's hyperparameter optimization tools, automatically creating organized child runs for parameter search experiments. This makes it easy to track and compare different parameter combinations:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\n# Enable autologging with hyperparameter tuning support\nmlflow.sklearn.autolog(max_tuning_runs=10)  # Track top 10 parameter combinations\n\n# Load data\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, test_size=0.2, random_state=42\n)\n\n# Define parameter grid\nparam_grid = {\n    "n_estimators": [50, 100, 200],\n    "max_depth": [5, 10, 15, None],\n    "min_samples_split": [2, 5, 10],\n    "min_samples_leaf": [1, 2, 4],\n}\n\nwith mlflow.start_run(run_name="Random Forest Hyperparameter Tuning"):\n    # Create and fit GridSearchCV\n    rf = RandomForestClassifier(random_state=42)\n    grid_search = GridSearchCV(\n        rf, param_grid, cv=5, scoring="accuracy", n_jobs=-1, verbose=1\n    )\n\n    grid_search.fit(X_train, y_train)\n\n    # Best model evaluation\n    best_score = grid_search.score(X_test, y_test)\n    print(f"Best parameters: {grid_search.best_params_}")\n    print(f"Best cross-validation score: {grid_search.best_score_:.3f}")\n    print(f"Test score: {best_score:.3f}")\n'})}),(0,a.jsx)(n.p,{children:"MLflow automatically creates a parent run containing the overall search results and child runs for each parameter combination, making it easy to analyze which parameters work best."})]}),(0,a.jsxs)(l.A,{value:"randomsearch",label:"RandomizedSearchCV",children:[(0,a.jsx)(n.p,{children:"For more efficient hyperparameter exploration, especially with large parameter spaces, RandomizedSearchCV provides a great alternative. MLflow handles this just as seamlessly as GridSearchCV:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\n# Define parameter distributions for more efficient exploration\nparam_distributions = {\n    "n_estimators": randint(50, 300),\n    "max_depth": randint(5, 20),\n    "min_samples_split": randint(2, 20),\n    "min_samples_leaf": randint(1, 10),\n    "max_features": uniform(0.1, 0.9),\n}\n\nwith mlflow.start_run(run_name="Randomized Hyperparameter Search"):\n    rf = RandomForestClassifier(random_state=42)\n    random_search = RandomizedSearchCV(\n        rf,\n        param_distributions,\n        n_iter=50,  # Try 50 random combinations\n        cv=5,\n        scoring="accuracy",\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    random_search.fit(X_train, y_train)\n\n    # MLflow automatically creates child runs for parameter combinations\n    # The parent run contains the best model and overall results\n'})}),(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"max_tuning_runs"})," parameter in autolog controls how many of the best parameter combinations get their own child runs, helping you focus on the most promising results."]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"model-evaluation-with-mlflow",children:"Model Evaluation with MLflow"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"mlflow-evaluate",label:"MLflow Evaluate API",default:!0,children:[(0,a.jsx)(n.p,{children:"MLflow provides a comprehensive evaluation API that automatically generates metrics, visualizations, and diagnostic tools for scikit-learn models:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom mlflow.models import infer_signature\n\n# Load data and train model\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(\n    wine.data, wine.target, test_size=0.2, random_state=42\n)\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data = pd.DataFrame(eval_data, columns=wine.feature_names)\neval_data["label"] = y_test\n\nwith mlflow.start_run():\n    # Log model with signature\n    signature = infer_signature(X_test, model.predict(X_test))\n    mlflow.sklearn.log_model(model, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n\n    # Comprehensive evaluation with MLflow\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="label",\n        model_type="classifier",  # or "regressor" for regression\n        evaluators=["default"],\n    )\n\n    # Access automatic metrics\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\n    print(f"ROC AUC: {result.metrics[\'roc_auc\']:.3f}")\n\n    # Access generated artifacts\n    print("Generated artifacts:")\n    for artifact_name, path in result.artifacts.items():\n        print(f"  {artifact_name}: {path}")\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Automatic Generation Includes:"})}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Performance Metrics"})," such as accuracy, precision, recall, F1-score, ROC-AUC for classification. ",(0,a.jsx)(n.strong,{children:"Visualizations"})," including confusion matrix, ROC curve, precision-recall curve. ",(0,a.jsx)(n.strong,{children:"Feature Importance"})," with SHAP values and feature contribution analysis. ",(0,a.jsx)(n.strong,{children:"Model Artifacts"})," where all plots and diagnostic information are saved to MLflow."]})]}),(0,a.jsxs)(l.A,{value:"regression-evaluation",label:"Regression Evaluation",children:[(0,a.jsx)(n.p,{children:"For scikit-learn regression models, MLflow automatically provides regression-specific metrics:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\n\n# Load regression dataset\nhousing = fetch_california_housing(as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    housing.data, housing.target, test_size=0.2, random_state=42\n)\n\n# Train regression model\nreg_model = LinearRegression()\nreg_model.fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["target"] = y_test\n\nwith mlflow.start_run():\n    # Log and evaluate regression model\n    signature = infer_signature(X_train, reg_model.predict(X_train))\n    mlflow.sklearn.log_model(reg_model, name="model", signature=signature)\n    model_uri = mlflow.get_artifact_uri("model")\n\n    result = mlflow.evaluate(\n        model_uri,\n        eval_data,\n        targets="target",\n        model_type="regressor",\n        evaluators=["default"],\n    )\n\n    print(f"MAE: {result.metrics[\'mean_absolute_error\']:.3f}")\n    print(f"RMSE: {result.metrics[\'root_mean_squared_error\']:.3f}")\n    print(f"R\xb2 Score: {result.metrics[\'r2_score\']:.3f}")\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Automatic Regression Metrics:"})}),(0,a.jsx)(n.p,{children:"Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root MSE provide error magnitude assessment. R\xb2 Score and Adjusted R\xb2 measure model fit quality. Mean Absolute Percentage Error (MAPE) shows relative error rates. Residual plots and distribution analysis help identify model assumptions violations."})]}),(0,a.jsxs)(l.A,{value:"custom-evaluation",label:"Custom Metrics & Artifacts",children:[(0,a.jsx)(n.p,{children:"Extend MLflow evaluation with custom metrics and visualizations specific to your scikit-learn models:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.models import make_metric\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n\ndef business_value_metric(predictions, targets, sample_weights=None):\n    """Custom business metric: value from correct predictions."""\n    # Assume $50 value per correct prediction, $20 cost per error\n    correct_predictions = (predictions == targets).sum()\n    incorrect_predictions = len(predictions) - correct_predictions\n\n    business_value = (correct_predictions * 50) - (incorrect_predictions * 20)\n    return business_value\n\n\ndef create_feature_distribution_plot(eval_df, builtin_metrics, artifacts_dir):\n    """Create feature distribution plots for model analysis."""\n\n    # Select numeric features for distribution analysis\n    numeric_features = eval_df.select_dtypes(include=[np.number]).columns\n    numeric_features = [\n        col for col in numeric_features if col not in ["label", "prediction"]\n    ]\n\n    if len(numeric_features) > 0:\n        # Create subplot for feature distributions\n        n_features = min(6, len(numeric_features))  # Show up to 6 features\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        axes = axes.flatten()\n\n        for i, feature in enumerate(numeric_features[:n_features]):\n            axes[i].hist(eval_df[feature], bins=30, alpha=0.7, edgecolor="black")\n            axes[i].set_title(f"Distribution of {feature}")\n            axes[i].set_xlabel(feature)\n            axes[i].set_ylabel("Frequency")\n\n        # Hide unused subplots\n        for i in range(n_features, len(axes)):\n            axes[i].set_visible(False)\n\n        plt.tight_layout()\n\n        plot_path = os.path.join(artifacts_dir, "feature_distributions.png")\n        plt.savefig(plot_path)\n        plt.close()\n\n        return {"feature_distributions": plot_path}\n\n    return {}\n\n\n# Create custom metric\ncustom_business_value = make_metric(\n    eval_fn=business_value_metric, greater_is_better=True, name="business_value_score"\n)\n\n# Use custom metrics and artifacts\nresult = mlflow.evaluate(\n    model_uri,\n    eval_data,\n    targets="label",\n    model_type="classifier",\n    extra_metrics=[custom_business_value],\n    custom_artifacts=[create_feature_distribution_plot],\n)\n\nprint(f"Business Value Score: ${result.metrics[\'business_value_score\']:.2f}")\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"model-comparison-and-selection",children:"Model Comparison and Selection"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"model-comparison",label:"MLflow Model Comparison",default:!0,children:[(0,a.jsx)(n.p,{children:"Use MLflow evaluate to systematically compare multiple scikit-learn models:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# Define models to compare\nsklearn_models = {\n    "random_forest": RandomForestClassifier(n_estimators=100, random_state=42),\n    "gradient_boosting": GradientBoostingClassifier(n_estimators=100, random_state=42),\n    "logistic_regression": LogisticRegression(random_state=42, max_iter=1000),\n    "svm": SVC(probability=True, random_state=42),\n}\n\n# Evaluate each model systematically\ncomparison_results = {}\n\nfor model_name, model in sklearn_models.items():\n    with mlflow.start_run(run_name=f"eval_{model_name}"):\n        # Train model\n        model.fit(X_train, y_train)\n\n        # Log model\n        signature = infer_signature(X_train, model.predict(X_train))\n        mlflow.sklearn.log_model(model, name="model", signature=signature)\n        model_uri = mlflow.get_artifact_uri("model")\n\n        # Comprehensive evaluation with MLflow\n        result = mlflow.evaluate(\n            model_uri,\n            eval_data,\n            targets="label",\n            model_type="classifier",\n            evaluators=["default"],\n        )\n\n        comparison_results[model_name] = result.metrics\n\n        # Log key metrics for comparison\n        mlflow.log_metrics(\n            {\n                "accuracy": result.metrics["accuracy_score"],\n                "f1": result.metrics["f1_score"],\n                "roc_auc": result.metrics["roc_auc"],\n                "precision": result.metrics["precision_score"],\n                "recall": result.metrics["recall_score"],\n            }\n        )\n\n# Create comparison summary\nimport pandas as pd\n\ncomparison_df = pd.DataFrame(comparison_results).T\nprint("Model Comparison Summary:")\nprint(comparison_df[["accuracy_score", "f1_score", "roc_auc"]].round(3))\n\n# Identify best model\nbest_model = comparison_df["f1_score"].idxmax()\nprint(f"\\nBest model by F1 score: {best_model}")\n'})})]}),(0,a.jsxs)(l.A,{value:"hyperparameter-eval",label:"Hyperparameter Evaluation",children:[(0,a.jsx)(n.p,{children:"Combine hyperparameter tuning with MLflow evaluation for comprehensive assessment:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.model_selection import ParameterGrid\n\n# Define parameter grid for Random Forest\nparam_grid = {\n    "n_estimators": [50, 100, 200],\n    "max_depth": [5, 10, None],\n    "min_samples_split": [2, 5, 10],\n    "min_samples_leaf": [1, 2, 4],\n}\n\n# Evaluate each parameter combination\ngrid_results = []\n\nfor params in ParameterGrid(param_grid):\n    with mlflow.start_run(run_name=f"rf_grid_search"):\n        # Log parameters\n        mlflow.log_params(params)\n\n        # Train model with current parameters\n        model = RandomForestClassifier(**params, random_state=42)\n        model.fit(X_train, y_train)\n\n        # Log and evaluate\n        signature = infer_signature(X_train, model.predict(X_train))\n        mlflow.sklearn.log_model(model, name="model", signature=signature)\n        model_uri = mlflow.get_artifact_uri("model")\n\n        # MLflow evaluation\n        result = mlflow.evaluate(\n            model_uri,\n            eval_data,\n            targets="label",\n            model_type="classifier",\n            evaluators=["default"],\n        )\n\n        # Track results\n        grid_results.append(\n            {\n                **params,\n                "f1_score": result.metrics["f1_score"],\n                "roc_auc": result.metrics["roc_auc"],\n                "accuracy": result.metrics["accuracy_score"],\n            }\n        )\n\n        # Log selection metric\n        mlflow.log_metric("grid_search_score", result.metrics["f1_score"])\n\n# Find best parameters\nbest_result = max(grid_results, key=lambda x: x["f1_score"])\nprint(f"Best parameters: {best_result}")\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"model-validation-and-quality-gates",children:"Model Validation and Quality Gates"}),"\n",(0,a.jsx)(n.p,{children:"Use MLflow's validation API to ensure scikit-learn model quality:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.models import MetricThreshold\n\n# First, evaluate your scikit-learn model\nresult = mlflow.evaluate(model_uri, eval_data, targets="label", model_type="classifier")\n\n# Define quality thresholds for classification models\nquality_thresholds = {\n    "accuracy_score": MetricThreshold(threshold=0.85, greater_is_better=True),\n    "f1_score": MetricThreshold(threshold=0.80, greater_is_better=True),\n    "roc_auc": MetricThreshold(threshold=0.75, greater_is_better=True),\n}\n\n# Validate model meets quality standards\ntry:\n    mlflow.validate_evaluation_results(\n        candidate_result=result,\n        validation_thresholds=quality_thresholds,\n    )\n    print("\u2705 Scikit-learn model meets all quality thresholds")\nexcept mlflow.exceptions.ModelValidationFailedException as e:\n    print(f"\u274c Model failed validation: {e}")\n\n# Compare against baseline model (e.g., previous model version)\nbaseline_result = mlflow.evaluate(\n    baseline_model_uri, eval_data, targets="label", model_type="classifier"\n)\n\n# Validate improvement over baseline\nimprovement_thresholds = {\n    "f1_score": MetricThreshold(\n        threshold=0.02, greater_is_better=True  # Must be 2% better\n    ),\n}\n\ntry:\n    mlflow.validate_evaluation_results(\n        candidate_result=result,\n        baseline_result=baseline_result,\n        validation_thresholds=improvement_thresholds,\n    )\n    print("\u2705 New model improves over baseline")\nexcept mlflow.exceptions.ModelValidationFailedException as e:\n    print(f"\u274c Model doesn\'t improve sufficiently: {e}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"model-management",children:"Model Management"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"serialization",label:"Serialization & Formats",default:!0,children:[(0,a.jsx)(n.p,{children:"MLflow supports multiple serialization formats for scikit-learn models, each optimized for different deployment scenarios. Understanding these options helps you choose the right approach for your production needs:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow.sklearn\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# Cloudpickle format (default) - better cross-system compatibility\nmlflow.sklearn.log_model(\n    sk_model=model,\n    name="cloudpickle_model",\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n)\n\n# Pickle format - faster but less portable\nmlflow.sklearn.log_model(\n    sk_model=model,\n    name="pickle_model",\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE,\n)\n'})}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Cloudpickle"})," is the default format because it provides better cross-system compatibility by identifying and packaging code dependencies with the serialized model. ",(0,a.jsx)(n.strong,{children:"Pickle"})," is faster but less portable across different environments."]})]}),(0,a.jsxs)(l.A,{value:"signatures",label:"Model Signatures",children:[(0,a.jsx)(n.p,{children:"Model signatures describe input and output schemas, providing crucial validation for production deployment. They help catch data compatibility issues early and ensure your models receive the correct input format:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.models import infer_signature\nimport pandas as pd\n\n# Create model signature automatically\nX_sample = X_train[:100]\npredictions = model.predict(X_sample)\nsignature = infer_signature(X_sample, predictions)\n\n# Log model with signature for production safety\nmlflow.sklearn.log_model(\n    sk_model=model,\n    name="model_with_signature",\n    signature=signature,\n    input_example=X_sample[:5],  # Include example for documentation\n)\n'})}),(0,a.jsx)(n.p,{children:"Model signatures are automatically inferred when autologging is enabled, but you can also create them manually for more control over the schema validation process."})]}),(0,a.jsxs)(l.A,{value:"loading",label:"Loading & Usage",children:[(0,a.jsx)(n.p,{children:"MLflow provides flexible ways to load and use your saved models, depending on your deployment needs. You can load models as native scikit-learn objects or as generic Python functions:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Load model in different ways\nimport mlflow.sklearn\nimport mlflow.pyfunc\n\nrun_id = "your_run_id_here"\n\n# Load as scikit-learn model (preserves all sklearn functionality)\nsklearn_model = mlflow.sklearn.load_model(f"runs:/{run_id}/model")\npredictions = sklearn_model.predict(X_test)\n\n# Load as PyFunc model (generic Python function interface)\npyfunc_model = mlflow.pyfunc.load_model(f"runs:/{run_id}/model")\npredictions = pyfunc_model.predict(pd.DataFrame(X_test))\n\n# Load from model registry (production deployment)\nregistered_model = mlflow.pyfunc.load_model("models:/MyModel@champion")\n'})}),(0,a.jsx)(n.p,{children:"The PyFunc format is particularly useful for deployment scenarios where you need a consistent interface across different model types and frameworks."})]})]}),"\n",(0,a.jsx)(n.h2,{id:"production-deployment",children:"Production Deployment"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"registry",label:"Model Registry",default:!0,children:[(0,a.jsx)(n.p,{children:"The Model Registry provides centralized model management with version control and alias-based deployment. This is essential for managing models from development through production deployment:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Register model to MLflow Model Registry\nimport mlflow\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n\n# Log and register model in one step\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        name="model",\n        registered_model_name="CustomerChurnModel",\n        signature=signature,\n    )\n\n# Or register an existing model\nrun_id = "your_run_id"\nmodel_uri = f"runs:/{run_id}/model"\n\n# Register the model\nregistered_model = mlflow.register_model(model_uri=model_uri, name="CustomerChurnModel")\n\n# Use aliases instead of deprecated stages for deployment management\n# Set aliases for different deployment environments\nclient.set_registered_model_alias(\n    name="CustomerChurnModel",\n    alias="champion",  # Production model\n    version=registered_model.version,\n)\n\nclient.set_registered_model_alias(\n    name="CustomerChurnModel",\n    alias="challenger",  # A/B testing model\n    version=registered_model.version,\n)\n\n# Use tags to track model status and metadata\nclient.set_model_version_tag(\n    name="CustomerChurnModel",\n    version=registered_model.version,\n    key="validation_status",\n    value="approved",\n)\n\nclient.set_model_version_tag(\n    name="CustomerChurnModel",\n    version=registered_model.version,\n    key="deployment_date",\n    value="2025-05-29",\n)\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Modern Model Registry Features:"})}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Model Aliases"})," replace deprecated stages with flexible, named references. You can assign multiple aliases to any model version (e.g., ",(0,a.jsx)(n.code,{children:"champion"}),", ",(0,a.jsx)(n.code,{children:"challenger"}),", ",(0,a.jsx)(n.code,{children:"shadow"}),"), update aliases independently of model training for seamless deployments, and use them for A/B testing and gradual rollouts."]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Model Tags"})," provide rich metadata and status tracking. Track validation status with ",(0,a.jsx)(n.code,{children:"validation_status: approved"}),", mark deployment readiness with ",(0,a.jsx)(n.code,{children:"ready_for_prod: true"}),", and add team ownership with ",(0,a.jsx)(n.code,{children:"team: data-science"}),"."]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Environment-based Models"})," support mature MLOps workflows. Create separate registered models per environment: ",(0,a.jsx)(n.code,{children:"dev.CustomerChurnModel"}),", ",(0,a.jsx)(n.code,{children:"staging.CustomerChurnModel"}),", ",(0,a.jsx)(n.code,{children:"prod.CustomerChurnModel"}),", and use ",(0,a.jsx)(o.B,{fn:"mlflow.client.MlflowClient.copy_model_version",children:(0,a.jsx)(n.code,{children:"copy_model_version()"})})," to promote models across environments."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Promote model from staging to production environment\nclient.copy_model_version(\n    src_model_uri="models:/staging.CustomerChurnModel@candidate",\n    dst_name="prod.CustomerChurnModel",\n)\n'})})]}),(0,a.jsxs)(l.A,{value:"serving",label:"Model Serving",children:[(0,a.jsx)(n.p,{children:"MLflow provides built-in model serving capabilities that make it easy to deploy your scikit-learn models as REST APIs. This is perfect for development, testing, and small-scale production deployments:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Serve model using alias for production deployment\nmlflow models serve \\\n    -m "models:/CustomerChurnModel@champion" \\\n    -p 5000 \\\n    --no-conda\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Deployment Best Practices:"})}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Use aliases for production serving"})," by pointing to ",(0,a.jsx)(n.code,{children:"@champion"})," or ",(0,a.jsx)(n.code,{children:"@production"})," aliases instead of hard-coding version numbers. Implement ",(0,a.jsx)(n.strong,{children:"blue-green deployments"})," by updating aliases to switch traffic between model versions instantly. Ensure ",(0,a.jsx)(n.strong,{children:"model signatures"})," provide automatic input validation at serving time. Configure ",(0,a.jsx)(n.strong,{children:"environment variables"})," for serving endpoints with necessary authentication and configuration."]}),(0,a.jsx)(n.p,{children:"Once your model is served, you can make predictions by sending POST requests to the endpoint:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\n\n# Example prediction request\ndata = {"inputs": [[1.2, 0.8, 3.4, 2.1]]}  # Feature values\n\nresponse = requests.post(\n    "http://localhost:5000/invocations",\n    headers={"Content-Type": "application/json"},\n    data=json.dumps(data),\n)\n\npredictions = response.json()\n'})}),(0,a.jsx)(n.p,{children:"For larger production deployments, you can also deploy MLflow models to cloud platforms like AWS SageMaker, Azure ML, or deploy them as Docker containers for Kubernetes orchestration."})]})]}),"\n",(0,a.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.A,{value:"pipelines",label:"Pipeline Integration",default:!0,children:[(0,a.jsx)(n.p,{children:"Scikit-learn pipelines are first-class citizens in MLflow, providing end-to-end workflow tracking from data preprocessing through model training. This ensures reproducibility of your entire ML workflow:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Enable autologging for pipelines\nmlflow.sklearn.autolog()\n\n# Create a complex preprocessing and modeling pipeline\nnumeric_features = ["age", "income", "credit_score"]\ncategorical_features = ["occupation", "location"]\n\n# Preprocessing pipeline\nnumeric_transformer = Pipeline(\n    steps=[("scaler", StandardScaler()), ("selector", SelectKBest(f_regression, k=2))]\n)\n\ncategorical_transformer = Pipeline(\n    steps=[("encoder", OneHotEncoder(drop="first", sparse_output=False))]\n)\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ("num", numeric_transformer, numeric_features),\n        ("cat", categorical_transformer, categorical_features),\n    ]\n)\n\n# Complete pipeline with model\npipeline = Pipeline(\n    steps=[("preprocessor", preprocessor), ("regressor", LinearRegression())]\n)\n\n# Train pipeline - all steps are automatically logged\nwith mlflow.start_run(run_name="Complete Pipeline Experiment"):\n    pipeline.fit(X_train, y_train)\n\n    # Pipeline scoring is automatically captured\n    train_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n\n    print(f"Pipeline R\xb2 score: {test_score:.3f}")\n'})}),(0,a.jsx)(n.p,{children:"MLflow automatically logs parameters from each pipeline stage, making it easy to understand exactly how your data was processed and which model parameters were used."})]}),(0,a.jsxs)(l.A,{value:"configuration",label:"Autolog Configuration",children:[(0,a.jsx)(n.p,{children:"MLflow's autologging behavior can be customized to fit your specific workflow needs. Here are the key configuration options that control what gets logged and how:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Fine-tune autologging behavior\nmlflow.sklearn.autolog(\n    log_input_examples=True,  # Include input examples in logged models\n    log_model_signatures=True,  # Include model signatures\n    log_models=True,  # Log trained models\n    log_datasets=True,  # Log dataset information\n    max_tuning_runs=10,  # Limit hyperparameter search child runs\n    log_post_training_metrics=True,  # Enable post-training metric capture\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    pos_label=1,  # Specify positive label for binary classification\n    extra_tags={"team": "data-science", "project": "customer-churn"},\n)\n'})}),(0,a.jsxs)(n.p,{children:["These configuration options give you fine-grained control over the autologging behavior. ",(0,a.jsx)(n.strong,{children:"Dataset logging"})," tracks the data used for training and evaluation. ",(0,a.jsx)(n.strong,{children:"Input examples"})," and ",(0,a.jsx)(n.strong,{children:"signatures"})," are crucial for production deployment. ",(0,a.jsx)(n.strong,{children:"Max tuning runs"})," controls how many hyperparameter combinations get detailed tracking. ",(0,a.jsx)(n.strong,{children:"Extra tags"})," help organize experiments across teams and projects."]})]}),(0,a.jsxs)(l.A,{value:"organization",label:"Experiment Organization",children:[(0,a.jsx)(n.p,{children:"Proper experiment organization is crucial for team collaboration and project management. MLflow provides several features to help you structure and categorize your experiments effectively:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Organize experiments with descriptive names and tags\nexperiment_name = "Customer Churn Prediction - Q4 2024"\nmlflow.set_experiment(experiment_name)\n\nwith mlflow.start_run(run_name="Baseline Random Forest"):\n    # Use consistent tagging for easy filtering and organization\n    mlflow.set_tags(\n        {\n            "model_type": "ensemble",\n            "algorithm": "random_forest",\n            "dataset_version": "v2.1",\n            "feature_engineering": "standard",\n            "purpose": "baseline",\n        }\n    )\n\n    # Train model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n'})}),(0,a.jsx)(n.p,{children:"Consistent tagging and naming conventions make it much easier to find, compare, and understand experiments later. Consider establishing team-wide conventions for experiment names, tags, and run organization."})]})]}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"MLflow's scikit-learn integration provides a comprehensive solution for experiment tracking, model management, and deployment in traditional machine learning workflows. Whether you're using simple autologging for quick experiments or implementing complex production pipelines, MLflow scales to meet your needs."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Key benefits of using MLflow with scikit-learn:"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Effortless Experiment Tracking"})," provides one-line autologging that captures everything you need for reproducible ML. ",(0,a.jsx)(n.strong,{children:"Hyperparameter Optimization"})," includes built-in support for grid search with organized child runs and easy comparison. ",(0,a.jsx)(n.strong,{children:"Comprehensive Evaluation"})," offers automatic metrics generation, visualizations, and SHAP analysis through ",(0,a.jsx)(n.code,{children:"mlflow.evaluate()"}),". ",(0,a.jsx)(n.strong,{children:"Production-Ready Deployment"})," provides model registry integration with alias-based deployment and quality gates. ",(0,a.jsx)(n.strong,{children:"Team Collaboration"})," enables centralized experiment management with rich metadata and artifacts."]}),"\n",(0,a.jsx)(n.p,{children:"The patterns and examples in this guide provide a solid foundation for building scalable, reproducible machine learning systems with scikit-learn and MLflow. Start with autologging for immediate benefits, then gradually adopt more advanced features like model evaluation, registry, and custom configurations as your needs grow."})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},11470:(e,n,r)=>{r.d(n,{A:()=>b});var t=r(96540),a=r(34164),i=r(23104),o=r(56347),s=r(205),l=r(57485),d=r(31682),c=r(70679);function m(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:r}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return m(e).map((({props:{value:e,label:n,attributes:r,default:t}})=>({value:e,label:n,attributes:r,default:t})))}(r);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,r])}function u({value:e,tabValues:n}){return n.some((n=>n.value===e))}function f({queryString:e=!1,groupId:n}){const r=(0,o.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(a),(0,t.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(r.location.search);n.set(a,e),r.replace({...r.location,search:n.toString()})}),[a,r])]}function h(e){const{defaultValue:n,queryString:r=!1,groupId:a}=e,i=p(e),[o,l]=(0,t.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const r=n.find((e=>e.default))??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:i}))),[d,m]=f({queryString:r,groupId:a}),[h,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[r,a]=(0,c.Dv)(n);return[r,(0,t.useCallback)((e=>{n&&a.set(e)}),[n,a])]}({groupId:a}),g=(()=>{const e=d??h;return u({value:e,tabValues:i})?e:null})();(0,s.A)((()=>{g&&l(g)}),[g]);return{selectedValue:o,selectValue:(0,t.useCallback)((e=>{if(!u({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),m(e),_(e)}),[m,_,i]),tabValues:i}}var _=r(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=r(74848);function w({className:e,block:n,selectedValue:r,selectValue:t,tabValues:o}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),d=e=>{const n=e.currentTarget,a=s.indexOf(n),i=o[a].value;i!==r&&(l(n),t(i))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const r=s.indexOf(e.currentTarget)+1;n=s[r]??s[0];break}case"ArrowLeft":{const r=s.indexOf(e.currentTarget)-1;n=s[r]??s[s.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:o.map((({value:e,label:n,attributes:t})=>(0,y.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{s.push(e)},onKeyDown:c,onClick:d,...t,className:(0,a.A)("tabs__item",g.tabItem,t?.className,{"tabs__item--active":r===e}),children:n??e},e)))})}function x({lazy:e,children:n,selectedValue:r}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find((e=>e.props.value===r));return e?(0,t.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function v(e){const n=h(e);return(0,y.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,y.jsx)(w,{...n,...e}),(0,y.jsx)(x,{...n,...e})]})}function b(e){const n=(0,_.A)();return(0,y.jsx)(v,{...e,children:m(e.children)},String(n))}},19365:(e,n,r)=>{r.d(n,{A:()=>o});r(96540);var t=r(34164);const a={tabItem:"tabItem_Ymn6"};var i=r(74848);function o({children:e,hidden:n,className:r}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.A)(a.tabItem,r),hidden:n,children:e})}},28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var t=r(96540);const a={},i=t.createContext(a);function o(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(i.Provider,{value:n},e.children)}},49374:(e,n,r)=>{r.d(n,{B:()=>l});r(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var a=r(86025),i=r(28774),o=r(74848);const s=e=>{const n=e.split(".");for(let r=n.length;r>0;r--){const e=n.slice(0,r).join(".");if(t[e])return e}return null};function l({fn:e,children:n}){const r=s(e);if(!r)return(0,o.jsx)(o.Fragment,{children:n});const l=(0,a.Ay)(`/${t[r]}#${e}`);return(0,o.jsx)(i.A,{to:l,target:"_blank",children:n??(0,o.jsxs)("code",{children:[e,"()"]})})}}}]);