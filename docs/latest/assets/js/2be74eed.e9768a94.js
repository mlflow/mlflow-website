"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7897],{970:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/save_new_input-3735297c289381cea11cb1ee60a3dd4f.png"},4456:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/evaluate_metrics-bee252801c0dd3bc77ff472f8e7d4a48.png"},17225:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/eval_view_1-2fd57ddf3134c1c646a91d382010ff72.png"},26081:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/load_model-ab1fe012845f6f7cd5843d91ce3af1b7.png"},26401:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/prompt_modal_3-78c14b824dfceb50dfac7ef02ed4ccd0.png"},28179:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/prompt_modal_1-51ecbda29dcb90d4b7ed59a996470b87.png"},28321:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/prompt_eng_table_view-71af8b8369dddf64ff8a00327489d80f.png"},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var o=t(96540);const l={},a=o.createContext(l);function r(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),o.createElement(a.Provider,{value:n},e.children)}},29369:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/add_row_modal-bf5df9f1c08b8e62c70909dbb4263ea2.png"},31662:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/evaluate_all-6b9191a0abf9c0fe077631376c5142e1.png"},40751:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/new_run-2879a8f2ac95f77cfe3173cb320967c9.png"},43310:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/prompt_modal_4-9bcc677fe20c9bb937a25ac48e43e89e.png"},52430:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/prompt_eng_run_page-14d417e3ec68786b5d118ce95b8979e1.png"},53688:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/prompt_modal_2-449c061f1e6104c039029c4820d7142a.png"},61646:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/experiment_page-2a7cf0b96047619eac6845f09e01807d.png"},62564:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/evaluate_new_input-0ef075da2f4cfc8a430d76119686c836.png"},67756:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const o=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=t(29030),a=t(56289),r=t(74848);const i=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(o[e])return e}return null};function s(e){let{fn:n,children:t}=e;const s=i(n);if(!s)return(0,r.jsx)(r.Fragment,{children:t});const c=(0,l.Ay)(`/${o[s]}#${n}`);return(0,r.jsx)(a.A,{to:c,target:"_blank",children:t??(0,r.jsxs)("code",{children:[n,"()"]})})}},81929:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/evaluate_all_results-7232f268e5e9453ae3fb20cae9009bcf.png"},86011:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"llms/prompt-engineering/index","title":"Prompt Engineering UI (Experimental)","description":"Starting in MLflow 2.7, the MLflow Tracking UI provides a best-in-class experience for prompt","source":"@site/docs/llms/prompt-engineering/index.mdx","sourceDirName":"llms/prompt-engineering","slug":"/llms/prompt-engineering/","permalink":"/docs/latest/llms/prompt-engineering/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Optimize Prompts \ud83c\udd95","permalink":"/docs/latest/prompts/optimize-prompt"},"next":{"title":"MLflow Evaluation","permalink":"/docs/latest/model-evaluation/"}}');var l=t(74848),a=t(28453),r=t(56289),i=t(67756);const s={},c="Prompt Engineering UI (Experimental)",p={},d=[{value:"Quickstart",id:"prompt-engineering-quickstart",level:2},{value:"Step 1: Create an MLflow AI Gateway Completions or Chat Endpoint",id:"step-1-create-an-mlflow-ai-gateway-completions-or-chat-endpoint",level:3},{value:"Step 2: Connect the MLflow AI Gateway to your MLflow Tracking Server",id:"step-2-connect-the-mlflow-ai-gateway-to-your-mlflow-tracking-server",level:3},{value:"Step 3: Create or find an MLflow Experiment",id:"step-3-create-or-find-an-mlflow-experiment",level:3},{value:"Step 4: Create a run with prompt engineering",id:"step-4-create-a-run-with-prompt-engineering",level:3},{value:"Step 5: Select your endpoint and evaluate the example prompt",id:"step-5-select-your-endpoint-and-evaluate-the-example-prompt",level:3},{value:"Step 6: Try a prompt of your choosing",id:"step-6-try-a-prompt-of-your-choosing",level:3},{value:"Step 7: Capture your choice of LLM, prompt template, and parameters as an MLflow Run",id:"step-7-capture-your-choice-of-llm-prompt-template-and-parameters-as-an-mlflow-run",level:3},{value:"Step 8: Try new inputs",id:"step-8-try-new-inputs",level:3},{value:"Step 9: Adjust your prompt template and create a new Run",id:"step-9-adjust-your-prompt-template-and-create-a-new-run",level:3},{value:"Step 10: Evaluate the new prompt template on previous inputs",id:"step-10-evaluate-the-new-prompt-template-on-previous-inputs",level:3},{value:"Step 11: Load evaluation data programmatically",id:"step-11-load-evaluation-data-programmatically",level:3},{value:"Step 12: Generate predictions programmatically",id:"quickstart-score",level:3},{value:"Step 13: Perform metric-based evaluation of your model&#39;s outputs",id:"step-13-perform-metric-based-evaluation-of-your-models-outputs",level:2},{value:"Deployment for real-time serving",id:"deploy-prompt-serving",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"prompt-engineering-ui-experimental",children:"Prompt Engineering UI (Experimental)"})}),"\n",(0,l.jsxs)(n.p,{children:["Starting in MLflow 2.7, the MLflow Tracking UI provides a best-in-class experience for prompt\nengineering. With no code required, you can try out multiple LLMs from the\n",(0,l.jsx)(n.a,{href:"/llms/deployments",children:"MLflow AI Gateway"}),", parameter configurations, and prompts to build a variety of models for\nquestion answering, document summarization, and beyond. Using the embedded Evaluation UI, you can\nalso evaluate multiple models on a set of inputs and compare the responses to select the best one.\nEvery model created with the prompt engineering UI is stored in the ",(0,l.jsx)(n.a,{href:"/model",children:"MLflow Model"}),"\nformat and can be deployed for batch or real time inference. All configurations (prompt templates,\nchoice of LLM, parameters, etc.) are tracked as ",(0,l.jsx)(n.a,{href:"/tracking",children:"MLflow Runs"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"prompt-engineering-quickstart",children:"Quickstart"}),"\n",(0,l.jsx)(n.p,{children:"The following guide will get you started with MLflow's UI for prompt engineering."}),"\n",(0,l.jsx)(n.h3,{id:"step-1-create-an-mlflow-ai-gateway-completions-or-chat-endpoint",children:"Step 1: Create an MLflow AI Gateway Completions or Chat Endpoint"}),"\n",(0,l.jsxs)(n.p,{children:["To use the prompt engineering UI, you need to create one or more ",(0,l.jsx)(n.a,{href:"/llms/deployments",children:"MLflow AI Gateway"}),"\ncompletions or chat ",(0,l.jsx)(n.a,{href:"/llms/deployments#deployments-endpoints",children:"Endpoints"}),". Follow the\n",(0,l.jsx)(n.a,{href:"/llms/deployments#deployments-quickstart",children:"MLflow AI Gateway Quickstart guide"})," to easily create an endpoint in less than five\nminutes. If you already have access to an MLflow AI Gateway endpoint of type ",(0,l.jsx)(n.code,{children:"llm/v1/completions"}),"\nor ",(0,l.jsx)(n.code,{children:"llm/v1/chat"}),", you can skip this step."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow gateway start --config-path config.yaml --port 7000\n"})}),"\n",(0,l.jsx)(n.h3,{id:"step-2-connect-the-mlflow-ai-gateway-to-your-mlflow-tracking-server",children:"Step 2: Connect the MLflow AI Gateway to your MLflow Tracking Server"}),"\n",(0,l.jsxs)(n.p,{children:["The prompt engineering UI also requires a connection between the MLflow AI Gateway and the MLflow\nTracking Server. To connect the MLflow AI Gateway with the MLflow Tracking Server, simply set the\n",(0,l.jsx)(n.code,{children:"MLFLOW_DEPLOYMENTS_TARGET"})," environment variable in the environment where the server is running and\nrestart the server. For example, if the MLflow AI Gateway is running at ",(0,l.jsx)(n.code,{children:"http://localhost:7000"}),", you\ncan start an MLflow Tracking Server in a shell on your local machine and connect it to the\nMLflow AI Gateway using the ",(0,l.jsx)(r.A,{to:"/api_reference/cli.html",target:"_blank",children:"mlflow server"})," command as follows:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'export MLFLOW_DEPLOYMENTS_TARGET="http://127.0.0.1:7000"\nmlflow server --port 5000\n'})}),"\n",(0,l.jsx)(n.h3,{id:"step-3-create-or-find-an-mlflow-experiment",children:"Step 3: Create or find an MLflow Experiment"}),"\n",(0,l.jsx)(n.p,{children:"Next, open an existing MLflow Experiment in the MLflow UI, or create a new experiment."}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:650,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(61646).A+"",width:"2592",height:"1972"})})}),"\n",(0,l.jsx)(n.h3,{id:"step-4-create-a-run-with-prompt-engineering",children:"Step 4: Create a run with prompt engineering"}),"\n",(0,l.jsxs)(n.p,{children:["Once you have opened the Experiment, click the ",(0,l.jsx)(n.strong,{children:"New Run"})," button and select\n",(0,l.jsx)(n.em,{children:"using Prompt Engineering"}),". This will open the prompt engineering playground where you can try\nout different LLMs, parameters, and prompts."]}),"\n",(0,l.jsxs)("div",{style:{display:"flex",alignItems:"center",justifyContent:"center"},children:[(0,l.jsx)("div",{style:{width:"25%"},children:(0,l.jsx)(n.img,{src:t(40751).A+"",width:"1190",height:"556"})}),(0,l.jsx)("div",{style:{width:"70%"},children:(0,l.jsx)(n.img,{src:t(28179).A+"",width:"2404",height:"1792"})})]}),"\n",(0,l.jsx)(n.h3,{id:"step-5-select-your-endpoint-and-evaluate-the-example-prompt",children:"Step 5: Select your endpoint and evaluate the example prompt"}),"\n",(0,l.jsxs)(n.p,{children:["Next, click the ",(0,l.jsx)(n.em,{children:"Select endpoint"})," dropdown and select the MLflow AI Gateway completions endpoint you created in\nStep 1. Then, click the ",(0,l.jsx)(n.strong,{children:"Evaluate"})," button to test out an example prompt engineering use case\nfor generating product advertisements."]}),"\n",(0,l.jsxs)(n.p,{children:["MLflow will embed the specified ",(0,l.jsx)(n.em,{children:"stock_type"})," input\nvariable value - ",(0,l.jsx)(n.code,{children:'"books"'})," - into the specified ",(0,l.jsx)(n.em,{children:"prompt template"})," and send it to the LLM\nassociated with the MLflow AI Gateway endpoint with the configured ",(0,l.jsx)(n.em,{children:"temperature"})," (currently ",(0,l.jsx)(n.code,{children:"0.01"}),")\nand ",(0,l.jsx)(n.em,{children:"max_tokens"})," (currently 1000). The LLM response will appear in the ",(0,l.jsx)(n.em,{children:"Output"})," section."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(53688).A+"",width:"4126",height:"2266"})}),"\n",(0,l.jsx)(n.h3,{id:"step-6-try-a-prompt-of-your-choosing",children:"Step 6: Try a prompt of your choosing"}),"\n",(0,l.jsx)(n.p,{children:"Replace the prompt template from the previous step with a prompt template of your choosing.\nPrompts can define multiple variables. For example, you can use the following prompt template\nto instruct the LLM to answer questions about the MLflow documentation:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'Read the following article from the MLflow documentation that appears between triple\nbackticks. Then, answer the question about the documentation that appears between triple quotes.\nInclude relevant links and code examples in your answer.\n\n```{{article}}```\n\n"""\n{{question}}\n"""\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Then, fill in the input variables. For example, in the MLflow documentation\nuse case, the ",(0,l.jsx)(n.em,{children:"article"})," input variable can be set to the contents of\n",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/tracking.html#logging-data-to-runs",children:"https://mlflow.org/docs/latest/tracking.html#logging-data-to-runs"})," and the ",(0,l.jsx)(n.em,{children:"question"})," input variable\ncan be set to ",(0,l.jsx)(n.code,{children:'"How do I create a new MLflow Run using the Python API?"'}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["Finally, click the ",(0,l.jsx)(n.strong,{children:"Evaluate"})," button to see the new output. You can also try choosing a larger\nvalue of ",(0,l.jsx)(n.em,{children:"temperature"})," to observe how the LLM's output changes."]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:820,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(26401).A+"",width:"2360",height:"1732"})})}),"\n",(0,l.jsx)(n.h3,{id:"step-7-capture-your-choice-of-llm-prompt-template-and-parameters-as-an-mlflow-run",children:"Step 7: Capture your choice of LLM, prompt template, and parameters as an MLflow Run"}),"\n",(0,l.jsxs)(n.p,{children:["Once you're satisfied with your chosen prompt template and parameters, click the ",(0,l.jsx)(n.strong,{children:"Create Run"}),"\nbutton to store this information, along with your choice of LLM, as an MLflow Run. This will\ncreate a new Run with the prompt template, parameters, and choice of LLM stored as Run params.\nIt will also automatically create an MLflow Model with this information that can be used for batch\nor real-time inference."]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["To view this information, click the Run name to open the ",(0,l.jsx)(n.strong,{children:"Run"})," page:"]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:750,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(52430).A+"",width:"3014",height:"1990"})})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["You can also see the parameters and compare them with other configurations by opening the ",(0,l.jsx)(n.strong,{children:"Table"}),"\nview tab:"]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:750,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(28321).A+"",width:"2892",height:"852"})})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["After your Run is created, MLflow will open the ",(0,l.jsx)(n.strong,{children:"Evaluation"})," tab where you can see your latest\nplayground input & output and try out additional inputs:"]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:750,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(17225).A+"",width:"2898",height:"1468"})})}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"step-8-try-new-inputs",children:"Step 8: Try new inputs"}),"\n",(0,l.jsx)(n.p,{children:"To test the behavior of your chosen LLM, prompt template, and parameters on a new inputs:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Click the ",(0,l.jsx)(n.em,{children:"Add Row"})," button and fill in a value(s) your prompt template's input variable(s).\nFor example, in the MLflow documentation use case, you can try asking a question\nunrelated to MLflow to see how the LLM responds. This is important to ensure that the application\nis robust to irrelevant inputs."]}),"\n"]}),"\n",(0,l.jsxs)("div",{style:{display:"flex",alignItems:"center",justifyContent:"center"},children:[(0,l.jsx)("div",{style:{width:"10%"},children:(0,l.jsx)(n.img,{src:t(94611).A+"",width:"438",height:"128"})}),(0,l.jsx)("div",{style:{width:"50%"},children:(0,l.jsx)(n.img,{src:t(29369).A+"",width:"1264",height:"990"})})]}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Then, click the ",(0,l.jsx)(n.strong,{children:"Evaluate"})," button to see the output."]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:650,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(62564).A+"",width:"1636",height:"1206"})})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Finally, click the ",(0,l.jsx)(n.strong,{children:"Save"})," button to store the new inputs and output."]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:650,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(970).A+"",width:"1636",height:"1208"})})}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"step-9-adjust-your-prompt-template-and-create-a-new-run",children:"Step 9: Adjust your prompt template and create a new Run"}),"\n",(0,l.jsxs)(n.p,{children:["As you try additional inputs, you might discover scenarios where your choice of LLM, prompt\ntemplate, and parameters doesn't perform as well as you would like. For example, in the\nMLflow documentation use case, the LLM still attempts to answer irrelevant\nquestions about ",(0,l.jsx)(n.a,{href:"/projects",children:"MLflow Projects"})," even if the answer does not appear in the\nspecified article."]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["To improve performance, create a new Run by selecting the ",(0,l.jsx)(n.em,{children:"Duplicate run"})," option from the context\nmenu. For example, in the MLflow documentation use case, adding the following text to\nthe prompt template helps improve robustness to irrelevant questions:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'If the question does not relate to the article, respond exactly with the phrase\n"I do not know how to answer that question." Do not include any additional text in your\nresponse.\n'})}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:500,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(87802).A+"",width:"1096",height:"974"})})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Then, from the prompt engineering playground, adjust the prompt template (and / or choice of\nLLM and parameters), evaluate an input, and click the ",(0,l.jsx)(n.strong,{children:"Create Run"})," button to create a new Run."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(43310).A+"",width:"2376",height:"1754"})}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"step-10-evaluate-the-new-prompt-template-on-previous-inputs",children:"Step 10: Evaluate the new prompt template on previous inputs"}),"\n",(0,l.jsx)(n.p,{children:"Now that you've made an adjustment to your prompt template, it's important to make sure that\nthe new template performs well on the previous inputs and compare the outputs with older\nconfigurations."}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["From the ",(0,l.jsx)(n.strong,{children:"Evaluation"})," tab, click the ",(0,l.jsx)(n.strong,{children:"Evaluate all"})," button next to the new Run to evaluate\nall of the previous inputs."]}),"\n",(0,l.jsx)("div",{className:"center-div",style:{maxWidth:300,width:"100%"},children:(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(31662).A+"",width:"726",height:"996"})})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Click the ",(0,l.jsx)(n.strong,{children:"Save"})," button to store the results."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(81929).A+"",width:"2336",height:"1204"})}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"step-11-load-evaluation-data-programmatically",children:"Step 11: Load evaluation data programmatically"}),"\n",(0,l.jsxs)(n.p,{children:["All of the inputs and outputs produced by the MLflow prompt engineering UI and Evaluation UI are stored\nas artifacts in MLflow Runs. They can be accessed programmatically using the ",(0,l.jsx)(i.B,{fn:"mlflow.load_table"})," API\nas follows:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.set_experiment("/Path/to/your/prompt/engineering/experiment")\n\n# Load input and output data across all Runs (configurations) as a Pandas DataFrame\ninputs_outputs_pdf = mlflow.load_table(\n    # All inputs and outputs created from the MLflow UI are stored in an artifact called\n    # "eval_results_table.json"\n    artifact_file="eval_results_table.json",\n    # Include the run ID as a column in the table to distinguish inputs and outputs\n    # produced by different runs\n    extra_columns=["run_id"],\n)\n# Optionally convert the Pandas DataFrame to Spark where it can be stored as a Delta\n# table or joined with existing Delta tables\ninputs_outputs_sdf = spark.createDataFrame(inputs_outputs_pdf)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"quickstart-score",children:"Step 12: Generate predictions programmatically"}),"\n",(0,l.jsxs)(n.p,{children:["Once you have found a configuration of LLM, prompt template, and parameters that performs well, you\ncan generate predictions using the corresponding MLflow Model in a Python environment of your choosing,\nor you can ",(0,l.jsx)(n.a,{href:"#deploy-prompt-serving",children:"deploy it for real-time serving"}),"."]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["To load the MLflow Model in a notebook for batch inference, click on the Run's name to open the\n",(0,l.jsx)(n.strong,{children:"Run Page"})," and select the ",(0,l.jsx)(n.em,{children:"model"})," directory in the ",(0,l.jsx)(n.strong,{children:"Artifact Viewer"}),". Then, copy the first\nfew lines of code from the ",(0,l.jsx)(n.em,{children:"Predict on a Pandas DataFrame"})," section and run them in a Python\nenvironment of your choosing, for example:"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(26081).A+"",width:"3004",height:"1388"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nlogged_model = "runs:/8451075c46964f82b85fe16c3d2b7ea0/model"\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n'})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Then, to generate predictions, call the ",(0,l.jsx)(i.B,{fn:"mlflow.pyfunc.PyFuncModel.predict",children:(0,l.jsx)(n.code,{children:"predict()"})})," method\nand pass in a dictionary of input variables. For example:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'article_text = """\nAn MLflow Project is a format for packaging data science code in a reusable and reproducible way.\nThe MLflow Projects component includes an API and command-line tools for running projects, which\nalso integrate with the Tracking component to automatically record the parameters and git commit\nof your source code for reproducibility.\n\nThis article describes the format of an MLflow Project and how to run an MLflow project remotely\nusing the MLflow CLI, which makes it easy to vertically scale your data science code.\n"""\nquestion = "What is an MLflow project?"\n\nloaded_model.predict({"article": article_text, "question": question})\n'})}),"\n",(0,l.jsxs)(n.p,{children:["For more information about deployment for real-time serving with MLflow,\nsee the ",(0,l.jsx)(n.a,{href:"#deploy-prompt-serving",children:"instructions below"}),"."]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"step-13-perform-metric-based-evaluation-of-your-models-outputs",children:"Step 13: Perform metric-based evaluation of your model's outputs"}),"\n",(0,l.jsxs)(n.p,{children:["If you'd like to assess your model's performance on specific metrics, MLflow provides the ",(0,l.jsx)(i.B,{fn:"mlflow.evaluate"}),"\nAPI. Let's evaluate our model on some ",(0,l.jsx)(n.a,{href:"/llms/llm-evaluate#llm-eval-default-metrics",children:"pre-defined metrics"}),"\nfor text summarization:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\n\nlogged_model = "runs:/840a5c43f3fb46f2a2059b761557c1d0/model"\n\narticle_text = """\nAn MLflow Project is a format for packaging data science code in a reusable and reproducible way.\nThe MLflow Projects component includes an API and command-line tools for running projects, which\nalso integrate with the Tracking component to automatically record the parameters and git commit\nof your source code for reproducibility.\n\nThis article describes the format of an MLflow Project and how to run an MLflow project remotely\nusing the MLflow CLI, which makes it easy to vertically scale your data science code.\n"""\nquestion = "What is an MLflow project?"\n\ndata = pd.DataFrame(\n    {\n        "article": [article_text],\n        "question": [question],\n        "ground_truth": [\n            article_text\n        ],  # used for certain evaluation metrics, such as ROUGE score\n    }\n)\n\nwith mlflow.start_run():\n    results = mlflow.evaluate(\n        model=logged_model,\n        data=data,\n        targets="ground_truth",\n        model_type="text-summarization",\n    )\n\neval_table = results.tables["eval_results_table"]\nprint(f"See evaluation table below: \\n{eval_table}")\n'})}),"\n",(0,l.jsx)(n.p,{children:"The evaluation results can also be viewed in the MLflow Evaluation UI:"}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(4456).A+"",width:"2402",height:"1052"})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(i.B,{fn:"mlflow.evaluate"})," API also supports ",(0,l.jsx)(n.a,{href:"/llms/llm-evaluate#llm-eval-custom-metrics",children:"custom metrics"}),",\n",(0,l.jsx)(n.a,{href:"/llms/llm-evaluate#llm-eval-static-dataset",children:"static dataset evaluation"}),", and much more. For a\nmore in-depth guide, see ",(0,l.jsx)(n.a,{href:"/llms/llm-evaluate",children:"MLflow LLM Evaluation"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"deploy-prompt-serving",children:"Deployment for real-time serving"}),"\n",(0,l.jsx)(n.p,{children:"Once you have found a configuration of LLM, prompt template, and parameters that performs well, you\ncan deploy the corresponding MLflow Model for real-time serving as follows:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Register your model with the MLflow Model Registry. The following example registers\nan MLflow Model created from the ",(0,l.jsx)(n.a,{href:"#quickstart-score",children:"Quickstart"})," as Version 1 of the\nRegistered Model named ",(0,l.jsx)(n.code,{children:'"mlflow_docs_qa_model"'}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'mlflow.register_model(\n    model_uri="runs:/8451075c46964f82b85fe16c3d2b7ea0/model",\n    name="mlflow_docs_qa_model",\n)\n'})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"Define the following environment variables in the environment where you will run your\nMLflow Model Server, such as a shell on your local machine:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"MLFLOW_DEPLOYMENTS_TARGET"}),": The URL of the MLflow AI Gateway"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Use the ",(0,l.jsx)(r.A,{to:"/api_reference/cli.html",target:"_blank",children:"mlflow models serve"})," command to start the MLflow Model Server. For example,\nrunning the following command from a shell on your local machine will serve the model\non port 8000:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"mlflow models serve --model-uri models:/mlflow_docs_qa_model/1 --port 8000\n"})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"Once the server has been started, it can be queried via REST API call. For example:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'input=\'\n{\n    "dataframe_records": [\n        {\n            "article": "An MLflow Project is a format for packaging data science code...",\n            "question": "What is an MLflow Project?"\n        }\n    ]\n}\'\n\necho $input | curl \\\n  -s \\\n  -X POST \\\n  https://localhost:8000/invocations\n  -H \'Content-Type: application/json\' \\\n  -d @-\n'})}),"\n",(0,l.jsxs)(n.p,{children:["where ",(0,l.jsx)(n.code,{children:"article"})," and ",(0,l.jsx)(n.code,{children:"question"})," are replaced with the input variable(s) from your\nprompt template."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(h,{...e})}):h(e)}},87802:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/duplicate_run-71f18e13acc59fb8bc860b27f2c24529.png"},94611:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/add_row-c9216bca634ec1383758a84c35d17101.png"}}]);