"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4239],{15869:(e,n,t)=>{t.d(n,{A:()=>l});const l="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   width="451.52316mm"
   height="162.82199mm"
   viewBox="0 0 451.52316 162.82198"
   version="1.1"
   id="svg8"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/">
  <defs
     id="defs2" />
  <metadata
     id="metadata5">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     id="g2106"
     transform="matrix(0.96564264,0,0,0.96251987,-846.8299,244.29116)">
    <circle
       style="fill:#009688;fill-opacity:0.980392;stroke:none;stroke-width:0.141404;stop-color:#000000"
       id="path875-5-9-7-3-2-3-9-9-8-0-0-5-87-7"
       cx="964.56165"
       cy="-169.22266"
       r="33.234192" />
    <path
       id="rect1249-6-3-4-4-3-6-6-1-2"
       style="fill:#ffffff;fill-opacity:0.980392;stroke:none;stroke-width:0.146895;stop-color:#000000"
       d="m 962.2685,-187.40837 -6.64403,14.80375 -3.03599,6.76393 -6.64456,14.80375 30.59142,-21.56768 h -14.35312 l 20.99715,-14.80375 z" />
  </g>
  <path
     style="font-size:79.7151px;line-height:1.25;font-family:Ubuntu;-inkscape-font-specification:Ubuntu;letter-spacing:0px;word-spacing:0px;fill:#009688;stroke-width:1.99288"
     d="M 142.02261,108.83303 V 53.590465 h 33.32092 v 6.616353 h -25.58855 v 16.660457 h 22.71881 v 6.536638 h -22.71881 v 25.429117 z m 52.29297,-5.34091 q 2.6306,0 4.62348,-0.0797 2.07259,-0.15943 3.42774,-0.47829 V 90.578273 q -0.79715,-0.398576 -2.63059,-0.637721 -1.75374,-0.31886 -4.30462,-0.31886 -1.67402,0 -3.58718,0.239145 -1.83345,0.239145 -3.42775,1.036296 -1.51459,0.717436 -2.55088,2.072593 -1.0363,1.275442 -1.0363,3.427749 0,3.985755 2.55089,5.580055 2.55088,1.51459 6.93521,1.51459 z m -0.63772,-37.147239 q 4.46404,0 7.49322,1.195727 3.10889,1.116011 4.94233,3.268319 1.91317,2.072593 2.71032,5.022052 0.79715,2.869743 0.79715,6.377208 V 108.1156 q -0.95658,0.15943 -2.71031,0.47829 -1.67402,0.23914 -3.82633,0.47829 -2.15231,0.23914 -4.70319,0.39857 -2.47117,0.23915 -4.94234,0.23915 -3.50746,0 -6.45692,-0.71744 -2.94946,-0.71743 -5.10177,-2.23202 -2.1523,-1.5943 -3.34803,-4.14519 -1.19573,-2.55088 -1.19573,-6.13806 0,-3.427749 1.35516,-5.898917 1.43487,-2.471168 3.82632,-3.985755 2.39146,-1.514587 5.58006,-2.232023 3.18861,-0.717436 6.69607,-0.717436 1.11601,0 2.31174,0.15943 1.19572,0.07972 2.23202,0.31886 1.11601,0.159431 1.91316,0.318861 0.79715,0.15943 1.11601,0.239145 v -2.072593 q 0,-1.833447 -0.39857,-3.587179 -0.39858,-1.833448 -1.43487,-3.188604 -1.0363,-1.434872 -2.86975,-2.232023 -1.75373,-0.876866 -4.62347,-0.876866 -3.6669,0 -6.45693,0.558005 -2.71031,0.478291 -4.06547,1.036297 l -0.87686,-6.138063 q 1.43487,-0.637721 4.7829,-1.195727 3.34804,-0.637721 7.25408,-0.637721 z m 37.86462,37.147239 q 4.54377,0 6.69607,-1.19573 2.23203,-1.19572 2.23203,-3.826322 0,-2.710314 -2.15231,-4.304616 -2.15231,-1.594302 -7.09465,-3.587179 -2.39145,-0.956581 -4.62347,-1.913163 -2.15231,-1.036296 -3.74661,-2.391453 -1.5943,-1.355157 -2.55088,-3.268319 -0.95659,-1.913163 -0.95659,-4.703191 0,-5.500342 4.06547,-8.688946 4.06547,-3.26832 11.0804,-3.26832 1.75374,0 3.50747,0.239146 1.75373,0.15943 3.26832,0.47829 1.51458,0.239146 2.6306,0.558006 1.19572,0.31886 1.83344,0.558006 l -1.35515,6.377208 q -1.19573,-0.637721 -3.74661,-1.275442 -2.55089,-0.717436 -6.13807,-0.717436 -3.10889,0 -5.42062,1.275442 -2.31174,1.195727 -2.31174,3.826325 0,1.355157 0.47829,2.391453 0.55801,1.036296 1.5943,1.913163 1.11601,0.797151 2.71031,1.514587 1.59431,0.717436 3.82633,1.514587 2.94946,1.116011 5.2612,2.232022 2.31173,1.036297 3.90604,2.471169 1.67401,1.434871 2.55088,3.507464 0.87687,1.992878 0.87687,4.942337 0,5.739482 -4.30462,8.688942 -4.2249,2.94946 -12.1167,2.94946 -5.50034,0 -8.60923,-0.95658 -3.10889,-0.87686 -4.2249,-1.35515 l 1.35516,-6.37721 q 1.27544,0.47829 4.06547,1.43487 2.79003,0.95658 7.4135,0.95658 z m 32.84256,-36.110942 h 15.70387 v 6.217778 h -15.70387 v 19.131625 q 0,3.108889 0.47829,5.181481 0.47829,1.992878 1.43487,3.188608 0.95658,1.11601 2.39145,1.5943 1.43487,0.47829 3.34804,0.47829 3.34803,0 5.34091,-0.71744 2.07259,-0.79715 2.86974,-1.11601 l 1.43487,6.13806 q -1.11601,0.55801 -3.90604,1.35516 -2.79003,0.87687 -6.37721,0.87687 -4.2249,0 -7.01492,-1.0363 -2.71032,-1.11601 -4.38434,-3.26832 -1.67401,-2.15231 -2.39145,-5.2612 -0.63772,-3.188599 -0.63772,-7.333784 V 55.822488 l 7.41351,-1.275442 z m 62.49652,41.451852 q -1.35516,-3.58718 -2.55088,-7.01493 -1.19573,-3.507462 -2.47117,-7.094642 h -25.03054 l -5.02205,14.109572 h -8.05123 q 3.18861,-8.76866 5.97863,-16.182165 2.79003,-7.493219 5.42063,-14.189288 2.71031,-6.696069 5.34091,-12.754416 2.6306,-6.138063 5.50034,-12.116696 h 7.09465 q 2.86974,5.978633 5.50034,12.116696 2.6306,6.058347 5.2612,12.754416 2.71031,6.696069 5.50034,14.189288 2.79003,7.413505 5.97863,16.182165 z m -7.25407,-20.48678 q -2.55089,-6.935214 -5.10177,-13.392137 -2.47117,-6.536639 -5.18148,-12.515272 -2.79003,5.978633 -5.34091,12.515272 -2.47117,6.456923 -4.94234,13.392137 z m 37.86453,-35.313791 q 11.6384,0 17.85618,4.464046 6.29749,4.384331 6.29749,13.152992 0,4.782906 -1.75373,8.210656 -1.67402,3.348034 -4.94234,5.500342 -3.1886,2.072592 -7.81208,3.029174 -4.62347,0.956581 -10.44268,0.956581 h -6.13806 v 20.48678 h -7.73236 V 54.387616 q 3.26832,-0.797151 7.25407,-1.036296 4.06547,-0.318861 7.41351,-0.318861 z m 0.63772,6.775784 q -4.94234,0 -7.57294,0.239145 v 21.682508 h 5.8192 q 3.98576,0 7.17436,-0.47829 3.18861,-0.558006 5.34092,-1.753733 2.23202,-1.275441 3.42774,-3.427749 1.19573,-2.152308 1.19573,-5.500342 0,-3.188604 -1.27544,-5.261197 -1.19573,-2.072593 -3.34803,-3.268319 -2.0726,-1.275442 -4.86263,-1.753732 -2.79002,-0.478291 -5.89891,-0.478291 z m 33.16146,-6.217778 h 7.73237 v 55.242565 h -7.73237 z"
     id="text979-1"
     aria-label="FastAPI" />
  <rect
     y="7.1054274e-15"
     x="0"
     height="162.82199"
     width="451.52316"
     id="rect824"
     style="opacity:0.98000004;fill:none;fill-opacity:1;stroke-width:0.31103656" />
  <g
     id="layer1"
     transform="translate(83.131114,-6.0791148)" />
</svg>
"},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var l=t(96540);const r={},o=l.createContext(r);function i(e){const n=l.useContext(o);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),l.createElement(o.Provider,{value:n},e.children)}},37974:(e,n,t)=>{t.d(n,{A:()=>l});const l=t.p+"assets/images/seldon-mlserver-logo-1c6ba2f4aecdd4434e37221e5f4ceba4.png"},65537:(e,n,t)=>{t.d(n,{A:()=>x});var l=t(96540),r=t(34164),o=t(65627),i=t(56347),a=t(50372),s=t(30604),c=t(11861),d=t(78749);function h(e){return l.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,l.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:l,default:r}}=e;return{value:n,label:t,attributes:l,default:r}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function M(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:t}=e;const r=(0,i.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s.aZ)(o),(0,l.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(r.location.search);n.set(o,e),r.replace({...r.location,search:n.toString()})}),[o,r])]}function m(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,o=p(e),[i,s]=(0,l.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!M({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const l=t.find((e=>e.default))??t[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:o}))),[c,h]=u({queryString:t,groupId:r}),[m,j]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,o]=(0,d.Dv)(t);return[r,(0,l.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:r}),g=(()=>{const e=c??m;return M({value:e,tabValues:o})?e:null})();(0,a.A)((()=>{g&&s(g)}),[g]);return{selectedValue:i,selectValue:(0,l.useCallback)((e=>{if(!M({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),h(e),j(e)}),[h,j,o]),tabValues:o}}var j=t(9136);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function w(e){let{className:n,block:t,selectedValue:l,selectValue:i,tabValues:a}=e;const s=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{const n=e.currentTarget,t=s.indexOf(n),r=a[t].value;r!==l&&(c(n),i(r))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},n),children:a.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:l===n?0:-1,"aria-selected":l===n,ref:e=>{s.push(e)},onKeyDown:h,onClick:d,...o,className:(0,r.A)("tabs__item",g.tabItem,o?.className,{"tabs__item--active":l===n}),children:t??n},n)}))})}function N(e){let{lazy:n,children:t,selectedValue:o}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===o));return e?(0,l.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,l.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function f(e){const n=m(e);return(0,y.jsxs)("div",{className:(0,r.A)("tabs-container",g.tabList),children:[(0,y.jsx)(w,{...n,...e}),(0,y.jsx)(N,{...n,...e})]})}function x(e){const n=(0,j.A)();return(0,y.jsx)(f,{...e,children:h(e.children)},String(n))}},67756:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const l=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=t(29030),o=t(56289),i=t(74848);const a=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(l[e])return e}return null};function s(e){let{fn:n,children:t}=e;const s=a(n);if(!s)return(0,i.jsx)(i.Fragment,{children:t});const c=(0,r.Ay)(`/${l[s]}#${n}`);return(0,i.jsx)(o.A,{to:c,target:"_blank",children:t??(0,i.jsxs)("code",{children:[n,"()"]})})}},79329:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var l=t(34164);const r={tabItem:"tabItem_Ymn6"};var o=t(74848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,l.A)(r.tabItem,i),hidden:t,children:n})}},92098:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>h,default:()=>m,frontMatter:()=>d,metadata:()=>l,toc:()=>M});const l=JSON.parse('{"id":"deployment/deploy-model-locally/index","title":"Deploy MLflow Model as a Local Inference Server","description":"MLflow allows you to deploy your model locally using just a single command.","source":"@site/docs/deployment/deploy-model-locally/index.mdx","sourceDirName":"deployment/deploy-model-locally","slug":"/deployment/deploy-model-locally/","permalink":"/docs/latest/deployment/deploy-model-locally/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"MLflow Serving","permalink":"/docs/latest/deployment/"},"next":{"title":"Deploy MLflow Model to Amazon SageMaker","permalink":"/docs/latest/deployment/deploy-model-to-sagemaker/"}}');var r=t(74848),o=t(28453),i=t(56289),a=t(65537),s=t(79329),c=t(67756);const d={sidebar_position:1},h="Deploy MLflow Model as a Local Inference Server",p={},M=[{value:"Deploying Inference Server",id:"deploying-inference-server",level:2},{value:"Inference Server Specification",id:"local-inference-server-spec",level:2},{value:"Endpoints",id:"endpoints",level:3},{value:"Accepted Input Formats",id:"accepted-input-formats",level:3},{value:"CSV Input",id:"csv-input",level:4},{value:"JSON Input",id:"json-input",level:4},{value:"Wrapped Payload Dict",id:"wrapped-payload-dict",level:5},{value:"Raw Payload Dict",id:"raw-payload-dict",level:5},{value:"Encoding complex data",id:"encoding-complex-data",level:4},{value:"Serving Frameworks",id:"serving-frameworks",level:2},{value:"Running Batch Inference",id:"running-batch-inference",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"deploy-mlflow-model-as-a-local-inference-server",children:"Deploy MLflow Model as a Local Inference Server"})}),"\n",(0,r.jsx)(n.p,{children:"MLflow allows you to deploy your model locally using just a single command.\nThis approach is ideal for lightweight applications or for testing your model locally before moving\nit to a staging or production environment."}),"\n",(0,r.jsxs)(n.p,{children:["If you are new to MLflow model deployment, please read the guide on ",(0,r.jsx)(n.a,{href:"/deployment",children:"MLflow Deployment"}),"\nfirst to understand the basic concepts of MLflow models and deployments."]}),"\n",(0,r.jsx)(n.h2,{id:"deploying-inference-server",children:"Deploying Inference Server"}),"\n",(0,r.jsxs)(n.p,{children:["Before deploying, you must have an MLflow Model. If you don't have one, you can create a sample scikit-learn model by following the ",(0,r.jsx)(n.a,{href:"/getting-started",children:"MLflow Tracking Quickstart"}),".\nRemember to note down the model URI, such as ",(0,r.jsx)(n.code,{children:"runs:/<run_id>/<artifact_path>"})," (or ",(0,r.jsx)(n.code,{children:"models:/<model_name>/<model_version>"})," if you registered the model in the ",(0,r.jsx)(n.a,{href:"/model-registry",children:"MLflow Model Registry"}),")."]}),"\n",(0,r.jsxs)(n.p,{children:["Once you have the model ready, deploying to a local server is straightforward. Use the ",(0,r.jsx)(i.A,{to:"/api_reference/cli.html#mlflow-models-serve",target:"_blank",children:"mlflow models serve"})," command for a one-step deployment.\nThis command starts a local server that listens on the specified port and serves your model. Refer to the ",(0,r.jsx)(i.A,{to:"/api_reference/cli.html#mlflow-models-serve",target:"_blank",children:"CLI reference"})," for available options."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow models serve -m runs:/<run_id>/model -p 5000\n"})}),"\n",(0,r.jsx)(n.p,{children:"You can then send a test request to the server as follows:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:5000/invocations -H "Content-Type:application/json"  --data \'{"inputs": [[1, 2], [3, 4], [5, 6]]}\'\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Several command line options are available to customize the server's behavior. For instance, the ",(0,r.jsx)(n.code,{children:"--env-manager"})," option allows you to\nchoose a specific environment manager, like Anaconda, to create the virtual environment. The ",(0,r.jsx)(n.code,{children:"mlflow models"})," module also provides\nadditional useful commands, such as building a Docker image or generating a Dockerfile. For comprehensive details, please refer\nto the ",(0,r.jsx)(i.A,{to:"/api_reference/cli.html#mlflow-models",target:"_blank",children:"MLflow CLI Reference"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"local-inference-server-spec",children:"Inference Server Specification"}),"\n",(0,r.jsx)(n.h3,{id:"endpoints",children:"Endpoints"}),"\n",(0,r.jsx)(n.p,{children:"The inference server provides 4 endpoints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/invocations"}),": An inference endpoint that accepts POST requests with input data and returns predictions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/ping"}),": Used for health checks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/health"}),": Same as /ping"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/version"}),": Returns the MLflow version."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"accepted-input-formats",children:"Accepted Input Formats"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"/invocations"})," endpoint accepts CSV or JSON inputs. The input format must be specified in the\n",(0,r.jsx)(n.code,{children:"Content-Type"})," header as either ",(0,r.jsx)(n.code,{children:"application/json"})," or ",(0,r.jsx)(n.code,{children:"application/csv"}),"."]}),"\n",(0,r.jsx)(n.h4,{id:"csv-input",children:"CSV Input"}),"\n",(0,r.jsx)(n.p,{children:"CSV input must be a valid pandas.DataFrame CSV representation. For example:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/csv' --data '1,2,3,4'"})}),"\n",(0,r.jsx)(n.h4,{id:"json-input",children:"JSON Input"}),"\n",(0,r.jsx)(n.p,{children:"You can either pass a flat dictionary corresponding to the desired model payload or wrap the\npayload in a dict with a dict key that specifies your payload format."}),"\n",(0,r.jsx)(n.h5,{id:"wrapped-payload-dict",children:"Wrapped Payload Dict"}),"\n",(0,r.jsx)(n.p,{children:"If your model format is not supported above or you want to avoid transforming your input data to\nthe required payload format, you can leverage the dict payload structures below."}),"\n",(0,r.jsxs)("table",{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Field"}),(0,r.jsx)("th",{children:"Description"}),(0,r.jsx)("th",{children:"Example"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"dataframe_split"})}),(0,r.jsxs)("td",{children:["Pandas DataFrames in the ",(0,r.jsx)(n.code,{children:"split"})," orientation."]}),(0,r.jsx)("td",{children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{"dataframe_split": pandas_df.to_dict(orient="split")}\n'})})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"dataframe_records"})}),(0,r.jsxs)("td",{children:["Pandas DataFrame in the records orientation. ",(0,r.jsx)(n.strong,{children:"We do not recommend using this format because it is not guaranteed to preserve column ordering."})]}),(0,r.jsx)("td",{children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{"dataframe_records": pandas_df.to_dict(orient="records")}\n'})})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"instances"})}),(0,r.jsxs)("td",{children:["Tensor input formatted as described in ",(0,r.jsx)(n.a,{href:"https://www.tensorflow.org/tfx/serving/api_rest#request_format_2",children:"TF Serving\u2019s API docs"})," where the provided inputs will be cast to Numpy arrays."]}),(0,r.jsx)("td",{children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{"instances": [1.0, 2.0, 5.0]}\n'})})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"inputs"})}),(0,r.jsxs)("td",{children:["Same as ",(0,r.jsx)(n.code,{children:"instances"})," but with a different key."]}),(0,r.jsx)("td",{children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{"inputs": [["Cheese"], ["and", "Crackers"]]}\n'})})})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'# Prerequisite: serve a custom pyfunc OpenAI model (not mlflow.openai) on localhost:5678\n#   that defines inputs in the below format and params of `temperature` and `max_tokens`\n\nimport json\nimport requests\n\npayload = json.dumps(\n    {\n        "inputs": {"messages": [{"role": "user", "content": "Tell a joke!"}]},\n        "params": {\n            "temperature": 0.5,\n            "max_tokens": 20,\n        },\n    }\n)\nresponse = requests.post(\n    url=f"http://localhost:5678/invocations",\n    data=payload,\n    headers={"Content-Type": "application/json"},\n)\nprint(response.json())\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The JSON input can also include an optional ",(0,r.jsx)(n.code,{children:"params"})," field for passing additional parameters.\nValid parameter types are ",(0,r.jsx)(n.code,{children:"Union[DataType, List[DataType], None]"}),", where DataType\nis ",(0,r.jsx)(c.B,{fn:"mlflow.types.DataType",children:(0,r.jsx)(n.code,{children:"MLflow data types"})}),". To pass parameters,\na valid ",(0,r.jsx)(n.a,{href:"/model/signatures#model-signature",children:"Model Signature"})," with ",(0,r.jsx)(n.code,{children:"params"})," must be defined."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl http://127.0.0.1:5000/invocations -H \'Content-Type: application/json\' -d \'{\n    "inputs": {"question": ["What color is it?"],\n                "context": ["Some people said it was green but I know that it is pink."]},\n    "params": {"max_answer_len": 10}\n}\'\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Since JSON discards type information, MLflow will cast the JSON input to the input type specified\nin the model's schema if available. If your model is sensitive to input types, it is recommended that\na schema is provided for the model to ensure that type mismatch errors do not occur at inference time.\nIn particular, Deep Learning models are typically strict about input types and will need a model schema in order\nfor the model to score correctly. For complex data types, see ",(0,r.jsx)(n.a,{href:"#encoding-complex-data",children:"Encoding complex data"})," below."]})}),"\n",(0,r.jsx)(n.h5,{id:"raw-payload-dict",children:"Raw Payload Dict"}),"\n",(0,r.jsx)(n.p,{children:"If your payload is in a format that your mlflow served model will accept and it's in the supported\nmodels below, you can pass a raw payload dict."}),"\n",(0,r.jsxs)("table",{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Supported Request Format"}),(0,r.jsx)("th",{children:"Description"}),(0,r.jsx)("th",{children:"Example"})]})}),(0,r.jsx)("tbody",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"OpenAI Chat"}),(0,r.jsxs)("td",{children:[(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat/create",children:"OpenAI chat request payload"}),"\u2020"]}),(0,r.jsx)("td",{children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{\n    "messages": [{"role": "user", "content": "Tell a joke!"}],  # noqa\n    "temperature": 0.0,\n}\n'})})})]})})]}),"\n",(0,r.jsxs)(n.p,{children:["\u2020 Note that the ",(0,r.jsx)(n.code,{children:"model"})," argument ",(0,r.jsx)(n.strong,{children:"should not"})," be included when using the OpenAI APIs, due to its configuration being set by the MLflow model instance. All other parameters can be freely used, provided that they are defined within the ",(0,r.jsx)(n.code,{children:"params"})," argument within the logged model signature."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="Example"',children:'# Prerequisite: serve a Pyfunc model accepts OpenAI-compatible chat requests on localhost:5678 that defines\n#   `temperature` and `max_tokens` as parameters within the logged model signature\n\nimport json\nimport requests\n\npayload = json.dumps(\n    {\n        "messages": [{"role": "user", "content": "Tell a joke!"}],\n        "temperature": 0.5,\n        "max_tokens": 20,\n    }\n)\nrequests.post(\n    url=f"http://localhost:5678/invocations",\n    data=payload,\n    headers={"Content-Type": "application/json"},\n)\nprint(requests.json())\n'})}),"\n",(0,r.jsx)(n.h4,{id:"encoding-complex-data",children:"Encoding complex data"}),"\n",(0,r.jsx)(n.p,{children:"Complex data types, such as dates or binary, do not have a native JSON representation. If you include a model\nsignature, MLflow can automatically decode supported data types from JSON. The following data type conversions\nare supported:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"binary: data is expected to be base64 encoded, MLflow will automatically base64 decode."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["datetime: data is expected to be encoded as a string according to\n",(0,r.jsx)(n.a,{href:"https://www.iso.org/iso-8601-date-and-time-format.html",children:"ISO 8601 specification"}),".\nMLflow will parse this into the appropriate datetime representation on the given platform."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Example requests:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# record-oriented DataFrame input with binary column "b"\ncurl http://127.0.0.1:5000/invocations -H \'Content-Type: application/json\' -d \'[\n    {"a": 0, "b": "dGVzdCBiaW5hcnkgZGF0YSAw"},\n    {"a": 1, "b": "dGVzdCBiaW5hcnkgZGF0YSAx"},\n    {"a": 2, "b": "dGVzdCBiaW5hcnkgZGF0YSAy"}\n]\'\n\n# record-oriented DataFrame input with datetime column "b"\ncurl http://127.0.0.1:5000/invocations -H \'Content-Type: application/json\' -d \'[\n    {"a": 0, "b": "2020-01-01T00:00:00Z"},\n    {"a": 1, "b": "2020-02-01T12:34:56Z"},\n    {"a": 2, "b": "2021-03-01T00:00:00Z"}\n]\'\n'})}),"\n",(0,r.jsx)(n.h2,{id:"serving-frameworks",children:"Serving Frameworks"}),"\n",(0,r.jsxs)(n.p,{children:["By default, MLflow uses ",(0,r.jsx)(n.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"}),", a modern ASGI web application framework for Python, to serve the inference endpoint.\nFastAPI handles requests asynchronously and is recognized as one of the fastest Python frameworks. This production-ready framework works well for most use cases.\nAdditionally, MLflow also integrates with ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest",children:"MLServer"})," as an alternative serving engine. MLServer achieves\nhigher performance and scalability by leveraging asynchronous request/response paradigm and workload offloading. Also MLServer is used as the core Python\ninference server in Kubernetes-native frameworks like ",(0,r.jsx)(n.a,{href:"https://docs.seldon.io/projects/seldon-core/en/latest",children:"Seldon Core"})," and\n",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website",children:"KServe (formerly known as KFServing)"}),", hence which provides advanced features such as canary deployment and\nauto scaling out of the box."]}),"\n",(0,r.jsxs)("table",{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{}),(0,r.jsx)("th",{children:(0,r.jsx)("div",{className:"max-height-img-container",style:{maxHeight:60},children:(0,r.jsx)("span",{children:(0,r.jsx)(n.img,{src:t(15869).A+"",width:"1707",height:"615"})})})}),(0,r.jsx)("th",{children:(0,r.jsx)("div",{className:"max-height-img-container",style:{maxHeight:60},children:(0,r.jsx)("span",{children:(0,r.jsx)(n.img,{src:t(37974).A+"",width:"714",height:"194"})})})})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.strong,{children:"Use Case"})}),(0,r.jsx)("td",{children:"Standard usecases including local testing."}),(0,r.jsx)("td",{children:"High-scale production environment."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.strong,{children:"Set Up"})}),(0,r.jsx)("td",{children:"FastAPI is installed by default with MLflow."}),(0,r.jsx)("td",{children:"Needs to be installed separately."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.strong,{children:"Performance"})}),(0,r.jsxs)("td",{children:["FastAPI natively supports asynchronous request handling, making it well-suited for I/O-bound tasks including ML workloads. Refer to ",(0,r.jsx)(n.a,{href:"https://fastapi.tiangolo.com/benchmarks/",children:"FastAPI Benchmark"})," for the benchmarks and comparisons with other Python frameworks."]}),(0,r.jsxs)("td",{children:["Designed for high-performance ML workloads, often delivering better throughput and efficiency. MLServer support asynchronous request/response paradigm, by offloading ML inference workload to a separate worker pool (processes), so that the server can continue to accept new requests while the inference is being processed. Please refer to the ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html",children:"MLServer Parallel Inference"})," for more details on how they achieve this. Additionally, MLServer supports ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html",children:"Adaptive Bacthing"})," that transparently batch requests together to improve throughput and efficiency."]})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.strong,{children:"Scalability"})}),(0,r.jsxs)("td",{children:["While FastAPI works well in a distributed environment in general, MLflow simply runs it with ",(0,r.jsx)(n.code,{children:"uvicorn"})," and does not support holizontal scaling out of the box."]}),(0,r.jsxs)("td",{children:["Additionally to the support for parallel inference as mentioned above, MLServer is used as the core inference server in Kubernetes-native frameworks such as ",(0,r.jsx)(n.a,{href:"https://docs.seldon.io/projects/seldon-core/en/latest/",children:"Seldon Core"})," and ",(0,r.jsx)(n.a,{href:"https://kserve.github.io/website/",children:"KServe"})," (formerly known as KFServing). By deploying ",(0,r.jsx)(n.a,{href:"/deployment/deploy-model-to-kubernetes",children:"MLflow models to Kubernetes with MLServer"}),", you can leverage the advanced features of these frameworks such as autoscaling to achieve high scalability."]})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:["MLServer exposes the same scoring API through the ",(0,r.jsx)(n.code,{children:"/invocations"})," endpoint.\nTo deploy with MLServer, first install additional dependencies with ",(0,r.jsx)(n.code,{children:"pip install mlflow[extras]"}),",\nthen execute the deployment command with the ",(0,r.jsx)(n.code,{children:"--enable-mlserver"})," option. For example,"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow models serve -m runs:/<run_id>/model -p 5000 --enable-mlserver\n"})}),"\n",(0,r.jsxs)(n.p,{children:["To read more about the integration between MLflow and MLServer, please check the ",(0,r.jsx)(n.a,{href:"https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html",children:"end-to-end example"})," in the MLServer documentation.\nYou can also find guides to deploy MLflow models to a Kubernetes cluster using MLServer in ",(0,r.jsx)(n.a,{href:"/deployment/deploy-model-to-kubernetes",children:"Deploying a model to Kubernetes"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"running-batch-inference",children:"Running Batch Inference"}),"\n",(0,r.jsxs)(n.p,{children:["Instead of running an online inference endpoint, you can execute a single batch inference job on local files using\nthe ",(0,r.jsx)(i.A,{to:"/api_reference/cli.html#mlflow-models-predict",target:"_blank",children:"mlflow models predict"})," command. The following command runs the model\nprediction on ",(0,r.jsx)(n.code,{children:"input.csv"})," and outputs the results to ",(0,r.jsx)(n.code,{children:"output.csv"}),"."]}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsx)(s.A,{default:!0,label:"Bash",value:"bash",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow models predict -m runs:/<run_id>/model -i input.csv -o output.csv\n"})})}),(0,r.jsx)(s.A,{label:"Python",value:"python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmodel = mlflow.pyfunc.load_model("runs:/<run_id>/model")\npredictions = model.predict(pd.read_csv("input.csv"))\npredictions.to_csv("output.csv")\n'})})})]}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}}}]);