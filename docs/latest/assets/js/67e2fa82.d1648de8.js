"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6298],{1204:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>q,contentTitle:()=>A,default:()=>T,frontMatter:()=>k,metadata:()=>a,toc:()=>L});const a=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/make-judge","title":"Using make_judge for Custom LLM Evaluation","description":"The make_judge API is the recommended way to create custom LLM judges in MLflow. It provides a unified interface for all types of judge-based evaluation, from simple Q&A validation to complex agent debugging.","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/make-judge.mdx","sourceDirName":"eval-monitor/scorers/llm-judge","slug":"/eval-monitor/scorers/llm-judge/make-judge","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"LLM-based Scorers (LLM-as-a-Judge)","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/"},"next":{"title":"End-to-End Workflow","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/workflow"}}');var i=t(74848),r=t(28453),l=t(49374),o=t(11470),s=t(19365),c=t(6789),d=t(65592),u=t(82238),p=t(79206),m=t(47792),h=t(61878),g=t(46858),f=t(87073),j=t(98445),w=t(80827),v=t(47504),_=t(80964),x=t(51004),y=t(46816),b=t(60665);const k={},A="Using make_judge for Custom LLM Evaluation",q={},L=[{value:"Why Use make_judge?",id:"why-use-make_judge",level:2},{value:"Choosing the Right LLM for Your Judge",id:"choosing-the-right-llm-for-your-judge",level:3},{value:"Evaluation Modes",id:"evaluation-modes",level:2},{value:"Template Variables",id:"template-variables",level:2},{value:"How Template Variables Work",id:"how-template-variables-work",level:3},{value:"Variable Restrictions",id:"variable-restrictions",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Important Limitations",id:"important-limitations",level:2},{value:"Template Variable Restrictions",id:"template-variable-restrictions",level:3},{value:"Common Evaluation Patterns",id:"common-evaluation-patterns",level:2},{value:"Integration with MLflow Evaluation",id:"integration-with-mlflow-evaluation",level:2},{value:"Using Judges in mlflow.genai.evaluate",id:"using-judges-in-mlflowgenaievaluate",level:3},{value:"Registering and Versioning Judges",id:"registering-and-versioning-judges",level:2},{value:"Registering a Judge",id:"registering-a-judge",level:3},{value:"Retrieving Registered Judges",id:"retrieving-registered-judges",level:3},{value:"Migrating from Legacy Judges",id:"migrating-from-legacy-judges",level:2},{value:"Migration Example",id:"migration-example",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Working with Complex Data",id:"working-with-complex-data",level:3},{value:"Conditional Logic in Instructions",id:"conditional-logic-in-instructions",level:3},{value:"Advanced Workflows",id:"advanced-workflows",level:2},{value:"Complete Trace Evaluation Example",id:"complete-trace-evaluation-example",level:3},{value:"Combining with Human Feedback",id:"combining-with-human-feedback",level:3},{value:"Learn More",id:"learn-more",level:2}];function C(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"using-make_judge-for-custom-llm-evaluation",children:"Using make_judge for Custom LLM Evaluation"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(l.B,{fn:"mlflow.genai.judges.make_judge",children:"make_judge"})," API is the recommended way to create custom LLM judges in MLflow. It provides a unified interface for all types of judge-based evaluation, from simple Q&A validation to complex agent debugging."]}),"\n",(0,i.jsx)(n.admonition,{title:"Version Requirements",type:"note",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"make_judge"})," API requires ",(0,i.jsx)(n.strong,{children:"MLflow >= 3.4.0"}),". For earlier versions, use the legacy judge functions."]})}),"\n",(0,i.jsx)(n.h2,{id:"why-use-make_judge",children:"Why Use make_judge?"}),"\n",(0,i.jsx)(n.p,{children:"Creating effective LLM judges requires a balance of flexibility, maintainability, and accuracy. The make_judge API addresses these needs by providing a template-based approach with built-in versioning and optimization capabilities."}),"\n",(0,i.jsx)(n.h3,{id:"choosing-the-right-llm-for-your-judge",children:"Choosing the Right LLM for Your Judge"}),"\n",(0,i.jsx)(n.p,{children:"The choice of LLM model significantly impacts judge performance and cost. Here's guidance based on your development stage and use case:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Early Development Stage (Inner Loop)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recommended"}),": Start with powerful models like GPT-4o or Claude Opus"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Why"}),": When you're beginning your agent development journey, you typically lack:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use-case-specific grading criteria"}),"\n",(0,i.jsx)(n.li,{children:"Labeled data for optimization"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Benefits"}),": More intelligent models can deeply explore traces, identify patterns, and help you understand common issues in your system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Trade-off"}),": Higher cost, but lower evaluation volume during development makes this acceptable"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Production & Scaling Stage"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recommended"}),": Transition to smaller models (GPT-4o-mini, Claude Haiku) with smarter optimizers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Why"}),": As you move toward production:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You've collected labeled data and established grading criteria"}),"\n",(0,i.jsx)(n.li,{children:"Cost becomes a critical factor at scale"}),"\n",(0,i.jsx)(n.li,{children:"You can align smaller judges using more powerful optimizers"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Approach"}),": Use a smaller judge model paired with a powerful optimizer model (e.g., GPT-4o-mini judge aligned using Claude Opus optimizer)"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"General Guidelines"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Agent-as-a-judge evaluation"}),": Requires intelligent LLMs (GPT-4o, Claude Opus) to analyze complex multi-step reasoning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simple classification tasks"}),": Can work well with smaller models (GPT-4o-mini, Claude Haiku)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Domain-specific evaluation"}),": Start with powerful models, then optimize smaller models using your collected feedback"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'The key insight: You can achieve cost-effective evaluation by aligning "dumber" judges using "smarter" optimizers, allowing you to use less expensive models in production while maintaining accuracy.'}),"\n",(0,i.jsx)(u.A,{features:[{icon:m.A,title:"Unified Evaluation Interface",description:"One API for all judge types - from simple Q&A validation to complex agent debugging. No need to learn multiple judge functions."},{icon:h.A,title:"Registration & Collaboration",description:"Register judges to share across teams and ensure reproducible evaluations. Organize and manage your evaluation logic in one place."},{icon:g.A,title:"Dual Evaluation Modes",description:"Evaluate final outputs with field-based assessment or analyze complete execution flows with Agent-as-a-Judge evaluation."},{icon:f.A,title:"Template-Based Instructions",description:"Write evaluation criteria in natural language using template variables. Clear, maintainable, and easy to understand."}]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-modes",children:"Evaluation Modes"}),"\n",(0,i.jsx)(n.p,{children:"The make_judge API supports two distinct evaluation modes, each optimized for different scenarios. Choose field-based evaluation for evaluating specific inputs and outputs, or Agent-as-a-Judge evaluation for analyzing complete execution flows."}),"\n",(0,i.jsx)(p.A,{concepts:[{icon:f.A,title:"Field-Based Evaluation",description:"Assess specific inputs, outputs, and expectations. Mix variables from different data categories. Ideal for traditional Q&A, classification, and generation tasks where you need to evaluate final results."},{icon:j.A,title:"Agent-as-a-Judge Evaluation",description:"Analyze complete execution flows using the trace variable. Inspect intermediate steps, tool usage, and decision-making. Essential for debugging complex AI agents and multi-step workflows."}]}),"\n",(0,i.jsx)(n.h2,{id:"template-variables",children:"Template Variables"}),"\n",(0,i.jsx)(n.p,{children:"Judge instructions use template variables to reference evaluation data. These variables are automatically filled with your data at runtime. Understanding which variables to use is critical for creating effective judges."}),"\n",(0,i.jsx)(p.A,{concepts:[{icon:w.A,title:"inputs",description:"The input data provided to your AI system. Contains questions, prompts, or any data your model processes."},{icon:v.A,title:"outputs",description:"The generated response from your AI system. The actual output that needs evaluation."},{icon:m.A,title:"expectations",description:"Ground truth or expected outcomes. Reference answers for comparison and accuracy assessment."},{icon:j.A,title:"trace",description:"Complete execution flow including all spans. Cannot be mixed with other variables. Used for analyzing multi-step processes."}]}),"\n",(0,i.jsx)(n.h3,{id:"how-template-variables-work",children:"How Template Variables Work"}),"\n",(0,i.jsx)(n.p,{children:"When you use template variables in your instructions, MLflow processes them in two distinct ways depending on the variable type:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Direct Interpolation"})," (",(0,i.jsx)(n.code,{children:"inputs"}),", ",(0,i.jsx)(n.code,{children:"outputs"}),", ",(0,i.jsx)(n.code,{children:"expectations"}),"): These variables are directly interpolated into the prompt as formatted strings. The dictionaries you pass are converted to readable text and inserted into your instruction template. This gives you full control over how the data appears in the evaluation prompt."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Agent-as-a-Judge Analysis"})," (",(0,i.jsx)(n.code,{children:"trace"}),"): The trace variable works differently to handle complexity at scale. Instead of interpolating potentially massive JSON data directly into the prompt, the trace metadata (trace_id, experiment_id, request_id) is passed to an evaluation agent that fetches and analyzes the full trace details. This design enables Agent-as-a-Judge to handle large, complex execution flows without hitting token limits."]}),"\n",(0,i.jsx)(n.admonition,{title:"Trace Processing Behavior",type:"note",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"{{ trace }}"})," variable is NOT interpolated as JSON into the prompt. This is by design - traces can contain thousands of spans with extensive data that would overwhelm token limits. Instead, an intelligent agent fetches and analyzes the trace data, allowing it to focus on relevant aspects based on your evaluation instructions."]})}),"\n",(0,i.jsx)(n.h3,{id:"variable-restrictions",children:"Variable Restrictions"}),"\n",(0,i.jsx)(n.admonition,{title:"Only Reserved Variables Allowed",type:"warning",children:(0,i.jsxs)(n.p,{children:["You can only use the four reserved template variables shown above (",(0,i.jsx)(n.code,{children:"inputs"}),", ",(0,i.jsx)(n.code,{children:"outputs"}),", ",(0,i.jsx)(n.code,{children:"expectations"}),", ",(0,i.jsx)(n.code,{children:"trace"}),"). Custom variables like ",(0,i.jsx)(n.code,{children:"{{ question }}"}),", ",(0,i.jsx)(n.code,{children:"{{ response }}"}),", or ",(0,i.jsx)(n.code,{children:"{{ context }}"})," will cause validation errors. This restriction ensures consistent behavior and prevents template injection issues."]})}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsxs)(s.A,{value:"field",label:"Field-Based (Simple)",default:!0,children:[(0,i.jsx)(n.p,{children:"First, create a simple agent to evaluate:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Create a toy agent that responds to questions\ndef my_agent(question):\n    # Simple toy agent that echoes back\n    return f"You asked about: {question}"\n'})}),(0,i.jsx)(n.p,{children:"Then create a judge to evaluate the agent's responses:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\n\n# Create a judge that evaluates response quality\nquality_judge = make_judge(\n    name="response_quality",\n    instructions=(\n        "Evaluate if the response in {{ outputs }} correctly answers "\n        "the question in {{ inputs }}. The response should be accurate, "\n        "complete, and professional."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})}),(0,i.jsx)(n.p,{children:"Now evaluate the agent's response:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Get agent response\nquestion = "What is machine learning?"\nresponse = my_agent(question)\n\n# Evaluate the response\nfeedback = quality_judge(\n    inputs={"question": question},\n    outputs={"response": response},\n)\nprint(f"Score: {feedback.value}")\nprint(f"Rationale: {feedback.rationale}")\n'})})]}),(0,i.jsxs)(s.A,{value:"trace",label:"Trace-Based (Advanced)",children:[(0,i.jsx)(n.p,{children:"First, create a more complex agent with tracing:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n\n# Create a more complex toy agent with tracing\n@mlflow.trace\ndef my_complex_agent(query):\n    with mlflow.start_span("parse_query") as parse_span:\n        # Parse the user query\n        parsed = f"Parsed: {query}"\n        parse_span.set_inputs({"query": query})\n        parse_span.set_outputs({"parsed": parsed})\n\n    with mlflow.start_span("generate_response") as gen_span:\n        # Generate response\n        response = f"Response to: {parsed}"\n        gen_span.set_inputs({"parsed": parsed})\n        gen_span.set_outputs({"response": response})\n\n    return response\n'})}),(0,i.jsx)(n.p,{children:"Create a judge that analyzes execution traces:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\n\n# Create a judge that analyzes complete execution flows\ntrace_judge = make_judge(\n    name="agent_performance",\n    instructions=(\n        "Analyze the {{ trace }} to evaluate the agent\'s performance.\\n\\n"\n        "Check for:\\n"\n        "1. Efficient execution and tool usage\\n"\n        "2. Error handling and recovery\\n"\n        "3. Logical reasoning flow\\n"\n        "4. Performance bottlenecks\\n\\n"\n        "Provide a rating: \'excellent\', \'good\', or \'needs improvement\'"\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",  # Note: Cannot use \'databricks\' model\n)\n'})}),(0,i.jsx)(n.p,{children:"Execute the agent and evaluate its trace:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Execute agent and capture trace\nwith mlflow.start_span("agent_task") as span:\n    response = my_complex_agent("What is MLflow?")\n    trace_id = span.request_id\n\n# Get the trace\ntrace = mlflow.get_trace(trace_id)\n\n# Evaluate the trace\nfeedback = trace_judge(trace=trace)\nprint(f"Performance: {feedback.value}")\nprint(f"Analysis: {feedback.rationale}")\n'})})]})]}),"\n",(0,i.jsx)(n.h2,{id:"important-limitations",children:"Important Limitations"}),"\n",(0,i.jsx)(n.h3,{id:"template-variable-restrictions",children:"Template Variable Restrictions"}),"\n",(0,i.jsxs)(n.admonition,{title:"Important Limitations",type:"warning",children:[(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"make_judge"})," API has strict template variable requirements:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Only reserved variables allowed"}),": ",(0,i.jsx)(n.code,{children:"inputs"}),", ",(0,i.jsx)(n.code,{children:"outputs"}),", ",(0,i.jsx)(n.code,{children:"expectations"}),", ",(0,i.jsx)(n.code,{children:"trace"})]}),"\n",(0,i.jsxs)(n.li,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"No custom variables"}),": Variables like ",(0,i.jsx)(n.code,{children:"{{ question }}"}),", ",(0,i.jsx)(n.code,{children:"{{ response }}"}),", etc. are not supported"]}),"\n",(0,i.jsxs)(n.li,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Trace isolation"}),": When using ",(0,i.jsx)(n.code,{children:"trace"}),", cannot use ",(0,i.jsx)(n.code,{children:"inputs"}),", ",(0,i.jsx)(n.code,{children:"outputs"}),", or ",(0,i.jsx)(n.code,{children:"expectations"})]}),"\n",(0,i.jsxs)(n.li,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Model restrictions"}),": Cannot use the ",(0,i.jsx)(n.code,{children:"databricks"})," default model with Agent-as-a-Judge"]}),"\n"]}),(0,i.jsx)(n.p,{children:"All template variables referenced in instructions must be provided when calling the judge."})]}),"\n",(0,i.jsx)(n.h2,{id:"common-evaluation-patterns",children:"Common Evaluation Patterns"}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(s.A,{value:"trace-patterns",label:"Trace Evaluation (Recommended)",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Tool Usage Evaluation\ntool_judge = make_judge(\n    name="tool_usage",\n    instructions=(\n        "Examine the {{ trace }} for tool usage patterns.\\n"\n        "Check: tool selection, sequencing, output utilization, error handling.\\n"\n        "Rate as \'optimal\', \'acceptable\', or \'inefficient\'."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Reasoning Chain Evaluation\nreasoning_judge = make_judge(\n    name="reasoning",\n    instructions=(\n        "Analyze reasoning in {{ trace }}.\\n"\n        "Evaluate: logical progression, assumptions, conclusions.\\n"\n        "Score 0-100 for reasoning quality."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Error Recovery Evaluation\nerror_judge = make_judge(\n    name="error_recovery",\n    instructions=(\n        "Review {{ trace }} for error handling.\\n"\n        "Check: detection, recovery strategies, user impact.\\n"\n        "Rate as \'robust\', \'adequate\', or \'fragile\'."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,i.jsx)(s.A,{value:"field-patterns",label:"Field Evaluation",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Quality Evaluation\nquality_judge = make_judge(\n    name="quality",\n    instructions=(\n        "Evaluate the quality of {{ outputs }} for the question in {{ inputs }}.\\n"\n        "Consider completeness, accuracy, and helpfulness.\\n"\n        "Rate as \'excellent\', \'good\', or \'needs improvement\'."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Safety Evaluation\nsafety_judge = make_judge(\n    name="safety",\n    instructions=(\n        "Check if {{ outputs }} is safe and appropriate for the {{ inputs }}.\\n"\n        "Answer \'safe\' or \'unsafe\' with concerns."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Groundedness Evaluation\ngrounded_judge = make_judge(\n    name="groundedness",\n    instructions=(\n        "Verify {{ outputs }} is grounded in the context provided in {{ inputs }}.\\n"\n        "Rate: \'fully\', \'mostly\', \'partially\', or \'not\' grounded."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})})]}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-mlflow-evaluation",children:"Integration with MLflow Evaluation"}),"\n",(0,i.jsxs)(n.p,{children:["Judges created with ",(0,i.jsx)(n.code,{children:"make_judge"})," work seamlessly as scorers in MLflow's evaluation framework:"]}),"\n",(0,i.jsx)(n.h3,{id:"using-judges-in-mlflowgenaievaluate",children:"Using Judges in mlflow.genai.evaluate"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nfrom mlflow.genai.judges import make_judge\n\n# Create multiple judges for comprehensive evaluation\nquality_judge = make_judge(\n    name="quality",\n    instructions=(\n        "Rate the quality of {{ outputs }} for the question in {{ inputs }}. Score 1-5."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\naccuracy_judge = make_judge(\n    name="accuracy",\n    instructions=(\n        "Check if {{ outputs }} accurately answers the question in {{ inputs }}.\\n"\n        "Compare against {{ expectations }} for correctness.\\n"\n        "Answer \'accurate\' or \'inaccurate\'."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Prepare evaluation data\neval_data = pd.DataFrame(\n    {\n        "inputs": [{"question": "What is MLflow?"}],\n        "outputs": [\n            {"response": "MLflow is an open-source platform for ML lifecycle."}\n        ],\n        "expectations": [\n            {\n                "ground_truth": "MLflow is an open-source platform for managing the ML lifecycle."\n            }\n        ],\n    }\n)\n\n# Run evaluation with judges as scorers\nresults = mlflow.genai.evaluate(\n    data=eval_data,\n    scorers=[quality_judge, accuracy_judge],\n)\n\n# Access evaluation results\nprint(results.metrics)\nprint(results.tables["eval_results_table"])\n'})}),"\n",(0,i.jsx)(n.h2,{id:"registering-and-versioning-judges",children:"Registering and Versioning Judges"}),"\n",(0,i.jsx)(n.p,{children:"Judges can be registered to MLflow experiments for version control and team collaboration:"}),"\n",(0,i.jsx)(n.h3,{id:"registering-a-judge",children:"Registering a Judge"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.judges import make_judge\n\n# Set up tracking\nmlflow.set_tracking_uri("your-tracking-uri")\nexperiment_id = mlflow.create_experiment("evaluation-judges")\n\n# Create and register a judge\nquality_judge = make_judge(\n    name="response_quality",\n    instructions=("Evaluate if {{ outputs }} is high quality for {{ inputs }}."),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Register the judge\nregistered_judge = quality_judge.register(experiment_id=experiment_id)\nprint("Judge registered successfully")\n\n# Update and register a new version of the judge\nquality_judge_v2 = make_judge(\n    name="response_quality",  # Same name\n    instructions=(\n        "Evaluate if {{ outputs }} is high quality, accurate, and complete "\n        "for the question in {{ inputs }}."\n    ),\n    model="anthropic:/claude-3.5-sonnet-20241022",  # Updated model\n)\n\n# Register the updated judge\nregistered_v2 = quality_judge_v2.register(experiment_id=experiment_id)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"retrieving-registered-judges",children:"Retrieving Registered Judges"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import get_scorer, list_scorers\n\n# Get the latest version\nlatest_judge = get_scorer(name="response_quality", experiment_id=experiment_id)\n\n# Note: Version tracking is currently under development\n# For now, use the latest version retrieval shown above\n\n# List all judges in an experiment\nall_judges = list_scorers(experiment_id=experiment_id)\nfor judge in all_judges:\n    print(f"Judge: {judge.name}, Model: {judge.model}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"migrating-from-legacy-judges",children:"Migrating from Legacy Judges"}),"\n",(0,i.jsxs)(n.p,{children:["If you're using the older judge functions (",(0,i.jsx)(n.code,{children:"is_correct"}),", ",(0,i.jsx)(n.code,{children:"is_grounded"}),", etc.), migrating to ",(0,i.jsx)(n.code,{children:"make_judge"})," provides significant improvements in flexibility, maintainability, and accuracy."]}),"\n",(0,i.jsx)(u.A,{features:[{icon:_.A,title:"Unified API",description:"One function for all judge types instead of multiple specialized functions. Simplifies your codebase and learning curve."},{icon:x.A,title:"Structured Data Organization",description:"Clean separation of inputs, outputs, and expectations. Makes data flow explicit and debugging easier."},{icon:h.A,title:"Version Control & Collaboration",description:"Register and version judges for reproducibility. Share evaluation logic across teams and projects."},{icon:y.A,title:"Seamless Integration",description:"Works perfectly as a scorer in MLflow evaluation. Compatible with all evaluation workflows and patterns."}]}),"\n",(0,i.jsx)(n.h3,{id:"migration-example",children:"Migration Example"}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(s.A,{value:"legacy",label:"Legacy Approach",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import is_correct\n\n# Limited to predefined parameters\nfeedback = is_correct(\n    request="What is 2+2?",\n    response="4",\n    expected_response="4",\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,i.jsx)(s.A,{value:"new",label:"New Approach",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\n\n# Flexible template-based approach\ncorrectness_judge = make_judge(\n    name="correctness",\n    instructions=(\n        "Evaluate if {{ outputs }} correctly answers the question in {{ inputs }}.\\n"\n        "Compare with {{ expectations }} for the correct answer.\\n\\n"\n        "Consider partial credit for reasoning.\\n"\n        "Answer: \'correct\', \'partially correct\', or \'incorrect\'"\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\nfeedback = correctness_judge(\n    inputs={"question": "What is 2+2?"},\n    outputs={"response": "4"},\n    expectations={"expected_answer": "4"},\n)\n'})})})]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsx)(n.h3,{id:"working-with-complex-data",children:"Working with Complex Data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Judge that handles structured data within reserved variables\ncomprehensive_judge = make_judge(\n    name="comprehensive_eval",\n    instructions=(\n        "Evaluate the complete interaction:\\n\\n"\n        "Review the inputs including user profile, query, and context.\\n"\n        "Assess if the outputs appropriately respond to the inputs.\\n"\n        "Check against expectations for required topics.\\n\\n"\n        "The {{ inputs }} contain user information and context.\\n"\n        "The {{ outputs }} contain the model\'s response.\\n"\n        "The {{ expectations }} list required coverage.\\n\\n"\n        "Assess completeness, accuracy, and appropriateness."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Handle complex nested data within reserved variables\nfeedback = comprehensive_judge(\n    inputs={\n        "user_profile": {"expertise": "beginner", "domain": "ML"},\n        "query": "Explain neural networks",\n        "context": ["Document 1...", "Document 2..."],\n    },\n    outputs={"response": "Neural networks are..."},\n    expectations={"required_topics": ["layers", "neurons", "activation functions"]},\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"conditional-logic-in-instructions",children:"Conditional Logic in Instructions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'conditional_judge = make_judge(\n    name="adaptive_evaluator",\n    instructions=(\n        "Evaluate the {{ outputs }} based on the user level in {{ inputs }}:\\n\\n"\n        "If the user level in inputs is \'beginner\':\\n"\n        "- Check for simple language\\n"\n        "- Ensure no unexplained jargon\\n\\n"\n        "If the user level in inputs is \'expert\':\\n"\n        "- Check for technical accuracy\\n"\n        "- Ensure appropriate depth\\n\\n"\n        "Rate as \'appropriate\' or \'inappropriate\' for the user level."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-workflows",children:"Advanced Workflows"}),"\n",(0,i.jsx)(n.h3,{id:"complete-trace-evaluation-example",children:"Complete Trace Evaluation Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.judges import make_judge\n\n# Create a performance judge\nperf_judge = make_judge(\n    name="performance",\n    instructions=(\n        "Analyze {{ trace }} for: slow operations (>2s), redundancy, efficiency.\\n"\n        "Rate: \'fast\', \'acceptable\', or \'slow\'. List bottlenecks."\n    ),\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Prepare test data\nimport pandas as pd\n\ntest_queries = pd.DataFrame(\n    [\n        {"query": "What is MLflow?"},\n        {"query": "How to track experiments?"},\n        {"query": "What are MLflow models?"},\n    ]\n)\n\n\n# Define your agent function\ndef my_agent(query):\n    # Your actual agent processing\n    with mlflow.start_span("agent_processing") as span:\n        # Simulate some processing\n        response = f"Detailed answer about: {query}"\n        span.set_inputs({"query": query})\n        span.set_outputs({"response": response})\n    return response\n\n\n# Run evaluation with the performance judge\nresults = mlflow.genai.evaluate(\n    data=test_queries, predict_fn=my_agent, scorers=[perf_judge]\n)\n\n# View results - assessments are automatically logged to traces\nprint("Performance metrics:", results.metrics)\nprint("\\nDetailed evaluations:")\nprint(results.tables["eval_results_table"])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"combining-with-human-feedback",children:"Combining with Human Feedback"}),"\n",(0,i.jsx)(n.p,{children:"Automate initial analysis and flag traces for human review:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import AssessmentSource, AssessmentSourceType\n\n# Create a trace to evaluate\nwith mlflow.start_span("example_operation") as span:\n    # Your operation here\n    trace_id = span.trace_id\n\ntrace = mlflow.get_trace(trace_id)\n\n# Create quality judge\ntrace_quality_judge = make_judge(\n    name="quality",\n    instructions="Evaluate the quality of {{ trace }}. Rate as \'good\', \'poor\', or \'needs improvement\'.",\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n# Automated evaluation\nauto_feedback = trace_quality_judge(trace=trace)\n\n# Log automated feedback\nmlflow.log_feedback(\n    trace_id=trace_id,\n    name="quality_auto",\n    value=auto_feedback.value,\n    rationale=auto_feedback.rationale,\n    source=AssessmentSource(\n        source_type=AssessmentSourceType.LLM_JUDGE, source_id="quality_judge_v1"\n    ),\n)\n\n# View and review traces in the MLflow UI\n# - OSS MLflow: Navigate to the Traces tab in your experiment\n# - Databricks: Use Labeling sessions for structured review\n# Traces are automatically grouped by mlflow.genai.evaluate() runs for easy review\n'})}),"\n",(0,i.jsx)(n.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,i.jsxs)(d.A,{children:[(0,i.jsx)(c.A,{icon:v.A,iconSize:48,title:"Evaluation Quickstart",description:"Get started with MLflow's evaluation framework and learn best practices.",href:"/genai/eval-monitor/quickstart",linkText:"Start evaluating \u2192",containerHeight:64}),(0,i.jsx)(c.A,{icon:b.A,iconSize:48,title:"Predefined Judges",description:"Explore MLflow's built-in LLM judges for common evaluation tasks.",href:"/genai/eval-monitor/scorers/llm-judge/predefined",linkText:"View built-in judges \u2192",containerHeight:64}),(0,i.jsx)(c.A,{icon:h.A,iconSize:48,title:"Tracing Guide",description:"Learn how to collect and analyze traces for comprehensive evaluation.",href:"/genai/tracing",linkText:"Start tracing \u2192",containerHeight:64}),(0,i.jsx)(c.A,{icon:f.A,iconSize:48,title:"Human Feedback",description:"Learn how to collect and utilize human feedback for evaluation.",href:"/genai/assessments/feedback",linkText:"Collect feedback \u2192",containerHeight:64})]})]})}function T(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(C,{...e})}):C(e)}},6789:(e,n,t)=>{t.d(n,{A:()=>c});t(96540);var a=t(28774),i=t(34164);const r={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var l=t(86025),o=t(21122),s=t(74848);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:d,iconSize:u=32,containerHeight:p,title:m,description:h,href:g,linkText:f="Learn more \u2192",className:j}){if(!e&&!n)throw new Error("TileCard requires either an icon or image prop");const w=p?{height:`${p}px`}:{},v={};return c&&(v.width=`${c}px`),d&&(v.height=`${d}px`),(0,s.jsxs)(a.A,{href:g,className:(0,i.A)(r.tileCard,j),children:[(0,s.jsx)("div",{className:r.tileIcon,style:w,children:e?(0,s.jsx)(e,{size:u}):t?(0,s.jsx)(o.A,{sources:{light:(0,l.Ay)(n),dark:(0,l.Ay)(t)},alt:m,className:r.tileImage,style:v}):(0,s.jsx)("img",{src:(0,l.Ay)(n),alt:m,className:r.tileImage,style:v})}),(0,s.jsx)("h3",{children:m}),(0,s.jsx)("p",{children:h}),(0,s.jsx)("div",{className:r.tileLink,children:f})]})}},49374:(e,n,t)=>{t.d(n,{B:()=>o});t(96540);const a=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var i=t(86025),r=t(74848);const l=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(a[e])return e}return null};function o({fn:e,children:n,hash:t}){const o=l(e);if(!o)return(0,r.jsx)(r.Fragment,{children:n});const s=(0,i.Ay)(`/${a[o]}#${t??e}`);return(0,r.jsx)("a",{href:s,target:"_blank",children:n??(0,r.jsxs)("code",{children:[e,"()"]})})}},65592:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var a=t(34164);const i={tilesGrid:"tilesGrid_hB9N"};var r=t(74848);function l({children:e,className:n}){return(0,r.jsx)("div",{className:(0,a.A)(i.tilesGrid,n),children:e})}},79206:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);const a={conceptOverview:"conceptOverview_x8T_",overviewTitle:"overviewTitle_HyAI",conceptGrid:"conceptGrid_uJNV",conceptCard:"conceptCard_oday",conceptHeader:"conceptHeader_HCk5",conceptIcon:"conceptIcon_gejw",conceptTitle:"conceptTitle_TGMM",conceptDescription:"conceptDescription_ZyDn"};var i=t(74848);function r({concepts:e,title:n}){return(0,i.jsxs)("div",{className:a.conceptOverview,children:[n&&(0,i.jsx)("h3",{className:a.overviewTitle,children:n}),(0,i.jsx)("div",{className:a.conceptGrid,children:e.map((e,n)=>(0,i.jsxs)("div",{className:a.conceptCard,children:[(0,i.jsxs)("div",{className:a.conceptHeader,children:[e.icon&&(0,i.jsx)("div",{className:a.conceptIcon,children:(0,i.jsx)(e.icon,{size:20})}),(0,i.jsx)("h4",{className:a.conceptTitle,children:e.title})]}),(0,i.jsx)("p",{className:a.conceptDescription,children:e.description})]},n))})]})}},82238:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);const a={featureHighlights:"featureHighlights_Ardf",highlightItem:"highlightItem_XPnN",highlightIcon:"highlightIcon_SUR8",highlightContent:"highlightContent_d0XP"};var i=t(74848);function r({features:e}){return(0,i.jsx)("div",{className:a.featureHighlights,children:e.map((e,n)=>(0,i.jsxs)("div",{className:a.highlightItem,children:[e.icon&&(0,i.jsx)("div",{className:a.highlightIcon,children:(0,i.jsx)(e.icon,{size:24})}),(0,i.jsxs)("div",{className:a.highlightContent,children:[(0,i.jsx)("h4",{children:e.title}),(0,i.jsx)("p",{children:e.description})]})]},n))})}}}]);