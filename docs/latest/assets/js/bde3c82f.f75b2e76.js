"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9736],{20139:(e,n,l)=>{l.d(n,{A:()=>r});const r=l.p+"assets/images/search_syntax-8db1091a58dda699e58cd3846c12cace.png"},28453:(e,n,l)=>{l.d(n,{R:()=>o,x:()=>t});var r=l(96540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},30876:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"search/search-models/index","title":"Search Logged Models","description":"This guide will walk you through how to search for logged models in MLflow using both the MLflow UI and Python API. This resource will be valuable if you\'re interested in querying specific models based on their metrics, params, tags, or model metadata.","source":"@site/docs/classic-ml/search/search-models/index.mdx","sourceDirName":"search/search-models","slug":"/search/search-models/","permalink":"/docs/latest/ml/search/search-models/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"Remote Experiment Tracking with MLflow Tracking Server","permalink":"/docs/latest/ml/tracking/tutorials/remote-server/"},"next":{"title":"Search Runs","permalink":"/docs/latest/ml/search/search-runs/"}}');var i=l(74848),s=l(28453),o=l(49374);const t={},a="Search Logged Models",c={},d=[{value:"Search Logged Models Overview",id:"search-logged-models-overview",level:2},{value:"Create Example Logged Models",id:"create-example-logged-models",level:2},{value:"Search Query Syntax",id:"search-logged-models-syntax",level:2},{value:"Visual Representation of Search Components:",id:"visual-representation-of-search-components",level:3},{value:"Key Differences from search_runs:",id:"key-differences-from-search_runs",level:3},{value:"Syntax Rules:",id:"syntax-rules",level:3},{value:"Example Queries",id:"example-queries",level:2},{value:"1 - Searching By Metrics",id:"1---searching-by-metrics",level:3},{value:"2 - Searching By Parameters",id:"2---searching-by-parameters",level:3},{value:"3 - Searching By Model Name",id:"3---searching-by-model-name",level:3},{value:"4 - Searching By Model Attributes",id:"4---searching-by-model-attributes",level:3},{value:"5 - Dataset-Specific Metric Filtering",id:"5---dataset-specific-metric-filtering",level:3},{value:"6 - Complex Queries",id:"6---complex-queries",level:3},{value:"Programmatic Search with Python",id:"programmatic-search-with-python",level:2},{value:"Using the Fluent API",id:"using-the-fluent-api",level:3},{value:"Using the Client API",id:"using-the-client-api",level:3},{value:"Advanced Ordering",id:"advanced-ordering",level:3},{value:"Getting Top N Models",id:"getting-top-n-models",level:3},{value:"Searching Across Multiple Experiments",id:"searching-across-multiple-experiments",level:3},{value:"Common Use Cases",id:"common-use-cases",level:2},{value:"Model Selection for Deployment",id:"model-selection-for-deployment",level:3},{value:"Model Comparison",id:"model-comparison",level:3},{value:"Important Notes",id:"important-notes",level:2},{value:"Accessing Metrics from LoggedModel",id:"accessing-metrics-from-loggedmodel",level:3},{value:"Summary",id:"summary",level:2}];function m(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"search-logged-models",children:"Search Logged Models"})}),"\n",(0,i.jsx)(n.p,{children:"This guide will walk you through how to search for logged models in MLflow using both the MLflow UI and Python API. This resource will be valuable if you're interested in querying specific models based on their metrics, params, tags, or model metadata."}),"\n",(0,i.jsxs)(n.p,{children:["MLflow's model search functionality allows you to leverage SQL-like syntax to filter your logged models based on a variety of conditions. While the ",(0,i.jsx)(n.code,{children:"OR"})," keyword is not supported, the search functionality is powerful enough to handle complex queries for model discovery and comparison."]}),"\n",(0,i.jsx)(n.h2,{id:"search-logged-models-overview",children:"Search Logged Models Overview"}),"\n",(0,i.jsxs)(n.p,{children:["When working with MLflow in production environments, you'll often have hundreds or thousands of logged models across different experiments. The ",(0,i.jsx)(n.code,{children:"search_logged_models"})," API helps you find specific models based on their performance metrics, parameters, tags, and other attributes - making model selection and comparison much more efficient."]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["Looking for guidance on searching over Runs? See the ",(0,i.jsx)(n.a,{href:"/ml/search/search-runs",children:"Search Runs"})," documentation."]})}),"\n",(0,i.jsx)(n.h2,{id:"create-example-logged-models",children:"Create Example Logged Models"}),"\n",(0,i.jsx)(n.p,{children:"First, let's create some example logged models to demonstrate the search functionality. This documentation is based on models created with the below script. If you don't want to interactively explore this on your machine, skip this section."}),"\n",(0,i.jsx)(n.p,{children:"Before running the script, let's start the MLflow UI on a local host:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mlflow ui\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Visit ",(0,i.jsx)(n.code,{children:"http://localhost:5000/"})," in your web browser. Let's create some example models:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.sklearn\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom mlflow.models import infer_signature\nimport warnings\n\n# Suppress the MLflow model config warning if present\nwarnings.filterwarnings("ignore", message=".*Failed to log model config as params.*")\n\nmlflow.set_experiment("model-search-guide")\n\n# Model configurations\nmodel_configs = [\n    {"model_type": "RandomForest", "n_estimators": 100, "max_depth": 10},\n    {"model_type": "RandomForest", "n_estimators": 200, "max_depth": 20},\n    {"model_type": "LogisticRegression", "C": 1.0, "solver": "lbfgs"},\n    {"model_type": "LogisticRegression", "C": 0.1, "solver": "saga"},\n    {"model_type": "SVM", "kernel": "rbf", "C": 1.0},\n    {"model_type": "SVM", "kernel": "linear", "C": 0.5},\n]\n\n# Performance metrics (simulated)\naccuracy_scores = [0.92, 0.94, 0.88, 0.86, 0.90, 0.87]\nprecision_scores = [0.91, 0.93, 0.87, 0.85, 0.89, 0.86]\nrecall_scores = [0.93, 0.95, 0.89, 0.87, 0.91, 0.88]\nf1_scores = [0.92, 0.94, 0.88, 0.86, 0.90, 0.87]\n\n# Model metadata\nversions = ["v1.0", "v1.1", "v1.0", "v2.0", "v1.0", "v1.1"]\nenvironments = [\n    "production",\n    "staging",\n    "production",\n    "development",\n    "staging",\n    "production",\n]\nframeworks = ["sklearn", "sklearn", "sklearn", "sklearn", "sklearn", "sklearn"]\n\n# Create dummy training data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\n\n# Create input example for model signature\ninput_example = pd.DataFrame(X_train[:5], columns=[f"feature_{i}" for i in range(10)])\n\nfor i, config in enumerate(model_configs):\n    with mlflow.start_run():\n        # Create and train model based on type\n        if config["model_type"] == "RandomForest":\n            model = RandomForestClassifier(\n                n_estimators=config["n_estimators"],\n                max_depth=config["max_depth"],\n                random_state=42,\n            )\n            mlflow.log_param("n_estimators", config["n_estimators"])\n            mlflow.log_param("max_depth", config["max_depth"])\n        elif config["model_type"] == "LogisticRegression":\n            model = LogisticRegression(\n                C=config["C"],\n                solver=config["solver"],\n                random_state=42,\n                max_iter=1000,  # Increase iterations for convergence\n            )\n            mlflow.log_param("C", config["C"])\n            mlflow.log_param("solver", config["solver"])\n        else:  # SVM\n            model = SVC(\n                kernel=config["kernel"],\n                C=config["C"],\n                random_state=42,\n                probability=True,  # Enable probability estimates\n            )\n            mlflow.log_param("kernel", config["kernel"])\n            mlflow.log_param("C", config["C"])\n\n        # Log common parameters\n        mlflow.log_param("model_type", config["model_type"])\n\n        # Fit model\n        model.fit(X_train, y_train)\n\n        # Get predictions for signature\n        predictions = model.predict(X_train[:5])\n\n        # Create model signature\n        signature = infer_signature(X_train[:5], predictions)\n\n        # Log metrics\n        mlflow.log_metric("accuracy", accuracy_scores[i])\n        mlflow.log_metric("precision", precision_scores[i])\n        mlflow.log_metric("recall", recall_scores[i])\n        mlflow.log_metric("f1_score", f1_scores[i])\n\n        # Log tags\n        mlflow.set_tag("version", versions[i])\n        mlflow.set_tag("environment", environments[i])\n        mlflow.set_tag("framework", frameworks[i])\n\n        # Log the model with signature and input example\n        model_name = f"{config[\'model_type\']}_model_{i}"\n        mlflow.sklearn.log_model(\n            model,\n            name=model_name,\n            signature=signature,\n            input_example=input_example,\n            registered_model_name=f"SearchGuide{config[\'model_type\']}",\n        )\n'})}),"\n",(0,i.jsx)(n.p,{children:"After running this script, you should have 6 different models logged across your experiments, each with different parameters, metrics, and tags."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),': You may see a warning about "Failed to log model config as params" - this is a known MLflow internal warning that can be safely ignored. The models and their parameters are still logged correctly.']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"search-logged-models-syntax",children:"Search Query Syntax"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"search_logged_models"})," API uses a SQL-like Domain Specific Language (DSL) for querying logged models. While inspired by SQL, it has some specific limitations and features tailored for model search."]}),"\n",(0,i.jsx)(n.h3,{id:"visual-representation-of-search-components",children:"Visual Representation of Search Components:"}),"\n",(0,i.jsx)("div",{class:"center-div",style:{width:"30%"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"search components",src:l(20139).A+"",width:"439",height:"142"})})}),"\n",(0,i.jsx)(n.h3,{id:"key-differences-from-search_runs",children:"Key Differences from search_runs:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Default Entity"}),": When no prefix is specified, the field is treated as an attribute (not a metric)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Supported Prefixes"}),": ",(0,i.jsx)(n.code,{children:"metrics."}),", ",(0,i.jsx)(n.code,{children:"params."}),", or no prefix for attributes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dataset-Aware Metrics"}),": You can filter metrics based on specific datasets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"No Tag Support"}),": Unlike ",(0,i.jsx)(n.code,{children:"search_runs"}),", the ",(0,i.jsx)(n.code,{children:"search_logged_models"})," API does not support filtering by tags"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"syntax-rules",children:"Syntax Rules:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Left Side (Field) Syntax:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Fields without special characters can be referenced directly (e.g., ",(0,i.jsx)(n.code,{children:"creation_time"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["Use backticks for fields with special characters (e.g., ",(0,i.jsx)(n.code,{children:"metrics.`f1-score`"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["Double quotes are also acceptable (e.g., ",(0,i.jsx)(n.code,{children:'metrics."f1 score"'}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Right Side (Value) Syntax:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["String values must be enclosed in single quotes (e.g., ",(0,i.jsx)(n.code,{children:"params.model_type = 'RandomForest'"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["Numeric values for metrics don't need quotes (e.g., ",(0,i.jsx)(n.code,{children:"metrics.accuracy > 0.9"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"All non-metric values must be quoted, even if numeric"}),"\n",(0,i.jsxs)(n.li,{children:["For string attributes like ",(0,i.jsx)(n.code,{children:"name"}),", only ",(0,i.jsx)(n.code,{children:"="}),", ",(0,i.jsx)(n.code,{children:"!="}),", ",(0,i.jsx)(n.code,{children:"IN"}),", and ",(0,i.jsx)(n.code,{children:"NOT IN"})," comparators are supported (no ",(0,i.jsx)(n.code,{children:"LIKE"})," or ",(0,i.jsx)(n.code,{children:"ILIKE"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"example-queries",children:"Example Queries"}),"\n",(0,i.jsx)(n.p,{children:"Let's explore different ways to search for logged models using various filter criteria."}),"\n",(0,i.jsx)(n.h3,{id:"1---searching-by-metrics",children:"1 - Searching By Metrics"}),"\n",(0,i.jsxs)(n.p,{children:["Metrics represent model performance measurements. When searching by metrics, use the ",(0,i.jsx)(n.code,{children:"metrics."})," prefix:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Find high-performing models\nhigh_accuracy_models = mlflow.search_logged_models(\n    experiment_ids=["1"],  # Replace with your experiment ID\n    filter_string="metrics.accuracy > 0.9",\n)\n\n# Multiple metric conditions\nbalanced_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="metrics.precision > 0.88 AND metrics.recall > 0.90",\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2---searching-by-parameters",children:"2 - Searching By Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["Parameters capture model configuration. Use the ",(0,i.jsx)(n.code,{children:"params."})," prefix and remember that all parameter values are stored as strings:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Find specific model types\nrf_models = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="params.model_type = \'RandomForest\'"\n)\n\n# Parameter combination search\ntuned_rf_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="params.model_type = \'RandomForest\' AND params.n_estimators = \'200\'",\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3---searching-by-model-name",children:"3 - Searching By Model Name"}),"\n",(0,i.jsxs)(n.p,{children:["Model names are searchable as attributes. Use the ",(0,i.jsx)(n.code,{children:"name"})," field with supported comparators (",(0,i.jsx)(n.code,{children:"="}),", ",(0,i.jsx)(n.code,{children:"!="}),", ",(0,i.jsx)(n.code,{children:"IN"}),", ",(0,i.jsx)(n.code,{children:"NOT IN"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Exact name match\nspecific_model = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="name = \'SVM_model_5\'"\n)\n\n# Multiple model names\nmultiple_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="name IN (\'SVM_model_5\', \'RandomForest_model_0\')",\n)\n\n# Exclude specific model\nnot_svm = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="name != \'SVM_model_4\'"\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"4---searching-by-model-attributes",children:"4 - Searching By Model Attributes"}),"\n",(0,i.jsx)(n.p,{children:"Attributes include model metadata like creation time. No prefix is needed for attributes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Find recently created models (timestamp in milliseconds)\nimport time\n\nlast_week = int((time.time() - 7 * 24 * 60 * 60) * 1000)\n\nrecent_models = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string=f"creation_time > {last_week}"\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"5---dataset-specific-metric-filtering",children:"5 - Dataset-Specific Metric Filtering"}),"\n",(0,i.jsxs)(n.p,{children:["One powerful feature of ",(0,i.jsx)(n.code,{children:"search_logged_models"})," is the ability to filter metrics based on specific datasets:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Find models with high accuracy on test dataset\ntest_accurate_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="metrics.accuracy > 0.9",\n    datasets=[{"dataset_name": "test_dataset", "dataset_digest": "abc123"}],  # Optional\n)\n\n# Multiple dataset conditions\nmulti_dataset_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="metrics.accuracy > 0.85",\n    datasets=[{"dataset_name": "test_dataset"}, {"dataset_name": "validation_dataset"}],\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"6---complex-queries",children:"6 - Complex Queries"}),"\n",(0,i.jsx)(n.p,{children:"Combine multiple conditions for sophisticated model discovery:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Production-ready RandomForest models with high performance\nproduction_ready = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="""\n        params.model_type = \'RandomForest\'\n        AND metrics.accuracy > 0.9\n        AND metrics.precision > 0.88\n    """,\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"programmatic-search-with-python",children:"Programmatic Search with Python"}),"\n",(0,i.jsx)(n.p,{children:"The Python API provides powerful capabilities for searching logged models programmatically."}),"\n",(0,i.jsx)(n.h3,{id:"using-the-fluent-api",children:"Using the Fluent API"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(o.B,{fn:"mlflow.search_logged_models"})," provides a convenient interface for model search:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Basic search with pandas output (default)\nmodels_df = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="metrics.accuracy > 0.9"\n)\n\n# Check available columns\nprint("Available columns:", models_df.columns.tolist())\nprint("\\nModel information:")\nprint(models_df[["name", "source_run_id"]])\n\n# Get results as a list instead of DataFrame\nmodels_list = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="metrics.accuracy > 0.9", output_format="list"\n)\n\nfor model in models_list:\n    print(f"Model: {model.name}, Run ID: {model.source_run_id}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"using-the-client-api",children:"Using the Client API"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(o.B,{fn:"mlflow.client.MlflowClient.search_logged_models"})," offers more control with pagination support:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow import MlflowClient\n\nclient = MlflowClient()\n\n# Search with pagination\npage_token = None\nall_models = []\n\nwhile True:\n    result = client.search_logged_models(\n        experiment_ids=["1"],\n        filter_string="metrics.accuracy > 0.85",\n        max_results=10,\n        page_token=page_token,\n    )\n\n    all_models.extend(result.to_list())\n\n    if not result.token:\n        break\n    page_token = result.token\n\nprint(f"Found {len(all_models)} models")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-ordering",children:"Advanced Ordering"}),"\n",(0,i.jsxs)(n.p,{children:["Control the order of search results using the ",(0,i.jsx)(n.code,{children:"order_by"})," parameter:"]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"order_by"})," functionality for results sorting must be supplied as a list of dictionaries that contains ",(0,i.jsx)(n.code,{children:"field_name"}),". The ",(0,i.jsx)(n.code,{children:"ascending"})," key is optional."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Order by single metric\nbest_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="params.model_type = \'RandomForest\'",\n    order_by=[\n        {"field_name": "metrics.accuracy", "ascending": False}  # Highest accuracy first\n    ],\n)\n\n# Order by dataset-specific metric\ndataset_ordered = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    filter_string="metrics.f1_score > 0.8",\n    order_by=[\n        {\n            "field_name": "metrics.f1_score",\n            "ascending": False,\n            "dataset_name": "test_dataset",\n            "dataset_digest": "abc123",  # Optional\n        }\n    ],\n)\n\n# Multiple ordering criteria\ncomplex_order = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    order_by=[\n        {"field_name": "metrics.accuracy", "ascending": False},\n        {"field_name": "creation_time", "ascending": True},\n    ],\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"getting-top-n-models",children:"Getting Top N Models"}),"\n",(0,i.jsxs)(n.p,{children:["Combine ",(0,i.jsx)(n.code,{children:"max_results"})," with ",(0,i.jsx)(n.code,{children:"order_by"})," to get the best models:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Get top 5 models by accuracy\ntop_5_models = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    max_results=5,\n    order_by=[{"field_name": "metrics.accuracy", "ascending": False}],\n)\n\n# Get the single best model\nbest_model = mlflow.search_logged_models(\n    experiment_ids=["1"],\n    max_results=1,\n    order_by=[{"field_name": "metrics.f1_score", "ascending": False}],\n    output_format="list",\n)[0]\n\naccuracy_metric = next(\n    (metric for metric in best_model.metrics if metric.key == "accuracy"), None\n)\nprint(f"Model ID: {best_model.model_id}, Accuracy: {accuracy_metric.value}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"searching-across-multiple-experiments",children:"Searching Across Multiple Experiments"}),"\n",(0,i.jsx)(n.p,{children:"Search for models across different experiments:"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["Do not search over more than 10 experiments when using the ",(0,i.jsx)(n.code,{children:"search_logged_models"})," API. Excessive search space over\nexperiments will impact the tracking server's performance."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Search specific experiments\nmulti_exp_models = mlflow.search_logged_models(\n    experiment_ids=["1", "2", "3"], filter_string="metrics.accuracy > 0.9"\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,i.jsx)(n.h3,{id:"model-selection-for-deployment",children:"Model Selection for Deployment"}),"\n",(0,i.jsx)(n.p,{children:"Find the best model that meets production criteria:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'deployment_candidates = mlflow.search_logged_models(\n    experiment_ids=exp_ids,\n    filter_string="""\n        metrics.accuracy > 0.95\n        AND metrics.precision > 0.93\n    """,\n    datasets=[{"dataset_name": "production_test_set"}],\n    max_results=1,\n    order_by=[{"field_name": "metrics.f1_score", "ascending": False}],\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"model-comparison",children:"Model Comparison"}),"\n",(0,i.jsx)(n.p,{children:"Compare different model architectures:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Get best model of each type\nmodel_types = ["RandomForest", "LogisticRegression", "SVM"]\nbest_by_type = {}\n\nfor model_type in model_types:\n    models = mlflow.search_logged_models(\n        experiment_ids=["1"],\n        filter_string=f"params.model_type = \'{model_type}\'",\n        max_results=1,\n        order_by=[{"field_name": "metrics.accuracy", "ascending": False}],\n        output_format="list",\n    )\n    if models:\n        best_by_type[model_type] = models[0]\n\n# Compare results\nfor model_type, model in best_by_type.items():\n    # Find accuracy in the metrics list\n    accuracy = None\n    for metric in model.metrics:\n        if metric.key == "accuracy":\n            accuracy = metric.value\n            break\n\n    accuracy_display = f"{accuracy:.4f}" if accuracy is not None else "N/A"\n    print(\n        f"{model_type}: Model ID = {model.model_id}, Run ID = {model.source_run_id}, Accuracy = {accuracy_display}"\n    )\n'})}),"\n",(0,i.jsx)(n.h2,{id:"important-notes",children:"Important Notes"}),"\n",(0,i.jsx)(n.h3,{id:"accessing-metrics-from-loggedmodel",children:"Accessing Metrics from LoggedModel"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"LoggedModel"})," objects returned by ",(0,i.jsx)(n.code,{children:"search_logged_models"})," contain a ",(0,i.jsx)(n.code,{children:"metrics"})," field with a list of ",(0,i.jsx)(n.code,{children:"Metric"})," objects:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Option 1: Access metrics from LoggedModel objects (list output)\nmodels_list = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="metrics.accuracy > 0.9", output_format="list"\n)\n\nfor model in models_list:\n    print(f"\\nModel: {model.name}")\n    # Access metrics as a list of Metric objects\n    for metric in model.metrics:\n        print(f"  {metric.key}: {metric.value}")\n\n# Option 2: Use the DataFrame output which includes flattened metrics\nmodels_df = mlflow.search_logged_models(\n    experiment_ids=["1"], filter_string="metrics.accuracy > 0.9", output_format="pandas"\n)\n\n# The DataFrame has a \'metrics\' column containing the list of Metric objects\nfirst_model_metrics = models_df.iloc[0].get("metrics", [])\nfor metric in first_model_metrics:\n    print(f"{metric.key}: {metric.value}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"search_logged_models"})," API provides a powerful way to discover and compare models in MLflow. By combining flexible filtering, dataset-aware metrics, and ordering capabilities, you can efficiently find the best models for your use case from potentially thousands of candidates."]}),"\n",(0,i.jsx)(n.p,{children:"Key takeaways:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Use SQL-like syntax with ",(0,i.jsx)(n.code,{children:"metrics."}),", ",(0,i.jsx)(n.code,{children:"params."}),", and ",(0,i.jsx)(n.code,{children:"tags."})," prefixes"]}),"\n",(0,i.jsx)(n.li,{children:"Filter metrics by specific datasets for fair comparison"}),"\n",(0,i.jsx)(n.li,{children:"Combine multiple conditions with AND (OR is not supported)"}),"\n",(0,i.jsx)(n.li,{children:"Use ordering and max_results to find top performers"}),"\n",(0,i.jsx)(n.li,{children:"Choose between DataFrame or list output formats based on your needs"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Whether you're selecting models for deployment, comparing architectures, or tracking model evolution, mastering the search API will make your MLflow workflow more efficient and powerful."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},49374:(e,n,l)=>{l.d(n,{B:()=>a});l(96540);const r=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var i=l(86025),s=l(28774),o=l(74848);const t=e=>{const n=e.split(".");for(let l=n.length;l>0;l--){const e=n.slice(0,l).join(".");if(r[e])return e}return null};function a({fn:e,children:n}){const l=t(e);if(!l)return(0,o.jsx)(o.Fragment,{children:n});const a=(0,i.Ay)(`/${r[l]}#${e}`);return(0,o.jsx)(s.A,{to:a,target:"_blank",children:n??(0,o.jsxs)("code",{children:[e,"()"]})})}}}]);