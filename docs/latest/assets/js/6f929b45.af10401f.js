"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["8759"],{69937(n,e,t){t.r(e),t.d(e,{metadata:()=>r,default:()=>x,frontMatter:()=>u,contentTitle:()=>g,toc:()=>f,assets:()=>_});var r=JSON.parse('{"id":"deep-learning/pytorch/index","title":"MLflow PyTorch Integration","description":"Introduction","source":"@site/docs/classic-ml/deep-learning/pytorch/index.mdx","sourceDirName":"deep-learning/pytorch","slug":"/deep-learning/pytorch/","permalink":"/mlflow-website/docs/latest/ml/deep-learning/pytorch/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"Pytorch"},"sidebar":"classicMLSidebar","previous":{"title":"Overview","permalink":"/mlflow-website/docs/latest/ml/deep-learning/"},"next":{"title":"TensorFlow","permalink":"/mlflow-website/docs/latest/ml/deep-learning/tensorflow/"}}'),i=t(74848),a=t(28453),o=t(66497),l=t(33508),s=t(10440),d=t(77541),c=t(46858),m=t(61878),p=t(17133),h=t(60665);let u={sidebar_position:1,sidebar_label:"Pytorch"},g="MLflow PyTorch Integration",_={},f=[{value:"Introduction",id:"introduction",level:2},{value:"Why MLflow + PyTorch?",id:"why-mlflow--pytorch",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Manual Logging",id:"manual-logging",level:2},{value:"System Metrics Tracking",id:"system-metrics-tracking",level:2},{value:"Model Logging with Signatures",id:"model-logging-with-signatures",level:2},{value:"Checkpoint Tracking",id:"checkpoint-tracking",level:2},{value:"Model Loading",id:"model-loading",level:2},{value:"Hyperparameter Optimization",id:"hyperparameter-optimization",level:2},{value:"Model Registry Integration",id:"model-registry-integration",level:2},{value:"Distributed Training",id:"distributed-training",level:2},{value:"Learn More",id:"learn-more",level:2}];function y(n){let e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"mlflow-pytorch-integration",children:"MLflow PyTorch Integration"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"PyTorch"})," is an open-source deep learning framework developed by Meta's AI Research lab. It provides dynamic computation graphs and a Pythonic API for building neural networks, making it popular for both research and production deep learning applications."]}),"\n",(0,i.jsx)(e.p,{children:"MLflow's PyTorch integration provides experiment tracking, model versioning, and deployment capabilities for deep learning workflows."}),"\n",(0,i.jsx)("video",{src:(0,o.default)("/images/deep-learning/dl-training-ui.mp4"),controls:!0,loop:!0,autoPlay:!0,muted:!0,"aria-label":"Deep Learning Training"}),"\n",(0,i.jsx)(e.h2,{id:"why-mlflow--pytorch",children:"Why MLflow + PyTorch?"}),"\n",(0,i.jsx)(l.A,{features:[{icon:c.A,title:"Autologging",description:"Enable comprehensive experiment tracking with one line: mlflow.pytorch.autolog() automatically logs metrics, parameters, and models."},{icon:m.A,title:"Experiment Tracking",description:"Track training metrics, hyperparameters, model architectures, and artifacts across all PyTorch experiments."},{icon:p.A,title:"Model Registry",description:"Version, stage, and deploy PyTorch models with MLflow's model registry and serving infrastructure."},{icon:h.A,title:"Reproducibility",description:"Capture model states, random seeds, and environments for reproducible deep learning experiments."}]}),"\n",(0,i.jsx)(e.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,i.jsx)(e.p,{children:"Enable comprehensive experiment tracking with one line of code:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import mlflow\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Enable autologging\nmlflow.pytorch.autolog()\n\n# Create synthetic data\nX = torch.randn(1000, 784)\ny = torch.randint(0, 10, (1000,))\ntrain_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n\n# Your existing PyTorch code works unchanged\nmodel = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop - metrics, parameters, and models logged automatically\nfor epoch in range(10):\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,i.jsx)(e.p,{children:"Autologging captures training metrics, model parameters, optimizer configuration, and model checkpoints automatically."}),"\n",(0,i.jsx)(e.admonition,{title:"PyTorch Lightning Support",type:"note",children:(0,i.jsx)(e.p,{children:"MLflow's autologging works seamlessly with PyTorch Lightning. For vanilla PyTorch with custom training loops, use manual logging as shown in the section below."})}),"\n",(0,i.jsx)(e.h2,{id:"manual-logging",children:"Manual Logging"}),"\n",(0,i.jsx)(e.p,{children:"For standard PyTorch workflows, integrate MLflow logging into your training loop:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        return self.linear_relu_stack(x)\n\n\n# Training parameters\nparams = {\n    "epochs": 5,\n    "learning_rate": 1e-3,\n    "batch_size": 64,\n}\n\n# Training with MLflow logging\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_params(params)\n\n    # Initialize model and optimizer\n    model = NeuralNetwork()\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=params["learning_rate"])\n\n    # Training loop\n    for epoch in range(params["epochs"]):\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        for data, target in train_loader:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n\n        # Log metrics per epoch\n        avg_loss = train_loss / len(train_loader)\n        accuracy = 100.0 * correct / total\n\n        mlflow.log_metrics(\n            {"train_loss": avg_loss, "train_accuracy": accuracy}, step=epoch\n        )\n\n    # Log final model\n    mlflow.pytorch.log_model(model, name="model")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"system-metrics-tracking",children:"System Metrics Tracking"}),"\n",(0,i.jsx)(e.p,{children:"Track hardware resource utilization during training to monitor GPU usage, memory consumption, and system performance:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create data and model\nX = torch.randn(1000, 784)\ny = torch.randint(0, 10, (1000,))\ntrain_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n\nmodel = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Enable system metrics logging\nmlflow.enable_system_metrics_logging()\n\nwith mlflow.start_run():\n    mlflow.log_params({"learning_rate": 0.001, "batch_size": 32, "epochs": 10})\n\n    # Training loop - system metrics logged automatically\n    for epoch in range(10):\n        for data, target in train_loader:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n        mlflow.log_metric("loss", loss.item(), step=epoch)\n\n    mlflow.pytorch.log_model(model, name="model")\n'})}),"\n",(0,i.jsx)(e.p,{children:"System metrics logging automatically captures:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"GPU Metrics"}),": Utilization percentage, memory usage, temperature, and power consumption"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"CPU Metrics"}),": Utilization percentage and memory usage"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Disk I/O"}),": Read/write throughput and utilization"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Network I/O"}),": Network traffic statistics"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"For PyTorch training, monitoring GPU metrics is especially valuable for:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Identifying GPU underutilization that may indicate data loading bottlenecks"}),"\n",(0,i.jsx)(e.li,{children:"Detecting memory issues before out-of-memory errors occur"}),"\n",(0,i.jsx)(e.li,{children:"Optimizing batch sizes based on GPU memory usage"}),"\n",(0,i.jsx)(e.li,{children:"Comparing resource efficiency across different model architectures"}),"\n"]}),"\n",(0,i.jsx)(e.admonition,{title:"Advanced Configuration",type:"tip",children:(0,i.jsxs)(e.p,{children:["You can customize system metrics collection frequency and behavior. See the ",(0,i.jsx)(e.strong,{children:(0,i.jsx)(e.a,{href:"/ml/tracking/system-metrics",children:"System Metrics documentation"})})," for detailed configuration options."]})}),"\n",(0,i.jsx)(e.h2,{id:"model-logging-with-signatures",children:"Model Logging with Signatures"}),"\n",(0,i.jsx)(e.p,{children:"Log PyTorch models with input/output signatures for better model understanding:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport torch\nimport torch.nn as nn\nfrom mlflow.models import infer_signature\n\nmodel = nn.Sequential(nn.Linear(10, 50), nn.ReLU(), nn.Linear(50, 1))\n\n# Create sample input and output for signature\ninput_example = torch.randn(1, 10)\npredictions = model(input_example)\n\n# Infer signature from input/output\nsignature = infer_signature(input_example.numpy(), predictions.detach().numpy())\n\nwith mlflow.start_run():\n    # Log model with signature and input example\n    mlflow.pytorch.log_model(\n        model,\n        name="pytorch_model",\n        signature=signature,\n        input_example=input_example.numpy(),\n    )\n'})}),"\n",(0,i.jsx)(e.h2,{id:"checkpoint-tracking",children:"Checkpoint Tracking"}),"\n",(0,i.jsxs)(e.p,{children:["Track model checkpoints during training with MLflow 3's checkpoint versioning. Use the ",(0,i.jsx)(e.code,{children:"step"})," parameter to version checkpoints and link metrics to specific model versions:"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n\n# Helper function to prepare data\ndef prepare_data(df):\n    X = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n    y = torch.tensor(df.iloc[:, -1].values, dtype=torch.long)\n    return X, y\n\n\n# Helper function to compute accuracy\ndef compute_accuracy(model, X, y):\n    with torch.no_grad():\n        outputs = model(X)\n        _, predicted = torch.max(outputs, 1)\n        accuracy = (predicted == y).sum().item() / y.size(0)\n    return accuracy\n\n\n# Define a basic PyTorch classifier\nclass IrisClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n\n# Load Iris dataset and prepare the DataFrame\niris = load_iris()\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_df["target"] = iris.target\n\n# Split into training and testing datasets\ntrain_df, test_df = train_test_split(iris_df, test_size=0.2, random_state=42)\n\n# Prepare training data\ntrain_dataset = mlflow.data.from_pandas(train_df, name="iris_train")\nX_train, y_train = prepare_data(train_dataset.df)\n\n# Initialize model\ninput_size = X_train.shape[1]\nhidden_size = 16\noutput_size = len(iris.target_names)\nmodel = IrisClassifier(input_size, hidden_size, output_size)\n\n# Training configuration\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nwith mlflow.start_run() as run:\n    # Log parameters once at the start\n    mlflow.log_params(\n        {\n            "n_layers": 3,\n            "activation": "ReLU",\n            "criterion": "CrossEntropyLoss",\n            "optimizer": "Adam",\n            "learning_rate": 0.01,\n        }\n    )\n\n    for epoch in range(101):\n        # Training step\n        out = model(X_train)\n        loss = criterion(out, y_train)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Log a checkpoint every 10 epochs\n        if epoch % 10 == 0:\n            # Log model checkpoint with step parameter\n            model_info = mlflow.pytorch.log_model(\n                pytorch_model=model,\n                name=f"iris-checkpoint-{epoch}",\n                step=epoch,\n                input_example=X_train[:5].numpy(),\n            )\n\n            # Log metrics linked to this checkpoint and dataset\n            accuracy = compute_accuracy(model, X_train, y_train)\n            mlflow.log_metric(\n                key="train_accuracy",\n                value=accuracy,\n                step=epoch,\n                model_id=model_info.model_id,\n                dataset=train_dataset,\n            )\n\n# Search and rank checkpoints by performance\nranked_checkpoints = mlflow.search_logged_models(\n    filter_string=f"source_run_id=\'{run.info.run_id}\'",\n    order_by=[{"field_name": "metrics.train_accuracy", "ascending": False}],\n    output_format="list",\n)\n\nbest_checkpoint = ranked_checkpoints[0]\nprint(f"Best checkpoint: {best_checkpoint.name}")\nprint(f"Accuracy: {best_checkpoint.metrics[0].value}")\n'})}),"\n",(0,i.jsx)(e.p,{children:"This approach enables you to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Version checkpoints systematically"})," with the ",(0,i.jsx)(e.code,{children:"step"})," parameter"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Link metrics to specific models"})," using ",(0,i.jsx)(e.code,{children:"model_id"})," in ",(0,i.jsx)(e.code,{children:"log_metric()"})]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Associate metrics with datasets"})," for better tracking"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Rank and compare checkpoints"})," using ",(0,i.jsx)(e.code,{children:"mlflow.search_logged_models()"})]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"model-loading",children:"Model Loading"}),"\n",(0,i.jsx)(e.p,{children:"Load logged PyTorch models for inference:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Load as PyTorch model\nmodel_uri = "runs:/<run_id>/pytorch_model"\nloaded_model = mlflow.pytorch.load_model(model_uri)\n\n# Make predictions\ninput_tensor = torch.randn(5, 10)\npredictions = loaded_model(input_tensor)\n\n# Load as PyFunc for generic inference\npyfunc_model = mlflow.pyfunc.load_model(model_uri)\npredictions = pyfunc_model.predict(input_tensor.numpy())\n'})}),"\n",(0,i.jsx)(e.h2,{id:"hyperparameter-optimization",children:"Hyperparameter Optimization"}),"\n",(0,i.jsx)(e.p,{children:"Track hyperparameter tuning experiments with MLflow:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create synthetic dataset for demonstration\ninput_size = 784  # e.g., flattened 28x28 images\noutput_size = 10  # e.g., 10 classes\n\nX_train = torch.randn(1000, input_size)\ny_train = torch.randint(0, output_size, (1000,))\nX_val = torch.randn(200, input_size)\ny_val = torch.randint(0, output_size, (200,))\n\ntrain_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n\n\ndef train_and_evaluate(model, optimizer, train_loader, val_loader, epochs=5):\n    """Simple training loop for demonstration."""\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        model.train()\n        for data, target in train_loader:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            output = model(data)\n            val_loss += criterion(output, target).item()\n\n    return val_loss / len(val_loader)\n\n\ndef objective(trial):\n    """Optuna objective for hyperparameter tuning."""\n\n    with mlflow.start_run(nested=True):\n        # Define hyperparameter search space\n        params = {\n            "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-1, log=True),\n            "hidden_size": trial.suggest_int("hidden_size", 32, 512),\n            "dropout": trial.suggest_float("dropout", 0.1, 0.5),\n        }\n\n        # Log parameters\n        mlflow.log_params(params)\n\n        # Create model\n        model = nn.Sequential(\n            nn.Linear(input_size, params["hidden_size"]),\n            nn.ReLU(),\n            nn.Dropout(params["dropout"]),\n            nn.Linear(params["hidden_size"], output_size),\n        )\n\n        # Train model\n        optimizer = optim.Adam(model.parameters(), lr=params["learning_rate"])\n        val_loss = train_and_evaluate(model, optimizer, train_loader, val_loader)\n\n        # Log validation loss\n        mlflow.log_metric("val_loss", val_loss)\n\n        return val_loss\n\n\n# Run optimization\nwith mlflow.start_run(run_name="PyTorch HPO"):\n    study = optuna.create_study(direction="minimize")\n    study.optimize(objective, n_trials=50)\n\n    # Log best parameters\n    mlflow.log_params({f"best_{k}": v for k, v in study.best_params.items()})\n    mlflow.log_metric("best_val_loss", study.best_value)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"model-registry-integration",children:"Model Registry Integration"}),"\n",(0,i.jsx)(e.p,{children:"Register PyTorch models for version control and deployment:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport torch.nn as nn\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n\nwith mlflow.start_run():\n    # Create a simple model for demonstration\n    model = nn.Sequential(\n        nn.Conv2d(3, 32, 3),\n        nn.ReLU(),\n        nn.MaxPool2d(2),\n        nn.Flatten(),\n        nn.Linear(32 * 15 * 15, 10),\n    )\n\n    # Log model to registry\n    model_info = mlflow.pytorch.log_model(\n        model, name="pytorch_model", registered_model_name="ImageClassifier"\n    )\n\n    # Tag for tracking\n    mlflow.set_tags(\n        {"model_type": "cnn", "dataset": "imagenet", "framework": "pytorch"}\n    )\n\n# Set alias for production deployment\nclient.set_registered_model_alias(\n    name="ImageClassifier",\n    alias="champion",\n    version=model_info.registered_model_version,\n)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"distributed-training",children:"Distributed Training"}),"\n",(0,i.jsx)(e.p,{children:"Track distributed PyTorch training experiments:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import mlflow\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create synthetic dataset\nX_train = torch.randn(1000, 784)\ny_train = torch.randint(0, 10, (1000,))\ntrain_dataset = TensorDataset(X_train, y_train)\n\n# Simple model\nmodel = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\n\n\ndef train_epoch(model, train_loader):\n    """Simple training epoch for demonstration."""\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    total_loss = 0\n\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(train_loader)\n\n\ndef train_distributed():\n    # Initialize distributed training\n    dist.init_process_group(backend="nccl")\n    rank = dist.get_rank()\n\n    # Wrap model with DDP\n    model_ddp = DDP(model.to(rank), device_ids=[rank])\n\n    # Create distributed sampler\n    from torch.utils.data.distributed import DistributedSampler\n\n    sampler = DistributedSampler(\n        train_dataset, num_replicas=dist.get_world_size(), rank=rank\n    )\n    train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n\n    # Only log from rank 0\n    if rank == 0:\n        mlflow.start_run()\n        mlflow.log_params({"world_size": dist.get_world_size(), "backend": "nccl"})\n\n    # Training loop\n    epochs = 10\n    for epoch in range(epochs):\n        sampler.set_epoch(epoch)  # Shuffle data differently each epoch\n        train_loss = train_epoch(model_ddp, train_loader)\n\n        # Log metrics from rank 0 only\n        if rank == 0:\n            mlflow.log_metric("train_loss", train_loss, step=epoch)\n\n    # Save model from rank 0\n    if rank == 0:\n        mlflow.pytorch.log_model(model, name="distributed_model")\n        mlflow.end_run()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,i.jsxs)(s.A,{children:[(0,i.jsx)(d.A,{icon:p.A,title:"Model Registry",description:"Version and manage PyTorch models",href:"/ml/model-registry"}),(0,i.jsx)(d.A,{icon:m.A,title:"MLflow Tracking",description:"Track experiments, parameters, and metrics",href:"/ml/tracking"})]})]})}function x(n={}){let{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(y,{...n})}):y(n)}},75689(n,e,t){t.d(e,{A:()=>s});var r=t(96540);let i=n=>{let e=n.replace(/^([A-Z])|[\s-_]+(\w)/g,(n,e,t)=>t?t.toUpperCase():e.toLowerCase());return e.charAt(0).toUpperCase()+e.slice(1)},a=(...n)=>n.filter((n,e,t)=>!!n&&""!==n.trim()&&t.indexOf(n)===e).join(" ").trim();var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let l=(0,r.forwardRef)(({color:n="currentColor",size:e=24,strokeWidth:t=2,absoluteStrokeWidth:i,className:l="",children:s,iconNode:d,...c},m)=>(0,r.createElement)("svg",{ref:m,...o,width:e,height:e,stroke:n,strokeWidth:i?24*Number(t)/Number(e):t,className:a("lucide",l),...!s&&!(n=>{for(let e in n)if(e.startsWith("aria-")||"role"===e||"title"===e)return!0})(c)&&{"aria-hidden":"true"},...c},[...d.map(([n,e])=>(0,r.createElement)(n,e)),...Array.isArray(s)?s:[s]])),s=(n,e)=>{let t=(0,r.forwardRef)(({className:t,...o},s)=>(0,r.createElement)(l,{ref:s,iconNode:e,className:a(`lucide-${i(n).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${n}`,t),...o}));return t.displayName=i(n),t}},60665(n,e,t){t.d(e,{A:()=>r});let r=(0,t(75689).A)("book-open",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]])},61878(n,e,t){t.d(e,{A:()=>r});let r=(0,t(75689).A)("git-branch",[["line",{x1:"6",x2:"6",y1:"3",y2:"15",key:"17qcm7"}],["circle",{cx:"18",cy:"6",r:"3",key:"1h7g24"}],["circle",{cx:"6",cy:"18",r:"3",key:"fqmcym"}],["path",{d:"M18 9a9 9 0 0 1-9 9",key:"n2h4wq"}]])},17133(n,e,t){t.d(e,{A:()=>r});let r=(0,t(75689).A)("package",[["path",{d:"M11 21.73a2 2 0 0 0 2 0l7-4A2 2 0 0 0 21 16V8a2 2 0 0 0-1-1.73l-7-4a2 2 0 0 0-2 0l-7 4A2 2 0 0 0 3 8v8a2 2 0 0 0 1 1.73z",key:"1a0edw"}],["path",{d:"M12 22V12",key:"d0xqtd"}],["polyline",{points:"3.29 7 12 12 20.71 7",key:"ousv84"}],["path",{d:"m7.5 4.27 9 5.15",key:"1c824w"}]])},46858(n,e,t){t.d(e,{A:()=>r});let r=(0,t(75689).A)("zap",[["path",{d:"M4 14a1 1 0 0 1-.78-1.63l9.9-10.2a.5.5 0 0 1 .86.46l-1.92 6.02A1 1 0 0 0 13 10h7a1 1 0 0 1 .78 1.63l-9.9 10.2a.5.5 0 0 1-.86-.46l1.92-6.02A1 1 0 0 0 11 14z",key:"1xq2db"}]])},33508(n,e,t){t.d(e,{A:()=>i});var r=t(74848);t(96540);function i({features:n,col:e=2}){return(0,r.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${e}, 1fr)`},children:n.map((n,e)=>(0,r.jsxs)("div",{className:"highlightItem_XPnN",children:[n.icon&&(0,r.jsx)("div",{className:"highlightIcon_SUR8",children:(0,r.jsx)(n.icon,{size:24})}),(0,r.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,r.jsx)("h4",{children:n.title}),(0,r.jsx)("p",{children:n.description})]})]},e))})}},77541(n,e,t){t.d(e,{A:()=>d});var r=t(74848);t(96540);var i=t(95310),a=t(34164);let o="tileImage_O4So";var l=t(66497),s=t(92802);function d({icon:n,image:e,imageDark:t,imageWidth:d,imageHeight:c,iconSize:m=32,containerHeight:p,title:h,description:u,href:g,linkText:_="Learn more \u2192",className:f}){if(!n&&!e)throw Error("TileCard requires either an icon or image prop");let y=p?{height:`${p}px`}:{},x={};return d&&(x.width=`${d}px`),c&&(x.height=`${c}px`),(0,r.jsxs)(i.A,{href:g,className:(0,a.A)("tileCard_NHsj",f),children:[(0,r.jsx)("div",{className:"tileIcon_pyoR",style:y,children:n?(0,r.jsx)(n,{size:m}):t?(0,r.jsx)(s.A,{sources:{light:(0,l.default)(e),dark:(0,l.default)(t)},alt:h,className:o,style:x}):(0,r.jsx)("img",{src:(0,l.default)(e),alt:h,className:o,style:x})}),(0,r.jsx)("h3",{children:h}),(0,r.jsx)("p",{children:u}),(0,r.jsx)("div",{className:"tileLink_iUbu",children:_})]})}},10440(n,e,t){t.d(e,{A:()=>a});var r=t(74848);t(96540);var i=t(34164);function a({children:n,className:e}){return(0,r.jsx)("div",{className:(0,i.A)("tilesGrid_hB9N",e),children:n})}},28453(n,e,t){t.d(e,{R:()=>o,x:()=>l});var r=t(96540);let i={},a=r.createContext(i);function o(n){let e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);