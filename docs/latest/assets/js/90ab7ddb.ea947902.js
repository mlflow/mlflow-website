"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["5551"],{2498(e,n,t){t.r(n),t.d(n,{metadata:()=>a,default:()=>y,frontMatter:()=>p,contentTitle:()=>h,toc:()=>m,assets:()=>g});var a=JSON.parse('{"id":"governance/ai-gateway/legacy/usage","title":"AI Gateway Server Usage","description":"Learn how to query your AI Gateway endpoints, integrate with applications, and leverage different APIs and tools.","source":"@site/docs/genai/governance/ai-gateway/legacy/usage.mdx","sourceDirName":"governance/ai-gateway/legacy","slug":"/governance/ai-gateway/legacy/usage","permalink":"/mlflow-website/docs/latest/genai/governance/ai-gateway/legacy/usage","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Configuration","permalink":"/mlflow-website/docs/latest/genai/governance/ai-gateway/legacy/configuration"},"next":{"title":"Version Tracking for GenAI Applications","permalink":"/mlflow-website/docs/latest/genai/version-tracking/"}}'),i=t(74848),s=t(28453),r=t(10440),o=t(77541),l=t(45244);let c=(0,t(75689).A)("house",[["path",{d:"M15 21v-8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v8",key:"5wwlr5"}],["path",{d:"M3 10a2 2 0 0 1 .709-1.528l7-5.999a2 2 0 0 1 2.582 0l7 5.999A2 2 0 0 1 21 10v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z",key:"1d0kgt"}]]);var d=t(46816);let p={},h="AI Gateway Server Usage",g={},m=[{value:"Basic Querying",id:"basic-querying",level:2},{value:"REST API Requests",id:"rest-api-requests",level:3},{value:"Query Parameters",id:"query-parameters",level:3},{value:"Chat Completions",id:"chat-completions",level:4},{value:"Text Completions",id:"text-completions",level:4},{value:"Embeddings",id:"embeddings",level:4},{value:"Streaming Responses",id:"streaming-responses",level:3},{value:"Python Client Integration",id:"python-client-integration",level:2},{value:"OpenAI python SDK client (Recommended)",id:"openai-python-sdk-client-recommended",level:3},{value:"MLflow Deployments Client",id:"mlflow-deployments-client",level:3},{value:"Advanced Client Usage",id:"advanced-client-usage",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Streaming Responses",id:"streaming-responses-1",level:2},{value:"API Reference",id:"api-reference",level:2},{value:"Gateway Management",id:"gateway-management",level:3},{value:"Health Monitoring",id:"health-monitoring",level:3},{value:"Next Steps",id:"next-steps",level:2}];function u(e){let n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ai-gateway-server-usage",children:"AI Gateway Server Usage"})}),"\n","\n",(0,i.jsx)(n.p,{children:"Learn how to query your AI Gateway endpoints, integrate with applications, and leverage different APIs and tools."}),"\n",(0,i.jsx)(n.h2,{id:"basic-querying",children:"Basic Querying"}),"\n",(0,i.jsx)(n.h3,{id:"rest-api-requests",children:"REST API Requests"}),"\n",(0,i.jsx)(n.p,{children:"The gateway exposes REST endpoints that follow OpenAI-compatible patterns. Each endpoint / route accepts JSON payloads and returns structured responses. Use these when integrating with applications that don't have MLflow client libraries:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Chat completions\ncurl -X POST http://localhost:5000/gateway/chat/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      {"role": "user", "content": "Hello, how are you?"}\n    ]\n  }\'\n\n# Text completions\ncurl -X POST http://localhost:5000/gateway/completions/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "prompt": "The future of AI is",\n    "max_tokens": 100\n  }\'\n\n# Embeddings\ncurl -X POST http://localhost:5000/gateway/embeddings/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "input": "Text to embed"\n  }\'\n'})}),"\n",(0,i.jsx)(n.h3,{id:"query-parameters",children:"Query Parameters"}),"\n",(0,i.jsx)(n.p,{children:"These parameters control model behavior and are supported across most providers. Different models may support different subsets of these parameters:"}),"\n",(0,i.jsx)(n.h4,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "messages": [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is machine learning?"}\n  ],\n  "temperature": 0.7,\n  "max_tokens": 150,\n  "top_p": 0.9,\n  "frequency_penalty": 0.0,\n  "presence_penalty": 0.0,\n  "stop": ["\\n\\n"],\n  "stream": false\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"text-completions",children:"Text Completions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "prompt": "Once upon a time",\n  "temperature": 0.8,\n  "max_tokens": 100,\n  "top_p": 1.0,\n  "frequency_penalty": 0.0,\n  "presence_penalty": 0.0,\n  "stop": [".", "!"],\n  "stream": false\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"embeddings",children:"Embeddings"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "input": ["Text to embed", "Another text"],\n  "encoding_format": "float"\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"streaming-responses",children:"Streaming Responses"}),"\n",(0,i.jsx)(n.p,{children:"Enable streaming for real-time response generation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/chat/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [{"role": "user", "content": "Write a story"}],\n    "stream": true\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"python-client-integration",children:"Python Client Integration"}),"\n",(0,i.jsx)(n.h3,{id:"openai-python-sdk-client-recommended",children:"OpenAI python SDK client (Recommended)"}),"\n",(0,i.jsx)(n.p,{children:"MLflow gateway allows developers to use serving models through OpenAI's SDK."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://127.0.0.1:5000/v1",\n    # API key is not needed, it is configured in gateway server side.\n    api_key="",\n)\n\nmessages = [{"role": "user", "content": "How are you ?"}]\n\nresponse = client.chat.completions.create(\n    # The model name must be set to either endpoint name or route name\n    # that is configured in gateway YAML file.\n    model="chat",\n    messages=messages,\n)\nprint(response.choices[0].message)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Streaming API is also supported:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://127.0.0.1:5000/v1",\n    # API key is not needed, it is configured in gateway server side.\n    api_key="",\n)\n\nmessages = [{"role": "user", "content": "How are you ?"}]\n\nresponse = client.chat.completions.create(\n    # The model name must be set to either endpoint name or route name\n    # that is configured in gateway YAML file.\n    model="chat",\n    messages=messages,\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk)\n    print(chunk.choices[0].delta)\n    print("****************")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"mlflow-deployments-client",children:"MLflow Deployments Client"}),"\n",(0,i.jsx)(n.p,{children:"The MLflow deployments client provides a Python interface that handles authentication, error handling, and response parsing. Use this when building Python applications:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\n\n# Create a client for the gateway\nclient = get_deploy_client("http://localhost:5000")\n\n# Query a chat endpoint\nresponse = client.predict(\n    endpoint="chat",\n    inputs={"messages": [{"role": "user", "content": "What is MLflow?"}]},\n)\n\nprint(response["choices"][0]["message"]["content"])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-client-usage",children:"Advanced Client Usage"}),"\n",(0,i.jsx)(n.p,{children:"Build reusable functions for common operations like streaming responses and batch embedding generation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\n\n# Initialize client\nclient = get_deploy_client("http://localhost:5000")\n\n\n# Chat with streaming\ndef stream_chat(prompt):\n    response = client.predict(\n        endpoint="chat",\n        inputs={\n            "messages": [{"role": "user", "content": prompt}],\n            "stream": True,\n            "temperature": 0.7,\n        },\n    )\n\n    for chunk in response:\n        if chunk["choices"][0]["delta"].get("content"):\n            print(chunk["choices"][0]["delta"]["content"], end="")\n\n\n# Generate embeddings\ndef get_embeddings(texts):\n    response = client.predict(endpoint="embeddings", inputs={"input": texts})\n    return [item["embedding"] for item in response["data"]]\n\n\n# Example usage\nstream_chat("Explain quantum computing")\nembeddings = get_embeddings(["Hello world", "MLflow AI Gateway"])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(n.p,{children:"Proper error handling helps you distinguish between network issues, authentication problems, and model-specific errors:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.deployments import get_deploy_client\nfrom mlflow.exceptions import MlflowException\n\nclient = get_deploy_client("http://localhost:5000")\n\ntry:\n    response = client.predict(\n        endpoint="chat", inputs={"messages": [{"role": "user", "content": "Hello"}]}\n    )\n    print(response)\nexcept MlflowException as e:\n    print(f"MLflow error: {e}")\nexcept Exception as e:\n    print(f"Unexpected error: {e}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"streaming-responses-1",children:"Streaming Responses"}),"\n",(0,i.jsx)(n.p,{children:"For long-form content generation, enable streaming to receive partial responses as they're generated instead of waiting for the complete response:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/gateway/chat/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [{"role": "user", "content": "Write a story"}],\n    "stream": true\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,i.jsx)(n.h3,{id:"gateway-management",children:"Gateway Management"}),"\n",(0,i.jsx)(n.p,{children:"Query the gateway's current configuration and available endpoints programmatically:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client(\"http://localhost:5000\")\n\n# List available endpoints\nendpoints = client.list_endpoints()\nfor endpoint in endpoints:\n    print(f\"Endpoint: {endpoint['name']}\")\n\n# Get endpoint details\nendpoint_info = client.get_endpoint(\"chat\")\nprint(f\"Model: {endpoint_info.get('model', {}).get('name', 'N/A')}\")\nprint(f\"Provider: {endpoint_info.get('model', {}).get('provider', 'N/A')}\")\n\n# Note: Route creation, updates, and deletion are typically done\n# through configuration file changes, not programmatically\n"})}),"\n",(0,i.jsx)(n.h3,{id:"health-monitoring",children:"Health Monitoring"}),"\n",(0,i.jsx)(n.p,{children:"Monitor gateway availability and responsiveness for production deployments:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import requests\n\ntry:\n    response = requests.get("http://localhost:5000/health")\n    print(f"Status: {response.status_code}")\n    if response.status_code == 200:\n        print("Gateway is healthy")\nexcept requests.RequestException as e:\n    print(f"Health check failed: {e}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(r.A,{children:[(0,i.jsx)(o.A,{icon:l.A,title:"Gateway Overview",description:"Return to the AI Gateway overview page",href:"/genai/governance/ai-gateway",linkText:"View overview \u2192"}),(0,i.jsx)(o.A,{icon:c,title:"Configuration Guide",description:"Learn how to configure providers and advanced settings",href:"/genai/governance/ai-gateway/legacy/configuration",linkText:"Configure providers \u2192"}),(0,i.jsx)(o.A,{icon:d.A,title:"Setup Guide",description:"Get started with installation and environment setup",href:"/genai/governance/ai-gateway/legacy/setup",linkText:"View setup \u2192"})]})]})}function y(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},75689(e,n,t){t.d(n,{A:()=>l});var a=t(96540);let i=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,t)=>t?t.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},s=(...e)=>e.filter((e,n,t)=>!!e&&""!==e.trim()&&t.indexOf(e)===n).join(" ").trim();var r={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let o=(0,a.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:t=2,absoluteStrokeWidth:i,className:o="",children:l,iconNode:c,...d},p)=>(0,a.createElement)("svg",{ref:p,...r,width:n,height:n,stroke:e,strokeWidth:i?24*Number(t)/Number(n):t,className:s("lucide",o),...!l&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,n])=>(0,a.createElement)(e,n)),...Array.isArray(l)?l:[l]])),l=(e,n)=>{let t=(0,a.forwardRef)(({className:t,...r},l)=>(0,a.createElement)(o,{ref:l,iconNode:n,className:s(`lucide-${i(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,t),...r}));return t.displayName=i(e),t}},45244(e,n,t){t.d(n,{A:()=>a});let a=(0,t(75689).A)("book",[["path",{d:"M4 19.5v-15A2.5 2.5 0 0 1 6.5 2H19a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1H6.5a1 1 0 0 1 0-5H20",key:"k3hazp"}]])},46816(e,n,t){t.d(n,{A:()=>a});let a=(0,t(75689).A)("wrench",[["path",{d:"M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z",key:"cbrjhi"}]])},77541(e,n,t){t.d(n,{A:()=>c});var a=t(74848);t(96540);var i=t(95310),s=t(34164);let r="tileImage_O4So";var o=t(66497),l=t(92802);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:d,iconSize:p=32,containerHeight:h,title:g,description:m,href:u,linkText:y="Learn more \u2192",className:f}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let x=h?{height:`${h}px`}:{},v={};return c&&(v.width=`${c}px`),d&&(v.height=`${d}px`),(0,a.jsxs)(i.A,{href:u,className:(0,s.A)("tileCard_NHsj",f),children:[(0,a.jsx)("div",{className:"tileIcon_pyoR",style:x,children:e?(0,a.jsx)(e,{size:p}):t?(0,a.jsx)(l.A,{sources:{light:(0,o.default)(n),dark:(0,o.default)(t)},alt:g,className:r,style:v}):(0,a.jsx)("img",{src:(0,o.default)(n),alt:g,className:r,style:v})}),(0,a.jsx)("h3",{children:g}),(0,a.jsx)("p",{children:m}),(0,a.jsx)("div",{className:"tileLink_iUbu",children:y})]})}},10440(e,n,t){t.d(n,{A:()=>s});var a=t(74848);t(96540);var i=t(34164);function s({children:e,className:n}){return(0,a.jsx)("div",{className:(0,i.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,t){t.d(n,{R:()=>r,x:()=>o});var a=t(96540);let i={},s=a.createContext(i);function r(e){let n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);