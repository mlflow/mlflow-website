/*! For license information please see f4aac173.573e5286.js.LICENSE.txt */
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3093],{6789:(e,r,n)=>{n.d(r,{A:()=>c});n(96540);var l=n(28774),t=n(34164);const i={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var o=n(86025),s=n(21122),a=n(74848);function c({icon:e,image:r,imageDark:n,imageWidth:c,imageHeight:d,iconSize:m=32,containerHeight:p,title:h,description:f,href:u,linkText:w="Learn more \u2192",className:g}){if(!e&&!r)throw new Error("TileCard requires either an icon or image prop");const x=p?{height:`${p}px`}:{},y={};return c&&(y.width=`${c}px`),d&&(y.height=`${d}px`),(0,a.jsxs)(l.A,{href:u,className:(0,t.A)(i.tileCard,g),children:[(0,a.jsx)("div",{className:i.tileIcon,style:x,children:e?(0,a.jsx)(e,{size:m}):n?(0,a.jsx)(s.A,{sources:{light:(0,o.Ay)(r),dark:(0,o.Ay)(n)},alt:h,className:i.tileImage,style:y}):(0,a.jsx)("img",{src:(0,o.Ay)(r),alt:h,className:i.tileImage,style:y})}),(0,a.jsx)("h3",{children:h}),(0,a.jsx)("p",{children:f}),(0,a.jsx)("div",{className:i.tileLink,children:w})]})}},28453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>s});var l=n(96540);const t={},i=l.createContext(t);function o(e){const r=l.useContext(i);return l.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function s(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),l.createElement(i.Provider,{value:r},e.children)}},42640:(e,r,n)=>{n.d(r,{A:()=>l});const l=(0,n(84722).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},49374:(e,r,n)=>{n.d(r,{B:()=>s});n(96540);const l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var t=n(86025),i=n(74848);const o=e=>{const r=e.split(".");for(let n=r.length;n>0;n--){const e=r.slice(0,n).join(".");if(l[e])return e}return null};function s({fn:e,children:r,hash:n}){const s=o(e);if(!s)return(0,i.jsx)(i.Fragment,{children:r});const a=(0,t.Ay)(`/${l[s]}#${n??e}`);return(0,i.jsx)("a",{href:a,target:"_blank",children:r??(0,i.jsxs)("code",{children:[e,"()"]})})}},61878:(e,r,n)=>{n.d(r,{A:()=>l});const l=(0,n(84722).A)("git-branch",[["line",{x1:"6",x2:"6",y1:"3",y2:"15",key:"17qcm7"}],["circle",{cx:"18",cy:"6",r:"3",key:"1h7g24"}],["circle",{cx:"6",cy:"18",r:"3",key:"fqmcym"}],["path",{d:"M18 9a9 9 0 0 1-9 9",key:"n2h4wq"}]])},65592:(e,r,n)=>{n.d(r,{A:()=>o});n(96540);var l=n(34164);const t={tilesGrid:"tilesGrid_hB9N"};var i=n(74848);function o({children:e,className:r}){return(0,i.jsx)("div",{className:(0,l.A)(t.tilesGrid,r),children:e})}},66927:(e,r,n)=>{n.d(r,{A:()=>o});n(96540);const l={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var t=n(86025),i=n(74848);function o({src:e,alt:r,width:n,caption:o,className:s}){return(0,i.jsxs)("div",{className:`${l.container} ${s||""}`,children:[(0,i.jsx)("div",{className:l.imageWrapper,style:n?{width:n}:{},children:(0,i.jsx)("img",{src:(0,t.Ay)(e),alt:r,className:l.image})}),o&&(0,i.jsx)("p",{className:l.caption,children:o})]})}},84722:(e,r,n)=>{n.d(r,{A:()=>c});var l=n(96540);const t=e=>{const r=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,r,n)=>n?n.toUpperCase():r.toLowerCase()))(e);return r.charAt(0).toUpperCase()+r.slice(1)},i=(...e)=>e.filter((e,r,n)=>Boolean(e)&&""!==e.trim()&&n.indexOf(e)===r).join(" ").trim(),o=e=>{for(const r in e)if(r.startsWith("aria-")||"role"===r||"title"===r)return!0};var s={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const a=(0,l.forwardRef)(({color:e="currentColor",size:r=24,strokeWidth:n=2,absoluteStrokeWidth:t,className:a="",children:c,iconNode:d,...m},p)=>(0,l.createElement)("svg",{ref:p,...s,width:r,height:r,stroke:e,strokeWidth:t?24*Number(n)/Number(r):n,className:i("lucide",a),...!c&&!o(m)&&{"aria-hidden":"true"},...m},[...d.map(([e,r])=>(0,l.createElement)(e,r)),...Array.isArray(c)?c:[c]])),c=(e,r)=>{const n=(0,l.forwardRef)(({className:n,...o},s)=>{return(0,l.createElement)(a,{ref:s,iconNode:r,className:i(`lucide-${c=t(e),c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,n),...o});var c});return n.displayName=t(e),n}},93825:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>u,contentTitle:()=>f,default:()=>x,frontMatter:()=>h,metadata:()=>l,toc:()=>w});const l=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/predefined","title":"Use Predefined LLM Scorers","description":"MLflow provides several pre-configured LLM judge scorers optimized for common evaluation scenarios.","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/predefined.mdx","sourceDirName":"eval-monitor/scorers/llm-judge","slug":"/eval-monitor/scorers/llm-judge/predefined","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/predefined","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"LLM-based Scorers (LLM-as-a-Judge)","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/"},"next":{"title":"Create Custom LLM Scorers","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/guidelines"}}');var t=n(74848),i=n(28453),o=n(49374),s=n(66927),a=n(6789),c=n(65592);const d=(0,n(84722).A)("hammer",[["path",{d:"m15 12-8.373 8.373a1 1 0 1 1-3-3L12 9",key:"eefl8a"}],["path",{d:"m18 15 4-4",key:"16gjal"}],["path",{d:"m21.5 11.5-1.914-1.914A2 2 0 0 1 19 8.172V7l-2.26-2.26a6 6 0 0 0-4.202-1.756L9 2.96l.92.82A6.18 6.18 0 0 1 12 8.4V10l2 2h1.172a2 2 0 0 1 1.414.586L18.5 14.5",key:"b7pghm"}]]);var m=n(42640),p=n(61878);const h={},f="Use Predefined LLM Scorers",u={},w=[{value:"Example Usage",id:"example-usage",level:2},{value:"Available Scorers",id:"available-scorers",level:2},{value:"Selecting Judge Models",id:"selecting-judge-models",level:2},{value:"Output Format",id:"output-format",level:2},{value:"Next Steps",id:"next-steps",level:2}];function g(e){const r={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"use-predefined-llm-scorers",children:"Use Predefined LLM Scorers"})}),"\n",(0,t.jsx)(r.p,{children:"MLflow provides several pre-configured LLM judge scorers optimized for common evaluation scenarios."}),"\n",(0,t.jsxs)(r.admonition,{type:"tip",children:[(0,t.jsx)(r.p,{children:"Typically, you can get started with evaluation using predefined scorers. However, every AI application is unique and has domain-specific quality criteria. At some point, you'll need to create your own custom LLM scorers."}),(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Your application has complex inputs/outputs that predefined scorers can't parse"}),"\n",(0,t.jsx)(r.li,{children:"You need to evaluate specific business logic or domain-specific criteria"}),"\n",(0,t.jsx)(r.li,{children:"You want to combine multiple evaluation aspects into a single scorer"}),"\n"]}),(0,t.jsxs)(r.p,{children:["See ",(0,t.jsx)("ins",{children:(0,t.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge/guidelines",children:"custom LLM scorers"})})," guide for detailed examples."]})]}),"\n",(0,t.jsx)(r.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,t.jsxs)(r.p,{children:["To use the predefined LLM scorers, select the scorer class from the ",(0,t.jsx)(r.a,{href:"#available-scorers",children:"available scorers"})," and pass it to the ",(0,t.jsx)(r.code,{children:"scorers"})," argument of the ",(0,t.jsx)(o.B,{fn:"mlflow.genai.evaluate",children:"evaluate"})," function."]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Correctness, RelevanceToQuery, Guidelines\n\neval_dataset = [\n    {\n        "inputs": {"query": "What is the most common aggregate function in SQL?"},\n        "outputs": "The most common aggregate function in SQL is SUM().",\n        # Correctness scorer requires an "expected_facts" field.\n        "expectations": {\n            "expected_facts": ["Most common aggregate function in SQL is COUNT()."],\n        },\n    },\n    {\n        "inputs": {"query": "How do I use MLflow?"},\n        # verbose answer\n        "outputs": "Hi, I\'m a chatbot that answers questions about MLflow. Thank you for asking a great question! I know MLflow well and I\'m glad to help you with that. You will love it! MLflow is a Python-based platform that provides a comprehensive set of tools for logging, tracking, and visualizing machine learning models and experiments throughout their entire lifecycle. It consists of four main components: MLflow Tracking for experiment management, MLflow Projects for reproducible runs, MLflow Models for standardized model packaging, and MLflow Model Registry for centralized model lifecycle management. To get started, simply install it with \'pip install mlflow\' and then use mlflow.start_run() to begin tracking your experiments with automatic logging of parameters, metrics, and artifacts. The platform creates a beautiful web UI where you can compare different runs, visualize metrics over time, and manage your entire ML workflow efficiently. MLflow integrates seamlessly with popular ML libraries like scikit-learn, TensorFlow, PyTorch, and many others, making it incredibly easy to incorporate into your existing projects!",\n        "expectations": {\n            "expected_facts": [\n                "MLflow is a tool for managing and tracking machine learning experiments."\n            ],\n        },\n    },\n]\n\nresults = mlflow.genai.evaluate(\n    data=eval_dataset,\n    scorers=[\n        Correctness(),\n        RelevanceToQuery(),\n        # Guidelines is a special scorer that takes user-defined criteria for evaluation.\n        # See the "Customizing LLM Judges" section below for more details.\n        Guidelines(\n            name="is_concise",\n            guidelines="The answer must be concise and straight to the point.",\n        ),\n    ],\n)\n'})}),"\n",(0,t.jsx)(s.A,{src:"/images/mlflow-3/eval-monitor/scorers/predefined-scorers-results.png",alt:"Predefined LLM scorers result"}),"\n",(0,t.jsx)(r.h2,{id:"available-scorers",children:"Available Scorers"}),"\n",(0,t.jsxs)(r.table,{children:[(0,t.jsx)(r.thead,{children:(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.th,{children:"Scorer"}),(0,t.jsx)(r.th,{children:"What does it evaluate?"}),(0,t.jsx)(r.th,{children:"Requires ground-truth?"})]})}),(0,t.jsxs)(r.tbody,{children:[(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.RelevanceToQuery",children:"RelevanceToQuery"})}),(0,t.jsx)(r.td,{children:"Does the app's response directly address the user's input?"}),(0,t.jsx)(r.td,{children:"No"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.Correctness",children:"Correctness"})}),(0,t.jsx)(r.td,{children:"Is the app's response correct compared to ground-truth?"}),(0,t.jsx)(r.td,{children:"Yes"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.Guidelines",children:"Guidelines"})}),(0,t.jsx)(r.td,{children:"Does the response adhere to provided guidelines?"}),(0,t.jsx)(r.td,{children:"Yes"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.ExpectationsGuidelines",children:"ExpectationsGuidelines"})}),(0,t.jsx)(r.td,{children:"Does the response meet specific expectations and guidelines?"}),(0,t.jsx)(r.td,{children:"Yes"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.Safety",children:"Safety"})}),(0,t.jsx)(r.td,{children:"Does the app's response avoid harmful or toxic content?"}),(0,t.jsx)(r.td,{children:"No"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.RetrievalGroundedness",children:"RetrievalGroundedness"})}),(0,t.jsx)(r.td,{children:"Is the app's response grounded in retrieved information?"}),(0,t.jsx)(r.td,{children:"No"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.RetrievalRelevance",children:"RetrievalRelevance"})}),(0,t.jsx)(r.td,{children:"Are retrieved documents relevant to the user's request?"}),(0,t.jsx)(r.td,{children:"No"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.RetrievalSufficiency",children:"RetrievalSufficiency"})}),(0,t.jsx)(r.td,{children:"Do retrieved documents contain all necessary information?"}),(0,t.jsx)(r.td,{children:"Yes"})]})]})]}),"\n",(0,t.jsx)(r.admonition,{title:"Availability",type:"note",children:(0,t.jsxs)(r.p,{children:["Safety and RetrievalRelevance scorers are currently only available in ",(0,t.jsx)(r.a,{href:"https://docs.databricks.com/mlflow3/genai/eval-monitor/",children:"Databricks managed MLflow"})," and will be open-sourced soon."]})}),"\n",(0,t.jsx)(r.admonition,{title:"Evaluating Retrieval",type:"tip",children:(0,t.jsxs)(r.p,{children:["Built-in scorers for evaluating retrieval (",(0,t.jsx)(r.code,{children:"RetrievalGroundedness"}),", ",(0,t.jsx)(r.code,{children:"RetrievalRelevance"}),", ",(0,t.jsx)(r.code,{children:"RetrievalSufficiency"}),") require traces to include one or more spans with type ",(0,t.jsx)(r.code,{children:"RETRIEVER"}),". If you are using ",(0,t.jsx)("ins",{children:(0,t.jsx)(r.a,{href:"/genai/tracing/app-instrumentation/automatic",children:"automatic tracing integration"})}),", MLflow will automatically set the type of spans for you."]})}),"\n",(0,t.jsx)(r.h2,{id:"selecting-judge-models",children:"Selecting Judge Models"}),"\n",(0,t.jsx)(r.p,{children:"MLflow supports all major LLM providers, such as OpenAI, Anthropic, Google, xAI, and more."}),"\n",(0,t.jsxs)(r.p,{children:["See ",(0,t.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge#supported-models",children:"Supported Models"})," for more details."]}),"\n",(0,t.jsx)(r.h2,{id:"output-format",children:"Output Format"}),"\n",(0,t.jsx)(r.p,{children:"Predefined LLM-based scorers in MLflow return structured assessments with three key components:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Score"}),": Binary output (",(0,t.jsx)(r.code,{children:"yes"}),"/",(0,t.jsx)(r.code,{children:"no"}),") renders as ",(0,t.jsx)("div",{className:"inline-flex rounded-sm bg-green-100 px-2 py-1 text-sm text-green-600",children:"Pass"})," or ",(0,t.jsx)("div",{className:"inline-flex rounded-sm bg-red-100 px-2 py-1 text-sm text-red-800",children:"Fail"})," in the UI."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Rationale"}),": Detailed explanation of why the judge made its decision"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Source"}),": Metadata about the evaluation source"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{children:'score: "yes"  # or "no"\nrationale: "The response accurately addresses the user\'s question about machine learning concepts, providing clear definitions and relevant examples. The information is factually correct and well-structured."\nsource: AssessmentSource(\n    source_type="LLM_JUDGE",\n    source_id="openai:/gpt-4o-mini"\n)\n'})}),"\n",(0,t.jsx)(r.admonition,{title:"Why Binary Scores?",type:"info",children:(0,t.jsx)(r.p,{children:"Binary scoring provides clearer, more consistent evaluations compared to numeric scales (1-5). Research shows that LLMs produce more reliable judgments when asked to make binary decisions rather than rating on a scale. Binary outputs also simplify threshold-based decision making in production systems."})}),"\n",(0,t.jsx)(r.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(c.A,{children:[(0,t.jsx)(a.A,{icon:d,title:"Guidelines Scorer",description:"Learn how to use the Guidelines scorer to evaluate responses against custom criteria",href:"/genai/eval-monitor/scorers/llm-judge/guidelines"}),(0,t.jsx)(a.A,{icon:m.A,title:"Evaluate Agents",description:"Learn how to evaluate AI agents with specialized techniques and scorers",href:"/genai/eval-monitor/running-evaluation/agents"}),(0,t.jsx)(a.A,{icon:p.A,title:"Evaluate Traces",description:"Evaluate production traces to understand and improve your AI application's behavior",href:"/genai/eval-monitor/running-evaluation/traces"})]})]})}function x(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(g,{...e})}):g(e)}}}]);