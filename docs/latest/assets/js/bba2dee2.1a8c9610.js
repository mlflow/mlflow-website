"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2426],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var t=s(96540);const o={},i=t.createContext(o);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(i.Provider,{value:n},e.children)}},55269:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"serving/custom-apps","title":"Custom Serving Applications","description":"MLflow\'s custom serving applications allow you to build sophisticated model serving solutions that go beyond simple prediction endpoints. Using the PyFunc framework, you can create custom applications with complex preprocessing, postprocessing, multi-model inference, and business logic integration.","source":"@site/docs/genai/serving/custom-apps.mdx","sourceDirName":"serving","slug":"/serving/custom-apps","permalink":"/docs/latest/genai/serving/custom-apps","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Responses Agent","permalink":"/docs/latest/genai/serving/responses-agent"},"next":{"title":"MLflow AI Gateway","permalink":"/docs/latest/genai/governance/ai-gateway/"}}');var o=s(74848),i=s(28453);const r={},l="Custom Serving Applications",a={},p=[{value:"Overview",id:"overview",level:2},{value:"Custom PyFunc Model",id:"custom-pyfunc-model",level:2},{value:"Custom Model",id:"custom-model",level:3},{value:"Multi-Model Ensemble",id:"multi-model-ensemble",level:3},{value:"Serving Custom Applications",id:"serving-custom-applications",level:2},{value:"Local Serving",id:"local-serving",level:3},{value:"Docker Deployment",id:"docker-deployment",level:3},{value:"Testing Custom Applications",id:"testing-custom-applications",level:3},{value:"Best Practices for Custom Applications",id:"best-practices-for-custom-applications",level:2},{value:"Error Handling",id:"error-handling",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Integration with Databricks",id:"integration-with-databricks",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Tips",id:"debugging-tips",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"custom-serving-applications",children:"Custom Serving Applications"})}),"\n",(0,o.jsx)(n.p,{children:"MLflow's custom serving applications allow you to build sophisticated model serving solutions that go beyond simple prediction endpoints. Using the PyFunc framework, you can create custom applications with complex preprocessing, postprocessing, multi-model inference, and business logic integration."}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:["Custom serving applications in MLflow are built using the ",(0,o.jsx)(n.code,{children:"mlflow.pyfunc.PythonModel"})," class, which provides a flexible framework for creating deployable models with custom logic. This approach is ideal when you need to:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\ud83d\udd04 Implement advanced preprocessing and postprocessing logic"}),"\n",(0,o.jsx)(n.li,{children:"\ud83e\udde0 Combine multiple models within a single serving pipeline"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Apply business rules and custom validation checks"}),"\n",(0,o.jsx)(n.li,{children:"\ud83d\udd23 Support diverse input and output data formats"}),"\n",(0,o.jsx)(n.li,{children:"\ud83c\udf10 Integrate seamlessly with external systems or databases"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"custom-pyfunc-model",children:"Custom PyFunc Model"}),"\n",(0,o.jsx)(n.h3,{id:"custom-model",children:"Custom Model"}),"\n",(0,o.jsx)(n.p,{children:"Here's an example of a custom PyFunc model:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nimport json\nfrom typing import Dict, List, Any\nimport openai  # or any other LLM client\n\n\nclass CustomLLMModel(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """Load LLM configuration and initialize client"""\n        # Load model configuration from artifacts\n        config_path = context.artifacts.get("config", "config.json")\n        with open(config_path, "r") as f:\n            self.config = json.load(f)\n\n        # Initialize LLM client\n        self.client = openai.OpenAI(api_key=self.config["api_key"])\n        self.model_name = self.config["model_name"]\n        self.system_prompt = self.config.get(\n            "system_prompt", "You are a helpful assistant."\n        )\n\n    def predict(self, context, model_input):\n        """Core LLM prediction logic"""\n        if isinstance(model_input, pd.DataFrame):\n            # Handle DataFrame input with prompts\n            responses = []\n            for _, row in model_input.iterrows():\n                user_prompt = row.get("prompt", row.get("input", ""))\n                processed_prompt = self._preprocess_prompt(user_prompt)\n                response = self._generate_response(processed_prompt)\n                post_processed = self._postprocess_response(response)\n                responses.append(post_processed)\n            return pd.DataFrame({"response": responses})\n        elif isinstance(model_input, dict):\n            # Handle single prompt\n            user_prompt = model_input.get("prompt", model_input.get("input", ""))\n            processed_prompt = self._preprocess_prompt(user_prompt)\n            response = self._generate_response(processed_prompt)\n            return self._postprocess_response(response)\n        else:\n            # Handle string input\n            processed_prompt = self._preprocess_prompt(str(model_input))\n            response = self._generate_response(processed_prompt)\n            return self._postprocess_response(response)\n\n    def _preprocess_prompt(self, prompt: str) -> str:\n        """Custom prompt preprocessing logic"""\n        # Example: Add context, format prompt, apply templates\n        template = self.config.get("prompt_template", "{prompt}")\n        return template.format(prompt=prompt)\n\n    def _generate_response(self, prompt: str) -> str:\n        """Core LLM inference"""\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": prompt},\n                ],\n                temperature=self.config.get("temperature", 0.7),\n                max_tokens=self.config.get("max_tokens", 1000),\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            return f"Error generating response: {str(e)}"\n\n    def _postprocess_response(self, response: str) -> str:\n        """Custom response postprocessing logic"""\n        # Example: format output, apply filters, extract specific parts\n        if self.config.get("strip_whitespace", True):\n            response = response.strip()\n\n        max_length = self.config.get("max_response_length")\n        if max_length and len(response) > max_length:\n            response = response[:max_length] + "..."\n\n        return response\n\n\n# Example configuration\nconfig = {\n    "api_key": "your-api-key",\n    "model_name": "gpt-4",\n    "system_prompt": "You are an expert data analyst. Provide clear, concise answers.",\n    "temperature": 0.3,\n    "max_tokens": 500,\n    "prompt_template": "Context: Data Analysis Task\\n\\nQuestion: {prompt}\\n\\nAnswer:",\n    "strip_whitespace": True,\n    "max_response_length": 1000,\n}\n\n# Save configuration\nwith open("config.json", "w") as f:\n    json.dump(config, f)\n\n# Log the model\nwith mlflow.start_run():\n    # Log configuration as artifact\n    mlflow.log_artifact("config.json")\n\n    # Create input example\n    input_example = pd.DataFrame(\n        {"prompt": ["What is machine learning?", "Explain neural networks"]}\n    )\n\n    model_info = mlflow.pyfunc.log_model(\n        name="custom_llm_model",\n        python_model=CustomLLMModel(),\n        artifacts={"config": "config.json"},\n        input_example=input_example,\n    )\n'})}),"\n",(0,o.jsx)(n.h3,{id:"multi-model-ensemble",children:"Multi-Model Ensemble"}),"\n",(0,o.jsx)(n.p,{children:"Create a custom application that combines multiple LLMs with different strengths:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.pyfunc\nimport pandas as pd\nimport json\nimport openai\nimport anthropic\nfrom typing import List, Dict, Any\n\n\nclass MultiLLMEnsemble(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """Load multiple LLM configurations from artifacts"""\n        # Load ensemble configuration\n        config_path = context.artifacts["ensemble_config"]\n        with open(config_path, "r") as f:\n            self.config = json.load(f)\n\n        # Initialize multiple LLM clients\n        self.llm_clients = {}\n\n        # OpenAI client\n        if "openai" in self.config["models"]:\n            self.llm_clients["openai"] = openai.OpenAI(\n                api_key=self.config["models"]["openai"]["api_key"]\n            )\n\n        # Anthropic client\n        if "anthropic" in self.config["models"]:\n            self.llm_clients["anthropic"] = anthropic.Anthropic(\n                api_key=self.config["models"]["anthropic"]["api_key"]\n            )\n\n        # Add other LLM clients as needed\n\n        self.voting_strategy = self.config.get("voting_strategy", "weighted_average")\n        self.model_weights = self.config.get("model_weights", {})\n\n    def predict(self, context, model_input):\n        """Ensemble prediction with multiple LLMs"""\n        if isinstance(model_input, pd.DataFrame):\n            responses = []\n            for _, row in model_input.iterrows():\n                prompt = row.get("prompt", row.get("input", ""))\n                task_type = row.get("task_type", "general")\n                ensemble_response = self._generate_ensemble_response(prompt, task_type)\n                responses.append(ensemble_response)\n            return pd.DataFrame({"response": responses})\n        else:\n            prompt = model_input.get("prompt", str(model_input))\n            task_type = (\n                model_input.get("task_type", "general")\n                if isinstance(model_input, dict)\n                else "general"\n            )\n            return self._generate_ensemble_response(prompt, task_type)\n\n    def _generate_ensemble_response(\n        self, prompt: str, task_type: str = "general"\n    ) -> str:\n        """Generate responses from multiple LLMs and combine them"""\n        responses = {}\n\n        # Get task-specific model configuration\n        task_config = self.config.get("task_routing", {}).get(task_type, {})\n        active_models = task_config.get("models", list(self.llm_clients.keys()))\n\n        # Generate responses from each active model\n        for model_name in active_models:\n            if model_name in self.llm_clients:\n                response = self._generate_single_response(model_name, prompt, task_type)\n                responses[model_name] = response\n\n        # Combine responses based on voting strategy\n        return self._combine_responses(responses, task_type)\n\n    def _generate_single_response(\n        self, model_name: str, prompt: str, task_type: str\n    ) -> str:\n        """Generate response from a single LLM"""\n        model_config = self.config["models"][model_name]\n\n        try:\n            if model_name == "openai":\n                response = self.llm_clients["openai"].chat.completions.create(\n                    model=model_config["model_name"],\n                    messages=[\n                        {\n                            "role": "system",\n                            "content": model_config.get("system_prompt", ""),\n                        },\n                        {"role": "user", "content": prompt},\n                    ],\n                    temperature=model_config.get("temperature", 0.7),\n                    max_tokens=model_config.get("max_tokens", 1000),\n                )\n                return response.choices[0].message.content\n\n            elif model_name == "anthropic":\n                response = self.llm_clients["anthropic"].messages.create(\n                    model=model_config["model_name"],\n                    max_tokens=model_config.get("max_tokens", 1000),\n                    temperature=model_config.get("temperature", 0.7),\n                    messages=[{"role": "user", "content": prompt}],\n                )\n                return response.content[0].text\n\n            # Add other LLM implementations here\n\n        except Exception as e:\n            return f"Error from {model_name}: {str(e)}"\n\n    def _combine_responses(self, responses: Dict[str, str], task_type: str) -> str:\n        """Combine multiple LLM responses using specified strategy"""\n        if self.voting_strategy == "best_for_task":\n            # Route to best model for specific task type\n            task_config = self.config.get("task_routing", {}).get(task_type, {})\n            preferred_model = task_config.get("preferred_model")\n            if preferred_model and preferred_model in responses:\n                return responses[preferred_model]\n\n        elif self.voting_strategy == "consensus":\n            # Return response if multiple models agree (simplified)\n            response_list = list(responses.values())\n            if len(set(response_list)) == 1:\n                return response_list[0]\n            else:\n                # If no consensus, return the longest response\n                return max(response_list, key=len)\n\n        elif self.voting_strategy == "weighted_combination":\n            # Combine responses with weights (simplified text combination)\n            combined_response = "Combined insights:\\n\\n"\n            for model_name, response in responses.items():\n                weight = self.model_weights.get(model_name, 1.0)\n                combined_response += (\n                    f"[{model_name.upper()} - Weight: {weight}]: {response}\\n\\n"\n                )\n            return combined_response\n\n        # Default: return first available response\n        return list(responses.values())[0] if responses else "No response generated"\n\n\n# Example ensemble configuration\nensemble_config = {\n    "voting_strategy": "best_for_task",\n    "models": {\n        "openai": {\n            "api_key": "your-openai-key",\n            "model_name": "gpt-4",\n            "system_prompt": "You are a helpful assistant specialized in technical analysis.",\n            "temperature": 0.3,\n            "max_tokens": 800,\n        },\n        "anthropic": {\n            "api_key": "your-anthropic-key",\n            "model_name": "claude-3-sonnet-20240229",\n            "temperature": 0.5,\n            "max_tokens": 1000,\n        },\n    },\n    "task_routing": {\n        "code_analysis": {"models": ["openai"], "preferred_model": "openai"},\n        "creative_writing": {"models": ["anthropic"], "preferred_model": "anthropic"},\n        "general": {"models": ["openai", "anthropic"], "preferred_model": "openai"},\n    },\n    "model_weights": {"openai": 0.6, "anthropic": 0.4},\n}\n\n# Save configuration\nwith open("ensemble_config.json", "w") as f:\n    json.dump(ensemble_config, f)\n\n# Log the ensemble model\nwith mlflow.start_run():\n    # Log configuration as artifact\n    mlflow.log_artifact("ensemble_config.json")\n\n    # Create input example\n    input_example = pd.DataFrame(\n        {\n            "prompt": ["Explain quantum computing", "Write a creative story about AI"],\n            "task_type": ["general", "creative_writing"],\n        }\n    )\n\n    mlflow.pyfunc.log_model(\n        name="multi_llm_ensemble",\n        python_model=MultiLLMEnsemble(),\n        artifacts={"ensemble_config": "ensemble_config.json"},\n        input_example=input_example,\n    )\n'})}),"\n",(0,o.jsx)(n.h2,{id:"serving-custom-applications",children:"Serving Custom Applications"}),"\n",(0,o.jsx)(n.h3,{id:"local-serving",children:"Local Serving"}),"\n",(0,o.jsx)(n.p,{children:"Once you've created and saved your custom application, serve it locally:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Serve from saved model path\nmlflow models serve -m ./path/to/custom/model -p 5000\n\n# Serve from Model Registry\nmlflow models serve -m "models:/CustomApp/Production" -p 5000\n'})}),"\n",(0,o.jsx)(n.h3,{id:"docker-deployment",children:"Docker Deployment"}),"\n",(0,o.jsx)(n.p,{children:"Build a Docker image for your custom application:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Build Docker image\nmlflow models build-docker -m ./path/to/custom/model -n custom-app\n\n# Run the container\ndocker run -p 5000:8080 custom-app\n"})}),"\n",(0,o.jsx)(n.h3,{id:"testing-custom-applications",children:"Testing Custom Applications"}),"\n",(0,o.jsx)(n.p,{children:"Test your custom serving application:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import requests\nimport pandas as pd\nimport json\n\n# Prepare test data\ntest_data = pd.DataFrame(\n    {\n        "feature1": [1.0, 2.0, 3.0],\n        "feature2": [0.5, 1.5, 2.5],\n        "customer_value": [5000, 15000, 3000],\n    }\n)\n\n# Convert to the expected input format\ninput_data = {"inputs": test_data.to_dict("records")}\n\n# Make prediction request\nresponse = requests.post(\n    "http://localhost:5000/invocations",\n    headers={"Content-Type": "application/json"},\n    data=json.dumps(input_data),\n)\n\nprint("Response:", response.json())\n'})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-custom-applications",children:"Best Practices for Custom Applications"}),"\n",(0,o.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,o.jsx)(n.p,{children:"Implement comprehensive error handling:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def predict(self, context, model_input):\n    try:\n        # Validate input\n        self._validate_input(model_input)\n\n        # Process and predict\n        result = self._process_prediction(model_input)\n\n        return result\n\n    except ValueError as e:\n        # Handle validation errors\n        return {"error": f"Validation error: {str(e)}"}\n    except Exception as e:\n        # Handle unexpected errors\n        return {"error": f"Prediction failed: {str(e)}"}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\ud83d\udca4 Lazy Loading: Defer loading large artifacts until they\u2019re needed"}),"\n",(0,o.jsx)(n.li,{children:"\ud83d\uddc2\ufe0f Caching: Store and reuse results of frequent computations"}),"\n",(0,o.jsx)(n.li,{children:"\ud83d\udce6 Batch Processing: Handle multiple inputs in a single, efficient operation"}),"\n",(0,o.jsx)(n.li,{children:"\ud83e\uddf9 Memory Management: Release unused resources after each request or task"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\ud83e\uddea Unit Testing: Test individual components of your custom model in isolation"}),"\n",(0,o.jsx)(n.li,{children:"\ud83d\udd17 Integration Testing: Verify the full prediction pipeline end-to-end"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Output Validation: Ensure correct output formats and robust error handling"}),"\n",(0,o.jsx)(n.li,{children:"\ud83d\ude80 Performance Testing: Evaluate scalability using realistic data volumes and loads"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,o.jsx)(n.p,{children:"Document your custom applications thoroughly:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\ud83d\udce5 Input/Output Specifications: Clearly define expected input formats and output structures"}),"\n",(0,o.jsx)(n.li,{children:"\u2699\ufe0f Business Logic: Document the core logic and decision-making rules"}),"\n",(0,o.jsx)(n.li,{children:"\u26a1 Performance Characteristics: Describe expected throughput, latency, and resource usage"}),"\n",(0,o.jsx)(n.li,{children:"\u2757 Error Handling: Specify how errors are detected, managed, and communicated"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-databricks",children:"Integration with Databricks"}),"\n",(0,o.jsx)(n.p,{children:"In Databricks Managed MLflow, custom applications can take advantage of additional features:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\u2601\ufe0f Serverless Compute"}),": Automatic scaling based on demand"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83d\udd10 Security Integration"}),": Built-in authentication and authorization"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83d\udcc8 Monitoring"}),": Advanced metrics and logging capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83d\uddc2\ufe0f Version Management"}),": Seamless model version management with Unity Catalog"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Note that the creation and management of serving endpoints is handled differently in Databricks compared to MLflow OSS, with additional UI and API capabilities for enterprise deployment."}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Import Errors"}),": Ensure all dependencies are specified in the conda environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Artifact Loading"}),": Verify artifact paths are correct and accessible"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Memory Issues"}),": Monitor memory usage with large models or datasets"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Serialization"}),": Use ",(0,o.jsx)(n.a,{href:"/ml/model/models-from-code/",children:(0,o.jsx)(n.code,{children:"models-from-code"})})," feature when logging models that are not picklable"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\ud83e\uddfe ",(0,o.jsxs)(n.strong,{children:["Enable ",(0,o.jsx)(n.a,{href:"/genai/tracing",children:"tracing"})]})," to track execution flow"]}),"\n",(0,o.jsxs)(n.li,{children:["\ud83e\uddea ",(0,o.jsx)(n.strong,{children:"Test components individually"})," before integration"]}),"\n",(0,o.jsxs)(n.li,{children:["\ud83d\udcca ",(0,o.jsx)(n.strong,{children:"Use small test datasets"})," for initial validation"]}),"\n",(0,o.jsxs)(n.li,{children:["\ud83d\udda5\ufe0f ",(0,o.jsx)(n.strong,{children:"Monitor resource usage"})," during development"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Custom serving applications provide the flexibility to build production-ready ML systems that integrate seamlessly with your business requirements while maintaining the reliability and scalability of MLflow's serving infrastructure."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);