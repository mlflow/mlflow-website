"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8137],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var o=i(96540);const t={},r=o.createContext(t);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(r.Provider,{value:n},e.children)}},52922:(e,n,i)=>{i.d(n,{A:()=>o});const o=i.p+"assets/images/RLHF-architecture-8865bb6fa6abd94c3f2fd3d31a290ea6.png"},57942:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"llms/openai/guide/index","title":"OpenAI within MLflow","description":"The openai flavor is under active development and is marked as Experimental. Public APIs may change and new features are","source":"@site/docs/llms/openai/guide/index.mdx","sourceDirName":"llms/openai/guide","slug":"/llms/openai/guide/","permalink":"/docs/latest/llms/openai/guide/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Detailed Guide","sidebar_position":3},"sidebar":"docsSidebar","previous":{"title":"Autologging","permalink":"/docs/latest/llms/openai/autologging/"},"next":{"title":"MLflow LangChain Flavor","permalink":"/docs/latest/llms/langchain/"}}');var t=i(74848),r=i(28453);const a={sidebar_label:"Detailed Guide",sidebar_position:3},s="OpenAI within MLflow",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Beyond Simple Deployment: Building Powerful NLP Applications with OpenAI and MLflow",id:"beyond-simple-deployment-building-powerful-nlp-applications-with-openai-and-mlflow",level:2},{value:"Craft Task-Specific Services",id:"craft-task-specific-services",level:3},{value:"Simplify Deployment and Comparison",id:"simplify-deployment-and-comparison",level:3},{value:"Advanced Prompt Engineering and Version Tracking with MLflow and OpenAI: Unleashing the True Potential of LLMs",id:"advanced-prompt-engineering-and-version-tracking-with-mlflow-and-openai-unleashing-the-true-potential-of-llms",level:2},{value:"Beyond the Basics: Embracing Iterative Experimentation",id:"beyond-the-basics-embracing-iterative-experimentation",level:3},{value:"Refining for Optimum Results: A/B Testing and Fine-Tuning",id:"refining-for-optimum-results-ab-testing-and-fine-tuning",level:3},{value:"Collaboration and Sharing: Fueling Innovation and Progress",id:"collaboration-and-sharing-fueling-innovation-and-progress",level:3},{value:"Leveraging MLflow for Optimized Prompt Engineering",id:"leveraging-mlflow-for-optimized-prompt-engineering",level:3},{value:"Real-World Impact",id:"real-world-impact",level:3},{value:"Direct OpenAI Service Usage",id:"direct-openai-service-usage",level:2},{value:"Azure OpenAI Service Integration",id:"azure-openai-service-integration",level:2},{value:"Environment Configuration for Azure Integration",id:"environment-configuration-for-azure-integration",level:3},{value:"Azure OpenAI Service in MLflow",id:"azure-openai-service-in-mlflow",level:3},{value:"OpenAI Autologging",id:"openai-autologging",level:2},{value:"Next Steps in Your NLP Journey",id:"next-steps-in-your-nlp-journey",level:2},{value:"Supplementary Learnings",id:"supplementary-learnings",level:2},{value:"RLHF in GPT Models",id:"rlhf-in-gpt-models",level:3},{value:"The RLHF Process",id:"the-rlhf-process",level:4},{value:"Why RLHF Matters",id:"why-rlhf-matters",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"openai-within-mlflow",children:"OpenAI within MLflow"})}),"\n",(0,t.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"openai"})," flavor is under active development and is marked as Experimental. Public APIs may change and new features are\nsubject to be added as additional functionality is brought to the flavor."]})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The integration of OpenAI's advanced language models within MLflow opens up new frontiers in creating and using NLP-based applications. It enables users to harness\nthe cutting-edge capabilities of models like GPT-4 for varied tasks, ranging from conversational AI to complex text analysis\nand embeddings generation. This integration is a leap forward in making advanced NLP accessible and manageable within a robust framework like MLflow."}),"\n",(0,t.jsx)(n.h2,{id:"beyond-simple-deployment-building-powerful-nlp-applications-with-openai-and-mlflow",children:"Beyond Simple Deployment: Building Powerful NLP Applications with OpenAI and MLflow"}),"\n",(0,t.jsx)(n.p,{children:"While the openai flavor within MLflow simplifies the logging and deployment of OpenAI models, its true potential lies in unlocking the full power of NLP\napplications. By seamlessly integrating with MLflow, you can:"}),"\n",(0,t.jsx)(n.h3,{id:"craft-task-specific-services",children:"Craft Task-Specific Services"}),"\n",(0,t.jsx)(n.p,{children:"Raw access to a large language model doesn't guarantee a valuable service. While powerful, unprompted models can be overly general, leading to unintended\noutputs or inappropriate responses for the intent of the application. MLflow enables users to tailor models for specific tasks, achieving desired functionalities\nwhile ensuring context and control."}),"\n",(0,t.jsx)(n.p,{children:"This allows you to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define prompts and parameters"}),": Instead of relying on open-ended inputs, you can define specific prompts and parameters that guide the model's responses, focusing its capabilities on the desired task."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Save and deploy customized models"}),": The saved models, along with their prompts and parameters, can be easily deployed and shared, ensuring consistent behavior and performance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perform champion/challenger evaluations"}),": MLflow allows users to easily compare different prompts, parameters, and deployment configurations, facilitating the selection of the most effective model for a specific task."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"simplify-deployment-and-comparison",children:"Simplify Deployment and Comparison"}),"\n",(0,t.jsx)(n.p,{children:"MLflow streamlines the deployment process, enabling you to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Package and deploy models as applications"}),": The openai flavor simplifies model packaging, including prompts, configuration parameters, and inference parameters, into a single, portable artifact."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compare different approaches"}),": With consistent packaging, you can easily compare different models, prompts, configurations, and deployment options, facilitating informed decision-making."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Leverage MLflow's ecosystem"}),": MLflow integrates with various tools and platforms, allowing users to deploy models on diverse environments, from cloud platforms to local servers."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-prompt-engineering-and-version-tracking-with-mlflow-and-openai-unleashing-the-true-potential-of-llms",children:"Advanced Prompt Engineering and Version Tracking with MLflow and OpenAI: Unleashing the True Potential of LLMs"}),"\n",(0,t.jsx)(n.p,{children:"The integration of MLflow and OpenAI marks a paradigm shift in the field of prompt engineering for large language models (LLMs). While basic prompts can\nenable rudimentary functionalities, this powerful combination unlocks the full potential of LLMs, empowering developers and data scientists to meticulously\ncraft and refine prompts, ushering in a new era of targeted and impactful applications."}),"\n",(0,t.jsx)(n.h3,{id:"beyond-the-basics-embracing-iterative-experimentation",children:"Beyond the Basics: Embracing Iterative Experimentation"}),"\n",(0,t.jsx)(n.p,{children:"Forget static prompts and limited applications! MLflow and OpenAI revolutionize the process by facilitating iterative experimentation through:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tracking and Comparison"}),": MLflow logs and meticulously tracks every iteration of a prompt alongside its performance metrics. This allows for a granular comparison of different versions, enabling informed decisions and identification of the most effective prompts."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Version Control for Reproducible Experimentation"}),": Each prompt iteration is safely stored and version-controlled within MLflow. This allows for easy rollback and comparison, fostering experimentation and refinement while ensuring reproducibility, a crucial aspect of scientific advancement."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible Parameterization"}),": MLflow enables control over which parameters are permitted to be modified at inference time, giving you the power to control creativity (temperature) and maximum token length (for cost)."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"refining-for-optimum-results-ab-testing-and-fine-tuning",children:"Refining for Optimum Results: A/B Testing and Fine-Tuning"}),"\n",(0,t.jsx)(n.p,{children:"MLflow and OpenAI empower you to push the boundaries of LLM performance by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A/B Testing for Optimal Prompt Selection"}),": Perform efficient A/B testing of different prompt variations and parameter configurations. This allows for the identification of the most effective combination for specific tasks and user profiles, leading to remarkable performance gains."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tailoring Prompts for Desired Outcomes"}),": Iterative and organized experimentation allows you to focus on what makes the most sense for your applications. Whether you prioritize factual accuracy, creative expression, or conversational fluency, MLflow and OpenAI empower you to tailor prompts to optimize specific performance metrics. This ensures that your LLM applications deliver the desired results, time and time again."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"collaboration-and-sharing-fueling-innovation-and-progress",children:"Collaboration and Sharing: Fueling Innovation and Progress"}),"\n",(0,t.jsx)(n.p,{children:"The power of MLflow and OpenAI extends beyond individual projects. By facilitating collaboration and sharing, they accelerate the advancement of LLM applications:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Shareable Artifacts for Collaborative Innovation"}),": MLflow packages prompts, parameters, model versions, and performance metrics into shareable artifacts. This enables researchers and developers to collaborate seamlessly, leveraging each other's insights and refined prompts to accelerate progress."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"leveraging-mlflow-for-optimized-prompt-engineering",children:"Leveraging MLflow for Optimized Prompt Engineering"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Iterative Improvement"}),": MLflow's tracking system supports an iterative approach to prompt engineering. By logging each experiment, users can incrementally refine their prompts, driving towards the most effective model interaction."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collaborative Experimentation"}),": MLflow's collaborative features enable teams to share and discuss prompt versions and experiment results, fostering a collaborative environment for prompt development."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"real-world-impact",children:"Real-World Impact"}),"\n",(0,t.jsx)(n.p,{children:"In real-world applications, the ability to track and refine prompts using MLflow and OpenAI leads to more accurate, reliable, and efficient language model\nimplementations. Whether in customer service chatbots, content generation, or complex decision support systems, the meticulous management of prompts\nand model versions directly translates to enhanced performance and user experience."}),"\n",(0,t.jsx)(n.p,{children:"This integration not only simplifies the complexities of working with advanced LLMs but also opens up new avenues for innovation in NLP applications,\nensuring that each prompt-driven interaction is as effective and impactful as possible."}),"\n",(0,t.jsx)(n.h2,{id:"direct-openai-service-usage",children:"Direct OpenAI Service Usage"}),"\n",(0,t.jsx)(n.p,{children:"Direct usage of OpenAI's service through MLflow allows for seamless interaction with the latest GPT models for a variety of NLP tasks."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import logging\nimport os\n\nimport openai\nimport pandas as pd\n\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema\n\nlogging.getLogger("mlflow").setLevel(logging.ERROR)\n\n# Uncomment the following lines to run this script without using a real OpenAI API key.\n# os.environ["MLFLOW_TESTING"] = "true"\n# os.environ["OPENAI_API_KEY"] = "test"\n\nassert (\n    "OPENAI_API_KEY" in os.environ\n), "Please set the OPENAI_API_KEY environment variable."\n\n\nprint(\n    """\n# ******************************************************************************\n# Single variable\n# ******************************************************************************\n"""\n)\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model="gpt-4o-mini",\n        task=openai.chat.completions,\n        name="model",\n        messages=[{"role": "user", "content": "Tell me a joke about {animal}."}],\n    )\n\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        "animal": [\n            "cats",\n            "dogs",\n        ]\n    }\n)\nprint(model.predict(df))\n\nlist_of_dicts = [\n    {"animal": "cats"},\n    {"animal": "dogs"},\n]\nprint(model.predict(list_of_dicts))\n\nlist_of_strings = [\n    "cats",\n    "dogs",\n]\nprint(model.predict(list_of_strings))\nprint(\n    """\n# ******************************************************************************\n# Multiple variables\n# ******************************************************************************\n"""\n)\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model="gpt-4o-mini",\n        task=openai.chat.completions,\n        name="model",\n        messages=[\n            {"role": "user", "content": "Tell me a {adjective} joke about {animal}."}\n        ],\n    )\n\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        "adjective": ["funny", "scary"],\n        "animal": ["cats", "dogs"],\n    }\n)\nprint(model.predict(df))\n\n\nlist_of_dicts = [\n    {"adjective": "funny", "animal": "cats"},\n    {"adjective": "scary", "animal": "dogs"},\n]\nprint(model.predict(list_of_dicts))\n\nprint(\n    """\n# ******************************************************************************\n# Multiple prompts\n# ******************************************************************************\n"""\n)\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model="gpt-4o-mini",\n        task=openai.chat.completions,\n        name="model",\n        messages=[\n            {"role": "system", "content": "You are {person}"},\n            {"role": "user", "content": "Let me hear your thoughts on {topic}"},\n        ],\n    )\n\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        "person": ["Elon Musk", "Jeff Bezos"],\n        "topic": ["AI", "ML"],\n    }\n)\nprint(model.predict(df))\n\nlist_of_dicts = [\n    {"person": "Elon Musk", "topic": "AI"},\n    {"person": "Jeff Bezos", "topic": "ML"},\n]\nprint(model.predict(list_of_dicts))\n\n\nprint(\n    """\n# ******************************************************************************\n# No input variables\n# ******************************************************************************\n"""\n)\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model="gpt-4o-mini",\n        task=openai.chat.completions,\n        name="model",\n        messages=[{"role": "system", "content": "You are Elon Musk"}],\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        "question": [\n            "Let me hear your thoughts on AI",\n            "Let me hear your thoughts on ML",\n        ],\n    }\n)\nprint(model.predict(df))\n\nlist_of_dicts = [\n    {"question": "Let me hear your thoughts on AI"},\n    {"question": "Let me hear your thoughts on ML"},\n]\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(model.predict(list_of_dicts))\n\nlist_of_strings = [\n    "Let me hear your thoughts on AI",\n    "Let me hear your thoughts on ML",\n]\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(model.predict(list_of_strings))\n\n\nprint(\n    """\n# ******************************************************************************\n# Inference parameters with chat completions\n# ******************************************************************************\n"""\n)\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model="gpt-4o-mini",\n        task=openai.chat.completions,\n        name="model",\n        messages=[{"role": "user", "content": "Tell me a joke about {animal}."}],\n        signature=ModelSignature(\n            inputs=Schema([ColSpec(type="string", name=None)]),\n            outputs=Schema([ColSpec(type="string", name=None)]),\n            params=ParamSchema(\n                [\n                    ParamSpec(name="temperature", default=0, dtype="float"),\n                ]\n            ),\n        ),\n    )\n\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        "animal": [\n            "cats",\n            "dogs",\n        ]\n    }\n)\nprint(model.predict(df, params={"temperature": 1}))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"azure-openai-service-integration",children:"Azure OpenAI Service Integration"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"openai"})," flavor supports logging models that use the ",(0,t.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/openai-service",children:"Azure OpenAI Service"}),".\nThere are a few notable differences between the Azure OpenAI Service and the OpenAI Service that need to be considered when logging models that target Azure endpoints."]}),"\n",(0,t.jsx)(n.h3,{id:"environment-configuration-for-azure-integration",children:"Environment Configuration for Azure Integration"}),"\n",(0,t.jsx)(n.p,{children:"To successfully log a model targeting Azure OpenAI Service, specific environment variables are essential for authentication and functionality."}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["The following environment variables contain ",(0,t.jsx)(n.strong,{children:"highly sensitive access keys"}),". Ensure that you do not commit these values to source control or declare them in an interactive\nenvironment. Environment variables should be set from within your terminal via an ",(0,t.jsx)(n.code,{children:"export"})," command, an addition to your user profile configurations (i.e., .bashrc or .zshrc),\nor set through your IDE's environment variable configuration. Please do not leak your credentials."]})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OPENAI_API_KEY"}),': The API key for the Azure OpenAI Service. This can be found in the Azure Portal under the "Keys and Endpoint" section of the "Keys and Endpoint" tab. You can use either ',(0,t.jsx)(n.code,{children:"KEY1"})," or ",(0,t.jsx)(n.code,{children:"KEY2"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OPENAI_API_BASE"}),": The base endpoint for your Azure OpenAI resource (e.g., ",(0,t.jsx)(n.code,{children:"https://<your-service-name>.openai.azure.com/"}),"). Within the Azure OpenAI documentation and guides, this key is referred to as ",(0,t.jsx)(n.code,{children:"AZURE_OPENAI_ENDPOINT"})," or simply ",(0,t.jsx)(n.code,{children:"ENDPOINT"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OPENAI_API_VERSION"}),": The API version to use for the Azure OpenAI Service. More information can be found in the ",(0,t.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/ai-services/openai/reference",children:"Azure OpenAI documentation"}),", including up-to-date lists of supported versions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OPENAI_API_TYPE"}),": If using Azure OpenAI endpoints, this value should be set to ",(0,t.jsx)(n.code,{children:'"azure"'}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OPENAI_DEPLOYMENT_NAME"}),": The deployment name that you chose when you deployed the model in Azure. To learn more, visit the ",(0,t.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal",children:"Azure OpenAI deployment documentation"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"azure-openai-service-in-mlflow",children:"Azure OpenAI Service in MLflow"}),"\n",(0,t.jsx)(n.p,{children:"Integrating Azure OpenAI models within MLflow follows similar procedures to direct OpenAI service usage, with additional Azure-specific configurations."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nimport pandas as pd\n\nimport mlflow\n\n"""\nSet environment variables for Azure OpenAI service\nexport OPENAI_API_KEY="<AZURE OPENAI KEY>"\n# OPENAI_API_BASE should be the endpoint of your Azure OpenAI resource\n# e.g. https://<service-name>.openai.azure.com/\nexport OPENAI_API_BASE="<AZURE OPENAI BASE>"\n# OPENAI_API_VERSION e.g. 2023-05-15\nexport OPENAI_API_VERSION="<AZURE OPENAI API VERSION>"\nexport OPENAI_API_TYPE="azure"\nexport OPENAI_DEPLOYMENT_NAME="<AZURE OPENAI DEPLOYMENT ID OR NAME>"\n"""\n\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        # Your Azure OpenAI model e.g. gpt-4o-mini\n        model="<YOUR AZURE OPENAI MODEL>",\n        task=openai.chat.completions,\n        name="model",\n        messages=[{"role": "user", "content": "Tell me a joke about {animal}."}],\n    )\n\n# Load native OpenAI model\nnative_model = mlflow.openai.load_model(model_info.model_uri)\ncompletion = openai.chat.completions.create(\n    deployment_id=native_model["deployment_id"],\n    messages=native_model["messages"],\n)\nprint(completion["choices"][0]["message"]["content"])\n\n\n# Load as Pyfunc model\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        "animal": [\n            "cats",\n            "dogs",\n        ]\n    }\n)\nprint(model.predict(df))\n\nlist_of_dicts = [\n    {"animal": "cats"},\n    {"animal": "dogs"},\n]\nprint(model.predict(list_of_dicts))\n\nlist_of_strings = [\n    "cats",\n    "dogs",\n]\nprint(model.predict(list_of_strings))\n\nlist_of_strings = [\n    "Let me hear your thoughts on AI",\n    "Let me hear your thoughts on ML",\n]\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(model.predict(list_of_strings))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"openai-autologging",children:"OpenAI Autologging"}),"\n",(0,t.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,t.jsx)(n.p,{children:"Autologging is only supported for OpenAI >= 1.17."})}),"\n",(0,t.jsxs)(n.p,{children:["To learn more about autologging support for the OpenAI flavor, please ",(0,t.jsx)(n.a,{href:"/llms/openai/autologging",children:"see the autologging guide"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["For more examples, please click ",(0,t.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/blob/master/examples/openai/autologging",children:"here"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps-in-your-nlp-journey",children:"Next Steps in Your NLP Journey"}),"\n",(0,t.jsx)(n.p,{children:"We invite you to harness the combined power of MLflow and OpenAI for developing innovative NLP applications. Whether it's creating interactive\nAI-driven platforms, enhancing data analysis with deep NLP insights, or exploring new frontiers in AI, this integration serves as a robust foundation\nfor your explorations"}),"\n",(0,t.jsx)(n.h2,{id:"supplementary-learnings",children:"Supplementary Learnings"}),"\n",(0,t.jsx)(n.p,{children:"If you're a bit curious about what really sets apart OpenAI's GPT models from other language models, we've included a brief (and heavily simplified) overview of\ntheir training process below.\nThis is but one small aspect of why they're so good and capable of responding in such a human-like manner, but it's a fascinating insight into how different the\nfine-tuning process is for these models as compared to the more familiar process of traditional supervised machine learning."}),"\n",(0,t.jsx)(n.h3,{id:"rlhf-in-gpt-models",children:"RLHF in GPT Models"}),"\n",(0,t.jsx)(n.p,{children:"One of the defining features of OpenAI's GPT models is their training process, particularly the use of Reinforcement Learning from Human Feedback\n(RLHF). This methodology sets GPT models apart from traditional language models in several ways (although they are not the only organization to use this\nstrategy, it is a key process component that greatly helps to enhance the quality of their services)."}),"\n",(0,t.jsx)(n.h4,{id:"the-rlhf-process",children:"The RLHF Process"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Supervised Fine-Tuning (SFT)"}),": Initially, GPT models undergo supervised fine-tuning using a large dataset of text. This process imparts the basic understanding of language and context."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reward Modeling (RM)"}),": Human trainers review the model's outputs and rate them based on criteria such as relevance, accuracy, and safety. This feedback is used to create a 'reward model'\u2014a system that evaluates the quality of the model's responses."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Proximal Policy Optimization (PPO)"}),": In this stage, the model is trained using reinforcement learning techniques, guided by the reward model. The model learns to generate responses that are more aligned with the values and preferences as judged by human trainers."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Iterative Improvement"}),": The model undergoes continuous refinement through human feedback, ensuring that it evolves and adapts to produce responses that are aligned with the feedback preferences provided by the human reviewers."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"why-rlhf-matters",children:"Why RLHF Matters"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human-Like Responses"}),": RLHF enables GPT models to generate responses that closely mimic human thought processes, making them more relatable and effective in practical applications."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety and Relevance"}),": Through human feedback, GPT models learn to avoid generating harmful or irrelevant content, thereby increasing their reliability and applicability."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost-Effective Training"}),": RLHF allows for more efficient and cost-effective training compared to extensively curating the training dataset to ensure that only desired outputs are generated."]}),"\n"]}),"\n",(0,t.jsx)("figure",{className:"center-div",style:{width:"90%",textAlign:"center"},children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.img,{alt:"A primer on RLHF for sophisticated LLM training",src:i(52922).A+"",width:"1617",height:"2096"}),"\n",(0,t.jsx)("figcaption",{children:"Simplified overview of RLHF"})]})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);