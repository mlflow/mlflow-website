"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["3576"],{49758(e,n,t){t.r(n),t.d(n,{metadata:()=>l,default:()=>v,frontMatter:()=>h,contentTitle:()=>m,toc:()=>f,assets:()=>d});var l=JSON.parse('{"id":"eval-monitor/quickstart","title":"GenAI Evaluation Quickstart","description":"Need help setting up evaluation? Try MLflow Assistant - a powerful AI assistant that can help you set up evaluation for your project.","source":"@site/docs/genai/eval-monitor/quickstart.mdx","sourceDirName":"eval-monitor","slug":"/eval-monitor/quickstart","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/quickstart","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Evaluating LLMs/Agents with MLflow","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/"},"next":{"title":"Examples","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/running-evaluation/eval-examples"}}'),r=t(74848),a=t(28453),o=t(54725),i=t(46077),s=t(74990),c=t(78010),p=t(57250),u=t(95986);let h={},m="GenAI Evaluation Quickstart",d={},f=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Set up your environment",id:"step-1-set-up-your-environment",level:2},...s.RM,{value:"Step 2: Create an evaluation script",id:"step-2-create-an-evaluation-script",level:2},{value:"Step 3: Define your mock agent&#39;s prediction function",id:"step-3-define-your-mock-agents-prediction-function",level:2},{value:"Step 4: Prepare an evaluation dataset",id:"step-4-prepare-an-evaluation-dataset",level:2},{value:"Step 5: Define evaluation criteria using Scorers",id:"step-5-define-evaluation-criteria-using-scorers",level:2},{value:"Step 6: Run the evaluation",id:"step-6-run-the-evaluation",level:2},{value:"Complete Script",id:"complete-script",level:2},{value:"Summary",id:"summary",level:2}];function w(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"genai-evaluation-quickstart",children:"GenAI Evaluation Quickstart"})}),"\n",(0,r.jsx)(n.admonition,{title:"MLflow Assistant",type:"tip",children:(0,r.jsxs)(n.p,{children:["Need help setting up evaluation? Try ",(0,r.jsx)("ins",{children:(0,r.jsx)(n.a,{href:"/genai/getting-started/try-assistant",children:"MLflow Assistant"})})," - a powerful AI assistant that can help you set up evaluation for your project."]})}),"\n",(0,r.jsx)(n.p,{children:"This quickstart guide will walk you through evaluating your GenAI applications with MLflow's comprehensive evaluation framework.\nIn less than 5 minutes, you'll learn how to evaluate LLM outputs, use built-in and custom evaluation criteria, and analyze results in the MLflow UI."}),"\n",(0,r.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/quickstart-eval-hero.png",alt:"Simple Evaluation Results"}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Depending on your Python environment, you may want to install the required packages by running the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"The code examples in this guide use the OpenAI SDK; however, MLflow's evaluation framework works with any LLM provider, including Anthropic, Google, Bedrock, and more."})}),"\n",(0,r.jsx)(n.h2,{id:"step-1-set-up-your-environment",children:"Step 1: Set up your environment"}),"\n",(0,r.jsxs)(n.p,{children:["MLflow stores evaluation results in a ",(0,r.jsx)(n.a,{href:"/self-hosting/architecture/tracking-server/",children:"MLflow Tracking Server"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Start a local MLflow Tracking Server by executing one of the following methods."}),"\n",(0,r.jsx)(s.Ay,{}),"\n",(0,r.jsx)(n.h2,{id:"step-2-create-an-evaluation-script",children:"Step 2: Create an evaluation script"}),"\n",(0,r.jsxs)(n.p,{children:["Create a file named ",(0,r.jsx)(n.code,{children:"quickstart_eval.py"}),". This script will contain your mock agent, evaluation dataset, scorers, and the evaluation execution. Alternatively, you may run this in a ",(0,r.jsx)(n.a,{href:"/genai/eval-monitor/notebooks/quickstart-eval",children:"notebook"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Start with the environment setup:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# quickstart_eval.py\nimport os\nimport mlflow\n\n# Configure environment\nos.environ["OPENAI_API_KEY"] = "your-api-key-here"  # Replace with your API key\nmlflow.set_experiment("GenAI Evaluation Quickstart")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-define-your-mock-agents-prediction-function",children:"Step 3: Define your mock agent's prediction function"}),"\n",(0,r.jsx)(n.p,{children:"First, we need to create a prediction function that takes a question and returns an answer. Here we use OpenAI's gpt-4o-mini model to generate the answer, but you can use any other LLM provider if you prefer."}),"\n",(0,r.jsxs)(n.p,{children:["Add your mock agent implementation to ",(0,r.jsx)(n.code,{children:"quickstart_eval.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\n\ndef my_agent(question: str) -> str:\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {\n                "role": "system",\n                "content": "You are a helpful assistant. Answer questions concisely.",\n            },\n            {"role": "user", "content": question},\n        ],\n    )\n    return response.choices[0].message.content\n\n\ndef qa_predict_fn(question: str) -> str:\n    """Wrapper function for evaluation using ``my_agent``."""\n    return my_agent(question)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"step-4-prepare-an-evaluation-dataset",children:"Step 4: Prepare an evaluation dataset"}),"\n",(0,r.jsxs)(n.p,{children:["The evaluation dataset is a list of samples, each with an ",(0,r.jsx)(n.code,{children:"inputs"})," and ",(0,r.jsx)(n.code,{children:"expectations"})," field."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"inputs"}),": The input to the ",(0,r.jsx)(n.code,{children:"predict_fn"})," function above. ",(0,r.jsxs)(n.strong,{children:["The key(s) must match the parameter name of the ",(0,r.jsx)(n.code,{children:"predict_fn"})," function"]}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"expectations"}),": The expected output from the ",(0,r.jsx)(n.code,{children:"predict_fn"})," function, namely, ground truth for the answer."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The dataset can be a list of dictionaries, a pandas DataFrame, a spark DataFrame. Here we use a list of dictionaries for simplicity."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Define a simple Q&A dataset with questions and expected answers\neval_dataset = [\n    {\n        "inputs": {"question": "What is the capital of France?"},\n        "expectations": {"expected_response": "Paris"},\n    },\n    {\n        "inputs": {"question": "Who was the first person to build an airplane?"},\n        "expectations": {"expected_response": "Wright Brothers"},\n    },\n    {\n        "inputs": {"question": "Who wrote Romeo and Juliet?"},\n        "expectations": {"expected_response": "William Shakespeare"},\n    },\n]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"step-5-define-evaluation-criteria-using-scorers",children:"Step 5: Define evaluation criteria using Scorers"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scorer"})," is a function that computes a score for a given input-output pair against various evaluation criteria.\nYou can use built-in scorers provided by MLflow for common evaluation criteria, as well as create your own custom scorers."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from mlflow.genai import scorer\nfrom mlflow.genai.scorers import Correctness, Guidelines\n\n\n@scorer\ndef is_concise(outputs: str) -> bool:\n    """Evaluate if the answer is concise (less than 5 words)"""\n    return len(outputs.split()) <= 5\n\n\nscorers = [\n    Correctness(),\n    Guidelines(name="is_english", guidelines="The answer must be in English"),\n    is_concise,\n]\n'})}),"\n",(0,r.jsx)(n.p,{children:"Here we use three scorers:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(o.B,{fn:"mlflow.genai.scorers.Correctness",children:"Correctness"}),': Evaluates if the answer is factually correct, using the "expected_response" field in the dataset.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(o.B,{fn:"mlflow.genai.scorers.Guidelines",children:"Guidelines"}),": Evaluates if the answer meets the given guidelines."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"is_concise"}),": A custom scorer defined using the ",(0,r.jsx)(o.B,{fn:"mlflow.genai.scorers.scorer",children:"scorer"})," decorator to judge if the answer is concise (less than 5 words)."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The first two scorers use LLMs to evaluate the response, so-called ",(0,r.jsx)(n.strong,{children:"LLM-as-a-Judge"}),". This is a powerful technique to assess the quality of the response, because it provides a human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation."]}),"\n",(0,r.jsx)(n.p,{children:"The Scorer interface allows you to define various types of quality metrics for your application in a simple way. From a simple natural language guideline to a code function with the full control of the evaluation logic."}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsxs)(n.p,{children:["The default model used for LLM-as-a-Judge scorers such as Correctness and Guidelines is OpenAI ",(0,r.jsx)(n.code,{children:"gpt-4o-mini"}),". MLflow supports all major LLM providers, such as Anthropic, Bedrock, Google, xAI, and more, through the built-in adopters and LiteLLM."]}),(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example of using different model providers for the judge model"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Anthropic\nCorrectness(model="anthropic:/claude-sonnet-4-20250514")\n\n# Bedrock\nCorrectness(model="bedrock:/anthropic.claude-sonnet-4-20250514")\n\n# Google\n# Run `pip install litellm` to use Google as the judge model\nCorrectness(model="gemini/gemini-2.5-flash")\n\n# xAI\n# Run `pip install litellm` to use xAI as the judge model\nCorrectness(model="xai/grok-2-latest")\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"step-6-run-the-evaluation",children:"Step 6: Run the evaluation"}),"\n",(0,r.jsx)(n.p,{children:"Now we have all three components of the evaluation: dataset, prediction function, and scorers. Let's run the evaluation!"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Run evaluation\nif __name__ == "__main__":\n    results = mlflow.genai.evaluate(\n        data=eval_dataset,\n        predict_fn=qa_predict_fn,\n        scorers=scorers,\n    )\n'})}),"\n",(0,r.jsx)(n.p,{children:"Now run your evaluation script:"}),"\n",(0,r.jsx)(u.A,{children:(0,r.jsxs)(c.A,{children:[(0,r.jsx)(p.A,{value:"uv",label:"uv",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"uv run --with openai,mlflow quickstart_eval.py\n"})})}),(0,r.jsx)(p.A,{value:"python",label:"Python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"python quickstart_eval.py\n"})})})]})}),"\n",(0,r.jsx)(n.h2,{id:"complete-script",children:"Complete Script"}),"\n",(0,r.jsxs)(n.p,{children:["Here's the complete ",(0,r.jsx)(n.code,{children:"quickstart_eval.py"})," for reference:"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"View complete script"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# quickstart_eval.py\nimport os\nimport mlflow\nfrom openai import OpenAI\nfrom mlflow.genai import scorer\nfrom mlflow.genai.scorers import Correctness, Guidelines\n\n# Use different env variable when using a different LLM provider\nos.environ["OPENAI_API_KEY"] = "your-api-key-here"\nmlflow.set_experiment("GenAI Evaluation Quickstart")\n\n# Your agent implementation\nclient = OpenAI()\n\n\ndef my_agent(question: str) -> str:\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {\n                "role": "system",\n                "content": "You are a helpful assistant. Answer questions concisely.",\n            },\n            {"role": "user", "content": question},\n        ],\n    )\n    return response.choices[0].message.content\n\n\n# Wrapper function for evaluation\ndef qa_predict_fn(question: str) -> str:\n    return my_agent(question)\n\n\n# Evaluation dataset\neval_dataset = [\n    {\n        "inputs": {"question": "What is the capital of France?"},\n        "expectations": {"expected_response": "Paris"},\n    },\n    {\n        "inputs": {"question": "Who was the first person to build an airplane?"},\n        "expectations": {"expected_response": "Wright Brothers"},\n    },\n    {\n        "inputs": {"question": "Who wrote Romeo and Juliet?"},\n        "expectations": {"expected_response": "William Shakespeare"},\n    },\n]\n\n\n# Scorers\n@scorer\ndef is_concise(outputs: str) -> bool:\n    return len(outputs.split()) <= 5\n\n\nscorers = [\n    Correctness(),\n    Guidelines(name="is_english", guidelines="The answer must be in English"),\n    is_concise,\n]\n\n# Run evaluation\nif __name__ == "__main__":\n    results = mlflow.genai.evaluate(\n        data=eval_dataset,\n        predict_fn=qa_predict_fn,\n        scorers=scorers,\n    )\n'})})]}),"\n",(0,r.jsx)(n.p,{children:'After running the code above, go to the MLflow UI and navigate to the "GenAI Evaluation Quickstart" experiment. You\'ll see the evaluation results with detailed metrics for each scorer.'}),"\n",(0,r.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/quickstart-eval-result.png",alt:"Detailed Evaluation Results",width:"90%"}),"\n",(0,r.jsx)(n.p,{children:"By clicking on the each row in the table, you can see the detailed rationale behind the score and the trace of the prediction."}),"\n",(0,r.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/quickstart-eval-trace.png",alt:"Detailed Score Rationale",width:"90%"}),"\n",(0,r.jsx)(n.p,{children:'You can compare evaluation runs, too. Click on "Evaluation runs" menu (on the left) and select a run that you want to compare to a baseline run.'}),"\n",(0,r.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/quickstart-eval-runs-compare.png",alt:"Compare Evaluation Runs",width:"90%"}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Congratulations! You've successfully:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Set up MLflow GenAI Evaluation for your applications"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Evaluated a Q&A application with built-in scorers"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Created custom evaluation guidelines"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Learned to analyze results in the MLflow UI"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"MLflow's evaluation framework provides comprehensive tools for assessing GenAI application quality, helping you build more reliable and effective AI systems."})]})}function v(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(w,{...e})}):w(e)}},74990(e,n,t){t.d(n,{Ay:()=>p,RM:()=>s});var l=t(74848),r=t(28453),a=t(78010),o=t(57250),i=t(95986);let s=[];function c(e){let n={a:"a",code:"code",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,l.jsx)(i.A,{children:(0,l.jsxs)(a.A,{children:[(0,l.jsxs)(o.A,{value:"uv",label:"Local (uv)",default:!0,children:[(0,l.jsxs)(n.p,{children:["Install the Python package manager ",(0,l.jsx)(n.a,{href:"https://docs.astral.sh/uv/getting-started/installation/",children:"uv"}),"\n(that will also install ",(0,l.jsxs)(n.a,{href:"https://docs.astral.sh/uv/guides/tools/",children:[(0,l.jsx)(n.code,{children:"uvx"})," command"]})," to invoke Python tools without installing them)."]}),(0,l.jsx)(n.p,{children:"Start a MLflow server locally."}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:"uvx mlflow server\n"})})]}),(0,l.jsxs)(o.A,{value:"local",label:"Local (pip)",children:[(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Python Environment"}),": Python 3.10+"]}),(0,l.jsxs)(n.p,{children:["Install the ",(0,l.jsx)(n.code,{children:"mlflow"})," Python package via ",(0,l.jsx)(n.code,{children:"pip"})," and start a MLflow server locally."]}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:"pip install --upgrade 'mlflow[genai]'\nmlflow server\n"})})]}),(0,l.jsxs)(o.A,{value:"docker",label:"Local (docker)",children:[(0,l.jsx)(n.p,{children:"MLflow provides a Docker Compose file to start a local MLflow server with a PostgreSQL database and a MinIO server."}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:"git clone --depth 1 --filter=blob:none --sparse https://github.com/mlflow/mlflow.git\ncd mlflow\ngit sparse-checkout set docker-compose\ncd docker-compose\ncp .env.dev.example .env\ndocker compose up -d\n"})}),(0,l.jsxs)(n.p,{children:["Refer to the ",(0,l.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/tree/master/docker-compose/README.md",children:"instruction"})," for more details (e.g., overriding the default environment variables)."]})]})]})})}function p(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},57250(e,n,t){t.d(n,{A:()=>a});var l=t(74848);t(96540);var r=t(34164);function a({children:e,hidden:n,className:t}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,r.A)("tabItem_Ymn6",t),hidden:n,children:e})}},78010(e,n,t){t.d(n,{A:()=>_});var l=t(74848),r=t(96540),a=t(34164),o=t(88287),i=t(28584),s=t(56347),c=t(99989),p=t(96629),u=t(80618),h=t(41367);function m(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function d({value:e,tabValues:n}){return n.some(n=>n.value===e)}var f=t(19863);function w({className:e,block:n,selectedValue:t,selectValue:r,tabValues:o}){let s=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),p=e=>{let n=e.currentTarget,l=o[s.indexOf(n)].value;l!==t&&(c(n),r(l))},u=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{let t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{let t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1]}}n?.focus()};return(0,l.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:r})=>(0,l.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:u,onClick:p,...r,className:(0,a.A)("tabs__item","tabItem_LNqP",r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:t}){let o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=o.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,l.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function g(e){let n=function(e){let n,{defaultValue:t,queryString:l=!1,groupId:a}=e,o=function(e){let{values:n,children:t}=e;return(0,r.useMemo)(()=>{let e=n??m(t).map(({props:{value:e,label:n,attributes:t,default:l}})=>({value:e,label:n,attributes:t,default:l})),l=(0,u.XI)(e,(e,n)=>e.value===n.value);if(l.length>0)throw Error(`Docusaurus error: Duplicate values "${l.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,t])}(e),[i,f]=(0,r.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!d({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let t=n.find(e=>e.default)??n[0];if(!t)throw Error("Unexpected error: 0 tabValues");return t.value})({defaultValue:t,tabValues:o})),[w,v]=function({queryString:e=!1,groupId:n}){let t=(0,s.W6)(),l=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,p.aZ)(l),(0,r.useCallback)(e=>{if(!l)return;let n=new URLSearchParams(t.location.search);n.set(l,e),t.replace({...t.location,search:n.toString()})},[l,t])]}({queryString:l,groupId:a}),[g,_]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[t,l]=(0,h.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&l.set(e)},[n,l])]}({groupId:a}),y=d({value:n=w??g,tabValues:o})?n:null;return(0,c.A)(()=>{y&&f(y)},[y]),{selectedValue:i,selectValue:(0,r.useCallback)(e=>{if(!d({value:e,tabValues:o}))throw Error(`Can't select invalid tab value=${e}`);f(e),v(e),_(e)},[v,_,o]),tabValues:o}}(e);return(0,l.jsxs)("div",{className:(0,a.A)(o.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,l.jsx)(w,{...n,...e}),(0,l.jsx)(v,{...n,...e})]})}function _(e){let n=(0,f.A)();return(0,l.jsx)(g,{...e,children:m(e.children)},String(n))}},54725(e,n,t){t.d(n,{B:()=>o});var l=t(74848);t(96540);var r=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),a=t(66497);function o({fn:e,children:n,hash:t}){let o=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(r[t])return t}return null})(e);if(!o)return(0,l.jsx)(l.Fragment,{children:n});let i=(0,a.default)(`/${r[o]}#${t??e}`);return(0,l.jsx)("a",{href:i,target:"_blank",children:n??(0,l.jsxs)("code",{children:[e,"()"]})})}},46077(e,n,t){t.d(n,{A:()=>a});var l=t(74848);t(96540);var r=t(66497);function a({src:e,alt:n,width:t,caption:a,className:o}){return(0,l.jsxs)("div",{className:`container_JwLF ${o||""}`,children:[(0,l.jsx)("div",{className:"imageWrapper_RfGN",style:t?{width:t}:{},children:(0,l.jsx)("img",{src:(0,r.default)(e),alt:n,className:"image_bwOA"})}),a&&(0,l.jsx)("p",{className:"caption_jo2G",children:a})]})}},95986(e,n,t){t.d(n,{A:()=>r});var l=t(74848);t(96540);function r({children:e}){return(0,l.jsx)("div",{className:"wrapper_sf5q",children:e})}},28453(e,n,t){t.d(n,{R:()=>o,x:()=>i});var l=t(96540);let r={},a=l.createContext(r);function o(e){let n=l.useContext(a);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),l.createElement(a.Provider,{value:n},e.children)}}}]);