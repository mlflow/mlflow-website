"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[676],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>i});var t=s(96540);const o={},l=t.createContext(o);function r(e){const n=t.useContext(l);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(l.Provider,{value:n},e.children)}},93759:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"data-model/runs","title":"MLflow Runs Data Model for GenAI","description":"In MLflow 3, Runs represent evaluation iterations of your GenAI application and are attached directly to Models as part of the model\'s evolution. Unlike earlier MLflow versions where runs lived under experiments, runs now capture specific evaluation sessions that test and validate model performance.","source":"@site/docs/genai/data-model/runs.mdx","sourceDirName":"data-model","slug":"/data-model/runs","permalink":"/docs/latest/genai/data-model/runs","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Logged Model","permalink":"/docs/latest/genai/data-model/logged-model"},"next":{"title":"Traces","permalink":"/docs/latest/genai/data-model/traces"}}');var o=s(74848),l=s(28453);const r={},i="MLflow Runs Data Model for GenAI",a={},d=[{value:"Overview",id:"overview",level:2},{value:"Runs as Model Evolution Snapshots",id:"runs-as-model-evolution-snapshots",level:2},{value:"Relationship to Other Entities",id:"relationship-to-other-entities",level:2},{value:"Benefits of Model-Attached Runs",id:"benefits-of-model-attached-runs",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"mlflow-runs-data-model-for-genai",children:"MLflow Runs Data Model for GenAI"})}),"\n",(0,o.jsxs)(n.p,{children:["In MLflow 3, ",(0,o.jsx)(n.strong,{children:"Runs"})," represent evaluation iterations of your GenAI application and are attached directly to Models as part of the model's evolution. Unlike earlier MLflow versions where runs lived under experiments, runs now capture specific evaluation sessions that test and validate model performance."]}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"A Run represents a single evaluation iteration of your GenAI model - think of it as a snapshot of how your model or application performed during a specific testing session."}),"\n",(0,o.jsx)(n.mermaid,{value:'graph TB\n    subgraph MODEL[" "]\n        direction TB\n        TITLE[\ud83e\udd16 GenAI Model: Customer Support Bot]\n\n        subgraph RUNS[\ud83d\ude80 Model Runs - Evaluation Iterations]\n            direction TB\n            R1[\ud83d\udcca Run 1: Initial Evaluation]\n            R2[\ud83d\udcca Run 2: Post-Training Eval]\n            R3[\ud83d\udcca Run 3: Production Validation]\n            R4[\ud83d\udcca Run 4: A/B Test Results]\n        end\n\n        R1 -.-> R2\n        R2 -.-> R3\n        R3 -.-> R4\n    end\n\n    classDef modelStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    classDef runStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef titleStyle fill:#f5f5f5,stroke:#424242,stroke-width:2px,color:#000\n\n    class MODEL modelStyle\n    class RUNS,R1,R2,R3,R4 runStyle\n    class TITLE titleStyle'}),"\n",(0,o.jsx)(n.h2,{id:"runs-as-model-evolution-snapshots",children:"Runs as Model Evolution Snapshots"}),"\n",(0,o.jsx)(n.p,{children:"Each Run captures a specific moment in your model's development lifecycle:"}),"\n",(0,o.jsx)(n.mermaid,{value:"graph TB\n    subgraph RUN[\ud83d\udcca Individual Run Contains]\n        direction TB\n        RC1[\ud83d\udcc8 Evaluation Metrics]\n        RC2[\ud83d\udccb Test Dataset Used]\n        RC3[\u2699\ufe0f Model Configuration]\n        RC4[\ud83d\udcdd Generated Traces]\n        RC5[\ud83c\udfaf Quality Scores]\n    end\n\n    subgraph PURPOSE[\ud83c\udfaf Run Purpose]\n        direction TB\n        P1[\ud83d\udd0d Performance Validation]\n        P2[\ud83d\udcca Quality Assessment]\n        P3[\ud83c\udd9a Comparative Analysis]\n        P4[\u2705 Release Readiness]\n    end\n\n    RUN --\x3e PURPOSE\n\n    classDef runStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef purposeStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000\n\n    class RUN,RC1,RC2,RC3,RC4,RC5 runStyle\n    class PURPOSE,P1,P2,P3,P4 purposeStyle"}),"\n",(0,o.jsx)(n.h2,{id:"relationship-to-other-entities",children:"Relationship to Other Entities"}),"\n",(0,o.jsx)(n.p,{children:"Runs connect your model development to systematic evaluation:"}),"\n",(0,o.jsx)(n.mermaid,{value:"graph TD\n    subgraph ECOSYSTEM[\ud83c\udf1f Run Ecosystem]\n        direction TB\n\n        M[\ud83e\udd16 Model] --\x3e R[\ud83d\udcca Run]\n        R --\x3e T[\ud83d\udcdd Traces]\n        R --\x3e A[\ud83d\udcca Assessments]\n\n        ED[\ud83d\udccb Evaluation Dataset] --\x3e R\n        S[\ud83c\udfaf Scorers] --\x3e R\n\n        R --\x3e RM[\ud83d\udcc8 Run Metrics]\n        R --\x3e RR[\ud83d\udccb Run Results]\n    end\n\n    classDef modelStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef runStyle fill:#e3f2fd,stroke:#0277bd,stroke-width:3px,color:#000\n    classDef entityStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000\n    classDef resultStyle fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000\n\n    class M modelStyle\n    class R runStyle\n    class T,A,ED,S entityStyle\n    class RM,RR resultStyle"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Key relationships:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Runs can link to Models"}),": Iterative model or app development is tracked through runs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Runs generate Traces"}),": Evaluation execution creates trace records"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Runs produce Assessments"}),": Quality judgments on model performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Runs use Datasets"}),": Systematic testing against curated examples"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Runs apply Scorers"}),": Automated evaluation functions"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"benefits-of-model-attached-runs",children:"Benefits of Model-Attached Runs"}),"\n",(0,o.jsx)(n.p,{children:"The MLflow 3 approach of attaching runs to models provides:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83c\udfaf Model-Centric Organization"}),": Evaluation history travels with the model"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83d\udcc8 Evolution Tracking"}),": Clear progression of model performance over time"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83d\udd0d Focused Analysis"}),": Evaluation results directly tied to specific logged model"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"\ud83d\ude80 Simplified Workflows"}),": Natural connection between model development and testing"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This model-centric approach makes it easier to understand how your GenAI application has evolved and which evaluation iterations led to improvements or regressions."})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);