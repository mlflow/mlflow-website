/*! For license information please see 9bb70d52.04443402.js.LICENSE.txt */
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[3678],{6789:(e,r,o)=>{o.d(r,{A:()=>p});o(96540);var t=o(28774),n=o(34164);const l={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var i=o(86025),a=o(21122),s=o(74848);function p({icon:e,image:r,imageDark:o,imageWidth:p,imageHeight:m,iconSize:c=32,containerHeight:h,title:d,description:f,href:u,linkText:g="Learn more \u2192",className:w}){if(!e&&!r)throw new Error("TileCard requires either an icon or image prop");const _=h?{height:`${h}px`}:{},y={};return p&&(y.width=`${p}px`),m&&(y.height=`${m}px`),(0,s.jsxs)(t.A,{href:u,className:(0,n.A)(l.tileCard,w),children:[(0,s.jsx)("div",{className:l.tileIcon,style:_,children:e?(0,s.jsx)(e,{size:c}):o?(0,s.jsx)(a.A,{sources:{light:(0,i.Ay)(r),dark:(0,i.Ay)(o)},alt:d,className:l.tileImage,style:y}):(0,s.jsx)("img",{src:(0,i.Ay)(r),alt:d,className:l.tileImage,style:y})}),(0,s.jsx)("h3",{children:d}),(0,s.jsx)("p",{children:f}),(0,s.jsx)("div",{className:l.tileLink,children:g})]})}},28453:(e,r,o)=>{o.d(r,{R:()=>i,x:()=>a});var t=o(96540);const n={},l=t.createContext(n);function i(e){const r=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),t.createElement(l.Provider,{value:r},e.children)}},42640:(e,r,o)=>{o.d(r,{A:()=>t});const t=(0,o(84722).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},43586:(e,r,o)=>{o.r(r),o.d(r,{assets:()=>u,contentTitle:()=>f,default:()=>_,frontMatter:()=>d,metadata:()=>t,toc:()=>g});const t=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/prompt","title":"Bring Your Own Prompts","description":"The custompromptjudge API is being phased out. We strongly recommend using the make_judge API instead, which provides:","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/prompt.mdx","sourceDirName":"eval-monitor/scorers/llm-judge","slug":"/eval-monitor/scorers/llm-judge/prompt","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/prompt","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var n=o(74848),l=o(28453),i=o(49374),a=o(66927),s=o(6789),p=o(65592),m=o(42640),c=o(60665),h=o(47504);const d={},f="Bring Your Own Prompts",u={},g=[{value:"Example Usage",id:"example-usage",level:2},{value:"Prompt requirements",id:"prompt-requirements",level:2},{value:"Maintaining Your Prompt",id:"maintaining-your-prompt",level:2},{value:"Selecting Judge Models",id:"selecting-judge-models",level:2},{value:"Next Steps",id:"next-steps",level:2}];function w(e){const r={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.header,{children:(0,n.jsx)(r.h1,{id:"bring-your-own-prompts",children:"Bring Your Own Prompts"})}),"\n",(0,n.jsxs)(r.admonition,{title:"Recommendation: Use make_judge Instead",type:"note",children:[(0,n.jsxs)(r.p,{children:["The ",(0,n.jsx)(r.code,{children:"custom_prompt_judge"})," API is being phased out. We strongly recommend using the ",(0,n.jsx)(i.B,{fn:"mlflow.genai.judges.make_judge",children:"make_judge"})," API instead, which provides:"]}),(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"More flexible template-based instructions"}),"\n",(0,n.jsx)(r.li,{children:"Better version control and collaboration features"}),"\n",(0,n.jsx)(r.li,{children:"Support for both field-based and Agent-as-a-Judge evaluation"}),"\n",(0,n.jsx)(r.li,{children:"Alignment capabilities with human feedback"}),"\n"]}),(0,n.jsxs)(r.p,{children:["See the ",(0,n.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge/make-judge",children:"make_judge documentation"})," for migration guidance."]})]}),"\n",(0,n.jsxs)(r.p,{children:["The ",(0,n.jsx)(i.B,{fn:"mlflow.genai.judges.custom_prompt_judge",children:"custom_prompt_judge"}),' API is designed to help you quickly and easily create LLM scorers when you need full control over the judge\'s prompt or need to return multiple output values beyond "pass" / "fail", for example, "great", "ok", "bad".']}),"\n",(0,n.jsx)(r.p,{children:"You provide a prompt template that has placeholders for specific fields in your app's trace and define the output choices the judge can select. The LLM judge model uses these inputs to select the best output choice and provides a rationale for its selection."}),"\n",(0,n.jsx)(r.admonition,{type:"tip",children:(0,n.jsxs)(r.p,{children:["We recommend starting with ",(0,n.jsx)("ins",{children:(0,n.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge/guidelines",children:"guidelines-based judges"})})," and only using prompt-based judges if you need more control or can't write your evaluation criteria as pass/fail guidelines. Guidelines-based judges have the distinct advantage of being easy to explain to business stakeholders and can often be directly written by domain experts."]})}),"\n",(0,n.jsx)(r.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'from mlflow.genai.judges import custom_prompt_judge\nfrom mlflow.genai.scorers import scorer\n\n\nissue_resolution_prompt = """\nEvaluate the entire conversation between a customer and an LLM-based agent. Determine if the issue was resolved in the conversation.\n\nYou must choose one of the following categories.\n\n[[fully_resolved]]: The response directly and comprehensively addresses the user\'s question or problem, providing a clear solution or answer. No further immediate action seems required from the user on the same core issue.\n[[partially_resolved]]: The response offers some help or relevant information but doesn\'t completely solve the problem or answer the question. It might provide initial steps, require more information from the user, or address only a part of a multi-faceted query.\n[[needs_follow_up]]: The response does not adequately address the user\'s query, misunderstands the core issue, provides unhelpful or incorrect information, or inappropriately deflects the question. The user will likely need to re-engage or seek further assistance.\n\nConversation to evaluate: {{conversation}}\n"""\n\n\n# Define a custom scorer that wraps the custom prompt judge to check if the issue was resolved\n@scorer\ndef is_issue_resolved(inputs, outputs):\n    issue_judge = custom_prompt_judge(\n        name="issue_resolution",\n        prompt_template=issue_resolution_prompt,\n        # Optionally map the categories to numeric values for ease\n        # of aggregation and comparison. When not provided, the judge\n        # directly returns the choice value as a string.\n        numeric_values={\n            "fully_resolved": 1,\n            "partially_resolved": 0.5,\n            "needs_follow_up": 0,\n        },\n    )\n\n    # Pass values for the placeholders ({{conversation}}) as kwargs\n    conversation = inputs["messages"] + outputs["messages"]\n    return issue_judge(conversation=conversation)\n'})}),"\n",(0,n.jsx)(r.h2,{id:"prompt-requirements",children:"Prompt requirements"}),"\n",(0,n.jsx)(r.p,{children:"The prompt template for the judge must have:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["Placeholders for input values with ",(0,n.jsx)(r.strong,{children:"double curly braces"}),", e.g., ",(0,n.jsx)(r.code,{children:"{{conversation}}"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["Choices for the judge to select from as output, enclosed in ",(0,n.jsx)(r.strong,{children:"square brackets"}),", e.g., ",(0,n.jsx)(r.code,{children:"[[fully_resolved]]"}),". The choice name can contain alphanumeric characters and underscores."]}),"\n"]}),"\n",(0,n.jsx)(r.admonition,{title:"Handling Parsing Errors",type:"tip",children:(0,n.jsxs)(r.p,{children:["MLflow uses raw prompt-based instructions for handling structured outputs to make the API generic to all LLM providers. This may not be strict enough to enforce structured outputs in all cases. If you see output parsing errors frequently, consider using ",(0,n.jsx)("ins",{children:(0,n.jsx)(r.a,{href:"/genai/eval-monitor/scorers/custom",children:"code-based custom scorers"})})," and invoke the specific structured output API for the LLM provider you are using to get more reliable results."]})}),"\n",(0,n.jsx)(r.h2,{id:"maintaining-your-prompt",children:"Maintaining Your Prompt"}),"\n",(0,n.jsxs)(r.p,{children:["Writing good prompts for LLM judges requires iterative testing and refinement. ",(0,n.jsx)(r.a,{href:"/genai/prompt-registry",children:"MLflow Prompt Registry"})," is a great tool to help you manage and version control your prompts and share them with your team."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'from mlflow.genai import register_prompt\n\nregister_prompt(\n    name="issue_resolution",\n    template=issue_resolution_prompt,\n)\n'})}),"\n",(0,n.jsx)(a.A,{src:"/images/mlflow-3/eval-monitor/scorers/prompt-registry.png",alt:"Prompt Registry"}),"\n",(0,n.jsx)(r.h2,{id:"selecting-judge-models",children:"Selecting Judge Models"}),"\n",(0,n.jsxs)(r.p,{children:["MLflow supports all major LLM providers, such as OpenAI, Anthropic, Google, xAI, and more. See ",(0,n.jsx)(r.a,{href:"/genai/eval-monitor/scorers/llm-judge#supported-models",children:"Supported Models"})," for more details."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'from mlflow.genai.judges import custom_prompt_judge\n\ncustom_prompt_judge(\n    name="is_issue_resolved",\n    prompt_template=issue_resolution_prompt,\n    model="anthropic:/claude-3-opus",\n)\n'})}),"\n",(0,n.jsx)(r.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,n.jsxs)(p.A,{children:[(0,n.jsx)(s.A,{icon:m.A,title:"Evaluate Agents",description:"Learn how to evaluate AI agents with specialized techniques and scorers",href:"/genai/eval-monitor/running-evaluation/agents"}),(0,n.jsx)(s.A,{icon:c.A,title:"Prompt Registry",description:"Version control and manage your judge prompts with MLflow Prompt Registry",href:"/genai/prompt-registry"}),(0,n.jsx)(s.A,{icon:h.A,title:"Collect User Feedback",description:"Integrate user feedback to continuously improve your evaluation criteria and model performance",href:"/genai/assessments/feedback/"})]})]})}function _(e={}){const{wrapper:r}={...(0,l.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(w,{...e})}):w(e)}},47504:(e,r,o)=>{o.d(r,{A:()=>t});const t=(0,o(84722).A)("message-square",[["path",{d:"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z",key:"1lielz"}]])},49374:(e,r,o)=>{o.d(r,{B:()=>a});o(96540);const t=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var n=o(86025),l=o(74848);const i=e=>{const r=e.split(".");for(let o=r.length;o>0;o--){const e=r.slice(0,o).join(".");if(t[e])return e}return null};function a({fn:e,children:r,hash:o}){const a=i(e);if(!a)return(0,l.jsx)(l.Fragment,{children:r});const s=(0,n.Ay)(`/${t[a]}#${o??e}`);return(0,l.jsx)("a",{href:s,target:"_blank",children:r??(0,l.jsxs)("code",{children:[e,"()"]})})}},60665:(e,r,o)=>{o.d(r,{A:()=>t});const t=(0,o(84722).A)("book-open",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]])},65592:(e,r,o)=>{o.d(r,{A:()=>i});o(96540);var t=o(34164);const n={tilesGrid:"tilesGrid_hB9N"};var l=o(74848);function i({children:e,className:r}){return(0,l.jsx)("div",{className:(0,t.A)(n.tilesGrid,r),children:e})}},66927:(e,r,o)=>{o.d(r,{A:()=>i});o(96540);const t={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var n=o(86025),l=o(74848);function i({src:e,alt:r,width:o,caption:i,className:a}){return(0,l.jsxs)("div",{className:`${t.container} ${a||""}`,children:[(0,l.jsx)("div",{className:t.imageWrapper,style:o?{width:o}:{},children:(0,l.jsx)("img",{src:(0,n.Ay)(e),alt:r,className:t.image})}),i&&(0,l.jsx)("p",{className:t.caption,children:i})]})}},84722:(e,r,o)=>{o.d(r,{A:()=>p});var t=o(96540);const n=e=>{const r=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,r,o)=>o?o.toUpperCase():r.toLowerCase()))(e);return r.charAt(0).toUpperCase()+r.slice(1)},l=(...e)=>e.filter((e,r,o)=>Boolean(e)&&""!==e.trim()&&o.indexOf(e)===r).join(" ").trim(),i=e=>{for(const r in e)if(r.startsWith("aria-")||"role"===r||"title"===r)return!0};var a={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const s=(0,t.forwardRef)(({color:e="currentColor",size:r=24,strokeWidth:o=2,absoluteStrokeWidth:n,className:s="",children:p,iconNode:m,...c},h)=>(0,t.createElement)("svg",{ref:h,...a,width:r,height:r,stroke:e,strokeWidth:n?24*Number(o)/Number(r):o,className:l("lucide",s),...!p&&!i(c)&&{"aria-hidden":"true"},...c},[...m.map(([e,r])=>(0,t.createElement)(e,r)),...Array.isArray(p)?p:[p]])),p=(e,r)=>{const o=(0,t.forwardRef)(({className:o,...i},a)=>{return(0,t.createElement)(s,{ref:a,iconNode:r,className:l(`lucide-${p=n(e),p.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,o),...i});var p});return o.displayName=n(e),o}}}]);