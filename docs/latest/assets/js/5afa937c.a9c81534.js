"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["4529"],{408(e,n,t){t.r(n),t.d(n,{metadata:()=>o,default:()=>_,frontMatter:()=>f,contentTitle:()=>g,toc:()=>x,assets:()=>u});var o=JSON.parse('{"id":"prompt-registry/index","title":"Prompt Registry","description":"MLflow Prompt Registry","source":"@site/docs/genai/prompt-registry/index.mdx","sourceDirName":"prompt-registry","slug":"/prompt-registry/","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Prompt Registry","description":"MLflow Prompt Registry"},"sidebar":"genAISidebar","previous":{"title":"FAQ","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/faq"},"next":{"title":"Create and Edit Prompts","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/create-and-edit-prompts"}}'),i=t(74848),r=t(28453),a=t(54725),l=t(78010),s=t(57250),p=t(33508),m=t(61878),c=t(43197),d=t(44080),h=t(93893);let f={title:"Prompt Registry",description:"MLflow Prompt Registry"},g="Prompt Registry",u={},x=[{value:"Key Benefits",id:"key-benefits",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"1. Create a Prompt",id:"1-create-a-prompt",level:3},{value:"2. Update the Prompt with a New Version",id:"2-update-the-prompt-with-a-new-version",level:3},{value:"3. Compare the Prompt Versions",id:"3-compare-the-prompt-versions",level:3},{value:"4. Load and Use the Prompt",id:"4-load-and-use-the-prompt",level:3},{value:"5. Search Prompts",id:"5-search-prompts",level:3},{value:"Prompt Object",id:"prompt-object",level:2},{value:"Prompt Types",id:"prompt-types",level:3},{value:"Text Prompts",id:"text-prompts",level:4},{value:"Chat Prompts",id:"chat-prompts",level:4},{value:"Jinja2 Prompts",id:"jinja2-prompts",level:4},{value:"Response Format",id:"response-format",level:3},{value:"Manage Prompt and Version Tags",id:"manage-prompt-and-version-tags",level:3},{value:"Model Configuration",id:"model-configuration",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Using PromptModelConfig Class",id:"using-promptmodelconfig-class",level:3},{value:"Supported Configuration Parameters",id:"supported-configuration-parameters",level:3},{value:"Provider-Specific Parameters",id:"provider-specific-parameters",level:3},{value:"Managing Model Configuration",id:"managing-model-configuration",level:3},{value:"Setting or Updating Model Config",id:"setting-or-updating-model-config",level:4},{value:"Deleting Model Config",id:"deleting-model-config",level:4},{value:"Important Notes",id:"important-notes",level:4},{value:"Prompt Caching",id:"prompt-caching",level:2},{value:"Default Caching Behavior",id:"default-caching-behavior",level:3},{value:"Customizing Cache Behavior",id:"customizing-cache-behavior",level:3},{value:"Per-Request Cache Control",id:"per-request-cache-control",level:4},{value:"Global Cache Configuration",id:"global-cache-configuration",level:4},{value:"Cache Invalidation",id:"cache-invalidation",level:3},{value:"FAQ",id:"faq",level:2},{value:"Q: How do I delete a prompt version?",id:"q-how-do-i-delete-a-prompt-version",level:4},{value:"Q: Can I update the prompt template of an existing prompt version?",id:"q-can-i-update-the-prompt-template-of-an-existing-prompt-version",level:4},{value:"Q: How to dynamically load the latest version of a prompt?",id:"q-how-to-dynamically-load-the-latest-version-of-a-prompt",level:4},{value:"Q: Can I use prompt templates with frameworks like LangChain or LlamaIndex?",id:"q-can-i-use-prompt-templates-with-frameworks-like-langchain-or-llamaindex",level:4},{value:"Q: Is Prompt Registry integrated with the Prompt Engineering UI?",id:"q-is-prompt-registry-integrated-with-the-prompt-engineering-ui",level:4}];function w(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"prompt-registry",children:"Prompt Registry"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"MLflow Prompt Registry"})," is a powerful tool that streamlines prompt engineering and management in your Generative AI (GenAI) applications. It enables you to version, track, and reuse prompts across your organization, helping maintain consistency and improving collaboration in prompt development."]}),"\n",(0,i.jsx)(n.h2,{id:"key-benefits",children:"Key Benefits"}),"\n",(0,i.jsx)(p.A,{features:[{icon:m.A,title:"Version Control",description:"Track the evolution of your prompts with Git-inspired commit-based versioning and side-by-side comparison with diff highlighting. Prompt versions in MLflow are immutable, providing strong guarantees for reproducibility."},{icon:c.A,title:"Aliasing",description:"Build robust yet flexible deployment pipelines for prompts, allowing you to isolate prompt versions from main application code and perform tasks such as A/B testing and roll-backs with ease."},{icon:d.A,title:"Lineage",description:"Seamlessly integrate with MLflow's existing features such as model tracking and evaluation for end-to-end GenAI lifecycle management."},{icon:h.A,title:"Collaboration",description:"Share prompts across your organization with a centralized registry, enabling teams to build upon each other's work."}]}),"\n",(0,i.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,i.jsx)(n.h3,{id:"1-create-a-prompt",children:"1. Create a Prompt"}),"\n",(0,i.jsxs)(l.A,{children:[(0,i.jsx)(s.A,{value:"ui",label:"UI",default:!0,children:(0,i.jsxs)("div",{class:"flex-column",children:[(0,i.jsx)("div",{style:{width:"70%",margin:"20px"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Create Prompt UI",src:t(76599).A+"",width:"1239",height:"660"})})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Run ",(0,i.jsx)(n.code,{children:"mlflow server"})," in your terminal to start the MLflow UI."]}),"\n",(0,i.jsxs)(n.li,{children:["Navigate to the ",(0,i.jsx)(n.strong,{children:"Prompts"})," tab in the MLflow UI."]}),"\n",(0,i.jsxs)(n.li,{children:["Click on the ",(0,i.jsx)(n.strong,{children:"Create Prompt"})," button."]}),"\n",(0,i.jsx)(n.li,{children:"Fill in the prompt details such as name, prompt template text, and commit message (optional)."}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:"Create"})," to register the prompt."]}),"\n"]}),(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["Prompt template text can contain variables in ",(0,i.jsx)(n.code,{children:"{{variable}}"})," format. These variables can be filled with dynamic content when using the prompt in your GenAI application. MLflow also provides the ",(0,i.jsx)(n.code,{children:"to_single_brace_format()"})," API to convert templates into single brace format for frameworks like LangChain or LlamaIndex that require single brace interpolation."]})})]})}),(0,i.jsx)(s.A,{value:"python",label:"Python",default:!0,children:(0,i.jsxs)("div",{class:"flex-column",children:[(0,i.jsxs)(n.p,{children:["To create a new prompt using the Python API, use ",(0,i.jsx)(a.B,{fn:"mlflow.genai.register_prompt"})," API:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Use double curly braces for variables in the template\ninitial_template = """\\\nSummarize content you are provided with in {{ num_sentences }} sentences.\n\nSentences: {{ sentences }}\n"""\n\n# Register a new prompt\nprompt = mlflow.genai.register_prompt(\n    name="summarization-prompt",\n    template=initial_template,\n    # Optional: Provide a commit message to describe the changes\n    commit_message="Initial commit",\n    # Optional: Set tags applies to the prompt (across versions)\n    tags={\n        "author": "author@example.com",\n        "task": "summarization",\n        "language": "en",\n    },\n)\n\n# The prompt object contains information about the registered prompt\nprint(f"Created prompt \'{prompt.name}\' (version {prompt.version})")\n'})})]})})]}),"\n",(0,i.jsx)(n.p,{children:"This creates a new prompt with the specified template text and metadata. The prompt is now available in the MLflow UI for further management."}),"\n",(0,i.jsx)("div",{style:{width:"90%",margin:"10px"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Registered Prompt in UI",src:t(72406).A+"",width:"1287",height:"616"})})}),"\n",(0,i.jsx)(n.h3,{id:"2-update-the-prompt-with-a-new-version",children:"2. Update the Prompt with a New Version"}),"\n",(0,i.jsxs)(l.A,{children:[(0,i.jsx)(s.A,{value:"ui",label:"UI",default:!0,children:(0,i.jsxs)("div",{class:"flex-column",children:[(0,i.jsx)("div",{style:{width:"70%",margin:"20px"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Update Prompt UI",src:t(53296).A+"",width:"1150",height:"594"})})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["The previous step leads to the created prompt page. (If you closed the page, navigate to the ",(0,i.jsx)(n.strong,{children:"Prompts"})," tab in the MLflow UI and click on the prompt name.)"]}),"\n",(0,i.jsxs)(n.li,{children:["Click on the ",(0,i.jsx)(n.strong,{children:"Create prompt Version"})," button."]}),"\n",(0,i.jsx)(n.li,{children:"The popup dialog is pre-filled with the existing prompt text. Modify the prompt as you wish."}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:"Create"})," to register the new version."]}),"\n"]})]})}),(0,i.jsx)(s.A,{value:"python",label:"Python",default:!0,children:(0,i.jsxs)("div",{class:"flex-column",children:[(0,i.jsxs)(n.p,{children:["To update an existing prompt with a new version, use the ",(0,i.jsx)(a.B,{fn:"mlflow.genai.register_prompt"})," API with the existing prompt name:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nnew_template = """\\\nYou are an expert summarizer. Condense the following content into exactly {{ num_sentences }} clear and informative sentences that capture the key points.\n\nSentences: {{ sentences }}\n\nYour summary should:\n- Contain exactly {{ num_sentences }} sentences\n- Include only the most important information\n- Be written in a neutral, objective tone\n- Maintain the same level of formality as the original text\n"""\n\n# Register a new version of an existing prompt\nupdated_prompt = mlflow.genai.register_prompt(\n    name="summarization-prompt",  # Specify the existing prompt name\n    template=new_template,\n    commit_message="Improvement",\n    tags={\n        "author": "author@example.com",\n    },\n)\n'})})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"3-compare-the-prompt-versions",children:"3. Compare the Prompt Versions"}),"\n",(0,i.jsxs)(n.p,{children:["Once you have multiple versions of a prompt, you can compare them to understand the changes between versions. To compare prompt versions in the MLflow UI, click on the ",(0,i.jsx)(n.strong,{children:"Compare"})," tab in the prompt details page:"]}),"\n",(0,i.jsx)("div",{style:{width:"90%",margin:"10px"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Compare Prompt\nVersions",src:t(2371).A+"",width:"1267",height:"713"})})}),"\n",(0,i.jsx)(n.h3,{id:"4-load-and-use-the-prompt",children:"4. Load and Use the Prompt"}),"\n",(0,i.jsxs)(n.p,{children:["To use a prompt in your GenAI application, you can load it with the ",(0,i.jsx)(a.B,{fn:"mlflow.genai.load_prompt"})," API and fill in the variables using the ",(0,i.jsx)(a.B,{fn:"mlflow.entities.Prompt.format"})," method of the prompt object."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\n\ntarget_text = """\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle.\nIt tackles four primary functions in the ML lifecycle: Tracking experiments, packaging ML\ncode for reuse, managing and deploying models, and providing a central model registry.\nMLflow currently offers these functions as four components: MLflow Tracking,\nMLflow Projects, MLflow Models, and MLflow Registry.\n"""\n\n# Load the prompt\nprompt = mlflow.genai.load_prompt("prompts:/summarization-prompt/2")\n\n# Use the prompt with an LLM\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            "role": "user",\n            "content": prompt.format(num_sentences=1, sentences=target_text),\n        }\n    ],\n    model="gpt-4o-mini",\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"5-search-prompts",children:"5. Search Prompts"}),"\n",(0,i.jsx)(n.p,{children:"You can discover prompts by name, tag or other registry fields:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Fluent API: returns a flat list of all matching prompts\nprompts = mlflow.genai.search_prompts(filter_string="task=\'summarization\'")\nprint(f"Found {len(prompts)} prompts")\n\n# For pagination control, use the client API:\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nall_prompts = []\ntoken = None\nwhile True:\n    page = client.search_prompts(\n        filter_string="task=\'summarization\'",\n        max_results=50,\n        page_token=token,\n    )\n    all_prompts.extend(page)\n    token = page.token\n    if not token:\n        break\nprint(f"Total prompts across pages: {len(all_prompts)}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"prompt-object",children:"Prompt Object"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"Prompt"})," object is the core entity in MLflow Prompt Registry. It represents a versioned template text that can contain variables for dynamic content."]}),"\n",(0,i.jsx)(n.p,{children:"Key attributes of a Prompt object:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Name"}),": A unique identifier for the prompt."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Template"}),": The content of the prompt, which can be either:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A string containing text with variables in ",(0,i.jsx)(n.code,{children:"{{variable}}"})," format (text prompts)"]}),"\n",(0,i.jsx)(n.li,{children:"A list of dictionaries representing chat messages with 'role' and 'content' keys (chat prompts)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Version"}),": A sequential number representing the revision of the prompt."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Commit Message"}),": A description of the changes made in the prompt version, similar to Git commit messages."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Tags"}),": Optional key-value pairs assigned at the prompt version\nfor categorization and filtering. For example, you may add tags for project name, language, etc, which apply to all versions of the prompt."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Alias"}),": An mutable named reference to the prompt. For example, you can create an alias named ",(0,i.jsx)(n.code,{children:"production"})," to refer to the version used in your production system. See ",(0,i.jsx)(n.a,{href:"/genai/prompt-registry/manage-prompt-lifecycles-with-aliases",children:"Aliases"})," for more details."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"is_text_prompt"}),": A boolean property indicating whether the prompt is a text prompt (True) or chat prompt (False)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"response_format"}),": An optional property containing the expected response structure specification, which can be used to validate or structure outputs from LLM calls."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_config"}),": An optional dictionary containing model-specific configuration such as model name, temperature, max_tokens, and other inference parameters. See ",(0,i.jsx)(n.a,{href:"#model-configuration",children:"Model Configuration"})," for more details."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"prompt-types",children:"Prompt Types"}),"\n",(0,i.jsx)(n.h4,{id:"text-prompts",children:"Text Prompts"}),"\n",(0,i.jsx)(n.p,{children:"Text prompts use a simple string template with variables enclosed in double curly braces:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'text_template = "Hello {{ name }}, how are you today?"\n'})}),"\n",(0,i.jsx)(n.h4,{id:"chat-prompts",children:"Chat Prompts"}),"\n",(0,i.jsx)(n.p,{children:"Chat prompts use a list of message dictionaries, each with 'role' and 'content' keys:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'chat_template = [\n    {"role": "system", "content": "You are a helpful {{ style }} assistant."},\n    {"role": "user", "content": "{{ question }}"},\n]\n'})}),"\n",(0,i.jsx)(n.h4,{id:"jinja2-prompts",children:"Jinja2 Prompts"}),"\n",(0,i.jsxs)(n.p,{children:["For advanced templating needs, MLflow supports ",(0,i.jsx)(n.a,{href:"https://jinja.palletsprojects.com/",children:"Jinja2"})," templates with conditionals, loops, and filters. Jinja2 prompts are automatically detected when the template contains control flow syntax (",(0,i.jsx)(n.code,{children:"{% %}"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Jinja2 template with conditionals and loops\njinja_template = """\\\nHello {% if name %}{{ name }}{% else %}Guest{% endif %}!\n\n{% if items %}\nHere are your items:\n{% for item in items %}\n- {{ item }}\n{% endfor %}\n{% endif %}\n"""\n\n# Register the Jinja2 prompt\nprompt = mlflow.genai.register_prompt(\n    name="greeting-prompt",\n    template=jinja_template,\n)\n\n# Format with variables\nresult = prompt.format(name="Alice", items=["Book", "Pen", "Notebook"])\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Templates with only ",(0,i.jsx)(n.code,{children:"{{ variable }}"})," syntax are treated as ",(0,i.jsx)(n.strong,{children:"text prompts"})," and use simple string substitution"]}),"\n",(0,i.jsxs)(n.li,{children:["Templates containing ",(0,i.jsx)(n.code,{children:"{% %}"})," control flow syntax are treated as ",(0,i.jsx)(n.strong,{children:"Jinja2 prompts"})," and support the full Jinja2 feature set"]}),"\n",(0,i.jsxs)(n.li,{children:["Jinja2 rendering uses ",(0,i.jsx)(n.code,{children:"SandboxedEnvironment"})," by default for security. Pass ",(0,i.jsx)(n.code,{children:"use_jinja_sandbox=False"})," to ",(0,i.jsx)(n.code,{children:"format()"})," if you need unrestricted Jinja2 features"]}),"\n"]})}),"\n",(0,i.jsx)(n.h3,{id:"response-format",children:"Response Format"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"response_format"})," property allows you to specify the expected structure of responses from LLM calls. This can be either a Pydantic model class or a dictionary defining the schema:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel\n\n\nclass SummaryResponse(BaseModel):\n    summary: str\n    key_points: list[str]\n    word_count: int\n\n\n# Or as a dictionary\nresponse_format_dict = {\n    "type": "object",\n    "properties": {\n        "summary": {"type": "string"},\n        "key_points": {"type": "array", "items": {"type": "string"}},\n        "word_count": {"type": "integer"},\n    },\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"manage-prompt-and-version-tags",children:"Manage Prompt and Version Tags"}),"\n",(0,i.jsx)(n.p,{children:"MLflow lets you modify and inspect tags after a prompt has been registered. Tags can be\napplied either at the prompt level or to individual prompt versions."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Prompt-level tag operations\nmlflow.genai.set_prompt_tag("summarization-prompt", "language", "en")\nmlflow.genai.get_prompt_tags("summarization-prompt")\nmlflow.genai.delete_prompt_tag("summarization-prompt", "language")\n\n# Prompt-version tag operations\nmlflow.genai.set_prompt_version_tag("summarization-prompt", 1, "author", "alice")\nmlflow.genai.load_prompt("prompts:/summarization-prompt/1").tags\nmlflow.genai.delete_prompt_version_tag("summarization-prompt", 1, "author")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,i.jsx)(n.p,{children:"MLflow Prompt Registry allows you to store model-specific configuration alongside your prompts, ensuring reproducibility and clarity about which model and parameters were used with a particular prompt version. This is especially useful when you want to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Version both prompt templates and model parameters together"}),"\n",(0,i.jsx)(n.li,{children:"Share prompts with recommended model settings across your team"}),"\n",(0,i.jsx)(n.li,{children:"Reproduce exact inference configurations from previous experiments"}),"\n",(0,i.jsx)(n.li,{children:"Maintain different model configurations for different prompt versions"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,i.jsxs)(n.p,{children:["You can attach model configuration to a prompt by passing a ",(0,i.jsx)(n.code,{children:"model_config"})," parameter when registering:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Using a dictionary\nmodel_config = {\n    "model_name": "gpt-4",\n    "temperature": 0.7,\n    "max_tokens": 1000,\n    "top_p": 0.9,\n}\n\nmlflow.genai.register_prompt(\n    name="qa-prompt",\n    template="Answer the following question: {{question}}",\n    model_config=model_config,\n    commit_message="QA prompt with model config",\n)\n\n# Load and access the model config\nprompt = mlflow.genai.load_prompt("qa-prompt")\nprint(f"Model: {prompt.model_config[\'model_name\']}")\nprint(f"Temperature: {prompt.model_config[\'temperature\']}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"using-promptmodelconfig-class",children:"Using PromptModelConfig Class"}),"\n",(0,i.jsxs)(n.p,{children:["For better type safety and validation, you can use the ",(0,i.jsx)(a.B,{fn:"mlflow.entities.model_registry.PromptModelConfig"})," class:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities.model_registry import PromptModelConfig\n\n# Create a validated config object\nconfig = PromptModelConfig(\n    model_name="gpt-4-turbo",\n    temperature=0.5,\n    max_tokens=2000,\n    top_p=0.95,\n    frequency_penalty=0.2,\n    presence_penalty=0.1,\n    stop_sequences=["END", "\\n\\n"],\n)\n\nmlflow.genai.register_prompt(\n    name="creative-prompt",\n    template="Write a creative story about {{topic}}",\n    model_config=config,\n)\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"PromptModelConfig"})," class provides validation to catch errors early:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# This will raise a ValueError\nconfig = PromptModelConfig(temperature=-1.0)  # temperature must be non-negative\n\n# This will raise a ValueError\nconfig = PromptModelConfig(max_tokens=-100)  # max_tokens must be positive\n"})}),"\n",(0,i.jsx)(n.h3,{id:"supported-configuration-parameters",children:"Supported Configuration Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["The following standard parameters are supported in ",(0,i.jsx)(n.code,{children:"PromptModelConfig"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_name"}),' (str): The name or identifier of the model (e.g., "gpt-4", "claude-3-opus")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"temperature"})," (float): Sampling temperature for controlling randomness (typically 0.0-2.0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_tokens"})," (int): Maximum number of tokens to generate in the response"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"top_p"})," (float): Nucleus sampling parameter (typically 0.0-1.0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"top_k"})," (int): Top-k sampling parameter"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"frequency_penalty"})," (float): Penalty for token frequency (typically -2.0 to 2.0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"presence_penalty"})," (float): Penalty for token presence (typically -2.0 to 2.0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"stop_sequences"})," (list[str]): List of sequences that will cause the model to stop generating"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"extra_params"})," (dict): Additional provider-specific or experimental parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"provider-specific-parameters",children:"Provider-Specific Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["You can include provider-specific parameters using the ",(0,i.jsx)(n.code,{children:"extra_params"})," field:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Anthropic-specific configuration with extended thinking\n# See: https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking\nanthropic_thinking_config = PromptModelConfig(\n    model_name="claude-sonnet-4-20250514",\n    max_tokens=16000,\n    extra_params={\n        # Enable extended thinking for complex reasoning tasks\n        "thinking": {\n            "type": "enabled",\n            "budget_tokens": 10000,  # Max tokens for internal reasoning\n        },\n        # User tracking for abuse detection\n        "metadata": {\n            "user_id": "user-123",\n        },\n    },\n)\n\n# OpenAI-specific configuration with reproducibility and structured output\n# See: https://platform.openai.com/docs/api-reference/chat/create\nopenai_config = PromptModelConfig(\n    model_name="gpt-4o",\n    temperature=0.7,\n    max_tokens=2000,\n    extra_params={\n        # Seed for reproducible outputs\n        "seed": 42,\n        # Bias specific tokens (token_id: bias from -100 to 100)\n        "logit_bias": {"50256": -100},  # Discourage <|endoftext|>\n        # User identifier for abuse tracking\n        "user": "user-123",\n        # Service tier for priority processing\n        "service_tier": "default",\n    },\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"managing-model-configuration",children:"Managing Model Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Model configuration is mutable and can be updated after a prompt version is created. This makes it easy to fix mistakes or iterate on model parameters without creating new prompt versions."}),"\n",(0,i.jsx)(n.h4,{id:"setting-or-updating-model-config",children:"Setting or Updating Model Config"}),"\n",(0,i.jsxs)(n.p,{children:["Use ",(0,i.jsx)(a.B,{fn:"mlflow.genai.set_prompt_model_config"})," to set or update the model configuration for a prompt version:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities.model_registry import PromptModelConfig\n\n# Register a prompt without model config\nmlflow.genai.register_prompt(\n    name="my-prompt",\n    template="Analyze: {{text}}",\n)\n\n# Later, add model config\nmlflow.genai.set_prompt_model_config(\n    name="my-prompt",\n    version=1,\n    model_config={"model_name": "gpt-4", "temperature": 0.7},\n)\n\n# Or update existing model config\nmlflow.genai.set_prompt_model_config(\n    name="my-prompt",\n    version=1,\n    model_config={"model_name": "gpt-4-turbo", "temperature": 0.8, "max_tokens": 2000},\n)\n\n# Verify the update\nprompt = mlflow.genai.load_prompt("my-prompt", version=1)\nprint(prompt.model_config)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"deleting-model-config",children:"Deleting Model Config"}),"\n",(0,i.jsxs)(n.p,{children:["Use ",(0,i.jsx)(a.B,{fn:"mlflow.genai.delete_prompt_model_config"})," to remove model configuration from a prompt version:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Remove model config\nmlflow.genai.delete_prompt_model_config(name="my-prompt", version=1)\n\n# Verify removal\nprompt = mlflow.genai.load_prompt("my-prompt", version=1)\nassert prompt.model_config is None\n'})}),"\n",(0,i.jsx)(n.h4,{id:"important-notes",children:"Important Notes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Model config changes are ",(0,i.jsx)(n.strong,{children:"version-specific"})," - updating one version doesn't affect others"]}),"\n",(0,i.jsxs)(n.li,{children:["Model config is ",(0,i.jsx)(n.strong,{children:"mutable"})," - unlike the prompt template, it can be changed after creation"]}),"\n",(0,i.jsxs)(n.li,{children:["Changes are ",(0,i.jsx)(n.strong,{children:"immediate"})," - no need to create a new version to fix model parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validation applies"})," - The same validation rules apply when updating as when creating"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prompt-caching",children:"Prompt Caching"}),"\n",(0,i.jsxs)(n.p,{children:["MLflow automatically caches loaded prompts in memory to improve performance and reduce repeated API calls. The caching behavior differs based on whether you're loading a prompt by ",(0,i.jsx)(n.strong,{children:"version"})," or by ",(0,i.jsx)(n.strong,{children:"alias"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"default-caching-behavior",children:"Default Caching Behavior"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Version-based prompts"})," (e.g., ",(0,i.jsx)(n.code,{children:"prompts:/summarization-prompt/1"}),"): Cached with ",(0,i.jsx)(n.strong,{children:"infinite TTL"})," by default. Since prompt versions are immutable, they can be safely cached indefinitely."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Alias-based prompts"})," (e.g., ",(0,i.jsx)(n.code,{children:"prompts:/summarization-prompt@latest"})," or ",(0,i.jsx)(n.code,{children:"prompts:/summarization-prompt@production"}),"): Cached with ",(0,i.jsx)(n.strong,{children:"60 seconds TTL"})," by default. Aliases can point to different versions over time, so a shorter TTL ensures your application picks up updates."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"customizing-cache-behavior",children:"Customizing Cache Behavior"}),"\n",(0,i.jsx)(n.h4,{id:"per-request-cache-control",children:"Per-Request Cache Control"}),"\n",(0,i.jsxs)(n.p,{children:["You can control caching on a per-request basis using the ",(0,i.jsx)(n.code,{children:"cache_ttl_seconds"})," parameter:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Custom TTL: Cache for 5 minutes\nprompt = mlflow.genai.load_prompt(\n    "prompts:/summarization-prompt/1", cache_ttl_seconds=300\n)\n\n# Bypass cache entirely: Always fetch from registry\nprompt = mlflow.genai.load_prompt(\n    "prompts:/summarization-prompt@production", cache_ttl_seconds=0\n)\n\n# Use infinite TTL even for alias-based prompts\nprompt = mlflow.genai.load_prompt(\n    "prompts:/summarization-prompt@latest", cache_ttl_seconds=float("inf")\n)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"global-cache-configuration",children:"Global Cache Configuration"}),"\n",(0,i.jsx)(n.p,{children:"You can set default TTL values globally using environment variables:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Set alias-based prompt cache TTL to 5 minutes\nexport MLFLOW_ALIAS_PROMPT_CACHE_TTL_SECONDS=300\n\n# Set version-based prompt cache TTL to 1 hour (instead of infinite)\nexport MLFLOW_VERSION_PROMPT_CACHE_TTL_SECONDS=3600\n\n# Disable caching globally\nexport MLFLOW_ALIAS_PROMPT_CACHE_TTL_SECONDS=0\nexport MLFLOW_VERSION_PROMPT_CACHE_TTL_SECONDS=0\n"})}),"\n",(0,i.jsx)(n.h3,{id:"cache-invalidation",children:"Cache Invalidation"}),"\n",(0,i.jsx)(n.p,{children:"The cache is automatically invalidated when you modify the prompt version or alias, including the following operations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"mlflow.genai.set_prompt_version_tag"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"mlflow.genai.set_prompt_alias"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"mlflow.genai.delete_prompt_version_tag"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"mlflow.genai.delete_prompt_alias"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,i.jsx)(n.h4,{id:"q-how-do-i-delete-a-prompt-version",children:"Q: How do I delete a prompt version?"}),"\n",(0,i.jsx)(n.p,{children:"A: You can delete a prompt version using the MLflow UI or Python API:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Delete a prompt version\nclient = mlflow.MlflowClient()\nclient.delete_prompt_version("summarization-prompt", version=2)\n'})}),"\n",(0,i.jsx)(n.p,{children:"To avoid accidental deletion, you can only delete one version at a time via API. If you delete the all versions of a prompt, the prompt itself will be deleted."}),"\n",(0,i.jsx)(n.h4,{id:"q-can-i-update-the-prompt-template-of-an-existing-prompt-version",children:"Q: Can I update the prompt template of an existing prompt version?"}),"\n",(0,i.jsx)(n.p,{children:"A: No, prompt versions are immutable once created. To update a prompt, create a new version with the desired changes."}),"\n",(0,i.jsx)(n.h4,{id:"q-how-to-dynamically-load-the-latest-version-of-a-prompt",children:"Q: How to dynamically load the latest version of a prompt?"}),"\n",(0,i.jsxs)(n.p,{children:["A: You can load the latest version of a prompt by passing the name only, or using the ",(0,i.jsx)(n.code,{children:"@latest"})," alias. This is the reserved alias name and MLflow will automatically find the latest available version of the prompt."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'prompt = mlflow.genai.load_prompt("summarization-prompt")\n# or\nprompt = mlflow.genai.load_prompt("prompts:/summarization-prompt@latest")\n'})}),"\n",(0,i.jsx)(n.h4,{id:"q-can-i-use-prompt-templates-with-frameworks-like-langchain-or-llamaindex",children:"Q: Can I use prompt templates with frameworks like LangChain or LlamaIndex?"}),"\n",(0,i.jsxs)(n.p,{children:["A: Yes, you can load prompts from MLflow and use them with any framework. For example, the following example demonstrates how to use a prompt registered in MLflow with LangChain. Also refer to ",(0,i.jsx)(n.a,{href:"/genai/prompt-registry/log-with-model#example-1-logging-prompts-with-langchain",children:"Logging Prompts with LangChain"})," for more details."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import mlflow\nfrom langchain.prompts import PromptTemplate\n\n# Load prompt from MLflow\nprompt = mlflow.genai.load_prompt(\"question_answering\")\n\n# Convert the prompt to single brace format for LangChain (MLflow uses double braces),\n# using the `to_single_brace_format` method.\nlangchain_prompt = PromptTemplate.from_template(prompt.to_single_brace_format())\nprint(langchain_prompt.input_variables)\n# Output: ['num_sentences', 'sentences']\n"})}),"\n",(0,i.jsx)(n.h4,{id:"q-is-prompt-registry-integrated-with-the-prompt-engineering-ui",children:"Q: Is Prompt Registry integrated with the Prompt Engineering UI?"}),"\n",(0,i.jsx)(n.p,{children:"A. Direct integration between the Prompt Registry and the Prompt Engineering UI is coming soon. In the meantime, you can iterate on prompt template in the Prompt Engineering UI and register the final version in the Prompt Registry by manually copying the prompt template."})]})}function _(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(w,{...e})}):w(e)}},2371(e,n,t){t.d(n,{A:()=>o});let o=t.p+"assets/images/compare-prompt-versions-2082121aeaca4be99a0cf968535141ed.png"},76599(e,n,t){t.d(n,{A:()=>o});let o=t.p+"assets/images/create-prompt-ui-03c88144e65d28eb7847b2ae5d8dd49a.png"},72406(e,n,t){t.d(n,{A:()=>o});let o=t.p+"assets/images/registered-prompt-b8d47ff0d061d8703b61a9a6e94a77c3.png"},53296(e,n,t){t.d(n,{A:()=>o});let o=t.p+"assets/images/update-prompt-ui-74a489e65098893bbffe253f43fb210d.png"},54725(e,n,t){t.d(n,{B:()=>a});var o=t(74848);t(96540);var i=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),r=t(66497);function a({fn:e,children:n,hash:t}){let a=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(i[t])return t}return null})(e);if(!a)return(0,o.jsx)(o.Fragment,{children:n});let l=(0,r.default)(`/${i[a]}#${t??e}`);return(0,o.jsx)("a",{href:l,target:"_blank",children:n??(0,o.jsxs)("code",{children:[e,"()"]})})}},33508(e,n,t){t.d(n,{A:()=>i});var o=t(74848);t(96540);function i({features:e,col:n=2}){return(0,o.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${n}, 1fr)`},children:e.map((e,n)=>(0,o.jsxs)("div",{className:"highlightItem_XPnN",children:[e.icon&&(0,o.jsx)("div",{className:"highlightIcon_SUR8",children:(0,o.jsx)(e.icon,{size:24})}),(0,o.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,o.jsx)("h4",{children:e.title}),(0,o.jsx)("p",{children:e.description})]})]},n))})}}}]);