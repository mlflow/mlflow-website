"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["5129"],{35290(e,t,n){n.r(t),n.d(t,{metadata:()=>a,default:()=>A,frontMatter:()=>w,contentTitle:()=>j,toc:()=>k,assets:()=>y});var a=JSON.parse('{"id":"eval-monitor/running-evaluation/prompts","title":"Evaluating Prompts","description":"Prompts are the core components of GenAI applications. However, iterating over prompts can be challenging because it is hard to know if the new prompt is better than the old one. MLflow provides a framework to systematically evaluate prompt templates and track performance over time.","source":"@site/docs/genai/eval-monitor/running-evaluation/prompts.mdx","sourceDirName":"eval-monitor/running-evaluation","slug":"/eval-monitor/running-evaluation/prompts","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/running-evaluation/prompts","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Examples","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/running-evaluation/eval-examples"},"next":{"title":"Evaluate Agents","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/running-evaluation/agents"}}'),s=n(74848),o=n(28453),i=n(46077),r=n(10440),l=n(77541),c=n(81956),p=n(80827),d=n(51004),m=n(93164),u=n(44471),h=n(85731),v=n(47792),f=n(42640),g=n(46858),x=n(74990);let w={},j="Evaluating Prompts",y={},k=[{value:"Workflow",id:"workflow",level:2},{value:"Example: Evaluating a Prompt Template",id:"example-evaluating-a-prompt-template",level:2},{value:"Prerequisites",id:"prerequisites",level:3},...x.RM,{value:"Step 1: Create prompt templates",id:"step-1-create-prompt-templates",level:3},{value:"Step 2: Create evaluation dataset",id:"step-2-create-evaluation-dataset",level:3},{value:"Step 3: Create prediction function",id:"step-3-create-prediction-function",level:3},{value:"Step 4: Define task-specific scorers",id:"step-4-define-task-specific-scorers",level:3},{value:"Step 5: Run evaluation",id:"step-5-run-evaluation",level:3},{value:"Iterating on Prompts",id:"iterating-on-prompts",level:2},{value:"Compare Evaluation Results",id:"compare-evaluation-results",level:2},{value:"Next steps",id:"next-steps",level:2}];function _(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"evaluating-prompts",children:"Evaluating Prompts"})}),"\n",(0,s.jsx)(t.p,{children:"Prompts are the core components of GenAI applications. However, iterating over prompts can be challenging because it is hard to know if the new prompt is better than the old one. MLflow provides a framework to systematically evaluate prompt templates and track performance over time."}),"\n",(0,s.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-hero.png",alt:"Prompt Evaluation",width:"95%"}),"\n",(0,s.jsx)(t.h2,{id:"workflow",children:"Workflow"}),"\n",(0,s.jsx)(c.A,{steps:[{icon:p.A,title:"Create prompt template(s)",description:"Define and register your prompt templates in MLflow Prompt Registry for version control and easy access."},{icon:d.A,title:"Prepare evaluation dataset",description:"Create test cases with inputs and expected outcomes to systematically evaluate prompt performance."},{icon:m.A,title:"Define a wrapper function to generate responses",description:"Wrap your prompt in a function that takes dataset inputs and generates responses using your model."},{icon:u.A,title:"Define evaluation scorers",description:"Set up built-in and custom scorers to measure quality, accuracy, and task-specific criteria."},{icon:h.A,title:"Run evaluation",description:"Execute the evaluation and review results in MLflow UI to analyze performance and iterate."}]}),"\n",(0,s.jsx)(t.h2,{id:"example-evaluating-a-prompt-template",children:"Example: Evaluating a Prompt Template"}),"\n",(0,s.jsx)(t.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(t.p,{children:"First, install the required packages by running the following command:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"pip install --upgrade 'mlflow[genai]>=3.3' openai\n"})}),"\n",(0,s.jsx)(t.p,{children:"MLflow stores evaluation results in a tracking server. Connect your local environment to the tracking server by one of the following methods."}),"\n",(0,s.jsx)(x.Ay,{}),"\n",(0,s.jsx)(t.h3,{id:"step-1-create-prompt-templates",children:"Step 1: Create prompt templates"}),"\n",(0,s.jsxs)(t.p,{children:["Let's define a simple prompt template to evaluate. We use ",(0,s.jsx)(t.a,{href:"/genai/prompt-registry",children:"MLflow Prompt Registry"})," to save the prompt and version control it, but it is optional for evaluation."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\n\n# Define prompt templates. MLflow supports both text and chat format prompt templates.\nPROMPT_V1 = [\n    {\n        "role": "system",\n        "content": "You are a helpful assistant. Answer the following question.",\n    },\n    {\n        "role": "user",\n        # Use double curly braces to indicate variables.\n        "content": "Question: {{question}}",\n    },\n]\n\n# Register the prompt template to the MLflow Prompt Registry for version control\n# and convenience of loading the prompt template. This is optional.\nmlflow.genai.register_prompt(\n    name="qa_prompt",\n    template=PROMPT_V1,\n    commit_message="Initial prompt",\n)\n'})}),"\n",(0,s.jsx)(t.h3,{id:"step-2-create-evaluation-dataset",children:"Step 2: Create evaluation dataset"}),"\n",(0,s.jsxs)(t.p,{children:["The evaluation dataset is defined as a list of dictionaries, each with an ",(0,s.jsx)(t.code,{children:"inputs"}),", ",(0,s.jsx)(t.code,{children:"expectations"}),", and an optional ",(0,s.jsx)(t.code,{children:"tags"})," field."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'eval_dataset = [\n    {\n        "inputs": {"question": "What causes rain?"},\n        "expectations": {\n            "key_concepts": ["evaporation", "condensation", "precipitation"]\n        },\n        "tags": {"topic": "weather"},\n    },\n    {\n        "inputs": {"question": "Explain the difference between AI and ML"},\n        "expectations": {\n            "key_concepts": ["artificial intelligence", "machine learning", "subset"]\n        },\n        "tags": {"topic": "technology"},\n    },\n    {\n        "inputs": {"question": "How do vaccines work?"},\n        "expectations": {"key_concepts": ["immune", "antibodies", "protection"]},\n        "tags": {"topic": "medicine"},\n    },\n]\n'})}),"\n",(0,s.jsx)(t.h3,{id:"step-3-create-prediction-function",children:"Step 3: Create prediction function"}),"\n",(0,s.jsxs)(t.p,{children:["Now wrap the prompt template in a simple function that takes a question to generate responses using the prompt template. ",(0,s.jsxs)(t.strong,{children:["IMPORTANT: The function must take the keyword arguments used in the ",(0,s.jsx)(t.code,{children:"inputs"})," field of the dataset."]})," Therefore, we use ",(0,s.jsx)(t.code,{children:"question"})," as the argument of the function here."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\n\n@mlflow.trace\ndef predict_fn(question: str) -> str:\n    prompt = mlflow.genai.load_prompt("prompts:/qa_prompt@latest")\n    rendered_prompt = prompt.format(question=question)\n\n    response = client.chat.completions.create(\n        model="gpt-4.1-mini", messages=rendered_prompt\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,s.jsx)(t.h3,{id:"step-4-define-task-specific-scorers",children:"Step 4: Define task-specific scorers"}),"\n",(0,s.jsxs)(t.p,{children:["Finally, let's define a few ",(0,s.jsx)(t.a,{href:"/genai/eval-monitor/scorers",children:"scorers"})," that decide the evaluation criteria. Here we use two types of scorers:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Built-in LLM scorers for evaluating the qualitative aspects of the response."}),"\n",(0,s.jsx)(t.li,{children:"Custom heuristic scorer for evaluating the coverage of the key concepts."}),"\n"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from mlflow.entities import Feedback\nfrom mlflow.genai import scorer\nfrom mlflow.genai.scorers import Guidelines\n\n# Define LLM scorers\nis_concise = Guidelines(\n    name="is_concise", guidelines="The response should be concise and to the point."\n)\nis_professional = Guidelines(\n    name="is_professional", guidelines="The response should be in professional tone."\n)\n\n\n# Evaluate the coverage of the key concepts using custom scorer\n@scorer\ndef concept_coverage(outputs: str, expectations: dict) -> Feedback:\n    concepts = set(expectations.get("key_concepts", []))\n    included = {c for c in concepts if c.lower() in outputs.lower()}\n    return Feedback(\n        value=len(included) / len(concepts),\n        rationale=(\n            f"Included {len(included)} out of {len(concepts)} concepts. Missing: {concepts - included}"\n        ),\n    )\n'})}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsxs)(t.p,{children:["LLM scorers use OpenAI's GPT 4.1-mini by default. You can use different models by passing the ",(0,s.jsx)(t.code,{children:"model"})," parameter to the scorer constructor."]})}),"\n",(0,s.jsx)(t.h3,{id:"step-5-run-evaluation",children:"Step 5: Run evaluation"}),"\n",(0,s.jsx)(t.p,{children:"Now we are ready to run the evaluation!"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=predict_fn,\n    scorers=[is_concise, is_professional, concept_coverage],\n)\n"})}),"\n",(0,s.jsx)(t.p,{children:"Once the evaluation is done, open the MLflow UI in your browser and navigate to the experiment page. You should see MLflow creates a new Run and logs the evaluation results."}),"\n",(0,s.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-results.png",alt:"Prompt Evaluation",width:"95%"}),"\n",(0,s.jsx)(t.p,{children:"By clicking on the each row in the result, you can open the trace and see the detailed score and rationale."}),"\n",(0,s.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-trace.png",alt:"Prompt Evaluation",width:"95%"}),"\n",(0,s.jsx)(t.h2,{id:"iterating-on-prompts",children:"Iterating on Prompts"}),"\n",(0,s.jsx)(t.p,{children:"The prompt evaluation is an iterative process. You can register a new prompt version, run the same eval again, and compare the evaluation results. The prompt registry keep track of the version changes and lineage between the prompt versions and evaluation results."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'# Define V2 prompt template\nPROMPT_V2 = [\n    {\n        "role": "system",\n        "content": "You are a helpful assistant. Answer the following question in three sentences.",\n    },\n    {"role": "user", "content": "Question: {{question}}"},\n]\n\nmlflow.genai.register_prompt(name="qa_prompt", template=PROMPT_V2)\n\n# Run the same evaluation again.\n# MLflow automatically loads the latest prompt template via the `@latest` alias.\nmlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=predict_fn,\n    scorers=[is_concise, is_professional, concept_coverage],\n)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"compare-evaluation-results",children:"Compare Evaluation Results"}),"\n",(0,s.jsx)(t.p,{children:"Once you have multiple evaluation runs, you can compare the result side-by-side to analyze the performance changes. To see the comparison view, open the evaluation result page for one of the runs, and pick another run to compare from the dropdown on the top."}),"\n",(0,s.jsx)(t.p,{children:"To see the comparison view, open the evaluation result page for one of the runs, and pick another run to compare from the dropdown on the top."}),"\n",(0,s.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-dropdown.png",alt:"Prompt Evaluation"}),"\n",(0,s.jsx)(t.p,{children:"MLflow will load the evaluation results for the two runs and display the comparison view. In this example, you can see the overall concise scorer is improved 33%, but the concept coverage is dropped 11%. The little arrow \u2197\uFE0F/\u2198\uFE0F in each row indicates where the change is coming from."}),"\n",(0,s.jsx)(i.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-compare.png",alt:"Prompt Evaluation",width:"95%"}),"\n",(0,s.jsx)(t.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(l.A,{icon:v.A,iconSize:48,title:"Customize Scorers",description:"Build specialized evaluation metrics for your specific use cases and requirements.",href:"/genai/eval-monitor/scorers",linkText:"Learn about custom scorers \u2192",containerHeight:64}),(0,s.jsx)(l.A,{icon:f.A,iconSize:48,title:"Evaluate Agents",description:"Evaluate complex AI agents with tool calling and multi-step workflows.",href:"/genai/eval-monitor/running-evaluation/agents",linkText:"Evaluate agents \u2192",containerHeight:64}),(0,s.jsx)(l.A,{icon:g.A,iconSize:48,title:"Optimize Prompts",description:"Use automated optimization techniques to systematically improve your prompts.",href:"/genai/prompt-registry/optimize-prompts",linkText:"Optimize prompts \u2192",containerHeight:64})]})]})}function A(e={}){let{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(_,{...e})}):_(e)}},74990(e,t,n){n.d(t,{Ay:()=>p,RM:()=>l});var a=n(74848),s=n(28453),o=n(78010),i=n(57250),r=n(95986);let l=[];function c(e){let t={a:"a",code:"code",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsx)(r.A,{children:(0,a.jsxs)(o.A,{children:[(0,a.jsxs)(i.A,{value:"uv",label:"Local (uv)",default:!0,children:[(0,a.jsxs)(t.p,{children:["Install the Python package manager ",(0,a.jsx)(t.a,{href:"https://docs.astral.sh/uv/getting-started/installation/",children:"uv"}),"\n(that will also install ",(0,a.jsxs)(t.a,{href:"https://docs.astral.sh/uv/guides/tools/",children:[(0,a.jsx)(t.code,{children:"uvx"})," command"]})," to invoke Python tools without installing them)."]}),(0,a.jsx)(t.p,{children:"Start a MLflow server locally."}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"uvx mlflow server\n"})})]}),(0,a.jsxs)(i.A,{value:"local",label:"Local (pip)",children:[(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Python Environment"}),": Python 3.10+"]}),(0,a.jsxs)(t.p,{children:["Install the ",(0,a.jsx)(t.code,{children:"mlflow"})," Python package via ",(0,a.jsx)(t.code,{children:"pip"})," and start a MLflow server locally."]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"pip install --upgrade 'mlflow[genai]'\nmlflow server\n"})})]}),(0,a.jsxs)(i.A,{value:"docker",label:"Local (docker)",children:[(0,a.jsx)(t.p,{children:"MLflow provides a Docker Compose file to start a local MLflow server with a PostgreSQL database and a MinIO server."}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"git clone --depth 1 --filter=blob:none --sparse https://github.com/mlflow/mlflow.git\ncd mlflow\ngit sparse-checkout set docker-compose\ncd docker-compose\ncp .env.dev.example .env\ndocker compose up -d\n"})}),(0,a.jsxs)(t.p,{children:["Refer to the ",(0,a.jsx)(t.a,{href:"https://github.com/mlflow/mlflow/tree/master/docker-compose/README.md",children:"instruction"})," for more details (e.g., overriding the default environment variables)."]})]})]})})}function p(e={}){let{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},46077(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var s=n(66497);function o({src:e,alt:t,width:n,caption:o,className:i}){return(0,a.jsxs)("div",{className:`container_JwLF ${i||""}`,children:[(0,a.jsx)("div",{className:"imageWrapper_RfGN",style:n?{width:n}:{},children:(0,a.jsx)("img",{src:(0,s.default)(e),alt:t,className:"image_bwOA"})}),o&&(0,a.jsx)("p",{className:"caption_jo2G",children:o})]})}},95986(e,t,n){n.d(t,{A:()=>s});var a=n(74848);n(96540);function s({children:e}){return(0,a.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,t,n){n.d(t,{A:()=>c});var a=n(74848);n(96540);var s=n(95310),o=n(34164);let i="tileImage_O4So";var r=n(66497),l=n(92802);function c({icon:e,image:t,imageDark:n,imageWidth:c,imageHeight:p,iconSize:d=32,containerHeight:m,title:u,description:h,href:v,linkText:f="Learn more \u2192",className:g}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let x=m?{height:`${m}px`}:{},w={};return c&&(w.width=`${c}px`),p&&(w.height=`${p}px`),(0,a.jsxs)(s.A,{href:v,className:(0,o.A)("tileCard_NHsj",g),children:[(0,a.jsx)("div",{className:"tileIcon_pyoR",style:x,children:e?(0,a.jsx)(e,{size:d}):n?(0,a.jsx)(l.A,{sources:{light:(0,r.default)(t),dark:(0,r.default)(n)},alt:u,className:i,style:w}):(0,a.jsx)("img",{src:(0,r.default)(t),alt:u,className:i,style:w})}),(0,a.jsx)("h3",{children:u}),(0,a.jsx)("p",{children:h}),(0,a.jsx)("div",{className:"tileLink_iUbu",children:f})]})}},10440(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var s=n(34164);function o({children:e,className:t}){return(0,a.jsx)("div",{className:(0,s.A)("tilesGrid_hB9N",t),children:e})}},81956(e,t,n){n.d(t,{A:()=>o});var a=n(74848);n(96540);var s=n(46077);let o=({steps:e,title:t,screenshot:n,width:o="normal"})=>(0,a.jsxs)("div",{className:"workflowContainer__N1v",children:[t&&(0,a.jsx)("h3",{className:"workflowTitle_QrAr",children:t}),n&&(0,a.jsx)("div",{className:"screenshotContainer_OwzZ",children:(0,a.jsx)(s.A,{src:n.src,alt:n.alt,width:n.width||"90%"})}),(0,a.jsx)("div",{className:"stepsContainer_IGeu",style:{maxWidth:"wide"===o?"850px":"700px"},children:e.map((t,n)=>(0,a.jsxs)("div",{className:"stepItem_GyHJ",children:[(0,a.jsxs)("div",{className:"stepIndicator_U2Wb",children:[(0,a.jsx)("div",{className:"stepNumber_vINc",children:t.icon?(0,a.jsx)(t.icon,{size:16}):(0,a.jsx)("span",{className:"stepNumberText_eLd7",children:n+1})}),n<e.length-1&&(0,a.jsx)("div",{className:"stepConnector_Si86"})]}),(0,a.jsxs)("div",{className:"stepContent_D0CA",children:[(0,a.jsx)("h4",{className:"stepTitle_wujx",children:t.title}),(0,a.jsx)("p",{className:"stepDescription_PIaE",children:t.description})]})]},n))})]})}}]);