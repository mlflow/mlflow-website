"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["418"],{45421(e,n,t){t.r(n),t.d(n,{metadata:()=>a,default:()=>y,frontMatter:()=>g,contentTitle:()=>f,toc:()=>j,assets:()=>_});var a=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge","title":"Create a custom judge using make_judge()","description":"Custom judges are LLM-based judges that evaluate your GenAI agents against specific quality criteria. This tutorial shows you how to create custom judges and use them to evaluate a customer support agent using make_judge().","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge.mdx","sourceDirName":"eval-monitor/scorers/llm-judge/custom-judges","slug":"/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges/create-custom-judge","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Supported Judge Models","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges/supported-models"},"next":{"title":"Create custom code-based scorers","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom/"}}'),l=t(74848),s=t(28453),r=t(10440),o=t(77541),i=t(78010),c=t(57250),u=t(95986),d=t(54725),p=t(22864),h=t(93893),m=t(46077);let g={},f="Create a custom judge using make_judge()",_={},j=[{value:"Step 1: Create an agent to evaluate",id:"step-1-create-an-agent-to-evaluate",level:2},{value:"Step 2: Define custom judges",id:"step-2-define-custom-judges",level:2},{value:"Example judge 1: Evaluate issue resolution",id:"example-judge-1-evaluate-issue-resolution",level:3},{value:"Example judge 2: Check expected behaviors",id:"example-judge-2-check-expected-behaviors",level:3},{value:"Example judge 3: Validate tool calls using a trace-based judge",id:"example-judge-3-validate-tool-calls-using-a-trace-based-judge",level:3},{value:"Step 3: Create a sample evaluation dataset",id:"step-3-create-a-sample-evaluation-dataset",level:2},{value:"Step 4: Evaluate your agent using the judges",id:"step-4-evaluate-your-agent-using-the-judges",level:2},{value:"Advanced Examples",id:"advanced-examples",level:2},{value:"Debugging Agent Judges",id:"debugging-agent-judges",level:2},{value:"Next steps",id:"next-steps",level:2}];function w(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsxs)(n.h1,{id:"create-a-custom-judge-using-make_judge",children:["Create a custom judge using ",(0,l.jsx)(n.code,{children:"make_judge()"})]})}),"\n",(0,l.jsxs)(n.p,{children:["Custom judges are LLM-based judges that evaluate your GenAI agents against specific quality criteria. This tutorial shows you how to create custom judges and use them to evaluate a customer support agent using ",(0,l.jsx)(d.B,{fn:"mlflow.genai.judges.make_jduge",children:(0,l.jsx)(n.code,{children:"make_judge()"})}),"."]}),"\n",(0,l.jsx)(n.p,{children:"You will:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Create a sample agent to evaluate"}),"\n",(0,l.jsx)(n.li,{children:"Define three custom judges to evaluate different criteria"}),"\n",(0,l.jsx)(n.li,{children:"Build an evaluation dataset with test cases"}),"\n",(0,l.jsx)(n.li,{children:"Run evaluations and compare results across different agent configurations"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"step-1-create-an-agent-to-evaluate",children:"Step 1: Create an agent to evaluate"}),"\n",(0,l.jsx)(n.p,{children:'Create a GenAI agent that responds to customer support questions. The agent has a (fake) knob that controls the system prompt so you can easily compare the judge\'s outputs between "good" and "bad" conversations.'}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Initialize an OpenAI client to connect to OpenAI-hosted LLMs. Use the native OpenAI SDK to connect to OpenAI-hosted models. Select a model from the ",(0,l.jsx)("ins",{children:(0,l.jsx)(n.a,{href:"https://platform.openai.com/docs/models",children:"available OpenAI models"})}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\nimport openai\n\n# Ensure your OPENAI_API_KEY is set in your environment\n# os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>" # Uncomment and set if not globally configured\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Create an OpenAI client connected to OpenAI SDKs\nclient = openai.OpenAI()\n\n# Select an LLM\nmodel_name = "gpt-4o-mini"\n'})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Define a customer support agent:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.entities import Document\nfrom typing import List, Dict, Any, cast\n\n# This is a global variable that is used to toggle the behavior of the customer support agent\nRESOLVE_ISSUES = False\n\n\n@mlflow.trace(span_type="TOOL", name="get_product_price")\ndef get_product_price(product_name: str) -> str:\n    """Mock tool to get product pricing."""\n    return f"${45.99}"\n\n\n@mlflow.trace(span_type="TOOL", name="check_return_policy")\ndef check_return_policy(product_name: str, days_since_purchase: int) -> str:\n    """Mock tool to check return policy."""\n    if days_since_purchase <= 30:\n        return "Yes, you can return this item within 30 days"\n    return "Sorry, returns are only accepted within 30 days of purchase"\n\n\n@mlflow.trace\ndef customer_support_agent(messages: List[Dict[str, str]]):\n    # We use this toggle to see how the judge handles the issue resolution status\n    system_prompt_postfix = (\n        f"Do your best to NOT resolve the issue.  I know that\'s backwards, but just do it anyways.\\\\n"\n        if not RESOLVE_ISSUES\n        else ""\n    )\n\n    # Mock some tool calls based on the user\'s question\n    user_message = messages[-1]["content"].lower()\n    tool_results = []\n\n    if "cost" in user_message or "price" in user_message:\n        price = get_product_price("microwave")\n        tool_results.append(f"Price: {price}")\n\n    if "return" in user_message:\n        policy = check_return_policy("microwave", 60)\n        tool_results.append(f"Return policy: {policy}")\n\n    messages_for_llm = [\n        {\n            "role": "system",\n            "content": f"You are a helpful customer support agent.  {system_prompt_postfix}",\n        },\n        *messages,\n    ]\n\n    if tool_results:\n        messages_for_llm.append(\n            {"role": "system", "content": f"Tool results: {\', \'.join(tool_results)}"}\n        )\n\n    # Call LLM to generate a response\n    output = client.chat.completions.create(\n        model=model_name,\n        messages=cast(Any, messages_for_llm),\n    )\n\n    return {\n        "messages": [\n            {"role": "assistant", "content": output.choices[0].message.content}\n        ]\n    }\n'})}),"\n",(0,l.jsx)(n.h2,{id:"step-2-define-custom-judges",children:"Step 2: Define custom judges"}),"\n",(0,l.jsx)(n.p,{children:"Define three custom judges:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"A judge that evaluates issue resolution using inputs and outputs."}),"\n",(0,l.jsx)(n.li,{children:"A judge that checks expected behaviors."}),"\n",(0,l.jsx)(n.li,{children:"A trace-based judge that validates tool calls by analyzing execution traces."}),"\n"]}),"\n",(0,l.jsx)(u.A,{children:(0,l.jsxs)(i.A,{groupId:"creation-method",children:[(0,l.jsxs)(c.A,{value:"ui",label:"UI",default:!0,children:[(0,l.jsx)(n.p,{children:"The MLflow UI provides a visual Judge Builder that lets you create custom LLM judges without writing code."}),(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Install and start MLflow:"}),"\n"]}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"pip install 'mlflow[genai]'\nmlflow server\n"})}),(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsxs)(n.li,{children:["Navigate to your experiment and select the ",(0,l.jsx)(n.strong,{children:"Judges"})," tab, then click ",(0,l.jsx)(n.strong,{children:"New LLM judge"})]}),"\n"]}),(0,l.jsx)(m.A,{src:"/images/mlflow-3/eval-monitor/scorers/judges-tab.png",alt:"Judges Tab"}),(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Select scope"}),": Choose what you want the judge to evaluate:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Traces"}),": Evaluate individual traces for quality and correctness"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sessions"}),": Evaluate entire multi-turn conversations for conversation quality and outcomes"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Configure the judge"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"LLM judge"}),': Select a built-in judge or "Custom judge" to create your own. Selecting a built-in judge pre-populates the instructions, which you can then modify to customize the evaluation criteria.']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Name"}),": A unique identifier for your judge"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Instructions"}),": Define your evaluation criteria using ",(0,l.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges/#prompts-and-template-variables",children:"template variables"}),". Use the ",(0,l.jsx)(n.strong,{children:"Add variable"})," button to insert variables into your prompt."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Output type"}),": Select the return type"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Model"}),': Select an endpoint from the dropdown (recommended) or click "enter model manually" to access models directly without AI Gateway. Endpoints can be configured using ',(0,l.jsx)(n.a,{href:"/genai/governance/ai-gateway/",children:"AI Gateway"}),", which centralizes API key management. Judges using direct model access require local API keys and cannot be run directly from the UI. See ",(0,l.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges#selecting-judge-models",children:"Supported Models"})," for details."]}),"\n"]}),"\n"]}),"\n"]}),(0,l.jsx)(m.A,{src:"/images/mlflow-3/eval-monitor/scorers/judge-builder-dialog.png",alt:"Judge Builder Dialog"}),(0,l.jsxs)(n.ol,{start:"5",children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Test your judge"})," (optional): Click the trace selector dropdown and choose ",(0,l.jsx)(n.strong,{children:"Select traces"})," to pick specific traces, then click ",(0,l.jsx)(n.strong,{children:"Run judge"})," to preview the evaluation result"]}),"\n"]}),(0,l.jsx)(m.A,{src:"/images/mlflow-3/eval-monitor/scorers/test-judge-output.png",alt:"Test Judge Output"}),(0,l.jsxs)(n.ol,{start:"6",children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Schedule automatic evaluation"})," (optional):"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Automatically evaluate future traces"}),": Enable to run this judge on new traces automatically"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sample rate"}),": Percentage of traces to evaluate (0-100%)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Filter string"}),": Only evaluate traces matching this filter (",(0,l.jsx)(n.a,{href:"/genai/tracing/search-traces/",children:"syntax"}),")"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Click ",(0,l.jsx)(n.strong,{children:"Create judge"})," to save your new LLM judge"]}),"\n"]}),"\n"]})]}),(0,l.jsxs)(c.A,{value:"sdk",label:"SDK",children:[(0,l.jsxs)(n.p,{children:["Judges created with ",(0,l.jsx)(n.code,{children:"make_judge()"})," return ",(0,l.jsx)(d.B,{fn:"mlflow.entities.Feedback",children:(0,l.jsx)(n.code,{children:"mlflow.entities.Feedback"})})," objects."]}),(0,l.jsx)(n.h3,{id:"example-judge-1-evaluate-issue-resolution",children:"Example judge 1: Evaluate issue resolution"}),(0,l.jsx)(n.p,{children:"This judge assesses whether customer issues were successfully resolved by analyzing the conversation history (inputs) and agent responses (outputs)."}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\nfrom typing import Literal\n\n\n# Create a judge that evaluates issue resolution using inputs and outputs\nissue_resolution_judge = make_judge(\n    name="issue_resolution",\n    instructions=(\n        "Evaluate if the customer\'s issue was resolved in the conversation.\\n\\n"\n        "User\'s messages: {{ inputs }}\\n"\n        "Agent\'s responses: {{ outputs }}"\n    ),\n    feedback_value_type=Literal[\n        "fully_resolved", "partially_resolved", "needs_follow_up"\n    ],\n)\n'})}),(0,l.jsx)(n.h3,{id:"example-judge-2-check-expected-behaviors",children:"Example judge 2: Check expected behaviors"}),(0,l.jsx)(n.p,{children:"This judge verifies that agent responses demonstrate specific expected behaviors (like providing pricing information or explaining return policies) by comparing outputs against predefined expectations."}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Create a judge that checks against expected behaviors\nexpected_behaviors_judge = make_judge(\n    name="expected_behaviors",\n    instructions=(\n        "Compare the agent\'s response in {{ outputs }} against the expected behaviors in {{ expectations }}.\\n\\n"\n        "User\'s question: {{ inputs }}"\n    ),\n    feedback_value_type=Literal[\n        "meets_expectations", "partially_meets", "does_not_meet"\n    ],\n)\n'})}),(0,l.jsx)(n.h3,{id:"example-judge-3-validate-tool-calls-using-a-trace-based-judge",children:"Example judge 3: Validate tool calls using a trace-based judge"}),(0,l.jsxs)(n.p,{children:["This judge analyzes execution traces to validate that appropriate tools were called. When you include ",(0,l.jsx)(n.code,{children:"{{ trace }}"})," in your instructions, the judge becomes trace-based and gains autonomous trace exploration capabilities."]}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Create a trace-based judge that validates tool calls from the trace\ntool_call_judge = make_judge(\n    name="tool_call_correctness",\n    instructions=(\n        "Analyze the execution {{ trace }} to determine if the agent called appropriate tools for the user\'s request.\\n\\n"\n        "Examine the trace to:\\n"\n        "1. Identify what tools were available and their purposes\\n"\n        "2. Determine which tools were actually called\\n"\n        "3. Assess whether the tool calls were reasonable for addressing the user\'s question"\n    ),\n    feedback_value_type=bool,\n    # To analyze a full trace with a trace-based judge, a model must be specified\n    model="openai:/gpt-5-mini",\n)\n'})})]})]})}),"\n",(0,l.jsx)(n.h2,{id:"step-3-create-a-sample-evaluation-dataset",children:"Step 3: Create a sample evaluation dataset"}),"\n",(0,l.jsxs)(n.p,{children:["Each ",(0,l.jsx)(n.code,{children:"inputs"})," is passed to the agent by ",(0,l.jsx)(d.B,{fn:"mlflow.genai.evaluate",children:(0,l.jsx)(n.code,{children:"mlflow.genai.evaluate"})}),". You can optionally include ",(0,l.jsx)(n.code,{children:"expectations"})," to enable the correctness checker."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'eval_dataset = [\n    {\n        "inputs": {\n            "messages": [\n                {"role": "user", "content": "How much does a microwave cost?"},\n            ],\n        },\n        "expectations": {\n            "should_provide_pricing": True,\n            "should_offer_alternatives": True,\n        },\n    },\n    {\n        "inputs": {\n            "messages": [\n                {\n                    "role": "user",\n                    "content": "Can I return the microwave I bought 2 months ago?",\n                },\n            ],\n        },\n        "expectations": {\n            "should_mention_return_policy": True,\n            "should_ask_for_receipt": False,\n        },\n    },\n    {\n        "inputs": {\n            "messages": [\n                {\n                    "role": "user",\n                    "content": "I\'m having trouble with my account.  I can\'t log in.",\n                },\n                {\n                    "role": "assistant",\n                    "content": "I\'m sorry to hear that you\'re having trouble with your account.  Are you using our website or mobile app?",\n                },\n                {"role": "user", "content": "Website"},\n            ],\n        },\n        "expectations": {\n            "should_provide_troubleshooting_steps": True,\n            "should_escalate_if_needed": True,\n        },\n    },\n    {\n        "inputs": {\n            "messages": [\n                {\n                    "role": "user",\n                    "content": "I\'m having trouble with my account.  I can\'t log in.",\n                },\n                {\n                    "role": "assistant",\n                    "content": "I\'m sorry to hear that you\'re having trouble with your account.  Are you using our website or mobile app?",\n                },\n                {"role": "user", "content": "JUST FIX IT FOR ME"},\n            ],\n        },\n        "expectations": {\n            "should_remain_calm": True,\n            "should_provide_solution": True,\n        },\n    },\n]\n'})}),"\n",(0,l.jsx)(n.h2,{id:"step-4-evaluate-your-agent-using-the-judges",children:"Step 4: Evaluate your agent using the judges"}),"\n",(0,l.jsx)(n.p,{children:"You can use multiple judges together to evaluate different aspects of your agent. Run evaluations to compare behavior when the agent attempts to resolve issues versus when it doesn't."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\n\n# Evaluate with all three judges when the agent does NOT try to resolve issues\nRESOLVE_ISSUES = False\n\nresult_unresolved = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=customer_support_agent,\n    scorers=[\n        issue_resolution_judge,  # Checks inputs/outputs\n        expected_behaviors_judge,  # Checks expected behaviors\n        tool_call_judge,  # Validates tool usage\n    ],\n)\n\n# Evaluate when the agent DOES try to resolve issues\nRESOLVE_ISSUES = True\n\nresult_resolved = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=customer_support_agent,\n    scorers=[\n        issue_resolution_judge,\n        expected_behaviors_judge,\n        tool_call_judge,\n    ],\n)\n"})}),"\n",(0,l.jsx)(n.p,{children:"The evaluation results show how each judge rates the agent:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"issue_resolution"}),": Rates conversations as 'fully_resolved', 'partially_resolved', or 'needs_follow_up'"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"expected_behaviors"}),": Checks if responses exhibit expected behaviors ('meets_expectations', 'partially_meets', 'does_not_meet')"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"tool_call_correctness"}),": Validates whether appropriate tools were called (true/false)"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"advanced-examples",children:"Advanced Examples"}),"\n",(0,l.jsx)(u.A,{children:(0,l.jsxs)(i.A,{children:[(0,l.jsx)(c.A,{value:"tool_usage",label:"Tool Usage Analysis",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'tool_optimization_judge = make_judge(\n    name="tool_optimizer",\n    instructions=(\n        "Analyze tool usage patterns in {{ trace }}.\\n\\n"\n        "Check for:\\n"\n        "1. Unnecessary tool calls (could be answered without tools)\\n"\n        "2. Wrong tool selection (better tool available)\\n"\n        "3. Inefficient sequencing (could parallelize or reorder)\\n"\n        "4. Missing tool usage (should have used a tool)\\n\\n"\n        "Provide specific optimization suggestions.\\n"\n        "Rate efficiency as: \'optimal\', \'good\', \'suboptimal\', or \'poor\'"\n    ),\n    feedback_value_type=Literal["optimal", "good", "suboptimal", "poor"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,l.jsx)(c.A,{value:"loop_detection",label:"Loop Detection",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'loop_detector_judge = make_judge(\n    name="loop_detector",\n    instructions=(\n        "Detect problematic loops in {{ trace }}.\\n\\n"\n        "Identify:\\n"\n        "1. Infinite loop risks\\n"\n        "2. Unnecessary iterations\\n"\n        "3. Circular reasoning patterns\\n"\n        "4. Recursive calls without proper termination\\n\\n"\n        "Report specific span patterns that indicate issues.\\n"\n        "Classify as: \'clean\', \'warning\', or \'critical\'"\n    ),\n    feedback_value_type=Literal["clean", "warning", "critical"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,l.jsx)(c.A,{value:"reasoning_chain",label:"Reasoning Analysis",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'reasoning_judge = make_judge(\n    name="reasoning_validator",\n    instructions=(\n        "Evaluate the reasoning chain in {{ trace }}.\\n\\n"\n        "Analysis criteria:\\n"\n        "1. Logical Progression: Does each step follow logically from the previous?\\n"\n        "2. Assumption Validity: Are assumptions reasonable and stated?\\n"\n        "3. Evidence Usage: Is evidence properly cited and used?\\n"\n        "4. Conclusion Soundness: Does the conclusion follow from the premises?\\n\\n"\n        "Identify specific reasoning flaws with span IDs.\\n"\n        "Score 1-100 for reasoning quality."\n    ),\n    feedback_value_type=int,\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,l.jsx)(c.A,{value:"rag",label:"RAG Agent Evaluation",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'rag_judge = make_judge(\n    name="rag_evaluator",\n    instructions=(\n        "Evaluate the RAG agent\'s behavior in {{ trace }}.\\n\\n"\n        "Check for:\\n"\n        "1. Were the right documents retrieved?\\n"\n        "2. Is the response grounded in the retrieved context?\\n"\n        "3. Are sources properly cited?\\n\\n"\n        "Rate as: \'good\', \'acceptable\', or \'poor\'"\n    ),\n    feedback_value_type=Literal["good", "acceptable", "poor"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n\n\n# Use with your RAG pipeline\n@mlflow.trace\ndef rag_pipeline(query):\n    docs = retrieve_documents(query)\n    response = generate_with_context(query, docs)\n    return response\n\n\nresult = rag_pipeline("What is MLflow?")\ntrace = mlflow.get_last_active_trace()\nevaluation = rag_judge(trace=trace)\n'})})}),(0,l.jsx)(c.A,{value:"error_handling",label:"Error Handling Assessment",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'error_handling_judge = make_judge(\n    name="error_handler_checker",\n    instructions=(\n        "Analyze error handling in the {{ trace }}.\\n\\n"\n        "Look for:\\n"\n        "1. Spans with error status or exceptions\\n"\n        "2. Retry attempts and their patterns\\n"\n        "3. Fallback mechanisms\\n"\n        "4. Error propagation and recovery\\n\\n"\n        "Identify specific error scenarios and how they were handled.\\n"\n        "Rate as: \'robust\', \'adequate\', or \'fragile\'"\n    ),\n    feedback_value_type=Literal["robust", "adequate", "fragile"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})})]})}),"\n",(0,l.jsx)(n.h2,{id:"debugging-agent-judges",children:"Debugging Agent Judges"}),"\n",(0,l.jsx)(n.p,{children:"To see the actual MCP tool calls that the Agent-as-a-Judge makes while analyzing your trace, enable debug logging:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import logging\n\n# Enable debug logging to see agent tool calls\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger("mlflow.genai.judges")\nlogger.setLevel(logging.DEBUG)\n\n# Now when you run the judge, you\'ll see detailed tool usage\nfeedback = performance_judge(trace=trace)\n'})}),"\n",(0,l.jsx)(n.p,{children:"With debug logging enabled, you'll see output like:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'DEBUG:mlflow.genai.judges:Calling tool: GetTraceInfo\nDEBUG:mlflow.genai.judges:Tool response: {"trace_id": "abc123", "duration_ms": 4000, ...}\nDEBUG:mlflow.genai.judges:Calling tool: ListSpans\nDEBUG:mlflow.genai.judges:Tool response: [{"span_id": "def456", "name": "fetch_data", ...}]\nDEBUG:mlflow.genai.judges:Calling tool: GetSpan with span_id=def456\nDEBUG:mlflow.genai.judges:Tool response: {"duration_ms": 2500, "inputs": {"query": "SELECT * FROM users"}, ...}\n'})}),"\n",(0,l.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,l.jsxs)(r.A,{children:[(0,l.jsx)(o.A,{icon:p.A,title:"Evaluate and improve a GenAI application",description:"Use custom judges in end-to-end evaluation workflows",href:"/genai/eval-monitor/quickstart",linkText:"Start evaluating \u2192"}),(0,l.jsx)(o.A,{icon:h.A,title:"Align judges with human feedback",description:"The base judge is a starting point. As you gather expert feedback on you application's outputs, align the LLM judges to the feedback to further improve judge accuracy",href:"/genai/eval-monitor/scorers/llm-judge/alignment",linkText:"Learn alignment \u2192"})]})]})}function y(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(w,{...e})}):w(e)}},75689(e,n,t){t.d(n,{A:()=>i});var a=t(96540);let l=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,t)=>t?t.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},s=(...e)=>e.filter((e,n,t)=>!!e&&""!==e.trim()&&t.indexOf(e)===n).join(" ").trim();var r={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let o=(0,a.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:t=2,absoluteStrokeWidth:l,className:o="",children:i,iconNode:c,...u},d)=>(0,a.createElement)("svg",{ref:d,...r,width:n,height:n,stroke:e,strokeWidth:l?24*Number(t)/Number(n):t,className:s("lucide",o),...!i&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(u)&&{"aria-hidden":"true"},...u},[...c.map(([e,n])=>(0,a.createElement)(e,n)),...Array.isArray(i)?i:[i]])),i=(e,n)=>{let t=(0,a.forwardRef)(({className:t,...r},i)=>(0,a.createElement)(o,{ref:i,iconNode:n,className:s(`lucide-${l(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,t),...r}));return t.displayName=l(e),t}},22864(e,n,t){t.d(n,{A:()=>a});let a=(0,t(75689).A)("chart-column",[["path",{d:"M3 3v16a2 2 0 0 0 2 2h16",key:"c24i48"}],["path",{d:"M18 17V9",key:"2bz60n"}],["path",{d:"M13 17V5",key:"1frdt8"}],["path",{d:"M8 17v-3",key:"17ska0"}]])},93893(e,n,t){t.d(n,{A:()=>a});let a=(0,t(75689).A)("users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["path",{d:"M16 3.128a4 4 0 0 1 0 7.744",key:"16gr8j"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}]])},57250(e,n,t){t.d(n,{A:()=>s});var a=t(74848);t(96540);var l=t(34164);function s({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,l.A)("tabItem_Ymn6",t),hidden:n,children:e})}},78010(e,n,t){t.d(n,{A:()=>w});var a=t(74848),l=t(96540),s=t(34164),r=t(88287),o=t(28584),i=t(56347),c=t(99989),u=t(96629),d=t(80618),p=t(41367);function h(e){return l.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}var g=t(19863);function f({className:e,block:n,selectedValue:t,selectValue:l,tabValues:r}){let i=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),u=e=>{let n=e.currentTarget,a=r[i.indexOf(n)].value;a!==t&&(c(n),l(a))},d=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{let t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{let t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1]}}n?.focus()};return(0,a.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:l})=>(0,a.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:u,...l,className:(0,s.A)("tabs__item","tabItem_LNqP",l?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:t}){let r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=r.find(e=>e.props.value===t);return e?(0,l.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,a.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,l.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function j(e){let n=function(e){let n,{defaultValue:t,queryString:a=!1,groupId:s}=e,r=function(e){let{values:n,children:t}=e;return(0,l.useMemo)(()=>{let e=n??h(t).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a})),a=(0,d.XI)(e,(e,n)=>e.value===n.value);if(a.length>0)throw Error(`Docusaurus error: Duplicate values "${a.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,t])}(e),[o,g]=(0,l.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let t=n.find(e=>e.default)??n[0];if(!t)throw Error("Unexpected error: 0 tabValues");return t.value})({defaultValue:t,tabValues:r})),[f,_]=function({queryString:e=!1,groupId:n}){let t=(0,i.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,u.aZ)(a),(0,l.useCallback)(e=>{if(!a)return;let n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})},[a,t])]}({queryString:a,groupId:s}),[j,w]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[t,a]=(0,p.Dv)(n);return[t,(0,l.useCallback)(e=>{n&&a.set(e)},[n,a])]}({groupId:s}),y=m({value:n=f??j,tabValues:r})?n:null;return(0,c.A)(()=>{y&&g(y)},[y]),{selectedValue:o,selectValue:(0,l.useCallback)(e=>{if(!m({value:e,tabValues:r}))throw Error(`Can't select invalid tab value=${e}`);g(e),_(e),w(e)},[_,w,r]),tabValues:r}}(e);return(0,a.jsxs)("div",{className:(0,s.A)(r.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,a.jsx)(f,{...n,...e}),(0,a.jsx)(_,{...n,...e})]})}function w(e){let n=(0,g.A)();return(0,a.jsx)(j,{...e,children:h(e.children)},String(n))}},54725(e,n,t){t.d(n,{B:()=>r});var a=t(74848);t(96540);var l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),s=t(66497);function r({fn:e,children:n,hash:t}){let r=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let t=n.slice(0,e).join(".");if(l[t])return t}return null})(e);if(!r)return(0,a.jsx)(a.Fragment,{children:n});let o=(0,s.default)(`/${l[r]}#${t??e}`);return(0,a.jsx)("a",{href:o,target:"_blank",children:n??(0,a.jsxs)("code",{children:[e,"()"]})})}},46077(e,n,t){t.d(n,{A:()=>s});var a=t(74848);t(96540);var l=t(66497);function s({src:e,alt:n,width:t,caption:s,className:r}){return(0,a.jsxs)("div",{className:`container_JwLF ${r||""}`,children:[(0,a.jsx)("div",{className:"imageWrapper_RfGN",style:t?{width:t}:{},children:(0,a.jsx)("img",{src:(0,l.default)(e),alt:n,className:"image_bwOA"})}),s&&(0,a.jsx)("p",{className:"caption_jo2G",children:s})]})}},95986(e,n,t){t.d(n,{A:()=>l});var a=t(74848);t(96540);function l({children:e}){return(0,a.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,n,t){t.d(n,{A:()=>c});var a=t(74848);t(96540);var l=t(95310),s=t(34164);let r="tileImage_O4So";var o=t(66497),i=t(92802);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:u,iconSize:d=32,containerHeight:p,title:h,description:m,href:g,linkText:f="Learn more \u2192",className:_}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let j=p?{height:`${p}px`}:{},w={};return c&&(w.width=`${c}px`),u&&(w.height=`${u}px`),(0,a.jsxs)(l.A,{href:g,className:(0,s.A)("tileCard_NHsj",_),children:[(0,a.jsx)("div",{className:"tileIcon_pyoR",style:j,children:e?(0,a.jsx)(e,{size:d}):t?(0,a.jsx)(i.A,{sources:{light:(0,o.default)(n),dark:(0,o.default)(t)},alt:h,className:r,style:w}):(0,a.jsx)("img",{src:(0,o.default)(n),alt:h,className:r,style:w})}),(0,a.jsx)("h3",{children:h}),(0,a.jsx)("p",{children:m}),(0,a.jsx)("div",{className:"tileLink_iUbu",children:f})]})}},10440(e,n,t){t.d(n,{A:()=>s});var a=t(74848);t(96540);var l=t(34164);function s({children:e,className:n}){return(0,a.jsx)("div",{className:(0,l.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,t){t.d(n,{R:()=>r,x:()=>o});var a=t(96540);let l={},s=a.createContext(l);function r(e){let n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);