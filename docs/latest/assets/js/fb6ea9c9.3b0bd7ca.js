"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5406],{11470:(e,n,t)=>{t.d(n,{A:()=>x});var i=t(96540),a=t(34164),r=t(23104),o=t(56347),s=t(205),l=t(57485),c=t(31682),p=t(70679);function u(e){return i.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??function(e){return u(e).map((({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i})))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m({value:e,tabValues:n}){return n.some((n=>n.value===e))}function f({queryString:e=!1,groupId:n}){const t=(0,o.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(a),(0,i.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})}),[a,t])]}function _(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,r=d(e),[o,l]=(0,i.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:r}))),[c,u]=f({queryString:t,groupId:a}),[_,h]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,p.Dv)(n);return[t,(0,i.useCallback)((e=>{n&&a.set(e)}),[n,a])]}({groupId:a}),g=(()=>{const e=c??_;return m({value:e,tabValues:r})?e:null})();(0,s.A)((()=>{g&&l(g)}),[g]);return{selectedValue:o,selectValue:(0,i.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),h(e)}),[u,h,r]),tabValues:r}}var h=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var w=t(74848);function y({className:e,block:n,selectedValue:t,selectValue:i,tabValues:o}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,r.a_)(),c=e=>{const n=e.currentTarget,a=s.indexOf(n),r=o[a].value;r!==t&&(l(n),i(r))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,w.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:o.map((({value:e,label:n,attributes:i})=>(0,w.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:p,onClick:c,...i,className:(0,a.A)("tabs__item",g.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function b({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find((e=>e.props.value===t));return e?(0,i.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,w.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function v(e){const n=_(e);return(0,w.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,w.jsx)(y,{...n,...e}),(0,w.jsx)(b,{...n,...e})]})}function x(e){const n=(0,h.A)();return(0,w.jsx)(v,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var i=t(34164);const a={tabItem:"tabItem_Ymn6"};var r=t(74848);function o({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,i.A)(a.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(96540);const a={},r=i.createContext(a);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},49374:(e,n,t)=>{t.d(n,{B:()=>l});t(96540);const i=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var a=t(86025),r=t(28774),o=t(74848);const s=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(i[e])return e}return null};function l({fn:e,children:n}){const t=s(e);if(!t)return(0,o.jsx)(o.Fragment,{children:n});const l=(0,a.Ay)(`/${i[t]}#${e}`);return(0,o.jsx)(r.A,{to:l,target:"_blank",children:n??(0,o.jsxs)("code",{children:[e,"()"]})})}},59897:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"evaluation/function-eval","title":"Function Evaluation","description":"Function evaluation allows you to assess Python functions directly without the overhead of logging models to MLflow. This lightweight approach is perfect for rapid prototyping, testing custom prediction logic, and evaluating complex business rules that may not fit the traditional model paradigm.","source":"@site/docs/classic-ml/evaluation/function-eval.mdx","sourceDirName":"evaluation","slug":"/evaluation/function-eval","permalink":"/docs/latest/ml/evaluation/function-eval","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Function Evaluation","sidebar_position":3},"sidebar":"classicMLSidebar","previous":{"title":"MLflow Evaluation","permalink":"/docs/latest/ml/evaluation/"},"next":{"title":"Dataset Evaluation","permalink":"/docs/latest/ml/evaluation/dataset-eval"}}');var a=t(74848),r=t(28453),o=(t(49374),t(11470)),s=t(19365);const l={title:"Function Evaluation",sidebar_position:3},c="Function Evaluation",p={},u=[{value:"Quick Start: Evaluating a Simple Function",id:"quick-start-evaluating-a-simple-function",level:2},{value:"Advanced Function Patterns",id:"advanced-function-patterns",level:2},{value:"Function Testing and Validation",id:"function-testing-and-validation",level:2},{value:"Key Use Cases and Benefits",id:"key-use-cases-and-benefits",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"function-evaluation",children:"Function Evaluation"})}),"\n",(0,a.jsx)(n.p,{children:"Function evaluation allows you to assess Python functions directly without the overhead of logging models to MLflow. This lightweight approach is perfect for rapid prototyping, testing custom prediction logic, and evaluating complex business rules that may not fit the traditional model paradigm."}),"\n",(0,a.jsx)(n.h2,{id:"quick-start-evaluating-a-simple-function",children:"Quick Start: Evaluating a Simple Function"}),"\n",(0,a.jsx)(n.p,{children:"The most straightforward function evaluation involves a callable that takes data and returns predictions:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Generate sample data\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train a model (we\'ll use this in our function)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n\n# Define a prediction function\ndef predict_function(input_data):\n    """Custom prediction function that can include business logic."""\n\n    # Get base model predictions\n    base_predictions = model.predict(input_data)\n\n    # Add custom business logic\n    # Example: Override predictions for specific conditions\n    feature_sum = input_data.sum(axis=1)\n    high_feature_mask = feature_sum > feature_sum.quantile(0.9)\n\n    # Custom rule: high feature sum values are always class 1\n    final_predictions = base_predictions.copy()\n    final_predictions[high_feature_mask] = 1\n\n    return final_predictions\n\n\n# Create evaluation dataset\neval_data = pd.DataFrame(X_test)\neval_data["target"] = y_test\n\nwith mlflow.start_run():\n    # Evaluate function directly - no model logging needed!\n    result = mlflow.evaluate(\n        predict_function,  # Function to evaluate\n        eval_data,  # Evaluation data\n        targets="target",  # Target column\n        model_type="classifier",  # Task type\n    )\n\n    print(f"Function Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"Function F1 Score: {result.metrics[\'f1_score\']:.3f}")\n'})}),"\n",(0,a.jsx)(n.p,{children:"This approach is ideal when:"}),"\n",(0,a.jsxs)("ul",{children:[(0,a.jsx)("li",{children:"You want to test functions quickly without model persistence"}),(0,a.jsx)("li",{children:"Your prediction logic includes complex business rules"}),(0,a.jsx)("li",{children:"You're prototyping custom algorithms or ensemble methods"}),(0,a.jsx)("li",{children:"You need to evaluate preprocessing + prediction pipelines"})]}),"\n",(0,a.jsx)(n.h2,{id:"advanced-function-patterns",children:"Advanced Function Patterns"}),"\n",(0,a.jsxs)(o.A,{children:[(0,a.jsxs)(s.A,{value:"pipeline-function",label:"Pipeline Functions",default:!0,children:[(0,a.jsx)(n.p,{children:"Evaluate complete data processing and prediction pipelines:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\ndef complete_pipeline_function(input_data):\n    """Function that includes preprocessing, feature engineering, and prediction."""\n\n    # Preprocessing\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(input_data)\n\n    # Feature engineering\n    pca = PCA(n_components=10)\n    pca_features = pca.fit_transform(scaled_data)\n\n    # Create additional features\n    feature_interactions = pca_features[:, 0] * pca_features[:, 1]\n    feature_ratios = pca_features[:, 0] / (pca_features[:, 1] + 1e-8)\n\n    # Combine features\n    enhanced_features = np.column_stack(\n        [\n            pca_features,\n            feature_interactions.reshape(-1, 1),\n            feature_ratios.reshape(-1, 1),\n        ]\n    )\n\n    # Model prediction\n    model = RandomForestClassifier(n_estimators=50, random_state=42)\n    # Note: In practice, you\'d fit this on training data separately\n    model.fit(enhanced_features, np.random.choice([0, 1], size=len(enhanced_features)))\n\n    predictions = model.predict(enhanced_features)\n    return predictions\n\n\nwith mlflow.start_run(run_name="Complete_Pipeline_Function"):\n    # Log pipeline configuration\n    mlflow.log_params(\n        {\n            "preprocessing": "StandardScaler + PCA",\n            "pca_components": 10,\n            "feature_engineering": "interactions + ratios",\n            "model": "RandomForest",\n        }\n    )\n\n    result = mlflow.evaluate(\n        complete_pipeline_function, eval_data, targets="target", model_type="classifier"\n    )\n\n    print(f"Pipeline Function Performance: {result.metrics[\'accuracy_score\']:.3f}")\n'})})]}),(0,a.jsxs)(s.A,{value:"ensemble-function",label:"Ensemble Functions",children:[(0,a.jsx)(n.p,{children:"Evaluate ensemble methods that combine multiple models:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndef ensemble_prediction_function(input_data):\n    """Ensemble function combining multiple base models."""\n\n    # Initialize base models\n    models = {\n        "rf": RandomForestClassifier(n_estimators=50, random_state=42),\n        "lr": LogisticRegression(random_state=42),\n        "svm": SVC(probability=True, random_state=42),\n        "nb": GaussianNB(),\n    }\n\n    # Train base models (in practice, these would be pre-trained)\n    predictions = {}\n    probabilities = {}\n\n    for name, model in models.items():\n        # Note: This is for demonstration - in practice, models would be pre-trained\n        model.fit(X_train, y_train)\n        predictions[name] = model.predict(input_data)\n\n        if hasattr(model, "predict_proba"):\n            probabilities[name] = model.predict_proba(input_data)[:, 1]\n        else:\n            probabilities[name] = predictions[name].astype(float)\n\n    # Ensemble strategies\n    # 1. Majority voting\n    pred_array = np.array(list(predictions.values()))\n    majority_vote = np.apply_along_axis(\n        lambda x: np.bincount(x).argmax(), axis=0, arr=pred_array\n    )\n\n    # 2. Weighted average of probabilities\n    prob_array = np.array(list(probabilities.values()))\n    weights = np.array([0.4, 0.3, 0.2, 0.1])  # RF, LR, SVM, NB\n    weighted_probs = np.average(prob_array, axis=0, weights=weights)\n    weighted_predictions = (weighted_probs > 0.5).astype(int)\n\n    # 3. Dynamic ensemble based on confidence\n    confidence_scores = np.std(prob_array, axis=0)\n    high_confidence_mask = confidence_scores < 0.2\n\n    final_predictions = majority_vote.copy()\n    final_predictions[high_confidence_mask] = weighted_predictions[high_confidence_mask]\n\n    return final_predictions\n\n\nwith mlflow.start_run(run_name="Ensemble_Function_Evaluation"):\n    # Log ensemble configuration\n    mlflow.log_params(\n        {\n            "base_models": "RF, LR, SVM, NB",\n            "ensemble_strategy": "dynamic_confidence_based",\n            "confidence_threshold": 0.2,\n            "weights": [0.4, 0.3, 0.2, 0.1],\n        }\n    )\n\n    result = mlflow.evaluate(\n        ensemble_prediction_function,\n        eval_data,\n        targets="target",\n        model_type="classifier",\n    )\n\n    print(f"Ensemble Function Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n'})})]}),(0,a.jsxs)(s.A,{value:"business-logic",label:"Business Logic Integration",children:[(0,a.jsx)(n.p,{children:"Evaluate functions that combine ML predictions with business rules:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def business_rule_function(input_data):\n    """Function combining ML predictions with business rules."""\n\n    # Get base ML predictions\n    ml_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    ml_model.fit(X_train, y_train)\n    ml_predictions = ml_model.predict(input_data)\n    ml_probabilities = ml_model.predict_proba(input_data)[:, 1]\n\n    # Business rules (example domain: loan approval)\n    feature_names = [f"feature_{i}" for i in range(input_data.shape[1])]\n\n    # Rule 1: High-risk features override ML prediction\n    high_risk_mask = input_data[:, 0] < -2  # Example: low credit score\n\n    # Rule 2: Low confidence ML predictions get conservative treatment\n    low_confidence_mask = np.abs(ml_probabilities - 0.5) < 0.1\n\n    # Rule 3: Combination rules for edge cases\n    edge_case_mask = (input_data[:, 1] > 2) & (input_data[:, 2] < -1)\n\n    # Apply business logic\n    final_predictions = ml_predictions.copy()\n\n    # Override high-risk cases\n    final_predictions[high_risk_mask] = 0  # Reject\n\n    # Conservative approach for low confidence\n    final_predictions[low_confidence_mask & (ml_probabilities < 0.6)] = 0\n\n    # Special handling for edge cases\n    final_predictions[edge_case_mask] = 1  # Approve with conditions\n\n    return final_predictions\n\n\nwith mlflow.start_run(run_name="Business_Rule_Function"):\n    # Log business rule configuration\n    mlflow.log_params(\n        {\n            "base_model": "RandomForest",\n            "business_rules": "high_risk_override, confidence_threshold, edge_cases",\n            "confidence_threshold": 0.6,\n            "high_risk_feature": "feature_0 < -2",\n        }\n    )\n\n    result = mlflow.evaluate(\n        business_rule_function, eval_data, targets="target", model_type="classifier"\n    )\n\n    # Calculate rule impact\n    ml_only_predictions = (\n        RandomForestClassifier(n_estimators=100, random_state=42)\n        .fit(X_train, y_train)\n        .predict(X_test)\n    )\n    rule_based_predictions = business_rule_function(X_test)\n\n    rule_changes = np.sum(ml_only_predictions != rule_based_predictions)\n    change_rate = rule_changes / len(ml_only_predictions)\n\n    mlflow.log_metrics(\n        {\n            "rule_changes": rule_changes,\n            "rule_change_rate": change_rate,\n            "ml_only_accuracy": (ml_only_predictions == y_test).mean(),\n        }\n    )\n\n    print(f"Business Rule Function Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"Rule Changes: {rule_changes} ({change_rate:.1%})")\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"function-testing-and-validation",children:"Function Testing and Validation"}),"\n",(0,a.jsxs)(o.A,{children:[(0,a.jsxs)(s.A,{value:"parameterized-testing",label:"Parameterized Testing",default:!0,children:[(0,a.jsx)(n.p,{children:"Test functions with different parameter configurations:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_parameterized_function(\n    threshold=0.5, use_scaling=True, feature_selection=None\n):\n    """Factory function that creates prediction functions with different parameters."""\n\n    def parameterized_prediction_function(input_data):\n        processed_data = input_data.copy()\n\n        # Optional scaling\n        if use_scaling:\n            from sklearn.preprocessing import StandardScaler\n\n            scaler = StandardScaler()\n            processed_data = scaler.fit_transform(processed_data)\n\n        # Optional feature selection\n        if feature_selection:\n            n_features = min(feature_selection, processed_data.shape[1])\n            processed_data = processed_data[:, :n_features]\n\n        # Model prediction\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\n        model.fit(\n            processed_data if use_scaling or feature_selection else X_train, y_train\n        )\n\n        probabilities = model.predict_proba(processed_data)[:, 1]\n        predictions = (probabilities > threshold).astype(int)\n\n        return predictions\n\n    return parameterized_prediction_function\n\n\n# Test different parameter combinations\nparameter_configs = [\n    {"threshold": 0.3, "use_scaling": True, "feature_selection": None},\n    {"threshold": 0.5, "use_scaling": True, "feature_selection": 10},\n    {"threshold": 0.7, "use_scaling": False, "feature_selection": None},\n    {"threshold": 0.5, "use_scaling": False, "feature_selection": 15},\n]\n\nresults = {}\n\nfor i, config in enumerate(parameter_configs):\n    with mlflow.start_run(run_name=f"Param_Config_{i+1}"):\n        # Log configuration\n        mlflow.log_params(config)\n\n        # Create and evaluate function\n        param_function = create_parameterized_function(**config)\n\n        result = mlflow.evaluate(\n            param_function, eval_data, targets="target", model_type="classifier"\n        )\n\n        results[f"config_{i+1}"] = {\n            "config": config,\n            "accuracy": result.metrics["accuracy_score"],\n            "f1_score": result.metrics["f1_score"],\n        }\n\n        print(f"Config {i+1}: Accuracy = {result.metrics[\'accuracy_score\']:.3f}")\n\n# Find best configuration\nbest_config = max(results.keys(), key=lambda k: results[k]["accuracy"])\nprint(f"Best configuration: {results[best_config][\'config\']}")\nprint(f"Best accuracy: {results[best_config][\'accuracy\']:.3f}")\n'})})]}),(0,a.jsxs)(s.A,{value:"production-monitoring",label:"Production Monitoring",children:[(0,a.jsx)(n.p,{children:"Create monitoring wrapper for production functions:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import time\n\n\ndef create_production_function_monitor(prediction_function):\n    """Create monitoring wrapper for production functions."""\n\n    def monitored_function(input_data):\n        """Wrapper that adds monitoring and health checks."""\n\n        start_time = time.time()\n\n        try:\n            # Input validation\n            if input_data is None or len(input_data) == 0:\n                raise ValueError("Empty input data")\n\n            # Health checks\n            if np.isnan(input_data).all():\n                raise ValueError("All input values are NaN")\n\n            if np.isinf(input_data).any():\n                raise ValueError("Input contains infinite values")\n\n            # Make prediction\n            predictions = prediction_function(input_data)\n\n            # Output validation\n            if predictions is None or len(predictions) == 0:\n                raise ValueError("Function returned empty predictions")\n\n            if len(predictions) != len(input_data):\n                raise ValueError(\n                    f"Prediction count mismatch: {len(predictions)} vs {len(input_data)}"\n                )\n\n            # Log successful execution\n            execution_time = time.time() - start_time\n\n            with mlflow.start_run(run_name="Function_Health_Check"):\n                mlflow.log_metrics(\n                    {\n                        "execution_time": execution_time,\n                        "input_samples": len(input_data),\n                        "predictions_generated": len(predictions),\n                        "nan_inputs": np.isnan(input_data).sum(),\n                        "success": 1,\n                    }\n                )\n\n                mlflow.log_params(\n                    {"function_status": "healthy", "timestamp": time.time()}\n                )\n\n            return predictions\n\n        except Exception as e:\n            # Log error\n            execution_time = time.time() - start_time\n\n            with mlflow.start_run(run_name="Function_Error"):\n                mlflow.log_metrics({"execution_time": execution_time, "success": 0})\n\n                mlflow.log_params(\n                    {\n                        "error": str(e),\n                        "function_status": "error",\n                        "timestamp": time.time(),\n                    }\n                )\n\n            raise\n\n    return monitored_function\n\n\n# Usage\nmonitored_function = create_production_function_monitor(predict_function)\n\n# Evaluate monitored function\nwith mlflow.start_run(run_name="Production_Function_Evaluation"):\n    result = mlflow.evaluate(\n        monitored_function, eval_data, targets="target", model_type="classifier"\n    )\n'})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"key-use-cases-and-benefits",children:"Key Use Cases and Benefits"}),"\n",(0,a.jsx)(n.p,{children:"Function evaluation in MLflow is particularly valuable for several scenarios:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Rapid Prototyping"})," - Perfect for testing prediction logic immediately without the overhead of model persistence. Developers can iterate quickly on algorithms and see results instantly."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Custom Business Logic"})," - Ideal for evaluating functions that combine machine learning predictions with domain-specific business rules, regulatory requirements, or operational constraints."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Ensemble Methods"})," - Excellent for testing custom ensemble approaches that may not fit standard model frameworks, including dynamic voting strategies and confidence-based combinations."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Pipeline Development"})," - Enables evaluation of complete data processing pipelines that include preprocessing, feature engineering, and prediction in a single function."]}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.p,{children:"When using function evaluation, consider these best practices:"}),"\n",(0,a.jsxs)("ul",{children:[(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Stateless Functions"}),": Design functions to be stateless when possible to ensure reproducible results"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Parameter Logging"}),": Always log function parameters and configuration for reproducibility"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Input Validation"}),": Include input validation and error handling in production functions"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Performance Monitoring"}),": Track execution time and resource usage for production functions"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Version Control"}),": Use MLflow's tagging and naming conventions to track function versions"]})]}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"Function evaluation in MLflow provides a lightweight, flexible approach to assessing prediction functions without the overhead of model persistence. This capability is essential for rapid prototyping, testing complex business logic, and evaluating custom algorithms that don't fit traditional model paradigms."}),"\n",(0,a.jsx)(n.p,{children:"Key advantages of function evaluation include:"}),"\n",(0,a.jsxs)("ul",{children:[(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Rapid Prototyping"}),": Test prediction logic immediately without model logging"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Business Logic Integration"}),": Evaluate functions that combine ML with business rules"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Custom Algorithms"}),": Assess non-standard prediction approaches and ensemble methods"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Development Workflow"}),": Seamlessly integrate with iterative development processes"]}),(0,a.jsxs)("li",{children:[(0,a.jsx)("strong",{children:"Performance Testing"}),": Benchmark and optimize function performance"]})]}),"\n",(0,a.jsx)(n.p,{children:"Whether you're developing custom ensemble methods, integrating business rules with ML predictions, or rapidly prototyping new approaches, MLflow's function evaluation provides the flexibility and insights needed for effective model development and testing."}),"\n",(0,a.jsx)(n.p,{children:"The lightweight nature of function evaluation makes it perfect for exploratory analysis, A/B testing different approaches, and validating custom prediction logic before committing to full model deployment workflows."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);