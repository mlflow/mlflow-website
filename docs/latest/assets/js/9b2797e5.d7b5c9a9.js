"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8329],{22450:(e,t,l)=>{l.d(t,{A:()=>n});const n=l.p+"assets/images/mistral-tracing-49245e61ba9c9c4969284b3d67c04058.png"},28453:(e,t,l)=>{l.d(t,{R:()=>r,x:()=>i});var n=l(96540);const a={},o=n.createContext(a);function r(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),n.createElement(o.Provider,{value:t},e.children)}},49374:(e,t,l)=>{l.d(t,{B:()=>s});l(96540);const n=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var a=l(86025),o=l(28774),r=l(74848);const i=e=>{const t=e.split(".");for(let l=t.length;l>0;l--){const e=t.slice(0,l).join(".");if(n[e])return e}return null};function s({fn:e,children:t}){const l=i(e);if(!l)return(0,r.jsx)(r.Fragment,{children:t});const s=(0,a.Ay)(`/${n[l]}#${e}`);return(0,r.jsx)(o.A,{to:s,target:"_blank",children:t??(0,r.jsxs)("code",{children:[e,"()"]})})}},87596:(e,t,l)=>{l.r(t),l.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>f,frontMatter:()=>i,metadata:()=>n,toc:()=>m});const n=JSON.parse('{"id":"tracing/integrations/listing/mistral","title":"Tracing Mistral","description":"Mistral tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/mistral.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/mistral","permalink":"/docs/latest/genai/tracing/integrations/listing/mistral","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":10.5,"frontMatter":{"sidebar_position":10.5,"sidebar_label":"Mistral"},"sidebar":"genAISidebar","previous":{"title":"Ollama","permalink":"/docs/latest/genai/tracing/integrations/listing/ollama"},"next":{"title":"Groq","permalink":"/docs/latest/genai/tracing/integrations/listing/groq"}}');var a=l(74848),o=l(28453),r=l(49374);const i={sidebar_position:10.5,sidebar_label:"Mistral"},s="Tracing Mistral",p={},m=[{value:"Example Usage",id:"example-usage",level:3},{value:"Token usage",id:"token-usage",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:2}];function c(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"tracing-mistral",children:"Tracing Mistral"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Mistral tracing via autolog",src:l(22450).A+"",width:"1684",height:"921"})}),"\n",(0,a.jsxs)(t.p,{children:["MLflow Tracing ensures observability for your interactions with Mistral AI models.\nWhen Mistral auto-tracing is enabled by calling the ",(0,a.jsx)(r.B,{fn:"mlflow.mistral.autolog"})," function,\nusage of the Mistral SDK will automatically record generated traces during interactive development."]}),"\n",(0,a.jsx)(t.p,{children:"Note that only synchronous calls to the Text Generation API are supported,\nand that asynchronous API and streaming methods are not traced."}),"\n",(0,a.jsx)(t.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import os\n\nfrom mistralai import Mistral\n\nimport mlflow\n\n# Turn on auto tracing for Mistral AI by calling mlflow.mistral.autolog()\nmlflow.mistral.autolog()\n\n# Configure your API key.\nclient = Mistral(api_key=os.environ["MISTRAL_API_KEY"])\n\n# Use the chat complete method to create new chat.\nchat_response = client.chat.complete(\n    model="mistral-small-latest",\n    messages=[\n        {\n            "role": "user",\n            "content": "Who is the best French painter? Answer in one short sentence.",\n        },\n    ],\n)\nprint(chat_response.choices[0].message)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"token-usage",children:"Token usage"}),"\n",(0,a.jsxs)(t.p,{children:["MLflow >= 3.2.0 supports token usage tracking for Mistral. The token usage for each LLM call will be logged in the ",(0,a.jsx)(t.code,{children:"mlflow.chat.tokenUsage"})," attribute. The total token usage throughout the trace will be\navailable in the ",(0,a.jsx)(t.code,{children:"token_usage"})," field of the trace info object."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import json\nimport mlflow\n\nmlflow.mistral.autolog()\n\n# Configure your API key.\nclient = Mistral(api_key=os.environ["MISTRAL_API_KEY"])\n\n# Use the chat complete method to create new chat.\nchat_response = client.chat.complete(\n    model="mistral-small-latest",\n    messages=[\n        {\n            "role": "user",\n            "content": "Who is the best French painter? Answer in one short sentence.",\n        },\n    ],\n)\n\n# Get the trace object just created\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Detailed usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 16\n  Output tokens: 25\n  Total tokens: 41\n\n== Detailed usage for each LLM call: ==\nChat.complete:\n  Input tokens: 16\n  Output tokens: 25\n  Total tokens: 41\n"})}),"\n",(0,a.jsx)(t.h2,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,a.jsxs)(t.p,{children:["Auto tracing for Mistral can be disabled globally by calling ",(0,a.jsx)(t.code,{children:"mlflow.mistral.autolog(disable=True)"})," or ",(0,a.jsx)(t.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function f(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);