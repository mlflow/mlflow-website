"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9843],{11470:(e,a,t)=>{t.d(a,{A:()=>j});var n=t(96540),s=t(34164),l=t(23104),r=t(56347),i=t(205),o=t(57485),d=t(31682),c=t(70679);function m(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:a,children:t}=e;return(0,n.useMemo)((()=>{const e=a??function(e){return m(e).map((({props:{value:e,label:a,attributes:t,default:n}})=>({value:e,label:a,attributes:t,default:n})))}(t);return function(e){const a=(0,d.XI)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,t])}function u({value:e,tabValues:a}){return a.some((a=>a.value===e))}function h({queryString:e=!1,groupId:a}){const t=(0,r.W6)(),s=function({queryString:e=!1,groupId:a}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:e,groupId:a});return[(0,o.aZ)(s),(0,n.useCallback)((e=>{if(!s)return;const a=new URLSearchParams(t.location.search);a.set(s,e),t.replace({...t.location,search:a.toString()})}),[s,t])]}function f(e){const{defaultValue:a,queryString:t=!1,groupId:s}=e,l=p(e),[r,o]=(0,n.useState)((()=>function({defaultValue:e,tabValues:a}){if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:a,tabValues:l}))),[d,m]=h({queryString:t,groupId:s}),[f,_]=function({groupId:e}){const a=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,c.Dv)(a);return[t,(0,n.useCallback)((e=>{a&&s.set(e)}),[a,s])]}({groupId:s}),g=(()=>{const e=d??f;return u({value:e,tabValues:l})?e:null})();(0,i.A)((()=>{g&&o(g)}),[g]);return{selectedValue:r,selectValue:(0,n.useCallback)((e=>{if(!u({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);o(e),m(e),_(e)}),[m,_,l]),tabValues:l}}var _=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var w=t(74848);function v({className:e,block:a,selectedValue:t,selectValue:n,tabValues:r}){const i=[],{blockElementScrollPositionUntilNextRender:o}=(0,l.a_)(),d=e=>{const a=e.currentTarget,s=i.indexOf(a),l=r[s].value;l!==t&&(o(a),n(l))},c=e=>{let a=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;a=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;a=i[t]??i[i.length-1];break}}a?.focus()};return(0,w.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":a},e),children:r.map((({value:e,label:a,attributes:n})=>(0,w.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:c,onClick:d,...n,className:(0,s.A)("tabs__item",g.tabItem,n?.className,{"tabs__item--active":t===e}),children:a??e},e)))})}function x({lazy:e,children:a,selectedValue:t}){const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(e){const e=l.find((e=>e.props.value===t));return e?(0,n.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,w.jsx)("div",{className:"margin-top--md",children:l.map(((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==t})))})}function y(e){const a=f(e);return(0,w.jsxs)("div",{className:(0,s.A)("tabs-container",g.tabList),children:[(0,w.jsx)(v,{...a,...e}),(0,w.jsx)(x,{...a,...e})]})}function j(e){const a=(0,_.A)();return(0,w.jsx)(y,{...e,children:m(e.children)},String(a))}},19365:(e,a,t)=>{t.d(a,{A:()=>r});t(96540);var n=t(34164);const s={tabItem:"tabItem_Ymn6"};var l=t(74848);function r({children:e,hidden:a,className:t}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,n.A)(s.tabItem,t),hidden:a,children:e})}},28453:(e,a,t)=>{t.d(a,{R:()=>r,x:()=>i});var n=t(96540);const s={},l=n.createContext(s);function r(e){const a=n.useContext(l);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(l.Provider,{value:a},e.children)}},29936:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/dataset-mlflow-ui-9c475f4c9e28b0c438dafb3b3e8c9db8.png"},49374:(e,a,t)=>{t.d(a,{B:()=>o});t(96540);const n=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var s=t(86025),l=t(28774),r=t(74848);const i=e=>{const a=e.split(".");for(let t=a.length;t>0;t--){const e=a.slice(0,t).join(".");if(n[e])return e}return null};function o({fn:e,children:a}){const t=i(e);if(!t)return(0,r.jsx)(r.Fragment,{children:a});const o=(0,s.Ay)(`/${n[t]}#${e}`);return(0,r.jsx)(l.A,{to:o,target:"_blank",children:a??(0,r.jsxs)("code",{children:[e,"()"]})})}},66046:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>m,contentTitle:()=>c,default:()=>h,frontMatter:()=>d,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"dataset/index","title":"MLflow Dataset Tracking","description":"The mlflow.data module is a comprehensive solution for dataset management throughout the machine learning lifecycle. It enables you to track, version, and manage datasets used in training, validation, and evaluation, providing complete lineage from raw data to model predictions.","source":"@site/docs/classic-ml/dataset/index.mdx","sourceDirName":"dataset","slug":"/dataset/","permalink":"/docs/latest/ml/dataset/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"Community-Managed Model Integrations","permalink":"/docs/latest/ml/community-model-flavors/"},"next":{"title":"MLflow Evaluation","permalink":"/docs/latest/ml/evaluation/"}}');var s=t(74848),l=t(28453),r=t(49374),i=t(11470),o=t(19365);const d={},c="MLflow Dataset Tracking",m={},p=[{value:"Why Dataset Tracking Matters",id:"why-dataset-tracking-matters",level:2},{value:"Core Components",id:"core-components",level:2},{value:"Dataset",id:"dataset",level:3},{value:"DatasetSource",id:"datasetsource",level:3},{value:"Quick Start: Basic Dataset Tracking",id:"quick-start-basic-dataset-tracking",level:2},{value:"Dataset Information and Metadata",id:"dataset-information-and-metadata",level:2},{value:"Dataset Sources and Lineage",id:"dataset-sources-and-lineage",level:2},{value:"Dataset Tracking in MLflow UI",id:"dataset-tracking-in-mlflow-ui",level:2},{value:"Integration with MLflow Evaluate",id:"integration-with-mlflow-evaluate",level:2},{value:"MLflow Evaluate Integration Example",id:"mlflow-evaluate-integration-example",level:2},{value:"Advanced Dataset Management",id:"advanced-dataset-management",level:2},{value:"Production Use Cases",id:"production-use-cases",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Key Benefits",id:"key-benefits",level:2}];function u(e){const a={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"mlflow-dataset-tracking",children:"MLflow Dataset Tracking"})}),"\n",(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"mlflow.data"})," module is a comprehensive solution for dataset management throughout the machine learning lifecycle. It enables you to track, version, and manage datasets used in training, validation, and evaluation, providing complete lineage from raw data to model predictions."]}),"\n",(0,s.jsx)(a.h2,{id:"why-dataset-tracking-matters",children:"Why Dataset Tracking Matters"}),"\n",(0,s.jsx)(a.p,{children:"Dataset tracking is essential for reproducible machine learning and provides several key benefits:"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"\u2022 Data Lineage"}),": Track the complete journey from raw data sources to model inputs\n",(0,s.jsx)(a.strong,{children:"\u2022 Reproducibility"}),": Ensure experiments can be reproduced with identical datasets\n",(0,s.jsx)(a.strong,{children:"\u2022 Version Control"}),": Manage different versions of datasets as they evolve\n",(0,s.jsx)(a.strong,{children:"\u2022 Collaboration"}),": Share datasets and their metadata across teams\n",(0,s.jsx)(a.strong,{children:"\u2022 Evaluation Integration"}),": Seamlessly integrate with MLflow's evaluation capabilities\n",(0,s.jsx)(a.strong,{children:"\u2022 Production Monitoring"}),": Track datasets used in production inference and evaluation"]}),"\n",(0,s.jsx)(a.h2,{id:"core-components",children:"Core Components"}),"\n",(0,s.jsx)(a.p,{children:"MLflow's dataset tracking revolves around two main abstractions:"}),"\n",(0,s.jsx)(a.h3,{id:"dataset",children:"Dataset"}),"\n",(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"Dataset"})," abstraction is a metadata tracking object that holds comprehensive information about a logged dataset. The information stored within a ",(0,s.jsx)(a.code,{children:"Dataset"})," object includes:"]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.strong,{children:"Core Properties:"})}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Name"}),': Descriptive identifier for the dataset (defaults to "dataset" if not specified)']}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Digest"}),": Unique hash/fingerprint for dataset identification (automatically computed)"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Source"}),": DatasetSource containing lineage information to the original data location"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Schema"}),": Optional dataset schema (implementation-specific, e.g., MLflow Schema)"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Profile"}),": Optional summary statistics (implementation-specific, e.g., row count, column stats)"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.strong,{children:"Supported Dataset Types:"})}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(r.B,{fn:"mlflow.data.pandas_dataset.PandasDataset",children:(0,s.jsx)(a.code,{children:"PandasDataset"})})," - For Pandas DataFrames"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(r.B,{fn:"mlflow.data.spark_dataset.SparkDataset",children:(0,s.jsx)(a.code,{children:"SparkDataset"})})," - For Apache Spark DataFrames"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(r.B,{fn:"mlflow.data.numpy_dataset.NumpyDataset",children:(0,s.jsx)(a.code,{children:"NumpyDataset"})})," - For NumPy arrays"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(r.B,{fn:"mlflow.data.huggingface_dataset.HuggingFaceDataset",children:(0,s.jsx)(a.code,{children:"HuggingFaceDataset"})})," - For Hugging Face datasets"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(r.B,{fn:"mlflow.data.tensorflow_dataset.TensorFlowDataset",children:(0,s.jsx)(a.code,{children:"TensorFlowDataset"})})," - For TensorFlow datasets"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(r.B,{fn:"mlflow.data.meta_dataset.MetaDataset",children:(0,s.jsx)(a.code,{children:"MetaDataset"})})," - For metadata-only datasets (no actual data storage)"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.strong,{children:"Special Dataset Types:"})}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"EvaluationDataset"})," - Internal dataset type used specifically with ",(0,s.jsx)(a.code,{children:"mlflow.evaluate()"})," for model evaluation workflows"]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"datasetsource",children:"DatasetSource"}),"\n",(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"DatasetSource"})," component provides linked lineage to the original source of the data, whether it's a file URL, S3 bucket, database table, or any other data source. This ensures you can always trace back to where your data originated."]}),"\n",(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"DatasetSource"})," can be retrieved using the ",(0,s.jsx)(r.B,{fn:"mlflow.data.get_source"})," API, which accepts instances of ",(0,s.jsx)(a.code,{children:"Dataset"}),", ",(0,s.jsx)(a.code,{children:"DatasetEntity"}),", or ",(0,s.jsx)(a.code,{children:"DatasetInput"}),"."]}),"\n",(0,s.jsx)(a.h2,{id:"quick-start-basic-dataset-tracking",children:"Quick Start: Basic Dataset Tracking"}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsxs)(o.A,{value:"simple-example",label:"Simple Example",default:!0,children:[(0,s.jsx)(a.p,{children:"Here's how to get started with basic dataset tracking:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'import mlflow.data\nimport pandas as pd\n\n# Load your data\ndataset_source_url = "https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv"\nraw_data = pd.read_csv(dataset_source_url, delimiter=";")\n\n# Create a Dataset object\ndataset = mlflow.data.from_pandas(\n    raw_data, source=dataset_source_url, name="wine-quality-white", targets="quality"\n)\n\n# Log the dataset to an MLflow run\nwith mlflow.start_run():\n    mlflow.log_input(dataset, context="training")\n\n    # Your training code here\n    # model = train_model(raw_data)\n    # mlflow.sklearn.log_model(model, "model")\n'})})]}),(0,s.jsxs)(o.A,{value:"metadata-only",label:"Metadata-Only Datasets",children:[(0,s.jsx)(a.p,{children:"For cases where you only want to log dataset metadata without the actual data:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'import mlflow.data\nfrom mlflow.data.meta_dataset import MetaDataset\nfrom mlflow.data.http_dataset_source import HTTPDatasetSource\nfrom mlflow.types import Schema, ColSpec, DataType\n\n# Create a metadata-only dataset for a remote data source\nsource = HTTPDatasetSource(\n    url="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"\n)\n\n# Option 1: Simple metadata dataset\nmeta_dataset = MetaDataset(source=source, name="imdb-sentiment-dataset")\n\n# Option 2: With schema information\nschema = Schema(\n    [\n        ColSpec(type=DataType.string, name="text"),\n        ColSpec(type=DataType.integer, name="label"),\n    ]\n)\n\nmeta_dataset_with_schema = MetaDataset(\n    source=source, name="imdb-sentiment-dataset-with-schema", schema=schema\n)\n\nwith mlflow.start_run():\n    # Log metadata-only dataset (no actual data stored)\n    mlflow.log_input(meta_dataset_with_schema, context="external_data")\n\n    # The dataset reference and schema are logged, but not the data itself\n    print(f"Logged dataset: {meta_dataset_with_schema.name}")\n    print(f"Data source: {meta_dataset_with_schema.source}")\n'})}),(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Use Cases for MetaDataset:"}),"\nReference datasets hosted on external servers or cloud storage, large datasets where you only want to track metadata and lineage, datasets with restricted access where actual data cannot be stored, and public datasets available via URLs that don't need to be duplicated."]})]}),(0,s.jsxs)(o.A,{value:"with-splits",label:"With Data Splits",children:[(0,s.jsx)(a.p,{children:"Track training, validation, and test splits separately:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'import mlflow.data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load and split your data\ndata = pd.read_csv("your_dataset.csv")\nX = data.drop("target", axis=1)\ny = data["target"]\n\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.4, random_state=42\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42\n)\n\n# Create dataset objects for each split\ntrain_data = pd.concat([X_train, y_train], axis=1)\nval_data = pd.concat([X_val, y_val], axis=1)\ntest_data = pd.concat([X_test, y_test], axis=1)\n\ntrain_dataset = mlflow.data.from_pandas(\n    train_data, source="your_dataset.csv", name="wine-quality-train", targets="target"\n)\nval_dataset = mlflow.data.from_pandas(\n    val_data, source="your_dataset.csv", name="wine-quality-val", targets="target"\n)\ntest_dataset = mlflow.data.from_pandas(\n    test_data, source="your_dataset.csv", name="wine-quality-test", targets="target"\n)\n\nwith mlflow.start_run():\n    # Log all dataset splits\n    mlflow.log_input(train_dataset, context="training")\n    mlflow.log_input(val_dataset, context="validation")\n    mlflow.log_input(test_dataset, context="testing")\n'})})]}),(0,s.jsxs)(o.A,{value:"with-predictions",label:"With Predictions",children:[(0,s.jsx)(a.p,{children:"Track datasets that include model predictions for evaluation:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'import mlflow.data\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train a model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Generate predictions\npredictions = model.predict(X_test)\nprediction_probs = model.predict_proba(X_test)[:, 1]\n\n# Create evaluation dataset with predictions\neval_data = X_test.copy()\neval_data["target"] = y_test\neval_data["prediction"] = predictions\neval_data["prediction_proba"] = prediction_probs\n\n# Create dataset with predictions specified\neval_dataset = mlflow.data.from_pandas(\n    eval_data,\n    source="your_dataset.csv",\n    name="wine-quality-evaluation",\n    targets="target",\n    predictions="prediction",\n)\n\nwith mlflow.start_run():\n    mlflow.log_input(eval_dataset, context="evaluation")\n\n    # This dataset can now be used directly with mlflow.evaluate()\n    result = mlflow.evaluate(data=eval_dataset, model_type="classifier")\n'})})]})]}),"\n",(0,s.jsx)(a.h2,{id:"dataset-information-and-metadata",children:"Dataset Information and Metadata"}),"\n",(0,s.jsx)(a.p,{children:"When you create a dataset, MLflow automatically captures rich metadata:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'# Access dataset metadata\nprint(f"Dataset name: {dataset.name}")  # Defaults to "dataset" if not specified\nprint(\n    f"Dataset digest: {dataset.digest}"\n)  # Unique hash identifier (computed automatically)\nprint(f"Dataset source: {dataset.source}")  # DatasetSource object\nprint(\n    f"Dataset profile: {dataset.profile}"\n)  # Optional: implementation-specific statistics\nprint(f"Dataset schema: {dataset.schema}")  # Optional: implementation-specific schema\n'})}),"\n",(0,s.jsx)(a.p,{children:"Example output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:'Dataset name: wine-quality-white\nDataset digest: 2a1e42c4\nDataset profile: {"num_rows": 4898, "num_elements": 58776}\nDataset schema: {"mlflow_colspec": [\n    {"type": "double", "name": "fixed acidity"},\n    {"type": "double", "name": "volatile acidity"},\n    ...\n    {"type": "long", "name": "quality"}\n]}\nDataset source: <DatasetSource object>\n'})}),"\n",(0,s.jsx)(a.admonition,{title:"Dataset Properties",type:"note",children:(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"profile"})," and ",(0,s.jsx)(a.code,{children:"schema"})," properties are implementation-specific and may vary depending on the dataset type (PandasDataset, SparkDataset, etc.). Some dataset types may return ",(0,s.jsx)(a.code,{children:"None"})," for these properties."]})}),"\n",(0,s.jsx)(a.h2,{id:"dataset-sources-and-lineage",children:"Dataset Sources and Lineage"}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsxs)(o.A,{value:"various-sources",label:"Various Data Sources",default:!0,children:[(0,s.jsx)(a.p,{children:"MLflow supports datasets from various sources:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'# From local file\nlocal_dataset = mlflow.data.from_pandas(\n    df, source="/path/to/local/file.csv", name="local-data"\n)\n\n# From cloud storage\ns3_dataset = mlflow.data.from_pandas(\n    df, source="s3://bucket/data.parquet", name="s3-data"\n)\n\n# From database\ndb_dataset = mlflow.data.from_pandas(\n    df, source="postgresql://user:pass@host/db", name="db-data"\n)\n\n# From URL\nurl_dataset = mlflow.data.from_pandas(\n    df, source="https://example.com/data.csv", name="web-data"\n)\n'})})]}),(0,s.jsxs)(o.A,{value:"retrieving-sources",label:"Retrieving Data Sources",children:[(0,s.jsx)(a.p,{children:"You can retrieve and reload data from logged datasets:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'# After logging a dataset, retrieve it later\nwith mlflow.start_run() as run:\n    mlflow.log_input(dataset, context="training")\n\n# Retrieve the run and dataset\nlogged_run = mlflow.get_run(run.info.run_id)\nlogged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n\n# Get the data source and reload data\ndataset_source = mlflow.data.get_source(logged_dataset)\nlocal_path = dataset_source.load()  # Downloads to local temp file\n\n# Reload the data\nreloaded_data = pd.read_csv(local_path, delimiter=";")\nprint(f"Reloaded {len(reloaded_data)} rows from {local_path}")\n'})})]}),(0,s.jsxs)(o.A,{value:"delta-tables",label:"Delta Tables",children:[(0,s.jsx)(a.p,{children:"Special support for Delta Lake tables:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'# For Delta tables (requires delta-lake package)\ndelta_dataset = mlflow.data.from_spark(\n    spark_df, source="delta://path/to/delta/table", name="delta-table-data"\n)\n\n# Can also specify version\nversioned_delta_dataset = mlflow.data.from_spark(\n    spark_df, source="delta://path/to/delta/table@v1", name="delta-table-v1"\n)\n'})})]})]}),"\n",(0,s.jsx)(a.h2,{id:"dataset-tracking-in-mlflow-ui",children:"Dataset Tracking in MLflow UI"}),"\n",(0,s.jsx)(a.p,{children:"When you log datasets to MLflow runs, they appear in the MLflow UI with comprehensive metadata. You can view dataset information, schema, and lineage directly in the interface."}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"Dataset in MLflow UI",src:t(29936).A+"",width:"1476",height:"763"})}),"\n",(0,s.jsx)(a.p,{children:"The UI displays:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Dataset name and digest"}),"\n",(0,s.jsx)(a.li,{children:"Schema information with column types"}),"\n",(0,s.jsx)(a.li,{children:"Profile statistics (row counts, etc.)"}),"\n",(0,s.jsx)(a.li,{children:"Source lineage information"}),"\n",(0,s.jsx)(a.li,{children:"Context in which the dataset was used"}),"\n"]}),"\n",(0,s.jsx)(a.h2,{id:"integration-with-mlflow-evaluate",children:"Integration with MLflow Evaluate"}),"\n",(0,s.jsxs)(a.p,{children:["One of the most powerful features of MLflow datasets is their seamless integration with MLflow's evaluation capabilities. MLflow automatically converts various data types to ",(0,s.jsx)(a.code,{children:"EvaluationDataset"})," objects internally when using ",(0,s.jsx)(a.code,{children:"mlflow.evaluate()"}),"."]}),"\n",(0,s.jsx)(a.admonition,{title:"EvaluationDataset",type:"info",children:(0,s.jsxs)(a.p,{children:["MLflow uses an internal ",(0,s.jsx)(a.code,{children:"EvaluationDataset"})," class when working with ",(0,s.jsx)(a.code,{children:"mlflow.evaluate()"}),". This dataset type is automatically created from your input data and provides optimized hashing and metadata tracking specifically for evaluation workflows."]})}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsxs)(o.A,{value:"basic-evaluation",label:"Basic Evaluation",default:!0,children:[(0,s.jsx)(a.p,{children:"Use datasets directly with MLflow evaluate:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'import mlflow\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data and train model\ndata = pd.read_csv("classification_data.csv")\nX = data.drop("target", axis=1)\ny = data["target"]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data["target"] = y_test\n\neval_dataset = mlflow.data.from_pandas(\n    eval_data, targets="target", name="evaluation-set"\n)\n\nwith mlflow.start_run():\n    # Log model\n    mlflow.sklearn.log_model(model, name="model", input_example=X_test)\n\n    # Evaluate using the dataset\n    result = mlflow.evaluate(\n        model="runs:/{}/model".format(mlflow.active_run().info.run_id),\n        data=eval_dataset,\n        model_type="classifier",\n    )\n\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n'})})]}),(0,s.jsxs)(o.A,{value:"static-predictions",label:"Static Predictions",children:[(0,s.jsx)(a.p,{children:"Evaluate pre-computed predictions without re-running the model:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'# Load previously computed predictions\nbatch_predictions = pd.read_parquet("batch_predictions.parquet")\n\n# Create dataset with existing predictions\nprediction_dataset = mlflow.data.from_pandas(\n    batch_predictions,\n    source="batch_predictions.parquet",\n    targets="true_label",\n    predictions="model_prediction",\n    name="batch-evaluation",\n)\n\nwith mlflow.start_run():\n    # Evaluate static predictions (no model needed!)\n    result = mlflow.evaluate(data=prediction_dataset, model_type="classifier")\n\n    # Dataset is automatically logged to the run\n    print("Evaluation completed on static predictions")\n'})})]}),(0,s.jsxs)(o.A,{value:"comparative-evaluation",label:"Comparative Evaluation",children:[(0,s.jsx)(a.p,{children:"Compare multiple models or datasets:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'def compare_model_performance(datasets_dict):\n    """Compare model performance across multiple evaluation datasets."""\n\n    results = {}\n\n    with mlflow.start_run(run_name="Model_Comparison"):\n        for dataset_name, dataset in datasets_dict.items():\n            with mlflow.start_run(run_name=f"Eval_{dataset_name}", nested=True):\n                result = mlflow.evaluate(\n                    model=model, data=dataset, model_type="classifier"\n                )\n                results[dataset_name] = result.metrics\n\n                # Log dataset metadata\n                mlflow.log_params(\n                    {"dataset_name": dataset_name, "dataset_size": len(dataset.df)}\n                )\n\n    return results\n\n\n# Usage\nevaluation_datasets = {\n    "validation": validation_dataset,\n    "test": test_dataset,\n    "holdout": holdout_dataset,\n}\n\ncomparison_results = compare_model_performance(evaluation_datasets)\n'})})]})]}),"\n",(0,s.jsx)(a.h2,{id:"mlflow-evaluate-integration-example",children:"MLflow Evaluate Integration Example"}),"\n",(0,s.jsx)(a.p,{children:"Here's a complete example showing how datasets integrate with MLflow's evaluation capabilities:"}),"\n",(0,s.jsx)("div",{className:"center-div",style:{width:"80%"},children:(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"Dataset Evaluation in MLflow UI",src:t(67429).A+"",width:"1034",height:"1003"})})}),"\n",(0,s.jsx)(a.p,{children:"The evaluation run shows how the dataset, model, metrics, and evaluation artifacts (like confusion matrices) are all logged together, providing a complete view of the evaluation process."}),"\n",(0,s.jsx)(a.h2,{id:"advanced-dataset-management",children:"Advanced Dataset Management"}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsxs)(o.A,{value:"versioning",label:"Dataset Versioning",default:!0,children:[(0,s.jsx)(a.p,{children:"Track dataset versions as they evolve:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'def create_versioned_dataset(data, version, base_name="customer-data"):\n    """Create a versioned dataset with metadata."""\n\n    dataset = mlflow.data.from_pandas(\n        data,\n        source=f"data_pipeline_v{version}",\n        name=f"{base_name}-v{version}",\n        targets="target",\n    )\n\n    with mlflow.start_run(run_name=f"Dataset_Version_{version}"):\n        mlflow.log_input(dataset, context="versioning")\n\n        # Log version metadata\n        mlflow.log_params(\n            {\n                "dataset_version": version,\n                "data_size": len(data),\n                "features_count": len(data.columns) - 1,\n                "target_distribution": data["target"].value_counts().to_dict(),\n            }\n        )\n\n        # Log data quality metrics\n        mlflow.log_metrics(\n            {\n                "missing_values_pct": (data.isnull().sum().sum() / data.size) * 100,\n                "duplicate_rows": data.duplicated().sum(),\n                "target_balance": data["target"].std(),\n            }\n        )\n\n    return dataset\n\n\n# Create multiple versions\nv1_dataset = create_versioned_dataset(data_v1, "1.0")\nv2_dataset = create_versioned_dataset(data_v2, "2.0")\nv3_dataset = create_versioned_dataset(data_v3, "3.0")\n'})})]}),(0,s.jsxs)(o.A,{value:"quality-monitoring",label:"Data Quality Monitoring",children:[(0,s.jsx)(a.p,{children:"Monitor data quality and drift over time:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'def monitor_dataset_quality(dataset, reference_dataset=None):\n    """Monitor dataset quality and compare against reference if provided."""\n\n    data = dataset.df if hasattr(dataset, "df") else dataset\n\n    quality_metrics = {\n        "total_rows": len(data),\n        "total_columns": len(data.columns),\n        "missing_values_total": data.isnull().sum().sum(),\n        "missing_values_pct": (data.isnull().sum().sum() / data.size) * 100,\n        "duplicate_rows": data.duplicated().sum(),\n        "duplicate_rows_pct": (data.duplicated().sum() / len(data)) * 100,\n    }\n\n    # Numeric column statistics\n    numeric_cols = data.select_dtypes(include=["number"]).columns\n    for col in numeric_cols:\n        quality_metrics.update(\n            {\n                f"{col}_mean": data[col].mean(),\n                f"{col}_std": data[col].std(),\n                f"{col}_missing_pct": (data[col].isnull().sum() / len(data)) * 100,\n            }\n        )\n\n    with mlflow.start_run(run_name="Data_Quality_Check"):\n        mlflow.log_input(dataset, context="quality_monitoring")\n        mlflow.log_metrics(quality_metrics)\n\n        # Compare with reference dataset if provided\n        if reference_dataset is not None:\n            ref_data = (\n                reference_dataset.df\n                if hasattr(reference_dataset, "df")\n                else reference_dataset\n            )\n\n            # Basic drift detection\n            drift_metrics = {}\n            for col in numeric_cols:\n                if col in ref_data.columns:\n                    mean_diff = abs(data[col].mean() - ref_data[col].mean())\n                    std_diff = abs(data[col].std() - ref_data[col].std())\n                    drift_metrics.update(\n                        {f"{col}_mean_drift": mean_diff, f"{col}_std_drift": std_diff}\n                    )\n\n            mlflow.log_metrics(drift_metrics)\n\n    return quality_metrics\n\n\n# Usage\nquality_report = monitor_dataset_quality(current_dataset, reference_dataset)\n'})})]}),(0,s.jsxs)(o.A,{value:"automated-tracking",label:"Automated Tracking",children:[(0,s.jsx)(a.p,{children:"Set up automated dataset tracking in your ML pipelines:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'class DatasetTracker:\n    """Automated dataset tracking for ML pipelines."""\n\n    def __init__(self, experiment_name="Dataset_Tracking"):\n        mlflow.set_experiment(experiment_name)\n        self.tracked_datasets = {}\n\n    def track_dataset(self, data, stage, source=None, name=None, **metadata):\n        """Track a dataset at a specific pipeline stage."""\n\n        dataset_name = name or f"{stage}_dataset"\n\n        dataset = mlflow.data.from_pandas(\n            data, source=source or f"pipeline_stage_{stage}", name=dataset_name\n        )\n\n        with mlflow.start_run(run_name=f"Pipeline_{stage}"):\n            mlflow.log_input(dataset, context=stage)\n\n            # Log stage metadata\n            mlflow.log_params(\n                {"pipeline_stage": stage, "dataset_name": dataset_name, **metadata}\n            )\n\n            # Automatic quality metrics\n            quality_metrics = {\n                "rows": len(data),\n                "columns": len(data.columns),\n                "missing_pct": (data.isnull().sum().sum() / data.size) * 100,\n            }\n            mlflow.log_metrics(quality_metrics)\n\n        self.tracked_datasets[stage] = dataset\n        return dataset\n\n    def compare_stages(self, stage1, stage2):\n        """Compare datasets between pipeline stages."""\n\n        if stage1 not in self.tracked_datasets or stage2 not in self.tracked_datasets:\n            raise ValueError("Both stages must be tracked first")\n\n        ds1 = self.tracked_datasets[stage1]\n        ds2 = self.tracked_datasets[stage2]\n\n        # Implementation of comparison logic\n        with mlflow.start_run(run_name=f"Compare_{stage1}_vs_{stage2}"):\n            comparison_metrics = {\n                "row_diff": len(ds2.df) - len(ds1.df),\n                "column_diff": len(ds2.df.columns) - len(ds1.df.columns),\n            }\n            mlflow.log_metrics(comparison_metrics)\n\n\n# Usage in a pipeline\ntracker = DatasetTracker()\n\n# Track at each stage\nraw_dataset = tracker.track_dataset(raw_data, "raw", source="raw_data.csv")\ncleaned_dataset = tracker.track_dataset(\n    cleaned_data, "cleaned", source="cleaned_data.csv"\n)\nfeatures_dataset = tracker.track_dataset(\n    feature_data, "features", source="feature_engineering"\n)\n\n# Compare stages\ntracker.compare_stages("raw", "cleaned")\ntracker.compare_stages("cleaned", "features")\n'})})]})]}),"\n",(0,s.jsx)(a.h2,{id:"production-use-cases",children:"Production Use Cases"}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsxs)(o.A,{value:"batch-monitoring",label:"Batch Prediction Monitoring",default:!0,children:[(0,s.jsx)(a.p,{children:"Monitor datasets used in production batch prediction:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'def monitor_batch_predictions(batch_data, model_version, date):\n    """Monitor production batch prediction datasets."""\n\n    # Create dataset for batch predictions\n    batch_dataset = mlflow.data.from_pandas(\n        batch_data,\n        source=f"production_batch_{date}",\n        name=f"batch_predictions_{date}",\n        targets="true_label" if "true_label" in batch_data.columns else None,\n        predictions="prediction" if "prediction" in batch_data.columns else None,\n    )\n\n    with mlflow.start_run(run_name=f"Batch_Monitor_{date}"):\n        mlflow.log_input(batch_dataset, context="production_batch")\n\n        # Log production metadata\n        mlflow.log_params(\n            {\n                "batch_date": date,\n                "model_version": model_version,\n                "batch_size": len(batch_data),\n                "has_ground_truth": "true_label" in batch_data.columns,\n            }\n        )\n\n        # Monitor prediction distribution\n        if "prediction" in batch_data.columns:\n            pred_metrics = {\n                "prediction_mean": batch_data["prediction"].mean(),\n                "prediction_std": batch_data["prediction"].std(),\n                "unique_predictions": batch_data["prediction"].nunique(),\n            }\n            mlflow.log_metrics(pred_metrics)\n\n        # Evaluate if ground truth is available\n        if all(col in batch_data.columns for col in ["prediction", "true_label"]):\n            result = mlflow.evaluate(data=batch_dataset, model_type="classifier")\n            print(f"Batch accuracy: {result.metrics.get(\'accuracy_score\', \'N/A\')}")\n\n    return batch_dataset\n\n\n# Usage\nbatch_dataset = monitor_batch_predictions(daily_batch_data, "v2.1", "2024-01-15")\n'})})]}),(0,s.jsxs)(o.A,{value:"ab-testing",label:"A/B Testing Datasets",children:[(0,s.jsx)(a.p,{children:"Track datasets used in A/B testing scenarios:"}),(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'def track_ab_test_data(control_data, treatment_data, test_name, test_date):\n    """Track datasets for A/B testing experiments."""\n\n    # Create datasets for each variant\n    control_dataset = mlflow.data.from_pandas(\n        control_data,\n        source=f"ab_test_{test_name}_control",\n        name=f"{test_name}_control_{test_date}",\n        targets="conversion" if "conversion" in control_data.columns else None,\n    )\n\n    treatment_dataset = mlflow.data.from_pandas(\n        treatment_data,\n        source=f"ab_test_{test_name}_treatment",\n        name=f"{test_name}_treatment_{test_date}",\n        targets="conversion" if "conversion" in treatment_data.columns else None,\n    )\n\n    with mlflow.start_run(run_name=f"AB_Test_{test_name}_{test_date}"):\n        # Log both datasets\n        mlflow.log_input(control_dataset, context="ab_test_control")\n        mlflow.log_input(treatment_dataset, context="ab_test_treatment")\n\n        # Log test parameters\n        mlflow.log_params(\n            {\n                "test_name": test_name,\n                "test_date": test_date,\n                "control_size": len(control_data),\n                "treatment_size": len(treatment_data),\n                "total_size": len(control_data) + len(treatment_data),\n            }\n        )\n\n        # Calculate and log comparison metrics\n        if (\n            "conversion" in control_data.columns\n            and "conversion" in treatment_data.columns\n        ):\n            control_rate = control_data["conversion"].mean()\n            treatment_rate = treatment_data["conversion"].mean()\n            lift = (treatment_rate - control_rate) / control_rate * 100\n\n            mlflow.log_metrics(\n                {\n                    "control_conversion_rate": control_rate,\n                    "treatment_conversion_rate": treatment_rate,\n                    "lift_percentage": lift,\n                }\n            )\n\n    return control_dataset, treatment_dataset\n\n\n# Usage\ncontrol_ds, treatment_ds = track_ab_test_data(\n    control_group_data, treatment_group_data, "new_recommendation_model", "2024-01-15"\n)\n'})})]})]}),"\n",(0,s.jsx)(a.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(a.p,{children:"When working with MLflow datasets, follow these best practices:"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Data Quality"}),": Always validate data quality before logging datasets. Check for missing values, duplicates, and data types."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Naming Conventions"}),": Use consistent, descriptive names for datasets that include version information and context."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Source Documentation"}),": Always specify meaningful source URLs or identifiers that allow you to trace back to the original data."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Context Specification"}),': Use clear context labels when logging datasets (e.g., "training", "validation", "evaluation", "production").']}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Metadata Logging"}),": Include relevant metadata about data collection, preprocessing steps, and data characteristics."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Version Control"}),": Track dataset versions explicitly, especially when data preprocessing or collection methods change."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Digest Computation"}),": Dataset digests are computed differently for different dataset types:"]}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Standard datasets"}),": Based on data content and structure"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"MetaDataset"}),": Based on metadata only (name, source, schema) - no actual data hashing"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"EvaluationDataset"}),": Optimized hashing using sample rows for large datasets"]}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Source Flexibility"}),": DatasetSource supports various source types including HTTP URLs, file paths, database connections, and cloud storage locations."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Evaluation Integration"}),": Design datasets with evaluation in mind by clearly specifying target and prediction columns."]}),"\n",(0,s.jsx)(a.h2,{id:"key-benefits",children:"Key Benefits"}),"\n",(0,s.jsx)(a.p,{children:"MLflow dataset tracking provides several key advantages for ML teams:"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Reproducibility"}),": Ensure experiments can be reproduced with identical datasets, even as data sources evolve."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Lineage Tracking"}),": Maintain complete data lineage from source to model predictions, enabling better debugging and compliance."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Collaboration"}),": Share datasets and their metadata across team members with consistent interfaces."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Evaluation Integration"}),": Seamlessly integrate with MLflow's evaluation capabilities for comprehensive model assessment."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Production Monitoring"}),": Track datasets used in production systems for performance monitoring and data drift detection."]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Quality Assurance"}),": Automatically capture data quality metrics and monitor changes over time."]}),"\n",(0,s.jsx)(a.p,{children:"Whether you're tracking training datasets, managing evaluation data, or monitoring production batch predictions, MLflow's dataset tracking capabilities provide the foundation for reliable, reproducible machine learning workflows."})]})}function h(e={}){const{wrapper:a}={...(0,l.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},67429:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/dataset-evaluate-0d449786c6575ea6b14818f6a3a92855.png"}}]);