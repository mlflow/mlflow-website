"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["2570"],{99194(e,n,t){t.r(n),t.d(n,{metadata:()=>i,default:()=>j,frontMatter:()=>g,contentTitle:()=>p,toc:()=>x,assets:()=>m});var i=JSON.parse('{"id":"tracing/prod-tracing","title":"Production Tracing and Monitoring","description":"When you deploy an agent or LLM application to production, real users behave differently than test data\u2014they find edge cases, ask unexpected questions, and expose issues you didn\'t anticipate. This guide covers how to configure MLflow Tracing for production environments\u2014including automatic (online) quality evaluation\u2014to catch these issues early and continuously improve your application.","source":"@site/docs/genai/tracing/prod-tracing.mdx","sourceDirName":"tracing","slug":"/tracing/prod-tracing","permalink":"/mlflow-website/docs/latest/genai/tracing/prod-tracing","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Delete Traces","permalink":"/mlflow-website/docs/latest/genai/tracing/observe-with-traces/delete-traces"},"next":{"title":"Production Tracing SDK","permalink":"/mlflow-website/docs/latest/genai/tracing/lightweight-sdk"}}'),r=t(74848),a=t(28453),s=t(3340),o=t(66497),l=t(10440),c=t(77541),d=t(46858),u=t(98445),h=t(93893);let g={},p="Production Tracing and Monitoring",m={},x=[{value:"Production Checklist",id:"production-checklist",level:2},{value:"Setting Up Tracing for Production Endpoints",id:"setting-up-tracing-for-production-endpoints",level:2},{value:"Using the Production Tracing SDK",id:"using-the-production-tracing-sdk",level:4},{value:"Automatic (Online) Quality Evaluation",id:"automatic-online-quality-evaluation",level:2},{value:"Setting Up Production Judges",id:"setting-up-production-judges",level:3},{value:"Production Tracing Configurations",id:"production-tracing-configurations",level:2},{value:"Asynchronous Trace Logging",id:"asynchronous-trace-logging",level:3},{value:"Sampling Traces",id:"sampling-traces",level:3},{value:"Adding Context to Production Traces",id:"adding-context-to-production-traces",level:2},{value:"Tracking Request, Session, and User Context",id:"tracking-request-session-and-user-context",level:3},{value:"Feedback Collection",id:"feedback-collection",level:3},{value:"Querying Traces with Context",id:"querying-traces-with-context",level:3},{value:"Next Steps",id:"next-steps",level:2}];function f(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"production-tracing-and-monitoring",children:"Production Tracing and Monitoring"})}),"\n",(0,r.jsx)(n.p,{children:"When you deploy an agent or LLM application to production, real users behave differently than test data\u2014they find edge cases, ask unexpected questions, and expose issues you didn't anticipate. This guide covers how to configure MLflow Tracing for production environments\u2014including automatic (online) quality evaluation\u2014to catch these issues early and continuously improve your application."}),"\n",(0,r.jsx)("video",{src:(0,o.default)("/images/llms/tracing/overview_demo.mp4"),controls:!0,loop:!0,autoPlay:!0,muted:!0,"aria-label":"Production Monitoring Overview"}),"\n",(0,r.jsx)(n.h2,{id:"production-checklist",children:"Production Checklist"}),"\n",(0,r.jsx)(n.p,{children:"We recommend the following steps before deploying to production. Each topic is covered in more detail below."}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"/self-hosting/architecture/backend-store#relational-database-default",children:"Use a production-grade SQL database"})})," \u2014 Use PostgreSQL, MySQL, or similar for reliability at scale"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#asynchronous-trace-logging",children:"Enable async trace logging"})})," \u2014 Upload traces in the background to avoid adding latency to your app"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","[Optional] ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#using-the-production-tracing-sdk",children:"Use the production tracing SDK"})})," \u2014 Faster startup and smaller footprint than the full ",(0,r.jsx)(n.code,{children:"mlflow"})," package"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","[Optional] ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#sampling-traces",children:"Configure trace sampling"})})," \u2014 Control costs by logging a percentage of traces for high-volume applications"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","[Optional] ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#automatic-online-quality-evaluation",children:"Enable automatic quality evaluation"})})," \u2014 Use LLM judges to monitor quality on production traffic"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","[Optional] ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#feedback-collection",children:"Collect end-user feedback"})})," \u2014 Capture ratings and comments to identify issues and improve quality"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","[Optional] ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#adding-context-to-production-traces",children:"Add application context to traces"})})," \u2014 Track user IDs, sessions, and metadata to debug and analyze behavior"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"setting-up-tracing-for-production-endpoints",children:"Setting Up Tracing for Production Endpoints"}),"\n",(0,r.jsxs)(n.p,{children:["For production deployments, we recommend the ",(0,r.jsx)(n.a,{href:"#using-the-production-tracing-sdk",children:"Production Tracing SDK"})," to minimize library dependencies and reduce startup time, and ",(0,r.jsx)(n.a,{href:"#production-tracing-configurations",children:"async logging with sampling"})," for better performance and cost control at scale."]}),"\n",(0,r.jsx)(s.Zp,{children:(0,r.jsx)("div",{className:"flex-column",children:(0,r.jsx)("div",{className:"flex-row",children:(0,r.jsxs)("div",{className:"flex-item",children:[(0,r.jsx)(n.h4,{id:"using-the-production-tracing-sdk",children:"Using the Production Tracing SDK"}),(0,r.jsxs)(n.p,{children:["The ",(0,r.jsxs)(n.a,{href:"/genai/tracing/lightweight-sdk",children:["Production Tracing SDK (",(0,r.jsx)(n.code,{children:"mlflow-tracing"}),")"]})," is a smaller package that only includes the minimum set of dependencies to instrument your code/models/agents with MLflow Tracing."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u26A1\uFE0F Faster Deployment"}),": Significantly smaller package size and fewer dependencies enable quicker deployments in containers and serverless environments"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u{1F4E6} Enhanced Portability"}),": Easily deploy across different platforms with minimal compatibility concerns"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u{1F680} Performance Optimizations"}),": Optimized for high-volume tracing in production environments"]})]})})})}),"\n",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.admonition,{title:"Compatibility Warning",type:"warning",children:(0,r.jsxs)(n.p,{children:["When installing the MLflow Tracing SDK, make sure the environment ",(0,r.jsx)(n.strong,{children:"does not have"})," the full MLflow package installed. Having both packages in the same environment might cause conflicts and unexpected behaviors."]})}),"\n",(0,r.jsx)(n.h2,{id:"automatic-online-quality-evaluation",children:"Automatic (Online) Quality Evaluation"}),"\n",(0,r.jsxs)(n.p,{children:["MLflow's ",(0,r.jsx)(n.a,{href:"/genai/eval-monitor/automatic-evaluations/",children:"automatic evaluation"})," enables continuous quality monitoring of production traffic using LLM judges. Judges run asynchronously on incoming traces without blocking your application, evaluating for issues like:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Hallucinations and factual accuracy"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"PII leakage and safety violations"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"User frustration in multi-turn conversations"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Response relevance and completeness"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-production-judges",children:"Setting Up Production Judges"}),"\n",(0,r.jsxs)(n.p,{children:["You can configure LLM judges to automatically evaluate a sample of your production traces using the UI or SDK. Judges can be set up with sampling rates to control costs and filter strings to target specific traces. For detailed setup instructions and configuration options, see ",(0,r.jsx)(n.a,{href:"/genai/eval-monitor/automatic-evaluations/",children:"Automatic Evaluation"}),"."]}),"\n",(0,r.jsx)("video",{src:(0,o.default)("/images/llms/tracing/automatic-evaluation-ui-setup.mp4"),controls:!0,loop:!0,autoPlay:!0,muted:!0,"aria-label":"Automatic Evaluation Setup"}),"\n",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n\nmlflow.set_experiment("production-genai-app")\n\n# Create a judge for detecting potential issues\nsafety_judge = Guidelines(\n    name="safety_check",\n    guidelines="The response must not contain PII, harmful content, or hallucinated information.",\n    model="gateway:/my-llm-endpoint",\n)\n\n# Register and start automatic evaluation\nregistered_judge = safety_judge.register(name="production_safety_check")\nregistered_judge.start(\n    sampling_config=ScorerSamplingConfig(\n        sample_rate=0.1,  # Evaluate 10% of traces\n        filter_string="metadata.environment = \'production\'",  # Only production traces\n    ),\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"production-tracing-configurations",children:"Production Tracing Configurations"}),"\n",(0,r.jsxs)(n.p,{children:["For production deployments, we recommend enabling ",(0,r.jsx)(n.a,{href:"#asynchronous-trace-logging",children:"asynchronous trace logging"})," to avoid blocking your application, and configuring ",(0,r.jsx)(n.a,{href:"#sampling-traces",children:"trace sampling"})," to control costs for high-volume traffic."]}),"\n",(0,r.jsx)(n.p,{children:"Example configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Required: Set MLflow Tracking URI\nexport MLFLOW_TRACKING_URI="http://your-mlflow-server:5000"\n\n# Optional: Configure the experiment name for organizing traces\nexport MLFLOW_EXPERIMENT_NAME="production-genai-app"\n\n# Optional: Configure async logging (recommended for production)\nexport MLFLOW_ENABLE_ASYNC_TRACE_LOGGING=true\nexport MLFLOW_ASYNC_TRACE_LOGGING_MAX_WORKERS=10\nexport MLFLOW_ASYNC_TRACE_LOGGING_MAX_QUEUE_SIZE=1000\n\n# Optional: Configure trace sampling ratio (default is 1.0)\nexport MLFLOW_TRACE_SAMPLING_RATIO=0.1\n'})}),"\n",(0,r.jsx)(n.h3,{id:"asynchronous-trace-logging",children:"Asynchronous Trace Logging"}),"\n",(0,r.jsx)(n.p,{children:"For production applications, MLflow logs traces asynchronously by default to prevent blocking your application:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Environment Variable"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Default Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MLFLOW_ENABLE_ASYNC_TRACE_LOGGING"})}),(0,r.jsxs)(n.td,{children:["Whether to log traces asynchronously. When set to ",(0,r.jsx)(n.code,{children:"False"}),", traces will be logged in a blocking manner."]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"True"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MLFLOW_ASYNC_TRACE_LOGGING_MAX_WORKERS"})}),(0,r.jsx)(n.td,{children:"The maximum number of worker threads to use for async trace logging per process. Increasing this allows higher throughput of trace logging, but also increases CPU usage and memory consumption."}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"10"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MLFLOW_ASYNC_TRACE_LOGGING_MAX_QUEUE_SIZE"})}),(0,r.jsx)(n.td,{children:"The maximum number of traces that can be queued before being logged to backend by the worker threads. When the queue is full, new traces will be discarded. Increasing this allows higher durability of trace logging, but also increases memory consumption."}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"1000"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MLFLOW_ASYNC_TRACE_LOGGING_RETRY_TIMEOUT"})}),(0,r.jsx)(n.td,{children:"The timeout in seconds for retrying failed trace logging. When a trace logging fails, it will be retried up to this timeout with backoff, after which it will be discarded."}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"500"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"sampling-traces",children:"Sampling Traces"}),"\n",(0,r.jsx)(n.p,{children:"For high-volume applications, you may want to reduce the number of traces exported to the backend. You can configure the sampling ratio to control the number of traces exported."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Environment Variable"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Default Value"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MLFLOW_TRACE_SAMPLING_RATIO"})}),(0,r.jsxs)(n.td,{children:["The sampling ratio for traces. When set to ",(0,r.jsx)(n.code,{children:"0.0"}),", no traces will be exported. When set to ",(0,r.jsx)(n.code,{children:"1.0"}),", all traces will be exported."]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"1.0"})})]})})]}),"\n",(0,r.jsxs)(n.p,{children:["The default value is ",(0,r.jsx)(n.code,{children:"1.0"}),", which means all traces will be exported. When set to less than ",(0,r.jsx)(n.code,{children:"1.0"}),", say ",(0,r.jsx)(n.code,{children:"0.1"}),", only 10% of the traces will be exported. The sampling is done at the trace level, meaning that all spans in some traces will be exported or discarded together."]}),"\n",(0,r.jsx)(n.h2,{id:"adding-context-to-production-traces",children:"Adding Context to Production Traces"}),"\n",(0,r.jsx)(n.p,{children:"Adding user IDs, session IDs, and environment metadata to your traces makes it easier to debug issues for specific users and analyze behavior across different segments."}),"\n",(0,r.jsx)(n.h3,{id:"tracking-request-session-and-user-context",children:"Tracking Request, Session, and User Context"}),"\n",(0,r.jsxs)(n.p,{children:["Production applications need to track multiple pieces of context simultaneously. For detailed guidance, see ",(0,r.jsx)(n.a,{href:"/genai/tracing/track-users-sessions",children:"Track Users & Sessions"}),". The following example demonstrates how to track all of these in a FastAPI application."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\nfrom fastapi import FastAPI, Request\nfrom pydantic import BaseModel\n\n# Initialize FastAPI app\napp = FastAPI()\n\n\nclass ChatRequest(BaseModel):\n    message: str\n\n\n@app.post("/chat")  # FastAPI decorator should be outermost\n@mlflow.trace  # Ensure @mlflow.trace is the inner decorator\ndef handle_chat(request: Request, chat_request: ChatRequest):\n    # Retrieve all context from request headers\n    client_request_id = request.headers.get("X-Request-ID")\n    session_id = request.headers.get("X-Session-ID")\n    user_id = request.headers.get("X-User-ID")\n\n    # Update the current trace with all context and environment metadata\n    mlflow.update_current_trace(\n        client_request_id=client_request_id,\n        tags={\n            # Session context - groups traces from multi-turn conversations\n            "mlflow.trace.session": session_id,\n            # User context - associates traces with specific users\n            "mlflow.trace.user": user_id,\n            # Environment metadata - tracks deployment context\n            "environment": "production",\n            "app_version": os.getenv("APP_VERSION", "1.0.0"),\n            "deployment_id": os.getenv("DEPLOYMENT_ID", "unknown"),\n            "region": os.getenv("REGION", "us-east-1"),\n        },\n    )\n\n    # Your application logic for processing the chat message\n    response_text = f"Processed message: \'{chat_request.message}\'"\n\n    return {"response": response_text}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"feedback-collection",children:"Feedback Collection"}),"\n",(0,r.jsxs)(n.p,{children:["Capturing user feedback on specific interactions is essential for understanding quality and improving your GenAI application. For detailed guidance, see ",(0,r.jsx)(n.a,{href:"/genai/tracing/collect-user-feedback",children:"Collect User Feedback"}),". The following example demonstrates how to collect feedback in a FastAPI application."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.client import MlflowClient\nfrom fastapi import FastAPI, Query, Request\nfrom pydantic import BaseModel\nfrom typing import Optional\nfrom mlflow.entities import AssessmentSource\n\napp = FastAPI()\n\n\nclass FeedbackRequest(BaseModel):\n    is_correct: bool  # True for correct, False for incorrect\n    comment: Optional[str] = None\n\n\n@app.post("/chat_feedback")\ndef handle_chat_feedback(\n    request: Request,\n    client_request_id: str = Query(\n        ..., description="The client request ID from the original chat request"\n    ),\n    feedback: FeedbackRequest = ...,\n):\n    """\n    Collect user feedback for a specific chat interaction identified by client_request_id.\n    """\n    # Search for the trace with the matching client_request_id\n    client = MlflowClient()\n    experiment = client.get_experiment_by_name("production-genai-app")\n    traces = client.search_traces(locations=[experiment.experiment_id])\n    traces = [\n        trace for trace in traces if trace.info.client_request_id == client_request_id\n    ][:1]\n\n    if not traces:\n        return {\n            "status": "error",\n            "message": f"Unable to find data for client request ID: {client_request_id}",\n        }, 500\n\n    # Log feedback using MLflow\'s log_feedback API\n    mlflow.log_feedback(\n        trace_id=traces[0].info.trace_id,\n        name="response_is_correct",\n        value=feedback.is_correct,\n        source=AssessmentSource(\n            source_type="HUMAN", source_id=request.headers.get("X-User-ID")\n        ),\n        rationale=feedback.comment,\n    )\n\n    return {\n        "status": "success",\n        "message": "Feedback recorded successfully",\n        "trace_id": traces[0].info.trace_id,\n    }\n'})}),"\n",(0,r.jsx)(n.h3,{id:"querying-traces-with-context",children:"Querying Traces with Context"}),"\n",(0,r.jsxs)(n.p,{children:["Once you've enriched traces with user, session, and environment context, you can query them to debug issues for specific users, analyze conversation flows within sessions, or compare behavior across deployments. For detailed guidance, see ",(0,r.jsx)(n.a,{href:"/genai/tracing/search-traces",children:"Search Traces"}),". The following example demonstrates how to query traces by user, session, and environment."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.set_experiment("production-genai-app")\n\n# Query traces by user\nuser_traces = mlflow.search_traces(\n    filter_string="tags.`mlflow.trace.user` = \'user-jane-doe-12345\'",\n    max_results=100,\n)\n\n# Query traces by session\nsession_traces = mlflow.search_traces(\n    filter_string="tags.`mlflow.trace.session` = \'session-def-456\'",\n    max_results=100,\n)\n\n# Query traces by environment\nproduction_traces = mlflow.search_traces(\n    filter_string="tags.environment = \'production\'",\n    max_results=100,\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(l.A,{children:[(0,r.jsx)(c.A,{icon:d.A,iconSize:48,title:"Automatic Evaluation",description:"Set up LLM judges to automatically monitor quality on production traffic.",href:"/genai/eval-monitor/automatic-evaluations",linkText:"Set up automatic evaluation \u2192",containerHeight:64}),(0,r.jsx)(c.A,{icon:u.A,iconSize:48,title:"Searching for Traces",description:"Understand how to access trace data for analysis with UI or API.",href:"/genai/tracing/search-traces",linkText:"Learn to search traces \u2192",containerHeight:64}),(0,r.jsx)(c.A,{icon:h.A,iconSize:48,title:"Track Users & Sessions",description:"Implement user and session context tracking for better monitoring insights.",href:"/genai/tracing/track-users-sessions",linkText:"Track users & sessions \u2192",containerHeight:64})]})]})}function j(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(f,{...e})}):f(e)}},75689(e,n,t){t.d(n,{A:()=>l});var i=t(96540);let r=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,t)=>t?t.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},a=(...e)=>e.filter((e,n,t)=>!!e&&""!==e.trim()&&t.indexOf(e)===n).join(" ").trim();var s={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let o=(0,i.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:t=2,absoluteStrokeWidth:r,className:o="",children:l,iconNode:c,...d},u)=>(0,i.createElement)("svg",{ref:u,...s,width:n,height:n,stroke:e,strokeWidth:r?24*Number(t)/Number(n):t,className:a("lucide",o),...!l&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,n])=>(0,i.createElement)(e,n)),...Array.isArray(l)?l:[l]])),l=(e,n)=>{let t=(0,i.forwardRef)(({className:t,...s},l)=>(0,i.createElement)(o,{ref:l,iconNode:n,className:a(`lucide-${r(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,t),...s}));return t.displayName=r(e),t}},98445(e,n,t){t.d(n,{A:()=>i});let i=(0,t(75689).A)("search",[["path",{d:"m21 21-4.34-4.34",key:"14j7rj"}],["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}]])},93893(e,n,t){t.d(n,{A:()=>i});let i=(0,t(75689).A)("users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["path",{d:"M16 3.128a4 4 0 0 1 0 7.744",key:"16gr8j"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}]])},46858(e,n,t){t.d(n,{A:()=>i});let i=(0,t(75689).A)("zap",[["path",{d:"M4 14a1 1 0 0 1-.78-1.63l9.9-10.2a.5.5 0 0 1 .86.46l-1.92 6.02A1 1 0 0 0 13 10h7a1 1 0 0 1 .78 1.63l-9.9 10.2a.5.5 0 0 1-.86-.46l1.92-6.02A1 1 0 0 0 11 14z",key:"1xq2db"}]])},3340(e,n,t){t.d(n,{WO:()=>d,$3:()=>u,jK:()=>h,_C:()=>c,Zp:()=>l,AC:()=>o});var i=t(74848),r=t(34164);let a={CardGroup:"CardGroup_P84T",NoGap:"NoGap_O9Dj",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardRounded:"SmallLogoCardRounded_X50_",SmallLogoCardImage:"SmallLogoCardImage_tPZl",SmallLogoCardImageWithTitle:"SmallLogoCardImageWithTitle_K6Vu",SmallLogoCardImageDefault:"SmallLogoCardImageDefault_nyru",SmallLogoCardRow:"SmallLogoCardRow_kcMP",SmallLogoCardLabel:"SmallLogoCardLabel_f5RQ",SmallLogoCardArrow:"SmallLogoCardArrow_J536",SmallLogoCardLink:"SmallLogoCardLink_Cm5j",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardHeader:"TitleCardHeader_fUQy",TitleCardHeaderRight:"TitleCardHeaderRight_iBLX",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var s=t(95310);let o=({children:e,isSmall:n,cols:t,noGap:s})=>(0,i.jsx)("div",{className:(0,r.A)(a.CardGroup,n?a.AutofillColumns:t?a[`Cols${t}`]:a.MaxThreeColumns,s&&a.NoGap),children:e}),l=({children:e,link:n="",style:t})=>n?(0,i.jsx)(s.A,{className:(0,r.A)(a.Link,a.Card,a.CardBordered),style:t,to:n,children:e}):(0,i.jsx)("div",{className:(0,r.A)(a.Card,a.CardBordered),style:t,children:e}),c=({headerText:e,link:n,text:t})=>(0,i.jsx)(l,{link:n,children:(0,i.jsxs)("span",{children:[(0,i.jsx)("div",{className:(0,r.A)(a.CardTitle,a.BoxRoot,a.PaddingBottom4),style:{pointerEvents:"none"},children:(0,i.jsx)("div",{className:(0,r.A)(a.BoxRoot,a.FlexFlex,a.FlexAlignItemsCenter,a.FlexDirectionRow,a.FlexJustifyContentFlexStart,a.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,i.jsx)("div",{className:(0,r.A)(a.BoxRoot,a.BoxHideIfEmpty,a.MarginTop4,a.MarginLeft4),style:{pointerEvents:"auto"},children:(0,i.jsx)("span",{className:"",children:e})})})}),(0,i.jsx)("span",{className:(0,r.A)(a.TextColor,a.CardBody),children:(0,i.jsx)("p",{children:t})})]})}),d=({description:e,children:n,link:t})=>(0,i.jsx)(l,{link:t,children:(0,i.jsxs)("div",{className:a.LogoCardContent,children:[(0,i.jsx)("div",{className:a.LogoCardImage,children:n}),(0,i.jsx)("p",{className:a.TextColor,children:e})]})}),u=({children:e,link:n,title:t=""})=>(0,i.jsx)(s.A,{className:(0,r.A)(a.Card,a.CardBordered,a.SmallLogoCardRounded,a.SmallLogoCardLink),to:n,children:(0,i.jsx)("div",{className:a.SmallLogoCardContent,children:t?(0,i.jsxs)("div",{className:a.SmallLogoCardRow,children:[(0,i.jsx)("div",{className:(0,r.A)("max-height-img-container",a.SmallLogoCardImage,a.SmallLogoCardImageWithTitle),children:e}),(0,i.jsx)("div",{className:a.SmallLogoCardLabel,children:(0,i.jsx)("span",{children:t})})]}):(0,i.jsx)("div",{className:(0,r.A)("max-height-img-container",a.SmallLogoCardImage,a.SmallLogoCardImageDefault),children:e})})}),h=({title:e,description:n,link:t="",headerRight:s,children:o})=>(0,i.jsx)(l,{link:t,children:(0,i.jsxs)("div",{className:a.TitleCardContent,children:[(0,i.jsxs)("div",{className:(0,r.A)(a.TitleCardHeader),children:[(0,i.jsx)("div",{className:(0,r.A)(a.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,i.jsx)("div",{className:a.TitleCardHeaderRight,children:s})]}),(0,i.jsx)("hr",{className:(0,r.A)(a.TitleCardSeparator),style:{margin:"12px 0"}}),o?(0,i.jsx)("div",{className:(0,r.A)(a.TextColor),children:o}):(0,i.jsx)("p",{className:(0,r.A)(a.TextColor),dangerouslySetInnerHTML:{__html:n}})]})})},77541(e,n,t){t.d(n,{A:()=>c});var i=t(74848);t(96540);var r=t(95310),a=t(34164);let s="tileImage_O4So";var o=t(66497),l=t(92802);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:d,iconSize:u=32,containerHeight:h,title:g,description:p,href:m,linkText:x="Learn more \u2192",className:f}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let j=h?{height:`${h}px`}:{},C={};return c&&(C.width=`${c}px`),d&&(C.height=`${d}px`),(0,i.jsxs)(r.A,{href:m,className:(0,a.A)("tileCard_NHsj",f),children:[(0,i.jsx)("div",{className:"tileIcon_pyoR",style:j,children:e?(0,i.jsx)(e,{size:u}):t?(0,i.jsx)(l.A,{sources:{light:(0,o.default)(n),dark:(0,o.default)(t)},alt:g,className:s,style:C}):(0,i.jsx)("img",{src:(0,o.default)(n),alt:g,className:s,style:C})}),(0,i.jsx)("h3",{children:g}),(0,i.jsx)("p",{children:p}),(0,i.jsx)("div",{className:"tileLink_iUbu",children:x})]})}},10440(e,n,t){t.d(n,{A:()=>a});var i=t(74848);t(96540);var r=t(34164);function a({children:e,className:n}){return(0,i.jsx)("div",{className:(0,r.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,t){t.d(n,{R:()=>s,x:()=>o});var i=t(96540);let r={},a=i.createContext(r);function s(e){let n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);