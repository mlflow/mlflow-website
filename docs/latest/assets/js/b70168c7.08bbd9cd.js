"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5067],{11470:(e,n,a)=>{a.d(n,{A:()=>b});var t=a(96540),r=a(34164),l=a(23104),s=a(56347),i=a(205),o=a(57485),c=a(31682),m=a(70679);function d(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:a}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return d(e).map((({props:{value:e,label:n,attributes:a,default:t}})=>({value:e,label:n,attributes:a,default:t})))}(a);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function p({value:e,tabValues:n}){return n.some((n=>n.value===e))}function h({queryString:e=!1,groupId:n}){const a=(0,s.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,o.aZ)(r),(0,t.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(a.location.search);n.set(r,e),a.replace({...a.location,search:n.toString()})}),[r,a])]}function f(e){const{defaultValue:n,queryString:a=!1,groupId:r}=e,l=u(e),[s,o]=(0,t.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:l}))),[c,d]=h({queryString:a,groupId:r}),[f,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[a,r]=(0,m.Dv)(n);return[a,(0,t.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:r}),g=(()=>{const e=c??f;return p({value:e,tabValues:l})?e:null})();(0,i.A)((()=>{g&&o(g)}),[g]);return{selectedValue:s,selectValue:(0,t.useCallback)((e=>{if(!p({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);o(e),d(e),_(e)}),[d,_,l]),tabValues:l}}var _=a(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var w=a(74848);function y({className:e,block:n,selectedValue:a,selectValue:t,tabValues:s}){const i=[],{blockElementScrollPositionUntilNextRender:o}=(0,l.a_)(),c=e=>{const n=e.currentTarget,r=i.indexOf(n),l=s[r].value;l!==a&&(o(n),t(l))},m=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=i.indexOf(e.currentTarget)+1;n=i[a]??i[0];break}case"ArrowLeft":{const a=i.indexOf(e.currentTarget)-1;n=i[a]??i[i.length-1];break}}n?.focus()};return(0,w.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:s.map((({value:e,label:n,attributes:t})=>(0,w.jsx)("li",{role:"tab",tabIndex:a===e?0:-1,"aria-selected":a===e,ref:e=>{i.push(e)},onKeyDown:m,onClick:c,...t,className:(0,r.A)("tabs__item",g.tabItem,t?.className,{"tabs__item--active":a===e}),children:n??e},e)))})}function v({lazy:e,children:n,selectedValue:a}){const l=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=l.find((e=>e.props.value===a));return e?(0,t.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,w.jsx)("div",{className:"margin-top--md",children:l.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==a})))})}function x(e){const n=f(e);return(0,w.jsxs)("div",{className:(0,r.A)("tabs-container",g.tabList),children:[(0,w.jsx)(y,{...n,...e}),(0,w.jsx)(v,{...n,...e})]})}function b(e){const n=(0,_.A)();return(0,w.jsx)(x,{...e,children:d(e.children)},String(n))}},19365:(e,n,a)=>{a.d(n,{A:()=>s});a(96540);var t=a(34164);const r={tabItem:"tabItem_Ymn6"};var l=a(74848);function s({children:e,hidden:n,className:a}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,t.A)(r.tabItem,a),hidden:n,children:e})}},20046:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>m,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"evaluation/dataset-eval","title":"Dataset Evaluation","description":"Dataset evaluation allows you to assess model performance on pre-computed predictions without re-running the model. This is particularly useful for evaluating large-scale batch inference results, historical predictions, or when you want to separate the prediction and evaluation phases.","source":"@site/docs/classic-ml/evaluation/dataset-eval.mdx","sourceDirName":"evaluation","slug":"/evaluation/dataset-eval","permalink":"/docs/latest/ml/evaluation/dataset-eval","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Dataset Evaluation","sidebar_position":2},"sidebar":"classicMLSidebar","previous":{"title":"Function Evaluation","permalink":"/docs/latest/ml/evaluation/function-eval"},"next":{"title":"Model Evaluation","permalink":"/docs/latest/ml/evaluation/model-eval"}}');var r=a(74848),l=a(28453),s=(a(49374),a(11470)),i=a(19365);const o={title:"Dataset Evaluation",sidebar_position:2},c="Dataset Evaluation",m={},d=[{value:"Quick Start: Evaluating Static Predictions",id:"quick-start-evaluating-static-predictions",level:2},{value:"Dataset Management",id:"dataset-management",level:2},{value:"Batch Evaluation Workflows",id:"batch-evaluation-workflows",level:2},{value:"Working with Large Datasets",id:"working-with-large-datasets",level:2},{value:"Key Use Cases and Benefits",id:"key-use-cases-and-benefits",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"dataset-evaluation",children:"Dataset Evaluation"})}),"\n",(0,r.jsx)(n.p,{children:"Dataset evaluation allows you to assess model performance on pre-computed predictions without re-running the model. This is particularly useful for evaluating large-scale batch inference results, historical predictions, or when you want to separate the prediction and evaluation phases."}),"\n",(0,r.jsx)(n.h2,{id:"quick-start-evaluating-static-predictions",children:"Quick Start: Evaluating Static Predictions"}),"\n",(0,r.jsx)(n.p,{children:"The simplest dataset evaluation involves a DataFrame with predictions and targets:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Generate sample data and train a model\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Generate predictions (this could be from a batch job, stored results, etc.)\npredictions = model.predict(X_test)\nprediction_probabilities = model.predict_proba(X_test)[:, 1]\n\n# Create evaluation dataset with predictions already computed\neval_dataset = pd.DataFrame(\n    {\n        "prediction": predictions,\n        "prediction_proba": prediction_probabilities,\n        "target": y_test,\n    }\n)\n\n# Add original features for analysis (optional)\nfeature_names = [f"feature_{i}" for i in range(X_test.shape[1])]\nfor i, feature_name in enumerate(feature_names):\n    eval_dataset[feature_name] = X_test[:, i]\n\nwith mlflow.start_run():\n    # Evaluate static dataset - no model needed!\n    result = mlflow.evaluate(\n        data=eval_dataset,\n        predictions="prediction",  # Column containing predictions\n        targets="target",  # Column containing true labels\n        model_type="classifier",\n    )\n\n    print(f"Accuracy: {result.metrics[\'accuracy_score\']:.3f}")\n    print(f"F1 Score: {result.metrics[\'f1_score\']:.3f}")\n    print(f"ROC AUC: {result.metrics[\'roc_auc\']:.3f}")\n'})}),"\n",(0,r.jsx)(n.p,{children:"This approach is perfect when:"}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"You have batch prediction results from a production system"}),(0,r.jsx)("li",{children:"You want to evaluate historical predictions"}),(0,r.jsx)("li",{children:"You're comparing different versions of the same model's outputs"}),(0,r.jsx)("li",{children:"You need to separate compute-intensive prediction from evaluation"})]}),"\n",(0,r.jsx)(n.h2,{id:"dataset-management",children:"Dataset Management"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsxs)(i.A,{value:"pandas-dataset",label:"MLflow PandasDataset",default:!0,children:[(0,r.jsx)(n.p,{children:"For more structured dataset management, use MLflow's PandasDataset:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow.data\n\n# Create MLflow dataset with prediction column specified\ndataset = mlflow.data.from_pandas(\n    eval_dataset,\n    predictions="prediction",  # Specify prediction column\n    targets="target",  # Specify target column\n)\n\nwith mlflow.start_run():\n    # Log the dataset\n    mlflow.log_input(dataset, context="evaluation")\n\n    # Evaluate using the dataset (predictions=None since specified in dataset)\n    result = mlflow.evaluate(\n        data=dataset,\n        predictions=None,  # Already specified in dataset creation\n        targets="target",\n        model_type="classifier",\n    )\n\n    print("Evaluation completed using MLflow PandasDataset")\n'})})]}),(0,r.jsxs)(i.A,{value:"multi-output",label:"Multi-Output Models",children:[(0,r.jsx)(n.p,{children:"When your model produces multiple outputs, you can evaluate different output columns:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Simulate multi-output model results\nmulti_output_data = pd.DataFrame(\n    {\n        "primary_prediction": predictions,\n        "confidence_score": prediction_probabilities,\n        "auxiliary_output": np.random.random(\n            len(predictions)\n        ),  # Additional model output\n        "target": y_test,\n    }\n)\n\nwith mlflow.start_run():\n    # Evaluate primary prediction\n    result = mlflow.evaluate(\n        data=multi_output_data,\n        predictions="primary_prediction",\n        targets="target",\n        model_type="classifier",\n    )\n\n    # Access other outputs for custom analysis\n    confidence_scores = multi_output_data["confidence_score"]\n    auxiliary_outputs = multi_output_data["auxiliary_output"]\n\n    # Log additional analysis\n    mlflow.log_metrics(\n        {\n            "avg_confidence": confidence_scores.mean(),\n            "confidence_std": confidence_scores.std(),\n            "avg_auxiliary": auxiliary_outputs.mean(),\n        }\n    )\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"batch-evaluation-workflows",children:"Batch Evaluation Workflows"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsxs)(i.A,{value:"large-scale-batch",label:"Large-Scale Batch Results",default:!0,children:[(0,r.jsx)(n.p,{children:"For production batch inference results:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def evaluate_batch_predictions(batch_results_path, batch_size=10000):\n    """Evaluate large batch prediction results efficiently."""\n\n    # Read batch results (could be from S3, database, etc.)\n    batch_df = pd.read_parquet(batch_results_path)\n\n    print(f"Evaluating {len(batch_df)} batch predictions")\n\n    with mlflow.start_run(run_name="Batch_Evaluation"):\n        # Log batch metadata\n        mlflow.log_params(\n            {\n                "batch_size": len(batch_df),\n                "batch_date": batch_df.get("prediction_date", "unknown").iloc[0]\n                if len(batch_df) > 0\n                else "unknown",\n                "data_source": batch_results_path,\n            }\n        )\n\n        # Evaluate full batch\n        result = mlflow.evaluate(\n            data=batch_df,\n            predictions="model_prediction",\n            targets="true_label",\n            model_type="classifier",\n        )\n\n        # Additional batch-specific analysis\n        if "prediction_timestamp" in batch_df.columns:\n            # Analyze performance over time\n            batch_df["hour"] = pd.to_datetime(batch_df["prediction_timestamp"]).dt.hour\n            hourly_accuracy = batch_df.groupby("hour").apply(\n                lambda x: (x["model_prediction"] == x["true_label"]).mean()\n            )\n\n            # Log time-based metrics\n            for hour, accuracy in hourly_accuracy.items():\n                mlflow.log_metric(f"accuracy_hour_{hour}", accuracy)\n\n        return result\n\n\n# Usage\n# result = evaluate_batch_predictions("s3://my-bucket/batch-predictions/2024-01-15.parquet")\n'})})]}),(0,r.jsxs)(i.A,{value:"historical-analysis",label:"Historical Performance Analysis",children:[(0,r.jsx)(n.p,{children:"Analyze model performance trends over time:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def analyze_historical_performance(historical_data, time_column="prediction_date"):\n    """Analyze model performance trends over time."""\n\n    historical_data[time_column] = pd.to_datetime(historical_data[time_column])\n\n    with mlflow.start_run(run_name="Historical_Performance_Analysis"):\n        # Overall historical evaluation\n        overall_result = mlflow.evaluate(\n            data=historical_data,\n            predictions="prediction",\n            targets="actual",\n            model_type="classifier",\n        )\n\n        # Time-based analysis\n        historical_data["month"] = historical_data[time_column].dt.to_period("M")\n        monthly_performance = []\n\n        for month in historical_data["month"].unique():\n            month_data = historical_data[historical_data["month"] == month]\n\n            if len(month_data) > 50:  # Minimum samples for reliable metrics\n                with mlflow.start_run(run_name=f"Month_{month}", nested=True):\n                    month_result = mlflow.evaluate(\n                        data=month_data,\n                        predictions="prediction",\n                        targets="actual",\n                        model_type="classifier",\n                    )\n\n                    monthly_performance.append(\n                        {\n                            "month": str(month),\n                            "accuracy": month_result.metrics["accuracy_score"],\n                            "f1": month_result.metrics["f1_score"],\n                            "sample_count": len(month_data),\n                        }\n                    )\n\n        # Log trend analysis\n        if monthly_performance:\n            perf_df = pd.DataFrame(monthly_performance)\n\n            # Calculate trends\n            accuracy_trend = np.polyfit(range(len(perf_df)), perf_df["accuracy"], 1)[0]\n            f1_trend = np.polyfit(range(len(perf_df)), perf_df["f1"], 1)[0]\n\n            mlflow.log_metrics(\n                {\n                    "accuracy_trend_slope": accuracy_trend,\n                    "f1_trend_slope": f1_trend,\n                    "performance_variance": perf_df["accuracy"].var(),\n                    "months_analyzed": len(monthly_performance),\n                }\n            )\n\n            # Save trend data\n            perf_df.to_csv("monthly_performance.csv", index=False)\n            mlflow.log_artifact("monthly_performance.csv")\n\n        return overall_result, monthly_performance\n\n\n# Usage example\n# historical_result, trends = analyze_historical_performance(historical_predictions_df)\n'})})]}),(0,r.jsxs)(i.A,{value:"comparative-datasets",label:"Comparative Dataset Evaluation",children:[(0,r.jsx)(n.p,{children:"Compare model performance across different datasets or data slices:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def compare_datasets(datasets_dict, model_type="classifier"):\n    """Compare model performance across multiple datasets."""\n\n    comparison_results = {}\n\n    with mlflow.start_run(run_name="Dataset_Comparison"):\n        for dataset_name, dataset in datasets_dict.items():\n            with mlflow.start_run(run_name=f"Dataset_{dataset_name}", nested=True):\n                result = mlflow.evaluate(\n                    data=dataset,\n                    predictions="prediction",\n                    targets="target",\n                    model_type=model_type,\n                )\n\n                comparison_results[dataset_name] = result.metrics\n\n                # Log dataset characteristics\n                mlflow.log_params(\n                    {\n                        "dataset_name": dataset_name,\n                        "dataset_size": len(dataset),\n                        "positive_rate": dataset["target"].mean()\n                        if model_type == "classifier"\n                        else None,\n                    }\n                )\n\n        # Log comparison metrics\n        if comparison_results:\n            accuracy_values = [\n                r.get("accuracy_score", 0) for r in comparison_results.values()\n            ]\n            mlflow.log_metrics(\n                {\n                    "max_accuracy": max(accuracy_values),\n                    "min_accuracy": min(accuracy_values),\n                    "accuracy_range": max(accuracy_values) - min(accuracy_values),\n                    "datasets_compared": len(comparison_results),\n                }\n            )\n\n    return comparison_results\n\n\n# Usage\ndatasets = {\n    "validation_set": validation_predictions_df,\n    "test_set": test_predictions_df,\n    "holdout_set": holdout_predictions_df,\n}\n\ncomparison = compare_datasets(datasets)\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"working-with-large-datasets",children:"Working with Large Datasets"}),"\n",(0,r.jsxs)(s.A,{children:[(0,r.jsxs)(i.A,{value:"chunked-evaluation",label:"Chunked Processing",default:!0,children:[(0,r.jsx)(n.p,{children:"For datasets too large to fit in memory:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def evaluate_large_dataset_in_chunks(data_path, chunk_size=50000):\n    """Evaluate very large datasets by processing in chunks."""\n\n    # Read data in chunks\n    chunk_results = []\n    total_samples = 0\n\n    with mlflow.start_run(run_name="Large_Dataset_Evaluation"):\n        for chunk_idx, chunk in enumerate(\n            pd.read_parquet(data_path, chunksize=chunk_size)\n        ):\n            chunk_size_actual = len(chunk)\n            total_samples += chunk_size_actual\n\n            # Evaluate chunk\n            with mlflow.start_run(run_name=f"Chunk_{chunk_idx}", nested=True):\n                chunk_result = mlflow.evaluate(\n                    data=chunk,\n                    predictions="prediction",\n                    targets="target",\n                    model_type="classifier",\n                )\n\n                # Weight metrics by chunk size for aggregation\n                weighted_metrics = {\n                    f"{k}_weighted": v * chunk_size_actual\n                    for k, v in chunk_result.metrics.items()\n                    if isinstance(v, (int, float))\n                }\n\n                chunk_results.append(\n                    {\n                        "chunk_idx": chunk_idx,\n                        "chunk_size": chunk_size_actual,\n                        "metrics": chunk_result.metrics,\n                        "weighted_metrics": weighted_metrics,\n                    }\n                )\n\n                mlflow.log_param("chunk_size", chunk_size_actual)\n\n        # Aggregate results across chunks\n        if chunk_results:\n            # Calculate weighted averages\n            total_weighted = {}\n            for chunk in chunk_results:\n                for metric, value in chunk["weighted_metrics"].items():\n                    total_weighted[metric] = total_weighted.get(metric, 0) + value\n\n            # Log aggregated metrics\n            aggregated_metrics = {\n                k.replace("_weighted", "_aggregate"): v / total_samples\n                for k, v in total_weighted.items()\n            }\n\n            mlflow.log_metrics(aggregated_metrics)\n            mlflow.log_params(\n                {\n                    "total_samples": total_samples,\n                    "chunks_processed": len(chunk_results),\n                    "avg_chunk_size": total_samples / len(chunk_results),\n                }\n            )\n\n    return chunk_results\n\n\n# Usage\n# results = evaluate_large_dataset_in_chunks("large_predictions.parquet")\n'})})]}),(0,r.jsxs)(i.A,{value:"sampling-evaluation",label:"Sampling-Based Evaluation",children:[(0,r.jsx)(n.p,{children:"For extremely large datasets, use statistical sampling:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def evaluate_with_sampling(large_dataset, sample_size=10000, n_samples=5):\n    """Evaluate large dataset using multiple random samples."""\n\n    sample_results = []\n\n    with mlflow.start_run(run_name="Sampling_Based_Evaluation"):\n        for sample_idx in range(n_samples):\n            # Create random sample\n            if len(large_dataset) > sample_size:\n                sample_data = large_dataset.sample(\n                    n=sample_size, random_state=sample_idx\n                )\n            else:\n                sample_data = large_dataset.copy()\n\n            with mlflow.start_run(run_name=f"Sample_{sample_idx}", nested=True):\n                sample_result = mlflow.evaluate(\n                    data=sample_data,\n                    predictions="prediction",\n                    targets="target",\n                    model_type="classifier",\n                )\n\n                sample_results.append(sample_result.metrics)\n\n                mlflow.log_params(\n                    {\n                        "sample_idx": sample_idx,\n                        "sample_size": len(sample_data),\n                        "random_seed": sample_idx,\n                    }\n                )\n\n        # Aggregate sample results\n        if sample_results:\n            # Calculate statistics across samples\n            metrics_df = pd.DataFrame(sample_results)\n\n            aggregated_stats = {}\n            for metric in metrics_df.columns:\n                if metrics_df[metric].dtype in ["float64", "int64"]:\n                    aggregated_stats.update(\n                        {\n                            f"{metric}_mean": metrics_df[metric].mean(),\n                            f"{metric}_std": metrics_df[metric].std(),\n                            f"{metric}_min": metrics_df[metric].min(),\n                            f"{metric}_max": metrics_df[metric].max(),\n                        }\n                    )\n\n            mlflow.log_metrics(aggregated_stats)\n            mlflow.log_params(\n                {\n                    "sampling_strategy": "random",\n                    "samples_taken": n_samples,\n                    "target_sample_size": sample_size,\n                }\n            )\n\n            # Save detailed results\n            metrics_df.to_csv("sample_results.csv", index=False)\n            mlflow.log_artifact("sample_results.csv")\n\n    return sample_results, aggregated_stats\n\n\n# Usage\n# samples, stats = evaluate_with_sampling(very_large_dataset, sample_size=5000, n_samples=10)\n'})})]}),(0,r.jsxs)(i.A,{value:"memory-optimization",label:"Memory Optimization",children:[(0,r.jsx)(n.p,{children:"Best practices for memory-efficient evaluation:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def memory_efficient_evaluation(large_dataset_path, chunk_size=10000):\n    """Memory-efficient evaluation of large datasets."""\n\n    chunk_metrics = []\n\n    with mlflow.start_run(run_name="Memory_Efficient_Evaluation"):\n        for chunk in pd.read_parquet(large_dataset_path, chunksize=chunk_size):\n            # Process chunk\n            chunk_result = mlflow.evaluate(\n                data=chunk,\n                predictions="prediction",\n                targets="target",\n                model_type="classifier",\n            )\n\n            # Store only essential metrics\n            chunk_metrics.append(\n                {\n                    "size": len(chunk),\n                    "accuracy": chunk_result.metrics["accuracy_score"],\n                    "f1_score": chunk_result.metrics["f1_score"],\n                }\n            )\n\n            # Clear chunk from memory\n            del chunk\n            del chunk_result\n\n        # Compute weighted averages\n        total_samples = sum(cm["size"] for cm in chunk_metrics)\n        weighted_accuracy = (\n            sum(cm["accuracy"] * cm["size"] for cm in chunk_metrics) / total_samples\n        )\n        weighted_f1 = (\n            sum(cm["f1_score"] * cm["size"] for cm in chunk_metrics) / total_samples\n        )\n\n        # Log final metrics\n        mlflow.log_metrics(\n            {\n                "final_weighted_accuracy": weighted_accuracy,\n                "final_weighted_f1": weighted_f1,\n                "total_samples_processed": total_samples,\n                "chunks_processed": len(chunk_metrics),\n            }\n        )\n\n    return weighted_accuracy, chunk_metrics\n\n\n# Usage\n# accuracy, metrics = memory_efficient_evaluation("very_large_predictions.parquet")\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Memory Management Tips:"})}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Chunked Processing"}),": Use chunked processing for datasets larger than 1GB"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Data Types"}),": Use appropriate data types (int32 vs int64) to reduce memory usage"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Garbage Collection"}),": Explicitly delete large variables when done processing"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Streaming"}),": Process data in streaming fashion when possible"]})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"key-use-cases-and-benefits",children:"Key Use Cases and Benefits"}),"\n",(0,r.jsx)(n.p,{children:"Dataset evaluation in MLflow is particularly valuable for several scenarios:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Batch Processing"})," - Perfect for evaluating large-scale batch prediction results from production systems without re-running expensive inference."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Historical Analysis"})," - Ideal for analyzing model performance trends over time using previously computed predictions and ground truth data."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Model Comparison"})," - Excellent for comparing different model versions' outputs on the same dataset without re-training or re-inference."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Production Monitoring"})," - Essential for automated evaluation pipelines that assess model performance on incoming batch predictions."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost Optimization"})," - Reduces computational costs by separating prediction generation from performance assessment, allowing evaluation without model re-execution."]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.p,{children:"When using dataset evaluation, consider these best practices:"}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Data Validation"}),": Always validate that prediction and target columns contain expected data types and ranges"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Missing Values"}),": Handle missing predictions or targets appropriately before evaluation"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Memory Management"}),": Use chunked processing or sampling for very large datasets"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Metadata Logging"}),": Log dataset characteristics, processing parameters, and evaluation context"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Storage Formats"}),": Use efficient formats like Parquet for large prediction datasets"]})]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Dataset evaluation in MLflow provides powerful capabilities for assessing model performance on pre-computed predictions. This approach is essential for production ML systems where you need to separate prediction generation from performance assessment."}),"\n",(0,r.jsx)(n.p,{children:"Key advantages of dataset evaluation include:"}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Flexibility"}),": Evaluate predictions from any source without re-running models"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Efficiency"}),": Skip expensive model inference when predictions are already available"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Scale"}),": Handle large-scale batch predictions and historical analysis"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Integration"}),": Seamlessly work with production prediction pipelines"]})]}),"\n",(0,r.jsx)(n.p,{children:"Whether you're analyzing batch predictions, conducting historical performance studies, or implementing automated evaluation pipelines, MLflow's dataset evaluation capabilities provide the tools needed for comprehensive model assessment at scale."})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>i});var t=a(96540);const r={},l=t.createContext(r);function s(e){const n=t.useContext(l);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(l.Provider,{value:n},e.children)}},49374:(e,n,a)=>{a.d(n,{B:()=>o});a(96540);const t=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=a(86025),l=a(28774),s=a(74848);const i=e=>{const n=e.split(".");for(let a=n.length;a>0;a--){const e=n.slice(0,a).join(".");if(t[e])return e}return null};function o({fn:e,children:n}){const a=i(e);if(!a)return(0,s.jsx)(s.Fragment,{children:n});const o=(0,r.Ay)(`/${t[a]}#${e}`);return(0,s.jsx)(l.A,{to:o,target:"_blank",children:n??(0,s.jsxs)("code",{children:[e,"()"]})})}}}]);