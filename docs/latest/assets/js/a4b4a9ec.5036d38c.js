"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["2347"],{27027(e,t,n){n.r(t),n.d(t,{metadata:()=>s,default:()=>C,frontMatter:()=>b,contentTitle:()=>k,toc:()=>A,assets:()=>N});var s=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/workflow","title":"End-to-End Judge Workflow","description":"Complete workflow for developing, testing, and deploying custom LLM judges","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/workflow.mdx","sourceDirName":"eval-monitor/scorers/llm-judge","slug":"/eval-monitor/scorers/llm-judge/workflow","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/workflow","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"End-to-End Judge Workflow","sidebar_label":"End-to-End Workflow","description":"Complete workflow for developing, testing, and deploying custom LLM judges"}}'),i=n(74848),r=n(28453),a=n(78010),l=n(57250),o=n(46077),d=n(95986),c=n(33508),u=n(10440),p=n(77541),h=n(33425),m=n(47792),g=n(93893),x=n(15977),j=n(22492),f=n(93164),v=n(87073),_=n(96393),w=n(96844),y=n(80827);let b={title:"End-to-End Judge Workflow",sidebar_label:"End-to-End Workflow",description:"Complete workflow for developing, testing, and deploying custom LLM judges"},k="End-to-End Judge Workflow",N={},A=[{value:"Why This Workflow Matters",id:"why-this-workflow-matters",level:2},{value:"The Development Cycle",id:"the-development-cycle",level:2},{value:"Step 1: Create Initial Judge",id:"step-1-create-initial-judge",level:2},{value:"Step 2: Generate Traces and Collect Feedback",id:"step-2-generate-traces-and-collect-feedback",level:2},{value:"Collecting Human Feedback",id:"collecting-human-feedback",level:3},{value:"How to Collect Feedback",id:"how-to-collect-feedback",level:3},{value:"Who Should Provide Feedback?",id:"who-should-provide-feedback",level:3},{value:"Step 3: Align Judge with Human Feedback",id:"step-3-align-judge-with-human-feedback",level:2},{value:"Step 4: Test and Register",id:"step-4-test-and-register",level:2},{value:"Step 5: Use the Registered Judge in Production",id:"step-5-use-the-registered-judge-in-production",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function S(e){let t={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"end-to-end-judge-workflow",children:"End-to-End Judge Workflow"})}),"\n",(0,i.jsx)(t.p,{children:"This guide walks through the complete lifecycle of developing and optimizing custom LLM judges using MLflow's judge APIs."}),"\n",(0,i.jsx)(t.h2,{id:"why-this-workflow-matters",children:"Why This Workflow Matters"}),"\n",(0,i.jsx)(c.A,{features:[{icon:m.A,title:"Systematic Development",description:"Move from subjective evaluation to data-driven judge development with clear metrics and goals."},{icon:g.A,title:"Human-AI Alignment",description:"Ensure your judges reflect human expertise and domain knowledge through structured feedback."},{icon:x.A,title:"Continuous Improvement",description:"Iterate and improve judge accuracy based on real-world performance and changing requirements."},{icon:j.A,title:"Production Ready",description:"Deploy judges with confidence knowing they've been tested and aligned with your quality standards."}]}),"\n",(0,i.jsx)(t.h2,{id:"the-development-cycle",children:"The Development Cycle"}),"\n",(0,i.jsx)(h.A,{steps:[{icon:f.A,title:"Create Judge",description:"Define evaluation criteria",detailedDescription:"Start with clear instructions that capture your domain expertise and evaluation requirements.",isFocus:!0},{icon:g.A,title:"Collect Feedback",description:"Gather human assessments",detailedDescription:"Run the judge on real traces and collect human feedback to establish ground truth."},{icon:v.A,title:"Align with Humans",description:"Optimize instructions",detailedDescription:"Use SIMBA optimizer to refine judge instructions based on human feedback."},{icon:_.A,title:"Test & Register",description:"Validate and deploy",detailedDescription:"Test the aligned judge and register it for production use when accuracy meets requirements."}],loopBackIcon:w.A,loopBackText:"Iterate",loopBackDescription:"Continue refining based on new data and requirements",circleSize:400}),"\n",(0,i.jsx)(t.h2,{id:"step-1-create-initial-judge",children:"Step 1: Create Initial Judge"}),"\n",(0,i.jsx)(t.p,{children:"Start by defining your evaluation criteria:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from typing import Literal\nimport mlflow\nfrom mlflow.genai.judges import make_judge\nfrom mlflow.entities import AssessmentSource, AssessmentSourceType\n\n# Create experiment for judge development\nexperiment_id = mlflow.create_experiment("support-judge-development")\nmlflow.set_experiment(experiment_id=experiment_id)\n\n# Create a judge for evaluating customer support responses\nsupport_judge = make_judge(\n    name="support_quality",\n    instructions="""\n    Evaluate the quality of this customer support response.\n\n    Rate as one of: excellent, good, needs_improvement, poor\n\n    Consider:\n    - Does it address the customer\'s issue?\n    - Is the tone professional and empathetic?\n    - Are next steps clear?\n\n    Focus on {{ outputs }} responding to {{ inputs }}.\n    """,\n    model="anthropic:/claude-opus-4-1-20250805",\n    feedback_value_type=Literal["excellent", "good", "needs_improvement", "poor"],\n)\n'})}),"\n",(0,i.jsx)(t.h2,{id:"step-2-generate-traces-and-collect-feedback",children:"Step 2: Generate Traces and Collect Feedback"}),"\n",(0,i.jsx)(t.p,{children:"Run your application to generate traces, then collect human feedback:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Generate traces from your application\n@mlflow.trace\ndef customer_support_app(issue):\n    # Your application logic here\n    return {"response": f"I\'ll help you with: {issue}"}\n\n\n# Run application to generate traces\nissues = [\n    "Password reset not working",\n    "Billing discrepancy",\n    "Feature request",\n    "Technical error",\n]\n\ntrace_ids = []\nfor issue in issues:\n    with mlflow.start_run(experiment_id=experiment_id):\n        result = customer_support_app(issue)\n        trace_id = mlflow.get_last_active_trace_id()\n        trace_ids.append(trace_id)\n\n        # Judge evaluates the trace\n        assessment = support_judge(inputs={"issue": issue}, outputs=result)\n\n        # Log judge\'s assessment\n        mlflow.log_assessment(trace_id=trace_id, assessment=assessment)\n'})}),"\n",(0,i.jsx)(t.h3,{id:"collecting-human-feedback",children:"Collecting Human Feedback"}),"\n",(0,i.jsx)(t.p,{children:"After running your judge on traces, collect human feedback to establish ground truth:"}),"\n",(0,i.jsx)(d.A,{children:(0,i.jsxs)(a.A,{children:[(0,i.jsxs)(l.A,{value:"ui",label:"MLflow UI (Recommended)",default:!0,children:[(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"When to use:"})," You need to collect human feedback for judge alignment."]}),(0,i.jsx)(t.p,{children:"The MLflow UI provides the most intuitive way to review traces and add feedback:"}),(0,i.jsx)(t.h3,{id:"how-to-collect-feedback",children:"How to Collect Feedback"}),(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Open the MLflow UI"})," and navigate to your experiment"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Go to the Traces tab"})," to see all generated traces"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Click on individual traces"})," to review:","\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Input data (customer issues)"}),"\n",(0,i.jsx)(t.li,{children:"Output responses"}),"\n",(0,i.jsx)(t.li,{children:"Judge's initial assessment"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Add your feedback"}),' by clicking "Add Feedback"']}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Select the assessment name"}),' matching your judge (e.g., "support_quality")']}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Provide your expert rating"})," (excellent, good, needs_improvement, or poor)"]}),"\n"]}),(0,i.jsx)(t.h3,{id:"who-should-provide-feedback",children:"Who Should Provide Feedback?"}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"If you're NOT the domain expert:"})}),(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Ask domain experts or other developers to provide labels through the MLflow UI"}),"\n",(0,i.jsx)(t.li,{children:"Distribute traces among team members with relevant expertise"}),"\n",(0,i.jsx)(t.li,{children:"Consider organizing feedback sessions where experts can review batches together"}),"\n"]}),(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"If you ARE the domain expert:"})}),(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Review traces directly in the MLflow UI and add your expert assessments"}),"\n",(0,i.jsx)(t.li,{children:"Create a rubric or guidelines document to ensure consistency"}),"\n",(0,i.jsx)(t.li,{children:"Document your evaluation criteria for future reference"}),"\n"]}),(0,i.jsx)(t.p,{children:"The UI automatically logs feedback in the correct format for alignment."}),(0,i.jsx)(o.A,{src:"/images/assessments/add_feedback_ui.png",alt:"Adding feedback through MLflow UI",width:"800px"})]}),(0,i.jsxs)(l.A,{value:"programmatic",label:"Programmatic (Existing Labels)",children:[(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"When to use:"})," You already have ground truth labels from your data."]}),(0,i.jsx)(t.p,{children:"If you have existing ground truth labels, log them programmatically:"}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Example: You have ground truth labels\nground_truth = {\n    trace_ids[0]: "excellent",  # Known good response\n    trace_ids[1]: "poor",  # Known bad response\n    trace_ids[2]: "good",  # Known acceptable response\n}\n\nfor trace_id, truth_value in ground_truth.items():\n    mlflow.log_feedback(\n        trace_id=trace_id,\n        name="support_quality",  # MUST match judge name\n        value=truth_value,\n        source=AssessmentSource(\n            source_type=AssessmentSourceType.HUMAN, source_id="ground_truth"\n        ),\n    )\n'})})]})]})}),"\n",(0,i.jsx)(t.h2,{id:"step-3-align-judge-with-human-feedback",children:"Step 3: Align Judge with Human Feedback"}),"\n",(0,i.jsx)(t.p,{children:"Use the SIMBA optimizer to improve judge accuracy:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Retrieve traces with both judge and human assessments\ntraces = mlflow.search_traces(locations=[experiment_id], return_type="list")\n\n# Filter for traces with both assessments\naligned_traces = []\nfor trace in traces:\n    assessments = trace.search_assessments(name="support_quality")\n    has_judge = any(\n        a.source.source_type == AssessmentSourceType.LLM_JUDGE for a in assessments\n    )\n    has_human = any(\n        a.source.source_type == AssessmentSourceType.HUMAN for a in assessments\n    )\n\n    if has_judge and has_human:\n        aligned_traces.append(trace)\n\nprint(f"Found {len(aligned_traces)} traces with both assessments")\n\n# Align the judge (requires at least 10 traces)\nif len(aligned_traces) >= 10:\n    # Option 1: Use default optimizer (recommended for simplicity)\n    aligned_judge = support_judge.align(aligned_traces)\n\n    # Option 2: Explicitly specify optimizer with custom model\n    # from mlflow.genai.judges.optimizers import SIMBAAlignmentOptimizer\n    # optimizer = SIMBAAlignmentOptimizer(model="anthropic:/claude-opus-4-1-20250805")\n    # aligned_judge = support_judge.align(aligned_traces, optimizer)\n\n    print("Judge aligned successfully!")\nelse:\n    print(f"Need at least 10 traces (have {len(aligned_traces)})")\n'})}),"\n",(0,i.jsx)(t.h2,{id:"step-4-test-and-register",children:"Step 4: Test and Register"}),"\n",(0,i.jsx)(t.p,{children:"Test the aligned judge and register it when ready:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# Test the aligned judge on new data\ntest_cases = [\n    {\n        "inputs": {"issue": "Can\'t log in"},\n        "outputs": {"response": "Let me reset your password for you."},\n    },\n    {\n        "inputs": {"issue": "Refund request"},\n        "outputs": {"response": "I\'ll process that refund immediately."},\n    },\n]\n\n# Evaluate with aligned judge\nfor case in test_cases:\n    assessment = aligned_judge(**case)\n    print(f"Issue: {case[\'inputs\'][\'issue\']}")\n    print(f"Judge rating: {assessment.value}")\n    print(f"Rationale: {assessment.rationale}\\n")\n\n# Register the aligned judge for production use\naligned_judge.register(experiment_id=experiment_id)\nprint("Judge registered and ready for deployment!")\n'})}),"\n",(0,i.jsx)(t.h2,{id:"step-5-use-the-registered-judge-in-production",children:"Step 5: Use the Registered Judge in Production"}),"\n",(0,i.jsxs)(t.p,{children:["Retrieve and use your registered judge with ",(0,i.jsx)(t.code,{children:"mlflow.genai.evaluate()"}),":"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from mlflow.genai import get_scorer\nimport pandas as pd\n\n# Retrieve the registered judge\nproduction_judge = get_scorer(name="support_quality", experiment_id=experiment_id)\n\n# Prepare evaluation data\neval_data = pd.DataFrame(\n    [\n        {\n            "inputs": {"issue": "Can\'t access my account"},\n            "outputs": {"response": "I\'ll help you regain access immediately."},\n        },\n        {\n            "inputs": {"issue": "Slow website performance"},\n            "outputs": {"response": "Let me investigate the performance issues."},\n        },\n    ]\n)\n\n# Run evaluation with the aligned judge\nresults = mlflow.genai.evaluate(data=eval_data, scorers=[production_judge])\n\n# View results and metrics\nprint("Evaluation metrics:", results.metrics)\nprint("\\nDetailed results:")\nprint(results.tables["eval_results_table"])\n\n# Assessments are automatically logged to the traces\n# You can view them in the MLflow UI Traces tab\n'})}),"\n",(0,i.jsx)(t.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(c.A,{features:[{icon:m.A,title:"Clear Instructions",description:"Start with specific, unambiguous evaluation criteria that reflect your domain requirements."},{icon:g.A,title:"Quality Feedback",description:"Ensure human feedback comes from domain experts who understand your evaluation standards."},{icon:_.A,title:"Sufficient Data",description:"Collect at least 10-15 traces with both assessments for effective alignment."},{icon:x.A,title:"Iterate Often",description:"Regularly re-align judges as your application evolves and new edge cases emerge."}]}),"\n",(0,i.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(u.A,{children:[(0,i.jsx)(p.A,{icon:v.A,iconSize:48,title:"Judge Alignment",description:"Deep dive into alignment techniques and optimization",link:"/genai/eval-monitor/scorers/llm-judge/alignment",linkText:"Learn alignment \u2192",containerHeight:64}),(0,i.jsx)(p.A,{icon:y.A,iconSize:48,title:"Dataset Integration",description:"Use judges with evaluation datasets for systematic testing",link:"/genai/eval-monitor/scorers/llm-judge/datasets",linkText:"Explore datasets \u2192",containerHeight:64}),(0,i.jsx)(p.A,{icon:f.A,iconSize:48,title:"Main Documentation",description:"Return to the custom judges overview",link:"/genai/eval-monitor/scorers/#llms-as-judges",linkText:"Back to overview \u2192",containerHeight:64})]})]})}function C(e={}){let{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(S,{...e})}):S(e)}},33425(e,t,n){n.d(t,{A:()=>l});var s=n(74848),i=n(96540);let r="loopContainer_P7aD",a="loopTitle_JPUj",l=({steps:e,title:t,loopBackIcon:n,loopBackText:l,loopBackDescription:o,circleSize:d=400})=>{let[c,u]=(0,i.useState)(null),[p,h]=(0,i.useState)(!1),[m,g]=(0,i.useState)({x:0,y:0}),[x,j]=(0,i.useState)(!1),f=(0,i.useRef)(null);i.useEffect(()=>{let e=()=>{j(window.innerWidth<=768)};return e(),window.addEventListener("resize",e),()=>window.removeEventListener("resize",e)},[]);let v=x?280:d,_=Math.max(x?100:140,Math.min(x?130:220,(d/2-(x?50:80))*Math.min(1.2,.8+.05*e.length))),w=v/2,y=v/2,b=t=>{let n=2*t*Math.PI/e.length-Math.PI/2;return{x:w+_*Math.cos(n),y:y+_*Math.sin(n)}},k=()=>{u(null)};return x?(0,s.jsxs)("div",{className:r,children:[t&&(0,s.jsx)("h3",{className:a,children:t}),(0,s.jsxs)("div",{className:"mobileLinearContent_PCYK",children:[e.map((t,n)=>(0,s.jsxs)("div",{className:"mobileStepItem_x9mX",children:[(0,s.jsxs)("div",{className:"mobileStepIndicator_zWzO",children:[(0,s.jsx)("div",{className:`mobileStepNumber_HnjD ${t.isFocus?"mobileFocusNode_FTRa":""}`,children:t.icon?(0,s.jsx)(t.icon,{size:20}):(0,s.jsx)("span",{children:n+1})}),n<e.length-1&&(0,s.jsx)("div",{className:"mobileStepConnector_kK9y"})]}),(0,s.jsxs)("div",{className:"mobileStepContent_jmKx",children:[(0,s.jsx)("h4",{className:"mobileStepTitle_P2DM",children:t.title}),(0,s.jsx)("p",{className:"mobileStepDescription_qbMN",children:t.detailedDescription||t.description})]})]},n)),n&&o&&(0,s.jsxs)("div",{className:"mobileLoopBack_nXtn",children:[(0,s.jsx)("div",{className:"mobileLoopIcon_FAGz",children:(0,s.jsx)(n,{size:24})}),(0,s.jsxs)("div",{className:"mobileLoopContent_BRFV",children:[(0,s.jsx)("h4",{className:"mobileLoopTitle_JcCt",children:l||"Iterate"}),(0,s.jsx)("p",{className:"mobileLoopDescription_5B8T",children:o})]})]})]})]}):(0,s.jsxs)("div",{className:r,children:[t&&(0,s.jsx)("h3",{className:a,children:t}),(0,s.jsx)("div",{className:"loopContent_d_OB",children:(0,s.jsxs)("div",{className:"circleContainer_r3vu",ref:f,style:{width:`${v}px`,height:`${v}px`},children:[(0,s.jsxs)("svg",{width:v,height:v,className:"svgCanvas_uDoP",children:[e.map((t,n)=>{let i,r,a,l,o,d,c,u,p,h=(n+1)%e.length;return(0,s.jsxs)("g",{children:[(0,s.jsx)("defs",{children:(0,s.jsx)("marker",{id:`arrowhead-${n}`,markerWidth:"6",markerHeight:"6",refX:"5",refY:"3",orient:"auto",children:(0,s.jsx)("path",{d:"M 0 0 L 6 3 L 0 6 L 1.5 3 Z",fill:"currentColor",opacity:"1",className:"arrowHead_pHvN"})})}),(0,s.jsx)("path",{d:(i=b(n),a=(r=b(h)).x-i.x,l=r.y-i.y,o=(i.x+r.x)/2,d=(i.y+r.y)/2,c=Math.sqrt(a*a+l*l),u=a/c,p=l/c,`M ${o-10*u} ${d-10*p} L ${o+10*u} ${d+10*p}`),fill:"none",stroke:"currentColor",strokeWidth:"2",strokeDasharray:"0",opacity:"0.9",markerEnd:`url(#arrowhead-${n})`,className:"arrowPath_C9al"})]},`arrow-${n}`)}),n&&(0,s.jsxs)("g",{className:"centerIcon_KAOa",onMouseEnter:()=>h(!0),onMouseLeave:()=>h(!1),style:{cursor:"pointer"},children:[(0,s.jsx)("foreignObject",{x:w-35,y:y-35,width:"70",height:"70",children:(0,s.jsx)("div",{className:"loopIconWrapper_xBPW",children:(0,s.jsx)(n,{size:32})})}),l&&(0,s.jsx)("text",{x:w,y:y+50,textAnchor:"middle",className:"loopText_T4eg",children:l})]})]}),e.map((e,t)=>{let n=b(t);return(0,s.jsxs)("div",{className:`stepNode_dfTI ${e.highlight?"highlighted_oNtg":""} ${e.isFocus?"focusNode_z3RB":""}`,style:{left:`${n.x}px`,top:`${n.y}px`,transform:"translate(-50%, -50%)"},onMouseEnter:e=>((e,t)=>{if(u(e),f.current){f.current.getBoundingClientRect();let t=b(e);g({x:t.x,y:t.y})}})(t,0),onMouseLeave:k,children:[(0,s.jsx)("div",{className:"stepNodeContent_qttg",children:e.icon?(0,s.jsx)(e.icon,{size:24}):(0,s.jsx)("span",{className:"stepNumber_LNrP",children:t+1})}),(0,s.jsx)("div",{className:"stepLabel_vl8R",children:e.title})]},t)}),null!==c&&(0,s.jsxs)("div",{className:"tooltip_UzKu",style:{left:`${m.x}px`,top:`${m.y}px`,transform:"translate(-50%, -120%)"},children:[(0,s.jsx)("h4",{className:"tooltipTitle_HAKW",children:e[c].title}),(0,s.jsx)("p",{className:"tooltipDescription_EDYJ",children:e[c].detailedDescription||e[c].description}),(0,s.jsx)("div",{className:"tooltipArrow_WNhr"})]}),p&&o&&(0,s.jsx)("div",{className:"centerTooltip_R18b",style:{left:`${w}px`,top:`${y}px`,transform:"translate(-50%, -50%)"},children:(0,s.jsx)("p",{className:"centerTooltipDescription_ttXB",children:o})})]})})]})}},33508(e,t,n){n.d(t,{A:()=>i});var s=n(74848);n(96540);function i({features:e,col:t=2}){return(0,s.jsx)("div",{className:"featureHighlights_Ardf",style:{gridTemplateColumns:`repeat(${t}, 1fr)`},children:e.map((e,t)=>(0,s.jsxs)("div",{className:"highlightItem_XPnN",children:[e.icon&&(0,s.jsx)("div",{className:"highlightIcon_SUR8",children:(0,s.jsx)(e.icon,{size:24})}),(0,s.jsxs)("div",{className:"highlightContent_d0XP",children:[(0,s.jsx)("h4",{children:e.title}),(0,s.jsx)("p",{children:e.description})]})]},t))})}},46077(e,t,n){n.d(t,{A:()=>r});var s=n(74848);n(96540);var i=n(66497);function r({src:e,alt:t,width:n,caption:r,className:a}){return(0,s.jsxs)("div",{className:`container_JwLF ${a||""}`,children:[(0,s.jsx)("div",{className:"imageWrapper_RfGN",style:n?{width:n}:{},children:(0,s.jsx)("img",{src:(0,i.default)(e),alt:t,className:"image_bwOA"})}),r&&(0,s.jsx)("p",{className:"caption_jo2G",children:r})]})}},95986(e,t,n){n.d(t,{A:()=>i});var s=n(74848);n(96540);function i({children:e}){return(0,s.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,t,n){n.d(t,{A:()=>d});var s=n(74848);n(96540);var i=n(95310),r=n(34164);let a="tileImage_O4So";var l=n(66497),o=n(92802);function d({icon:e,image:t,imageDark:n,imageWidth:d,imageHeight:c,iconSize:u=32,containerHeight:p,title:h,description:m,href:g,linkText:x="Learn more \u2192",className:j}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let f=p?{height:`${p}px`}:{},v={};return d&&(v.width=`${d}px`),c&&(v.height=`${c}px`),(0,s.jsxs)(i.A,{href:g,className:(0,r.A)("tileCard_NHsj",j),children:[(0,s.jsx)("div",{className:"tileIcon_pyoR",style:f,children:e?(0,s.jsx)(e,{size:u}):n?(0,s.jsx)(o.A,{sources:{light:(0,l.default)(t),dark:(0,l.default)(n)},alt:h,className:a,style:v}):(0,s.jsx)("img",{src:(0,l.default)(t),alt:h,className:a,style:v})}),(0,s.jsx)("h3",{children:h}),(0,s.jsx)("p",{children:m}),(0,s.jsx)("div",{className:"tileLink_iUbu",children:x})]})}},10440(e,t,n){n.d(t,{A:()=>r});var s=n(74848);n(96540);var i=n(34164);function r({children:e,className:t}){return(0,s.jsx)("div",{className:(0,i.A)("tilesGrid_hB9N",t),children:e})}}}]);