/*! For license information please see 337be522.0a3538a1.js.LICENSE.txt */
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[2332],{11470:(e,t,n)=>{n.d(t,{A:()=>b});var l=n(96540),a=n(34164),i=n(17559),r=n(23104),o=n(56347),s=n(205),c=n(57485),d=n(31682),h=n(70679);function m(e){return l.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:t,children:n}=e;return(0,l.useMemo)(()=>{const e=t??function(e){return m(e).map(({props:{value:e,label:t,attributes:n,default:l}})=>({value:e,label:t,attributes:n,default:l}))}(n);return function(e){const t=(0,d.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,n])}function p({value:e,tabValues:t}){return t.some(t=>t.value===e)}function f({queryString:e=!1,groupId:t}){const n=(0,o.W6)(),a=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c.aZ)(a),(0,l.useCallback)(e=>{if(!a)return;const t=new URLSearchParams(n.location.search);t.set(a,e),n.replace({...n.location,search:t.toString()})},[a,n])]}function w(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,i=u(e),[r,o]=(0,l.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find(e=>e.default)??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:i})),[c,d]=f({queryString:n,groupId:a}),[m,w]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,a]=(0,h.Dv)(t);return[n,(0,l.useCallback)(e=>{t&&a.set(e)},[t,a])]}({groupId:a}),g=(()=>{const e=c??m;return p({value:e,tabValues:i})?e:null})();(0,s.A)(()=>{g&&o(g)},[g]);return{selectedValue:r,selectValue:(0,l.useCallback)(e=>{if(!p({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);o(e),d(e),w(e)},[d,w,i]),tabValues:i}}var g=n(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=n(74848);function j({className:e,block:t,selectedValue:n,selectValue:l,tabValues:i}){const o=[],{blockElementScrollPositionUntilNextRender:s}=(0,r.a_)(),c=e=>{const t=e.currentTarget,a=o.indexOf(t),r=i[a].value;r!==n&&(s(t),l(r))},d=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=o.indexOf(e.currentTarget)+1;t=o[n]??o[0];break}case"ArrowLeft":{const n=o.indexOf(e.currentTarget)-1;t=o[n]??o[o.length-1];break}}t?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},e),children:i.map(({value:e,label:t,attributes:l})=>(0,v.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...l,className:(0,a.A)("tabs__item",x.tabItem,l?.className,{"tabs__item--active":n===e}),children:t??e},e))})}function y({lazy:e,children:t,selectedValue:n}){const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===n);return e?(0,l.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:i.map((e,t)=>(0,l.cloneElement)(e,{key:t,hidden:e.props.value!==n}))})}function _(e){const t=w(e);return(0,v.jsxs)("div",{className:(0,a.A)(i.G.tabs.container,"tabs-container",x.tabList),children:[(0,v.jsx)(j,{...t,...e}),(0,v.jsx)(y,{...t,...e})]})}function b(e){const t=(0,g.A)();return(0,v.jsx)(_,{...e,children:m(e.children)},String(t))}},19365:(e,t,n)=>{n.d(t,{A:()=>r});n(96540);var l=n(34164);const a={tabItem:"tabItem_Ymn6"};var i=n(74848);function r({children:e,hidden:t,className:n}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,l.A)(a.tabItem,n),hidden:t,children:e})}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>o});var l=n(96540);const a={},i=l.createContext(a);function r(e){const t=l.useContext(i);return l.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),l.createElement(i.Provider,{value:t},e.children)}},42640:(e,t,n)=>{n.d(t,{A:()=>l});const l=(0,n(84722).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},46543:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>f,contentTitle:()=>p,default:()=>x,frontMatter:()=>u,metadata:()=>l,toc:()=>w});const l=JSON.parse('{"id":"eval-monitor/legacy-llm-evaluation","title":"Migrating from Legacy LLM Evaluation","description":"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics.","source":"@site/docs/genai/eval-monitor/legacy-llm-evaluation.mdx","sourceDirName":"eval-monitor","slug":"/eval-monitor/legacy-llm-evaluation","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/legacy-llm-evaluation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"description":"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics."},"sidebar":"genAISidebar","previous":{"title":"AI Issue Discovery","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/ai-insights/ai-issue-discovery"},"next":{"title":"FAQ","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/faq"}}');var a=n(74848),i=n(28453),r=(n(11470),n(19365),n(49374)),o=(n(28774),n(66927)),s=n(96869),c=n(42640),d=n(51004),h=n(47792),m=n(85731);const u={description:"LLM evaluation involves assessing how well a model performs on a task. MLflow provides a simple API to evaluate your LLMs with popular metrics."},p="Migrating from Legacy LLM Evaluation",f={},w=[{value:"Why Migrate?",id:"why-migrate",level:2},{value:"1. Richer evaluation results",id:"1-richer-evaluation-results",level:5},{value:"2. More powerful and flexible LLM-as-a-Judge",id:"2-more-powerful-and-flexible-llm-as-a-judge",level:5},{value:"3. Integration with other MLflow GenAI capabilities",id:"3-integration-with-other-mlflow-genai-capabilities",level:5},{value:"4. Better future support",id:"4-better-future-support",level:5},{value:"Migration Steps",id:"migration-steps",level:2},{value:"1. Wrap Your Model in a Function",id:"1-wrap-your-model-in-a-function",level:3},{value:"2. Update the Dataset Format",id:"2-update-the-dataset-format",level:3},{value:"3. Migrate Metrics",id:"3-migrate-metrics",level:3},{value:"Example of custom LLM-as-a-Judge metrics",id:"example-of-custom-llm-as-a-judge-metrics",level:4},{value:"Example of custom heuristic metrics",id:"example-of-custom-heuristic-metrics",level:4},{value:"4. Run Evaluation",id:"4-run-evaluation",level:3},{value:"Other Changes",id:"other-changes",level:2},{value:"FAQ",id:"faq",level:2},{value:"Q: The feature I want is not supported in the new evaluation suite.",id:"q-the-feature-i-want-is-not-supported-in-the-new-evaluation-suite",level:3},{value:"Q: Where can I find the documentation for the legacy evaluation API?",id:"q-where-can-i-find-the-documentation-for-the-legacy-evaluation-api",level:3},{value:"Q: When will the legacy evaluation API be removed?",id:"q-when-will-the-legacy-evaluation-api-be-removed",level:3},{value:"Q: Should I migrate non-GenAI workloads to the new evaluation suite?",id:"q-should-i-migrate-non-genai-workloads-to-the-new-evaluation-suite",level:3}];function g(e){const t={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"migrating-from-legacy-llm-evaluation",children:"Migrating from Legacy LLM Evaluation"})}),"\n",(0,a.jsxs)(t.admonition,{type:"info",children:[(0,a.jsxs)(t.p,{children:["This is a migration guide for users who are using the legacy LLM evaluation capability through ",(0,a.jsx)(t.code,{children:"mlflow.evaluate"})," API and see the following warning while migrating to MLflow 3."]}),(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsx)(t.p,{children:"The mlflow.evaluate API has been deprecated as of MLflow 3.0.0."}),"\n"]}),(0,a.jsxs)(t.p,{children:["If you are new to MLflow or its evaluation capabilities, start from the ",(0,a.jsx)("ins",{children:(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/latest/genai/eval-monitor/index.html",children:"MLflow 3 GenAI Evaluation"})})," guide instead."]})]}),"\n",(0,a.jsx)(t.h2,{id:"why-migrate",children:"Why Migrate?"}),"\n",(0,a.jsxs)(t.p,{children:["MLflow 3 introduces a ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor",children:"new evaluation suite"})," that are optimized for evaluating LLMs and GenAI applications. Compared to the legacy evaluation through the ",(0,a.jsx)(t.code,{children:"mlflow.evaluate"})," API, the new suite offers the following benefits:"]}),"\n",(0,a.jsx)(t.h5,{id:"1-richer-evaluation-results",children:"1. Richer evaluation results"}),"\n",(0,a.jsx)(t.p,{children:"MLflow 3 displays the evaluation results with intuitive visualizations. Each prediction is recorded with a trace, which allows you to further investigate the result in details and identify the root cause of low quality predictions."}),"\n",(0,a.jsxs)("table",{children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Old Results"}),(0,a.jsx)("td",{children:(0,a.jsx)(o.A,{src:"/images/mlflow-3/eval-monitor/legacy-eval-result.png",alt:"Legacy Evaluation",width:"80%"})})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"New Results"}),(0,a.jsx)("td",{children:(0,a.jsx)(o.A,{src:"/images/mlflow-3/eval-monitor/prompt-evaluation-compare.png",alt:"New Evaluation",width:"80%"})})]})]}),"\n",(0,a.jsx)(t.h5,{id:"2-more-powerful-and-flexible-llm-as-a-judge",children:"2. More powerful and flexible LLM-as-a-Judge"}),"\n",(0,a.jsxs)(t.p,{children:["A rich set of built-in ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge",children:"LLM-as-a-Judge"})," scorers and a flexible toolset to build your own LLM-as-a-Judge supports you to evaluate various aspects of your LLM applications. Furthermore, the new ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/agentic-overview",children:"Agents-as-a-Judge"})," capability evaluates complex trace with minimum context window consumption and boilerplate code."]}),"\n",(0,a.jsx)(t.h5,{id:"3-integration-with-other-mlflow-genai-capabilities",children:"3. Integration with other MLflow GenAI capabilities"}),"\n",(0,a.jsxs)(t.p,{children:["The new evaluation suite is tightly integrated with other MLflow GenAI capabilities, such as ",(0,a.jsx)(t.a,{href:"/genai/tracing",children:"tracing"}),", ",(0,a.jsx)(t.a,{href:"/genai/prompt-registry",children:"prompt management"}),", ",(0,a.jsx)(t.a,{href:"/genai/prompt-registry/optimize-prompts",children:"prompt optimization"}),", making it an end-to-end solution for building high-quality LLM applications."]}),"\n",(0,a.jsx)(t.h5,{id:"4-better-future-support",children:"4. Better future support"}),"\n",(0,a.jsxs)(t.p,{children:["MLflow is rapidly evolving (",(0,a.jsx)(t.a,{href:"https://github.com/mlflow/mlflow/releases",children:"changelog"}),") and will continue strengthening its evaluation capabilities with the north star of ",(0,a.jsx)(t.strong,{children:"Deliver production-ready AI"}),". Migrating your workload to the new evaluation suite will ensure you have instant access to the latest and greatest features."]}),"\n",(0,a.jsx)(t.h2,{id:"migration-steps",children:"Migration Steps"}),"\n",(0,a.jsx)(s.A,{width:"wide",steps:[{icon:c.A,title:"Wrap your model in a function",description:"If you are evaluating an MLflow Model, wrap the model in a function and pass it to the new evaluation API."},{icon:d.A,title:"Update dataset format",description:"Update the inputs and ground truth format to match the new evaluation dataset format."},{icon:h.A,title:"Migrate metrics",description:"Update the metrics to use the new built-in or custom scorers offered by MLflow 3."},{icon:m.A,title:"Run evaluation",description:"Execute the evaluation and make sure the results are as expected."}]}),"\n",(0,a.jsx)(t.admonition,{title:"Before you start the migration",type:"tip",children:(0,a.jsxs)(t.p,{children:["Before starting the migration, we highly recommend you to visit the ",(0,a.jsx)("ins",{children:(0,a.jsx)(t.a,{href:"/genai/eval-monitor",children:"GenAI Evaluation Guide"})})," and go through the ",(0,a.jsx)("ins",{children:(0,a.jsx)(t.a,{href:"/genai/eval-monitor/quickstart",children:"Quickstart"})})," to get a sense of the new evaluation suite. Basic understanding of the concepts will help you to migrate your existing workload smoothly."]})}),"\n",(0,a.jsx)(t.h3,{id:"1-wrap-your-model-in-a-function",children:"1. Wrap Your Model in a Function"}),"\n",(0,a.jsxs)(t.p,{children:["The old evaluation API accepts MLflow model URI as an evaluation target. The new evaluation API accepts a callable function as ",(0,a.jsx)(t.code,{children:"predict_fn"})," argument instead, to provide more flexibility and control. This also eliminates the need of logging the model in MLflow before evaluation."]}),"\n",(0,a.jsxs)("table",{style:{tableLayout:"fixed",width:"100%"},children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Old Format"}),(0,a.jsx)("th",{children:"New Format"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,a.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,a.jsx)("code",{className:"language-python",children:'# Log the model first before evaluation\nwith mlflow.start_run() as run:\n    logged_model_info = mlflow.openai.log_model(\n        model="gpt-5-mini",\n        task=openai.chat.completions,\n        artifact_path="model",\n        messages=[\n            {"role": "system", "content": "Answer the following question in two sentences"},\n            {"role": "user", "content": "{question}"},\n        ],\n    )\n\n# Pass the model URI to the evaluation API.\nmlflow.evaluate(model=logged_model_info.model_uri, ...)\n'})})}),(0,a.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,a.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,a.jsx)("code",{className:"language-python",children:'# Define a function that runs predictions.\ndef predict_fn(question: str) -> str:\n  response = openai.OpenAI().chat.completions.create(\n      model="gpt-5-mini",\n      messages=[\n          {"role": "system", "content": "Answer the following question in two sentences"},\n          {"role": "user", "content": question},\n      ],\n  )\n  return response.choices[0].message.content\n\nmlflow.genai.evaluate(predict_fn=predict_fn, ...)\n'})})})]})]}),"\n",(0,a.jsx)(t.p,{children:"If you want to evaluate a pre-logged model with the new evaluation API, simply call the loaded model in the function."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# IMPORTANT: Load the model outside the predict_fn function. Otherwise the model will be loaded\n# for each input in the dataset and significantly slow down the evaluation.\nmodel = mlflow.pyfunc.load_model(model_uri)\n\n\ndef predict_fn(question: str) -> str:\n    return model.predict([question])[0]\n"})}),"\n",(0,a.jsx)(t.h3,{id:"2-update-the-dataset-format",children:"2. Update the Dataset Format"}),"\n",(0,a.jsx)(t.p,{children:"The dataset format has been changed to be more flexible and consistent. The new format requirements are:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"inputs"}),": The input to the predict_fn function. The key(s) must match the parameter name of the predict_fn function."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"expectations"}),": The expected output from the predict_fn function, namely, ground truth for the answer."]}),"\n",(0,a.jsxs)(t.li,{children:["Optionally, you can pass ",(0,a.jsx)(t.code,{children:"outputs"})," column or ",(0,a.jsx)(t.code,{children:"trace"})," column to evaluate pre-generated outputs and traces."]}),"\n"]}),"\n",(0,a.jsxs)("table",{style:{tableLayout:"fixed",width:"100%"},children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Old Format"}),(0,a.jsx)("th",{children:"New Format"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,a.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,a.jsx)("code",{className:"language-python",children:'eval_data = pd.DataFrame(\n  {\n      "inputs": [\n          "What is MLflow?",\n          "What is Spark?",\n      ],\n      "ground_truth": [\n          "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle.",\n          "Apache Spark is an open-source, distributed computing system designed for big data processing and analytics.",\n      ],\n      "predictions": [\n        "MLflow is an open-source MLOps platform",\n        "Apache Spark is an open-source distributed computing engine.",\n      ]\n  }\n)\n\nmlflow.evaluate(\n  data=eval_data, # Needed to specify the ground truth and prediction # columns name, otherwise MLflow does not recognize them.\n  targets="ground_truth",\n  predictions="predictions",\n  ...\n)\n'})})}),(0,a.jsx)("td",{style:{maxWidth:"50%",verticalAlign:"top"},children:(0,a.jsx)("pre",{style:{whiteSpace:"pre",overflowX:"auto",margin:0},children:(0,a.jsx)("code",{className:"language-python",children:'eval_data = [\n  {\n      "inputs": {"question": "What is MLflow?"},\n      "outputs": "MLflow is an open-source MLOps platform",\n      "expectations": {"answer": "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle."},\n  },\n  {\n      "inputs": {"question": "What is Spark?"},\n      "outputs": "Apache Spark is an open-source distributed computing engine.",\n      "expectations": {"answer": "Apache Spark is an open-source, distributed computing system designed for big data processing and analytics."},\n  },\n]\n\nmlflow.genai.evaluate(\n  data=eval_data,\n  ...\n)\n'})})})]})]}),"\n",(0,a.jsx)(t.h3,{id:"3-migrate-metrics",children:"3. Migrate Metrics"}),"\n",(0,a.jsx)(t.p,{children:"The new evaluation API supports a rich set of built-in and custom LLM-as-a-Judge metrics. The table below shows the mapping between the legacy metrics and the new metrics."}),"\n",(0,a.jsxs)("table",{children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Metric"}),(0,a.jsx)("th",{children:"Before"}),(0,a.jsx)("th",{children:"After"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:"Latency"}),(0,a.jsx)("td",{children:(0,a.jsx)(r.B,{fn:"mlflow.metrics.latency",children:(0,a.jsx)(t.code,{children:"latency"})})}),(0,a.jsxs)("td",{children:["Traces record latency and also span-level break down. You don't need to specify a metric to evaluate latency when running the new ",(0,a.jsx)(r.B,{fn:"mlflow.genai.evaluate"})," API."]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:"Token Count"}),(0,a.jsx)("td",{children:(0,a.jsx)(r.B,{fn:"mlflow.metrics.token_count",children:(0,a.jsx)(t.code,{children:"token_count"})})}),(0,a.jsxs)("td",{children:["Traces record token count for LLM calls for most of popular LLM providers. For other cases, you can use a ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/custom",children:"custom scorer"})," to calculate the token count."]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:"Heuristic NLP metrics"}),(0,a.jsxs)("td",{children:[(0,a.jsx)(r.B,{fn:"mlflow.metrics.toxicity",children:(0,a.jsx)(t.code,{children:"toxicity"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.flesch_kincaid_grade_level",children:(0,a.jsx)(t.code,{children:"flesch_kincaid_grade_level"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.ari_grade_level",children:(0,a.jsx)(t.code,{children:"ari_grade_level"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.exact_match",children:(0,a.jsx)(t.code,{children:"exact_match"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.rouge1",children:(0,a.jsx)(t.code,{children:"rouge1"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.rouge2",children:(0,a.jsx)(t.code,{children:"rouge2"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.rougeL",children:(0,a.jsx)(t.code,{children:"rougeL"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.rougeLsum",children:(0,a.jsx)(t.code,{children:"rougeLsum"})})]}),(0,a.jsxs)("td",{children:["Use a ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/custom",children:"Code-based Scorer"})," to implement the equivalent metrics. See the example below for reference."]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:"Retrieval metrics"}),(0,a.jsxs)("td",{children:[(0,a.jsx)(r.B,{fn:"mlflow.metrics.precision_at_k",children:(0,a.jsx)(t.code,{children:"precision_at_k"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.recall_at_k",children:(0,a.jsx)(t.code,{children:"recall_at_k"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.ndcg_at_k",children:(0,a.jsx)(t.code,{children:"ndcg_at_k"})})]}),(0,a.jsxs)("td",{children:["Use the new ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/predefined/#available-scorers",children:"built-in retrieval metrics"})," or define a custom code-based scorer."]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:"Built-in LLM-as-a-Judge metrics"}),(0,a.jsxs)("td",{children:[(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.answer_similarity",children:(0,a.jsx)(t.code,{children:"answer_similarity"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.answer_correctness",children:(0,a.jsx)(t.code,{children:"answer_correctness"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.answer_relevance",children:(0,a.jsx)(t.code,{children:"answer_relevance"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.relevance",children:(0,a.jsx)(t.code,{children:"relevance"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.faithfulness",children:(0,a.jsx)(t.code,{children:"faithfulness"})})]}),(0,a.jsxs)("td",{children:["Use the new ",(0,a.jsx)(t.a,{href:"http://localhost:3000/genai/eval-monitor/scorers/llm-judge/predefined/#available-scorers",children:"built-in LLM scorers"}),". If the metric is not supported out of the box, define a custom LLM-as-a-Judge scorer using the ",(0,a.jsx)(r.B,{fn:"mlflow.genai.judges.make_judge",children:(0,a.jsx)(t.code,{children:"make_judge"})})," API, following the example below."]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:"Custom LLM-as-a-Judge metrics"}),(0,a.jsxs)("td",{children:[(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.make_genai_metric",children:(0,a.jsx)(t.code,{children:"make_genai_metric"})}),", ",(0,a.jsx)(r.B,{fn:"mlflow.metrics.genai.make_genai_metric_from_prompt",children:(0,a.jsx)(t.code,{children:"make_genai_metric_from_prompt"})})]}),(0,a.jsxs)("td",{children:["Use the ",(0,a.jsx)(r.B,{fn:"mlflow.genai.judges.make_judge",children:(0,a.jsx)(t.code,{children:"make_judge"})})," API to define a custom LLM-as-a-Judge scorer, following the example below."]})]})]}),"\n",(0,a.jsx)(t.h4,{id:"example-of-custom-llm-as-a-judge-metrics",children:"Example of custom LLM-as-a-Judge metrics"}),"\n",(0,a.jsxs)(t.p,{children:["The new evaluation API supports defining custom LLM-as-a-Judge metrics from a custom prompt template. This eliminates a lot of complexity and over-abstractions from the previous ",(0,a.jsx)(t.code,{children:"make_genai_metric"})," and ",(0,a.jsx)(t.code,{children:"make_genai_metric_from_prompt"})," APIs."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from mlflow.genai import make_judge\n\nanswer_similarity = make_judge(\n    name="answer_similarity",\n    instructions=(\n        "Evaluated on the degree of semantic similarity of the provided output to the expected answer.\\n\\n"\n        "Output: {{ outputs }}\\n\\n"\n        "Expected: {{ expectations }}"\n    ),\n    feedback_value_type=int,\n)\n\n# Pass the scorer to the evaluation API.\nmlflow.genai.evaluate(scorers=[answer_similarity, ...])\n'})}),"\n",(0,a.jsxs)(t.p,{children:["See the ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge",children:"LLM-as-a-Judge Scorers"})," guide for more details."]}),"\n",(0,a.jsx)(t.h4,{id:"example-of-custom-heuristic-metrics",children:"Example of custom heuristic metrics"}),"\n",(0,a.jsxs)(t.p,{children:["Implementing a custom scorer for heuristic metrics is straightforward. You just need to define a function and decorate it with the ",(0,a.jsx)(r.B,{fn:"mlflow.genai.scorers.scorer",children:(0,a.jsx)(t.code,{children:"@scorer"})})," decorator. The example below shows how to implement the ",(0,a.jsx)(t.code,{children:"exact_match"})," metric."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'@scorer\ndef exact_match(outputs: dict, expectations: dict) -> bool:\n    return outputs == expectations["expected_response"]\n\n\n# Pass the scorer to the evaluation API.\nmlflow.genai.evaluate(scorers=[exact_match, ...])\n'})}),"\n",(0,a.jsxs)(t.p,{children:["See the ",(0,a.jsx)(t.a,{href:"/genai/eval-monitor/scorers/custom",children:"Code-based Scorers"})," guide for more details."]}),"\n",(0,a.jsx)(t.h3,{id:"4-run-evaluation",children:"4. Run Evaluation"}),"\n",(0,a.jsx)(t.p,{children:"Now you have migrated all components of the legacy evaluation API and are ready to run the evaluation!"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"mlflow.genai.evaluate(\n    data=eval_data,\n    predict_fn=predict_fn,\n    scorers=[answer_similarity, exact_match, ...],\n)\n"})}),"\n",(0,a.jsxs)(t.p,{children:["To view the evaluation results, click the link in the console output, or navigate to the ",(0,a.jsx)(t.strong,{children:"Evaluations"})," tab in the MLflow UI."]}),"\n",(0,a.jsx)(o.A,{src:"/images/mlflow-3/eval-monitor/evaluation-result-video.gif",alt:"Prompt Evaluation"}),"\n",(0,a.jsx)(t.h2,{id:"other-changes",children:"Other Changes"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["When using Databricks Model Serving endpoint as a LLM-judge model, use ",(0,a.jsx)(t.code,{children:"databricks:/<endpoint-name>"})," as model provider, rather than ",(0,a.jsx)(t.code,{children:"endpoints:/<endpoint-name>"})]}),"\n",(0,a.jsxs)(t.li,{children:["The evaluation results are shown in the ",(0,a.jsx)(t.code,{children:"Evaluations"})," tab in the MLflow UI."]}),"\n",(0,a.jsxs)(t.li,{children:["Lots of configuration knobs such as ",(0,a.jsx)(t.code,{children:"model_type"}),", ",(0,a.jsx)(t.code,{children:"targets"}),", ",(0,a.jsx)(t.code,{children:"feature_names"}),", ",(0,a.jsx)(t.code,{children:"env_manager"}),", are removed in the new evaluation API."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"faq",children:"FAQ"}),"\n",(0,a.jsx)(t.h3,{id:"q-the-feature-i-want-is-not-supported-in-the-new-evaluation-suite",children:"Q: The feature I want is not supported in the new evaluation suite."}),"\n",(0,a.jsxs)(t.p,{children:["Please open an feature request in ",(0,a.jsx)(t.a,{href:"https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml",children:"GitHub"}),"."]}),"\n",(0,a.jsx)(t.h3,{id:"q-where-can-i-find-the-documentation-for-the-legacy-evaluation-api",children:"Q: Where can I find the documentation for the legacy evaluation API?"}),"\n",(0,a.jsxs)(t.p,{children:["See ",(0,a.jsx)(t.a,{href:"https://mlflow.org/docs/2.22.1/llms/llm-evaluate",children:"MLflow 2 documentation"})," for the legacy evaluation API."]}),"\n",(0,a.jsx)(t.h3,{id:"q-when-will-the-legacy-evaluation-api-be-removed",children:"Q: When will the legacy evaluation API be removed?"}),"\n",(0,a.jsx)(t.p,{children:"It will likely be removed in MLflow 3.7.0 or a few releases after that."}),"\n",(0,a.jsx)(t.h3,{id:"q-should-i-migrate-non-genai-workloads-to-the-new-evaluation-suite",children:"Q: Should I migrate non-GenAI workloads to the new evaluation suite?"}),"\n",(0,a.jsxs)(t.p,{children:["No. The new evaluation suite is only for GenAI workloads. If you are not using GenAI, you should use the ",(0,a.jsx)(r.B,{fn:"mlflow.models.evaluate"})," API, which offers perfect compatibility with ",(0,a.jsx)(t.code,{children:"mlflow.evaluate"})," API but drops the GenAI-specific features."]})]})}function x(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}},47792:(e,t,n)=>{n.d(t,{A:()=>l});const l=(0,n(84722).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},49374:(e,t,n)=>{n.d(t,{B:()=>o});n(96540);const l=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var a=n(86025),i=n(74848);const r=e=>{const t=e.split(".");for(let n=t.length;n>0;n--){const e=t.slice(0,n).join(".");if(l[e])return e}return null};function o({fn:e,children:t,hash:n}){const o=r(e);if(!o)return(0,i.jsx)(i.Fragment,{children:t});const s=(0,a.default)(`/${l[o]}#${n??e}`);return(0,i.jsx)("a",{href:s,target:"_blank",children:t??(0,i.jsxs)("code",{children:[e,"()"]})})}},51004:(e,t,n)=>{n.d(t,{A:()=>l});const l=(0,n(84722).A)("database",[["ellipse",{cx:"12",cy:"5",rx:"9",ry:"3",key:"msslwz"}],["path",{d:"M3 5V19A9 3 0 0 0 21 19V5",key:"1wlel7"}],["path",{d:"M3 12A9 3 0 0 0 21 12",key:"mv7ke4"}]])},66927:(e,t,n)=>{n.d(t,{A:()=>r});n(96540);const l={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var a=n(86025),i=n(74848);function r({src:e,alt:t,width:n,caption:r,className:o}){return(0,i.jsxs)("div",{className:`${l.container} ${o||""}`,children:[(0,i.jsx)("div",{className:l.imageWrapper,style:n?{width:n}:{},children:(0,i.jsx)("img",{src:(0,a.default)(e),alt:t,className:l.image})}),r&&(0,i.jsx)("p",{className:l.caption,children:r})]})}},84722:(e,t,n)=>{n.d(t,{A:()=>c});var l=n(96540);const a=e=>{const t=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,n)=>n?n.toUpperCase():t.toLowerCase()))(e);return t.charAt(0).toUpperCase()+t.slice(1)},i=(...e)=>e.filter((e,t,n)=>Boolean(e)&&""!==e.trim()&&n.indexOf(e)===t).join(" ").trim(),r=e=>{for(const t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0};var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const s=(0,l.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:a,className:s="",children:c,iconNode:d,...h},m)=>(0,l.createElement)("svg",{ref:m,...o,width:t,height:t,stroke:e,strokeWidth:a?24*Number(n)/Number(t):n,className:i("lucide",s),...!c&&!r(h)&&{"aria-hidden":"true"},...h},[...d.map(([e,t])=>(0,l.createElement)(e,t)),...Array.isArray(c)?c:[c]])),c=(e,t)=>{const n=(0,l.forwardRef)(({className:n,...r},o)=>{return(0,l.createElement)(s,{ref:o,iconNode:t,className:i(`lucide-${c=a(e),c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,n),...r});var c});return n.displayName=a(e),n}},85731:(e,t,n)=>{n.d(t,{A:()=>l});const l=(0,n(84722).A)("play",[["polygon",{points:"6 3 20 12 6 21 6 3",key:"1oa8hb"}]])},96869:(e,t,n)=>{n.d(t,{A:()=>g});n(96540);const l="workflowContainer__N1v",a="workflowTitle_QrAr",i="stepsContainer_IGeu",r="screenshotContainer_OwzZ",o="stepItem_GyHJ",s="stepIndicator_U2Wb",c="stepNumber_vINc",d="stepNumberText_eLd7",h="stepConnector_Si86",m="stepContent_D0CA",u="stepTitle_wujx",p="stepDescription_PIaE";var f=n(66927),w=n(74848);const g=({steps:e,title:t,screenshot:n,width:g="normal"})=>(0,w.jsxs)("div",{className:l,children:[t&&(0,w.jsx)("h3",{className:a,children:t}),n&&(0,w.jsx)("div",{className:r,children:(0,w.jsx)(f.A,{src:n.src,alt:n.alt,width:n.width||"90%"})}),(0,w.jsx)("div",{className:i,style:{maxWidth:"wide"===g?"850px":"700px"},children:e.map((t,n)=>(0,w.jsxs)("div",{className:o,children:[(0,w.jsxs)("div",{className:s,children:[(0,w.jsx)("div",{className:c,children:t.icon?(0,w.jsx)(t.icon,{size:16}):(0,w.jsx)("span",{className:d,children:n+1})}),n<e.length-1&&(0,w.jsx)("div",{className:h})]}),(0,w.jsxs)("div",{className:m,children:[(0,w.jsx)("h4",{className:u,children:t.title}),(0,w.jsx)("p",{className:p,children:t.description})]})]},n))})]})}}]);