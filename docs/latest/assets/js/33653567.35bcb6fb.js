"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["2564"],{7695(e,n,s){s.r(n),s.d(n,{metadata:()=>r,default:()=>_,frontMatter:()=>x,contentTitle:()=>j,toc:()=>w,assets:()=>v});var r=JSON.parse('{"id":"serving/index","title":"MLflow Model Serving","description":"Transform your trained models into production-ready inference servers with MLflow\'s comprehensive serving capabilities. Deploy locally, in the cloud, or through managed endpoints with standardized REST APIs.","source":"@site/docs/genai/serving/index.mdx","sourceDirName":"serving","slug":"/serving/","permalink":"/mlflow-website/docs/latest/genai/serving/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"MLflow MCP Server","permalink":"/mlflow-website/docs/latest/genai/mcp/"},"next":{"title":"Agent Server","permalink":"/mlflow-website/docs/latest/genai/serving/agent-server"}}'),i=s(74848),o=s(28453),t=s(78010),l=s(57250),a=s(95986),d=s(10440),c=s(77541),p=s(34742),m=s(30684),h=s(93164),u=s(80964),g=s(46858),f=s(93893);let x={},j="MLflow Model Serving",v={},w=[{value:"Quick Start",id:"quick-start",level:2},{value:"How Model Serving Works",id:"how-model-serving-works",level:2},{value:"Server Startup and Model Loading",id:"server-startup-and-model-loading",level:3},{value:"Request Processing Pipeline",id:"request-processing-pipeline",level:3},{value:"Model Prediction and Response",id:"model-prediction-and-response",level:3},{value:"The Flavor System",id:"the-flavor-system",level:3},{value:"Error Handling and Debugging",id:"error-handling-and-debugging",level:3},{value:"Input Format Examples",id:"input-format-examples",level:3},{value:"Key Implementation Concepts",id:"key-implementation-concepts",level:2},{value:"Complete Example: Train to Production",id:"complete-example-train-to-production",level:2},{value:"Next Steps",id:"next-steps",level:2}];function y(e){let n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"mlflow-model-serving",children:"MLflow Model Serving"})}),"\n",(0,i.jsx)(n.p,{children:"Transform your trained models into production-ready inference servers with MLflow's comprehensive serving capabilities. Deploy locally, in the cloud, or through managed endpoints with standardized REST APIs."}),"\n",(0,i.jsx)(p.A,{concepts:[{icon:m.A,title:"REST API Endpoints",description:"Automatic generation of standardized REST endpoints for model inference with consistent request/response formats."},{icon:h.A,title:"Multi-Framework Support",description:"Serve models from any ML framework through MLflow's flavor system with unified deployment patterns."},{icon:u.A,title:"Custom Applications",description:"Build sophisticated serving applications with custom logic, preprocessing, and business rules."},{icon:g.A,title:"Scalable Deployment",description:"Deploy to various targets from local development servers to cloud platforms and Kubernetes clusters."}]}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.p,{children:"Get your model serving in minutes with these simple steps:"}),"\n",(0,i.jsx)(a.A,{children:(0,i.jsxs)(t.A,{children:[(0,i.jsxs)(l.A,{value:"serve",label:"1. Serve Model",default:!0,children:[(0,i.jsx)(n.p,{children:"Choose your serving approach:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Serve a logged model\nmlflow models serve -m "models:/<model-id>" -p 5000\n\n# Serve a registered model\nmlflow models serve -m "models:/<model-name>/<model-version>" -p 5000\n\n# Serve a model from local path\nmlflow models serve -m ./path/to/model -p 5000\n'})}),(0,i.jsxs)(n.p,{children:["Your model will be available at ",(0,i.jsx)(n.code,{children:"http://localhost:5000"})]})]}),(0,i.jsxs)(l.A,{value:"predict",label:"2. Make Predictions",children:[(0,i.jsx)(n.p,{children:"Send prediction requests via HTTP:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:5000/invocations \\\n  -H "Content-Type: application/json" \\\n  -d \'{"inputs": [[1, 2, 3, 4]]}\'\n'})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Using Python:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\n\ndata = {\n    "dataframe_split": {\n        "columns": ["feature1", "feature2", "feature3", "feature4"],\n        "data": [[1, 2, 3, 4]],\n    }\n}\n\nresponse = requests.post(\n    "http://localhost:5000/invocations",\n    headers={"Content-Type": "application/json"},\n    data=json.dumps(data),\n)\n\nprint(response.json())\n'})})]})]})}),"\n",(0,i.jsx)(n.h2,{id:"how-model-serving-works",children:"How Model Serving Works"}),"\n",(0,i.jsx)(n.p,{children:"MLflow transforms your trained models into production-ready HTTP servers through a carefully orchestrated process that handles everything from model loading to request processing."}),"\n",(0,i.jsx)(n.h3,{id:"server-startup-and-model-loading",children:"Server Startup and Model Loading"}),"\n",(0,i.jsxs)(n.p,{children:["When you run ",(0,i.jsx)(n.code,{children:"mlflow models serve"}),", MLflow begins by analyzing your model's metadata to determine how to load it. Each model contains an ",(0,i.jsx)(n.code,{children:"MLmodel"}),' file that specifies which "flavor" it uses - whether it\'s scikit-learn, PyTorch, TensorFlow, or a custom PyFunc model.']}),"\n",(0,i.jsx)(n.p,{children:"MLflow downloads the model artifacts to a local directory and creates a FastAPI server with standardized endpoints. The server loads your model using the appropriate flavor-specific loading logic. For example, a scikit-learn model is loaded using pickle, while a PyTorch model loads its state dictionary and model class."}),"\n",(0,i.jsx)(n.p,{children:"The server exposes four key endpoints:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"POST /invocations"})})," - The main prediction endpoint"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"GET /ping"})})," and ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"GET /health"})})," - Health checks for monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"GET /version"})})," - Returns server and model information"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"request-processing-pipeline",children:"Request Processing Pipeline"}),"\n",(0,i.jsxs)(n.p,{children:["When a prediction request arrives at ",(0,i.jsx)(n.code,{children:"/invocations"}),", MLflow processes it through several validation and transformation steps:"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Input Format Detection"}),": MLflow automatically detects which input format you're using. It supports multiple formats to accommodate different use cases:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dataframe_split"}),": Pandas DataFrame with separate columns and data arrays"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dataframe_records"}),": List of dictionaries representing rows"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"instances"}),": TensorFlow Serving format for individual predictions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"inputs"}),": Named tensor format for more complex inputs"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Schema Validation"}),": If your model includes a signature (input/output schema), MLflow validates the incoming data against it. This catches type mismatches and missing columns before they reach your model."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parameter Extraction"}),": MLflow separates prediction data from optional parameters. Parameters like ",(0,i.jsx)(n.code,{children:"temperature"})," for language models or ",(0,i.jsx)(n.code,{children:"threshold"})," for classifiers are extracted and passed separately to models that support them."]}),"\n",(0,i.jsx)(n.h3,{id:"model-prediction-and-response",children:"Model Prediction and Response"}),"\n",(0,i.jsxs)(n.p,{children:["Once the input is validated and formatted, MLflow calls your model's ",(0,i.jsx)(n.code,{children:"predict()"})," method. The framework automatically detects whether your model accepts parameters and calls it appropriately:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# For models that accept parameters\nraw_predictions = model.predict(data, params=params)\n\n# For traditional models\nraw_predictions = model.predict(data)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["MLflow then serializes the predictions back to JSON, handling various data types including NumPy arrays, pandas DataFrames, and Python lists. The response format depends on your input format - traditional requests get wrapped in a ",(0,i.jsx)(n.code,{children:"predictions"})," object, while LLM-style requests return unwrapped results."]}),"\n",(0,i.jsx)(n.h3,{id:"the-flavor-system",children:"The Flavor System"}),"\n",(0,i.jsx)(n.p,{children:"MLflow's flavor system is what makes serving work consistently across different ML frameworks. Each flavor implements framework-specific loading and prediction logic while exposing a unified interface."}),"\n",(0,i.jsxs)(n.p,{children:["When you log a model using ",(0,i.jsx)(n.code,{children:"mlflow.sklearn.log_model()"})," or ",(0,i.jsx)(n.code,{children:"mlflow.pytorch.log_model()"}),", MLflow creates both a flavor-specific representation and a PyFunc wrapper. The PyFunc wrapper provides the standardized ",(0,i.jsx)(n.code,{children:"predict()"})," interface that the serving layer expects, while the flavor handles the framework-specific details like tensor operations or data preprocessing."]}),"\n",(0,i.jsx)(n.p,{children:"This architecture means you can serve scikit-learn, PyTorch, TensorFlow, and custom models using identical serving commands and APIs."}),"\n",(0,i.jsx)(n.h3,{id:"error-handling-and-debugging",children:"Error Handling and Debugging"}),"\n",(0,i.jsx)(n.p,{children:"MLflow's serving infrastructure includes comprehensive error handling to help you debug issues:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Schema Errors"}),": Detailed messages about data type mismatches or missing columns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Format Errors"}),": Clear guidance when input format is incorrect or ambiguous"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Errors"}),": Full stack traces from your model's prediction code"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Server Errors"}),": Timeout and resource-related error handling"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The server logs all requests and errors, making it easier to diagnose production issues."}),"\n",(0,i.jsx)(n.h3,{id:"input-format-examples",children:"Input Format Examples"}),"\n",(0,i.jsx)(n.p,{children:"Here are the main input formats MLflow accepts:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'// dataframe_split format\n{\n  "dataframe_split": {\n    "columns": ["feature1", "feature2", "feature3"],\n    "data": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n  }\n}\n\n// dataframe_records format\n{\n  "dataframe_records": [\n    {"feature1": 1.0, "feature2": 2.0, "feature3": 3.0},\n    {"feature1": 4.0, "feature2": 5.0, "feature3": 6.0}\n  ]\n}\n\n// instances format (for simple models)\n{\n  "instances": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"All formats return a consistent response structure with your predictions and any additional metadata your model provides."}),"\n",(0,i.jsx)(n.h2,{id:"key-implementation-concepts",children:"Key Implementation Concepts"}),"\n",(0,i.jsx)(a.A,{children:(0,i.jsxs)(t.A,{children:[(0,i.jsxs)(l.A,{value:"model-preparation",label:"Model Preparation",default:!0,children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Prepare your models for successful serving:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Signatures"}),": Define input/output schemas for automatic request validation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Management"}),": Capture dependencies to ensure reproducible deployments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Registry"}),": Use aliases for seamless production updates"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metadata"}),": Include relevant context for debugging and monitoring"]}),"\n"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.models.signature import infer_signature\nfrom mlflow.tracking import MlflowClient\n\n# Log model with comprehensive serving metadata\nsignature = infer_signature(X_train, model.predict(X_train))\nmlflow.sklearn.log_model(\n    sk_model=model,\n    name="my_model",\n    signature=signature,\n    registered_model_name="production_model",\n    input_example=X_train[:5],  # Visible example for the MLflow UI\n)\n\n# Use aliases for production deployment\nclient = MlflowClient()\nclient.set_registered_model_alias(\n    name="production_model", alias="production", version="1"\n)\n'})})]}),(0,i.jsxs)(l.A,{value:"serving-config",label:"Server Configuration",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Configure your serving infrastructure for optimal performance:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Request Handling"}),": Set appropriate timeouts and batch sizes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Allocation"}),": Configure workers based on model complexity and load"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input Formats"}),": Choose the right format for your data type"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Handling"}),": Implement proper logging and monitoring"]}),"\n"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Configure server for production workloads\nexport MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT=60\n\n# For uvicorn-based deployments (Docker):\nexport MLFLOW_MODELS_WORKERS=4\n\n# For deployments that use gunicorn:\nexport GUNICORN_CMD_ARGS="--timeout 60 --workers 4"\n\n# Serve with optimal settings\nmlflow models serve \\\n  --model-uri models:/my_model@production \\\n  --port 5000 \\\n  --env-manager local  # For production, use conda or virtualenv\n'})})]}),(0,i.jsxs)(l.A,{value:"custom-pyfunc",label:"Advanced Patterns",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implement advanced serving patterns with custom PyFunc models:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preprocessing Logic"}),": Handle data transformation within the model"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Model Ensembles"}),": Combine predictions from multiple models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Business Logic"}),": Integrate validation and post-processing rules"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Optimization"}),": Batch processing and caching strategies"]}),"\n"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import joblib\nimport mlflow\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any\n\n\nclass EnsembleModel(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        """Load multiple models for ensemble prediction"""\n        self.model_a = joblib.load(context.artifacts["model_a"])\n        self.model_b = joblib.load(context.artifacts["model_b"])\n        self.preprocessor = joblib.load(context.artifacts["preprocessor"])\n        # Load ensemble weights from config\n        self.weights = context.model_config.get("weights", [0.5, 0.5])\n\n    def predict(self, model_input: pd.DataFrame) -> np.ndarray:\n        """Combine predictions from multiple models"""\n        # Preprocess input\n        processed = self.preprocessor.transform(model_input)\n\n        # Get predictions from both models\n        pred_a = self.model_a.predict(processed)\n        pred_b = self.model_b.predict(processed)\n\n        # Weighted average ensemble\n        ensemble_pred = self.weights[0] * pred_a + self.weights[1] * pred_b\n\n        return ensemble_pred\n\n\n# Log ensemble model with artifacts\nartifacts = {\n    "model_a": "path/to/model_a.pkl",\n    "model_b": "path/to/model_b.pkl",\n    "preprocessor": "path/to/preprocessor.pkl",\n}\n\nmlflow.pyfunc.log_model(\n    name="ensemble_model",\n    python_model=EnsembleModel(),\n    artifacts=artifacts,\n    model_config={"weights": [0.6, 0.4]},\n    pip_requirements=["scikit-learn", "pandas", "numpy"],\n)\n'})})]})]})}),"\n",(0,i.jsx)(n.h2,{id:"complete-example-train-to-production",children:"Complete Example: Train to Production"}),"\n",(0,i.jsx)(n.p,{children:"Follow this step-by-step guide to go from model training to a deployed REST API:"}),"\n",(0,i.jsx)(a.A,{children:(0,i.jsxs)(t.A,{children:[(0,i.jsxs)(l.A,{value:"train",label:"1. Train & Log",default:!0,children:[(0,i.jsx)(n.p,{children:"Train a simple model with automatic logging:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport mlflow.sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load sample data\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Enable sklearn autologging with model registration\nmlflow.sklearn.autolog(registered_model_name="iris_classifier")\n\n# Train model - MLflow automatically logs everything\nwith mlflow.start_run() as run:\n    model = RandomForestClassifier(n_estimators=10, random_state=42)\n    model.fit(X_train, y_train)\n\n    # Autologging automatically captures:\n    # - Model artifacts\n    # - Training parameters (n_estimators, random_state, etc.)\n    # - Training metrics (score on training data)\n    # - Model signature (inferred from training data)\n    # - Input example\n\n    # Optional: Log additional custom metrics\n    accuracy = model.score(X_test, y_test)\n    mlflow.log_metric("test_accuracy", accuracy)\n\n    print(f"Run ID: {run.info.run_id}")\n    print("Model automatically logged and registered!")\n'})})]}),(0,i.jsxs)(l.A,{value:"register",label:"2. Promote Model",children:[(0,i.jsx)(n.p,{children:"Set up model alias for production:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\n# Get the latest registered version (autologging creates version 1)\nmodel_version = client.get_registered_model("iris_classifier").latest_versions[0]\n\n# Set production alias (replaces deprecated stages)\nclient.set_registered_model_alias(\n    name="iris_classifier", alias="production", version=model_version.version\n)\n\nprint(f"Model version {model_version.version} tagged as \'production\'")\n\n# Model URI for serving (using alias)\nmodel_uri = "models:/iris_classifier@production"\nprint(f"Production model URI: {model_uri}")\n'})})]}),(0,i.jsxs)(l.A,{value:"serve",label:"3. Start Server",children:[(0,i.jsx)(n.p,{children:"Serve the registered model:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Serve using model alias (MLflow 3.x way)\nmlflow models serve \\\n  --model-uri "models:/iris_classifier@production" \\\n  --port 5000 \\\n  --env-manager local\n\n# Server will start at http://localhost:5000\n# Available endpoints:\n# - POST /invocations (predictions)\n# - GET /ping (health check)\n# - GET /version (model info)\n'})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Alternative serving approaches:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Serve by specific version number\nmlflow models serve \\\n  --model-uri "models:/iris_classifier/1" \\\n  --port 5000\n\n# Serve from run URI\nmlflow models serve \\\n  --model-uri "runs:/<run-id>/model" \\\n  --port 5000\n'})})]}),(0,i.jsxs)(l.A,{value:"predict",label:"4. Make Predictions",children:[(0,i.jsx)(n.p,{children:"Send requests to your served model:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\nimport pandas as pd\n\n# Prepare test data (same format as training)\ntest_data = {\n    "dataframe_split": {\n        "columns": [\n            "sepal length (cm)",\n            "sepal width (cm)",\n            "petal length (cm)",\n            "petal width (cm)",\n        ],\n        "data": [\n            [5.1, 3.5, 1.4, 0.2],  # setosa\n            [6.2, 2.9, 4.3, 1.3],  # versicolor\n            [7.3, 2.9, 6.3, 1.8],  # virginica\n        ],\n    }\n}\n\n# Make prediction request\nresponse = requests.post(\n    "http://localhost:5000/invocations",\n    headers={"Content-Type": "application/json"},\n    data=json.dumps(test_data),\n)\n\n# Parse response\nif response.status_code == 200:\n    predictions = response.json()\n    print("Predictions:", predictions)\n    # Output: {"predictions": [0, 1, 2]}\nelse:\n    print(f"Error: {response.status_code}, {response.text}")\n\n# Health check\nhealth = requests.get("http://localhost:5000/ping")\nprint("Health status:", health.status_code)  # Should be 200\n\n# Model info\ninfo = requests.get("http://localhost:5000/version")\nprint("Model version info:", info.json())\n'})})]})]})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Ready to build more advanced serving applications? Explore these specialized topics:"}),"\n",(0,i.jsx)(n.admonition,{title:"Get Started",type:"tip",children:(0,i.jsx)(n.p,{children:"The examples in each section are designed to be practical and ready-to-use. Start with the Quick Start above, then explore the use cases that match your deployment needs."})}),"\n",(0,i.jsxs)(d.A,{children:[(0,i.jsx)(c.A,{icon:h.A,title:"Custom Applications",description:"Build sophisticated serving logic with custom preprocessing, routing, and business rules",href:"./custom-apps",linkText:"Build custom apps \u2192"}),(0,i.jsx)(c.A,{icon:f.A,title:"Responses Agents",description:"Handle complex response patterns and multi-step inference workflows",href:"./responses-agent",linkText:"Learn about agents \u2192"})]})]})}function _(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(y,{...e})}):y(e)}},34742(e,n,s){s.d(n,{A:()=>i});var r=s(74848);s(96540);function i({concepts:e,title:n}){return(0,r.jsxs)("div",{className:"conceptOverview_x8T_",children:[n&&(0,r.jsx)("h3",{className:"overviewTitle_HyAI",children:n}),(0,r.jsx)("div",{className:"conceptGrid_uJNV",children:e.map((e,n)=>(0,r.jsxs)("div",{className:"conceptCard_oday",children:[(0,r.jsxs)("div",{className:"conceptHeader_HCk5",children:[e.icon&&(0,r.jsx)("div",{className:"conceptIcon_gejw",children:(0,r.jsx)(e.icon,{size:20})}),(0,r.jsx)("h4",{className:"conceptTitle_TGMM",children:e.title})]}),(0,r.jsx)("p",{className:"conceptDescription_ZyDn",children:e.description})]},n))})]})}},95986(e,n,s){s.d(n,{A:()=>i});var r=s(74848);s(96540);function i({children:e}){return(0,r.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,n,s){s.d(n,{A:()=>d});var r=s(74848);s(96540);var i=s(95310),o=s(34164);let t="tileImage_O4So";var l=s(66497),a=s(92802);function d({icon:e,image:n,imageDark:s,imageWidth:d,imageHeight:c,iconSize:p=32,containerHeight:m,title:h,description:u,href:g,linkText:f="Learn more \u2192",className:x}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let j=m?{height:`${m}px`}:{},v={};return d&&(v.width=`${d}px`),c&&(v.height=`${c}px`),(0,r.jsxs)(i.A,{href:g,className:(0,o.A)("tileCard_NHsj",x),children:[(0,r.jsx)("div",{className:"tileIcon_pyoR",style:j,children:e?(0,r.jsx)(e,{size:p}):s?(0,r.jsx)(a.A,{sources:{light:(0,l.default)(n),dark:(0,l.default)(s)},alt:h,className:t,style:v}):(0,r.jsx)("img",{src:(0,l.default)(n),alt:h,className:t,style:v})}),(0,r.jsx)("h3",{children:h}),(0,r.jsx)("p",{children:u}),(0,r.jsx)("div",{className:"tileLink_iUbu",children:f})]})}},10440(e,n,s){s.d(n,{A:()=>o});var r=s(74848);s(96540);var i=s(34164);function o({children:e,className:n}){return(0,r.jsx)("div",{className:(0,i.A)("tilesGrid_hB9N",n),children:e})}}}]);