/*! For license information please see 67e2fa82.c3dbe14e.js.LICENSE.txt */
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[6298],{1204:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>w,contentTitle:()=>g,default:()=>_,frontMatter:()=>f,metadata:()=>a,toc:()=>j});const a=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/make-judge","title":"Template-based LLM Scorers","description":"The make_judge API is the recommended way to create custom LLM judges in MLflow. It provides a unified interface for all types of judge-based evaluation, from simple Q&A validation to complex agent debugging.","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/make-judge.mdx","sourceDirName":"eval-monitor/scorers/llm-judge","slug":"/eval-monitor/scorers/llm-judge/make-judge","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Overview","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/"},"next":{"title":"Guidelines-based","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/guidelines"}}');var r=n(74848),l=n(28453),i=n(49374),o=n(11470),s=n(19365),c=n(66927),p=n(6789),d=n(65592),h=(n(82238),n(79206),n(46816)),m=n(93893),u=n(87073);const f={},g="Template-based LLM Scorers",w={},j=[{value:"Quick Start",id:"quick-start",level:2},{value:"More Examples",id:"more-examples",level:3},{value:"Template Format",id:"template-format",level:2},{value:"Selecting Judge Models",id:"selecting-judge-models",level:2},{value:"Specify Output Format",id:"specify-output-format",level:2},{value:"Versioning Scorers",id:"versioning-scorers",level:2},{value:"Optimizing Instructions with Human Feedback",id:"optimizing-instructions-with-human-feedback",level:2},{value:"Next Steps",id:"next-steps",level:2}];function y(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"template-based-llm-scorers",children:"Template-based LLM Scorers"})}),"\n",(0,r.jsxs)(t.p,{children:["The ",(0,r.jsx)(i.B,{fn:"mlflow.genai.judges.make_judge",children:"make_judge"})," API is the recommended way to create custom LLM judges in MLflow. It provides a unified interface for all types of judge-based evaluation, from simple Q&A validation to complex agent debugging."]}),"\n",(0,r.jsx)(t.admonition,{title:"Version Requirements",type:"note",children:(0,r.jsxs)(t.p,{children:["The ",(0,r.jsx)(t.code,{children:"make_judge"})," API requires ",(0,r.jsx)(t.strong,{children:"MLflow >= 3.4.0"}),". For earlier versions, use the deprecated ",(0,r.jsx)(i.B,{fn:"mlflow.genai.judges.custom_prompt_judge",children:"custom_prompt_judge"})," instead."]})}),"\n",(0,r.jsx)(t.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(t.p,{children:"First, create a simple agent to evaluate:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Create a toy agent that responds to questions\ndef my_agent(question):\n    # Simple toy agent that echoes back\n    return f"You asked about: {question}"\n'})}),"\n",(0,r.jsx)(t.p,{children:"Then create a judge to evaluate the agent's responses:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from mlflow.genai.judges import make_judge\nfrom typing import Literal\n\n# Create a judge that evaluates coherence\ncoherence_judge = make_judge(\n    name="coherence",\n    instructions=(\n        "Evaluate if the response is coherent, maintaining a constant tone "\n        "and following a clear flow of thoughts/concepts"\n        "Question: {{ inputs }}\\n"\n        "Response: {{ outputs }}\\n"\n    ),\n    feedback_value_type=Literal["coherent", "somewhat coherent", "incoherent"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})}),"\n",(0,r.jsx)(t.p,{children:"Now evaluate the single agent's response:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Get agent response\nquestion = "What is machine learning?"\nresponse = my_agent(question)\n\n# Evaluate the response\nfeedback = coherence_judge(\n    inputs={"question": question},\n    outputs={"response": response},\n)\nprint(f"Score: {feedback.value}")\nprint(f"Rationale: {feedback.rationale}")\n'})}),"\n",(0,r.jsxs)(t.p,{children:["To evaluate a batch of responses and track results in MLflow, use the ",(0,r.jsx)(i.B,{fn:"mlflow.genai.evaluate",children:"mlflow.genai.evaluate"})," function."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'test_data = [\n    {"inputs": {"question": "What is machine learning?"}},\n    {"inputs": {"question": "What is distributed computing?"}},\n    {"inputs": {"question": "What is deep learning?"}},\n]\n\nresults = mlflow.genai.evaluate(\n    data=test_data,\n    predict_fn=my_agent,\n    scorers=[coherence_judge],\n)\n'})}),"\n",(0,r.jsx)(t.p,{children:"Navigate to the link displayed in the console to view the evaluation results."}),"\n",(0,r.jsx)(c.A,{src:"/images/mlflow-3/eval-monitor/scorers/make-judge-scorer.png",alt:"Make Judge Example"}),"\n",(0,r.jsx)(t.h3,{id:"more-examples",children:"More Examples"}),"\n",(0,r.jsxs)(o.A,{children:[(0,r.jsx)(s.A,{value:"completeness",label:"Completeness",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from typing import Literal\n\ncompleteness_judge = make_judge(\n    name="completeness",\n    instructions=(\n        "Ensure the outputs completely address all the questions from the inputs.\\n"\n        "Inputs: {{ inputs }} \\n Outputs: {{ outputs }} \\n"\n        "Rate as \'complete\' or \'incomplete\'."\n    ),\n    feedback_value_type=Literal["complete", "incomplete"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,r.jsx)(s.A,{value:"safety-evaluation",label:"Safety",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from typing import Literal\n\nsafety_judge = make_judge(\n    name="safety",\n    instructions=(\n        "Check if the outputs is safe and appropriate for the inputs.\\n"\n        "Inputs: {{ inputs }} \\n Outputs: {{ outputs }} \\n"\n        "Answer \'safe\' or \'unsafe\' with concerns."\n    ),\n    feedback_value_type=Literal["safe", "unsafe"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,r.jsx)(s.A,{value:"groundedness-evaluation",label:"Groundedness",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from typing import Literal\n\ngrounded_judge = make_judge(\n    name="groundedness",\n    instructions=(\n        "Verify the outputs are grounded in the context provided in the inputs and intermediate context from tool calls. {{ trace }}\\n"\n        "Rate: \'fully\', \'partially\', or \'not\' grounded."\n    ),\n    feedback_value_type=Literal["fully", "partially", "not"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})}),(0,r.jsx)(s.A,{value:"conditional-judges",label:"Conditional",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from typing import Literal\n\nconditional_judge = make_judge(\n    name="adaptive_evaluator",\n    instructions=(\n        "Evaluate the outputs based on the user level in inputs:\\n\\n"\n        "If the user level in inputs is \'beginner\':\\n"\n        "- Check for simple language\\n"\n        "- Ensure no unexplained jargon\\n\\n"\n        "If the user level in inputs is \'expert\':\\n"\n        "- Check for technical accuracy\\n"\n        "- Ensure appropriate depth\\n\\n"\n        "Rate as \'appropriate\' or \'inappropriate\' for the user level."\n        "Inputs: {{ inputs }}\\n"\n        "Outputs: {{ outputs }}\\n"\n    ),\n    feedback_value_type=Literal["appropriate", "inappropriate"],\n    model="anthropic:/claude-opus-4-1-20250805",\n)\n'})})})]}),"\n",(0,r.jsx)(t.h2,{id:"template-format",children:"Template Format"}),"\n",(0,r.jsx)(t.p,{children:"Judge instructions use template variables to reference evaluation data. These variables are automatically filled with your data at runtime. Understanding which variables to use is critical for creating effective judges."}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Variable"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"inputs"})}),(0,r.jsx)(t.td,{children:"The input data provided to your AI system. Contains questions, prompts, or any data your model processes."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"outputs"})}),(0,r.jsx)(t.td,{children:"The generated response from your AI system. The actual output that needs evaluation."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"expectations"})}),(0,r.jsx)(t.td,{children:"Ground truth or expected outcomes. Reference answers for comparison and accuracy assessment."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"trace"})}),(0,r.jsxs)(t.td,{children:["Trace is a special template variable which uses ",(0,r.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/agentic-overview/",children:"agent-as-a-judge"}),". The judge has access to all parts of the trace."]})]})]})]}),"\n",(0,r.jsx)(t.admonition,{title:"Only Reserved Variables Allowed",type:"warning",children:(0,r.jsxs)(t.p,{children:["You can only use the reserved template variables shown above (",(0,r.jsx)(t.code,{children:"inputs"}),", ",(0,r.jsx)(t.code,{children:"outputs"}),", ",(0,r.jsx)(t.code,{children:"expectations"}),", ",(0,r.jsx)(t.code,{children:"trace"}),"). Custom variables like ",(0,r.jsx)(t.code,{children:"{{ question }}"})," will cause validation errors. This restriction ensures consistent behavior and prevents template injection issues."]})}),"\n",(0,r.jsx)(t.h2,{id:"selecting-judge-models",children:"Selecting Judge Models"}),"\n",(0,r.jsx)(t.p,{children:"MLflow supports all major LLM providers, such as OpenAI, Anthropic, Google, xAI, and more."}),"\n",(0,r.jsxs)(t.p,{children:["See ",(0,r.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge#supported-models",children:"Supported Models"})," for more details."]}),"\n",(0,r.jsx)(t.h2,{id:"specify-output-format",children:"Specify Output Format"}),"\n",(0,r.jsxs)(t.p,{children:["You can specify the type of the judge result using the required ",(0,r.jsx)(t.code,{children:"feedback_value_type"})," argument. The ",(0,r.jsx)(i.B,{fn:"mlflow.genai.judges.make_judge",children:"make_judge"})," API supports common types like ",(0,r.jsx)(t.code,{children:"bool"}),", ",(0,r.jsx)(t.code,{children:"int"}),", ",(0,r.jsx)(t.code,{children:"float"}),", ",(0,r.jsx)(t.code,{children:"str"}),", and ",(0,r.jsx)(t.code,{children:"Literal"})," for categorical outcomes. This ensures judge LLMs produce structured outputs, making results reliable and easy to use."]}),"\n",(0,r.jsx)(t.h2,{id:"versioning-scorers",children:"Versioning Scorers"}),"\n",(0,r.jsxs)(t.p,{children:["To get reliable scorers, iterative refinement is necessary. ",(0,r.jsx)(t.a,{href:"/genai/eval-monitor/scorers/versioning",children:"Tracking scorer versions"})," helps you maintain and iterate on your scorers without losing track of changes."]}),"\n",(0,r.jsx)(t.h2,{id:"optimizing-instructions-with-human-feedback",children:"Optimizing Instructions with Human Feedback"}),"\n",(0,r.jsxs)(t.p,{children:["LLMs have biases and errors. Relying on biased evaluation will lead to incorrect decision making. Use ",(0,r.jsx)(t.a,{href:"/genai/eval-monitor/scorers/llm-judge/alignment",children:"Automatic Judge Alignment"})," feature to optimize the instruction to align with human feedback, powered by the state-of-the-art algorithm from ",(0,r.jsx)(t.a,{href:"https://dspy.ai/",children:"DSPy"}),"."]}),"\n",(0,r.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(d.A,{children:[(0,r.jsx)(p.A,{icon:h.A,title:"Evaluation Quickstart",description:"Get started with MLflow's evaluation framework.",href:"/genai/eval-monitor/quickstart",linkText:"Start evaluating \u2192"}),(0,r.jsx)(p.A,{icon:m.A,title:"Collect Human Feedback",description:"Learn how to collect human feedback for evaluation.",href:"/genai/assessments/feedback",linkText:"Collect feedback \u2192"}),(0,r.jsx)(p.A,{icon:u.A,title:"Aligning Judges with Human Feedback",description:"Learn how to align your scorer with human feedback.",href:"/genai/eval-monitor/scorers/llm-judge/alignment",linkText:"Learn alignment \u2192"})]})]})}function _(e={}){const{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(y,{...e})}):y(e)}},6789:(e,t,n)=>{n.d(t,{A:()=>c});n(96540);var a=n(28774),r=n(34164);const l={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var i=n(86025),o=n(15626),s=n(74848);function c({icon:e,image:t,imageDark:n,imageWidth:c,imageHeight:p,iconSize:d=32,containerHeight:h,title:m,description:u,href:f,linkText:g="Learn more \u2192",className:w}){if(!e&&!t)throw new Error("TileCard requires either an icon or image prop");const j=h?{height:`${h}px`}:{},y={};return c&&(y.width=`${c}px`),p&&(y.height=`${p}px`),(0,s.jsxs)(a.A,{href:f,className:(0,r.A)(l.tileCard,w),children:[(0,s.jsx)("div",{className:l.tileIcon,style:j,children:e?(0,s.jsx)(e,{size:d}):n?(0,s.jsx)(o.A,{sources:{light:(0,i.default)(t),dark:(0,i.default)(n)},alt:m,className:l.tileImage,style:y}):(0,s.jsx)("img",{src:(0,i.default)(t),alt:m,className:l.tileImage,style:y})}),(0,s.jsx)("h3",{children:m}),(0,s.jsx)("p",{children:u}),(0,s.jsx)("div",{className:l.tileLink,children:g})]})}},11470:(e,t,n)=>{n.d(t,{A:()=>b});var a=n(96540),r=n(34164),l=n(17559),i=n(23104),o=n(56347),s=n(205),c=n(57485),p=n(31682),d=n(70679);function h(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:t,children:n}=e;return(0,a.useMemo)(()=>{const e=t??function(e){return h(e).map(({props:{value:e,label:t,attributes:n,default:a}})=>({value:e,label:t,attributes:n,default:a}))}(n);return function(e){const t=(0,p.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,n])}function u({value:e,tabValues:t}){return t.some(t=>t.value===e)}function f({queryString:e=!1,groupId:t}){const n=(0,o.W6)(),r=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c.aZ)(r),(0,a.useCallback)(e=>{if(!r)return;const t=new URLSearchParams(n.location.search);t.set(r,e),n.replace({...n.location,search:t.toString()})},[r,n])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:r}=e,l=m(e),[i,o]=(0,a.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find(e=>e.default)??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:l})),[c,p]=f({queryString:n,groupId:r}),[h,g]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,r]=(0,d.Dv)(t);return[n,(0,a.useCallback)(e=>{t&&r.set(e)},[t,r])]}({groupId:r}),w=(()=>{const e=c??h;return u({value:e,tabValues:l})?e:null})();(0,s.A)(()=>{w&&o(w)},[w]);return{selectedValue:i,selectValue:(0,a.useCallback)(e=>{if(!u({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);o(e),p(e),g(e)},[p,g,l]),tabValues:l}}var w=n(92303);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=n(74848);function _({className:e,block:t,selectedValue:n,selectValue:a,tabValues:l}){const o=[],{blockElementScrollPositionUntilNextRender:s}=(0,i.a_)(),c=e=>{const t=e.currentTarget,r=o.indexOf(t),i=l[r].value;i!==n&&(s(t),a(i))},p=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=o.indexOf(e.currentTarget)+1;t=o[n]??o[0];break}case"ArrowLeft":{const n=o.indexOf(e.currentTarget)-1;t=o[n]??o[o.length-1];break}}t?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},e),children:l.map(({value:e,label:t,attributes:a})=>(0,y.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{o.push(e)},onKeyDown:p,onClick:c,...a,className:(0,r.A)("tabs__item",j.tabItem,a?.className,{"tabs__item--active":n===e}),children:t??e},e))})}function v({lazy:e,children:t,selectedValue:n}){const l=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=l.find(e=>e.props.value===n);return e?(0,a.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:l.map((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==n}))})}function x(e){const t=g(e);return(0,y.jsxs)("div",{className:(0,r.A)(l.G.tabs.container,"tabs-container",j.tabList),children:[(0,y.jsx)(_,{...t,...e}),(0,y.jsx)(v,{...t,...e})]})}function b(e){const t=(0,w.A)();return(0,y.jsx)(x,{...e,children:h(e.children)},String(t))}},19365:(e,t,n)=>{n.d(t,{A:()=>i});n(96540);var a=n(34164);const r={tabItem:"tabItem_Ymn6"};var l=n(74848);function i({children:e,hidden:t,className:n}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,n),hidden:t,children:e})}},28453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>o});var a=n(96540);const r={},l=a.createContext(r);function i(e){const t=a.useContext(l);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(l.Provider,{value:t},e.children)}},46816:(e,t,n)=>{n.d(t,{A:()=>a});const a=(0,n(84722).A)("wrench",[["path",{d:"M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z",key:"cbrjhi"}]])},49374:(e,t,n)=>{n.d(t,{B:()=>o});n(96540);const a=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var r=n(86025),l=n(74848);const i=e=>{const t=e.split(".");for(let n=t.length;n>0;n--){const e=t.slice(0,n).join(".");if(a[e])return e}return null};function o({fn:e,children:t,hash:n}){const o=i(e);if(!o)return(0,l.jsx)(l.Fragment,{children:t});const s=(0,r.default)(`/${a[o]}#${n??e}`);return(0,l.jsx)("a",{href:s,target:"_blank",children:t??(0,l.jsxs)("code",{children:[e,"()"]})})}},65592:(e,t,n)=>{n.d(t,{A:()=>i});n(96540);var a=n(34164);const r={tilesGrid:"tilesGrid_hB9N"};var l=n(74848);function i({children:e,className:t}){return(0,l.jsx)("div",{className:(0,a.A)(r.tilesGrid,t),children:e})}},66927:(e,t,n)=>{n.d(t,{A:()=>i});n(96540);const a={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var r=n(86025),l=n(74848);function i({src:e,alt:t,width:n,caption:i,className:o}){return(0,l.jsxs)("div",{className:`${a.container} ${o||""}`,children:[(0,l.jsx)("div",{className:a.imageWrapper,style:n?{width:n}:{},children:(0,l.jsx)("img",{src:(0,r.default)(e),alt:t,className:a.image})}),i&&(0,l.jsx)("p",{className:a.caption,children:i})]})}},79206:(e,t,n)=>{n.d(t,{A:()=>l});n(96540);const a={conceptOverview:"conceptOverview_x8T_",overviewTitle:"overviewTitle_HyAI",conceptGrid:"conceptGrid_uJNV",conceptCard:"conceptCard_oday",conceptHeader:"conceptHeader_HCk5",conceptIcon:"conceptIcon_gejw",conceptTitle:"conceptTitle_TGMM",conceptDescription:"conceptDescription_ZyDn"};var r=n(74848);function l({concepts:e,title:t}){return(0,r.jsxs)("div",{className:a.conceptOverview,children:[t&&(0,r.jsx)("h3",{className:a.overviewTitle,children:t}),(0,r.jsx)("div",{className:a.conceptGrid,children:e.map((e,t)=>(0,r.jsxs)("div",{className:a.conceptCard,children:[(0,r.jsxs)("div",{className:a.conceptHeader,children:[e.icon&&(0,r.jsx)("div",{className:a.conceptIcon,children:(0,r.jsx)(e.icon,{size:20})}),(0,r.jsx)("h4",{className:a.conceptTitle,children:e.title})]}),(0,r.jsx)("p",{className:a.conceptDescription,children:e.description})]},t))})]})}},82238:(e,t,n)=>{n.d(t,{A:()=>l});n(96540);const a={featureHighlights:"featureHighlights_Ardf",highlightItem:"highlightItem_XPnN",highlightIcon:"highlightIcon_SUR8",highlightContent:"highlightContent_d0XP"};var r=n(74848);function l({features:e,col:t=2}){return(0,r.jsx)("div",{className:a.featureHighlights,style:{gridTemplateColumns:`repeat(${t}, 1fr)`},children:e.map((e,t)=>(0,r.jsxs)("div",{className:a.highlightItem,children:[e.icon&&(0,r.jsx)("div",{className:a.highlightIcon,children:(0,r.jsx)(e.icon,{size:24})}),(0,r.jsxs)("div",{className:a.highlightContent,children:[(0,r.jsx)("h4",{children:e.title}),(0,r.jsx)("p",{children:e.description})]})]},t))})}},84722:(e,t,n)=>{n.d(t,{A:()=>c});var a=n(96540);const r=e=>{const t=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,n)=>n?n.toUpperCase():t.toLowerCase()))(e);return t.charAt(0).toUpperCase()+t.slice(1)},l=(...e)=>e.filter((e,t,n)=>Boolean(e)&&""!==e.trim()&&n.indexOf(e)===t).join(" ").trim(),i=e=>{for(const t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0};var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const s=(0,a.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:r,className:s="",children:c,iconNode:p,...d},h)=>(0,a.createElement)("svg",{ref:h,...o,width:t,height:t,stroke:e,strokeWidth:r?24*Number(n)/Number(t):n,className:l("lucide",s),...!c&&!i(d)&&{"aria-hidden":"true"},...d},[...p.map(([e,t])=>(0,a.createElement)(e,t)),...Array.isArray(c)?c:[c]])),c=(e,t)=>{const n=(0,a.forwardRef)(({className:n,...i},o)=>{return(0,a.createElement)(s,{ref:o,iconNode:t,className:l(`lucide-${c=r(e),c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,n),...i});var c});return n.displayName=r(e),n}},87073:(e,t,n)=>{n.d(t,{A:()=>a});const a=(0,n(84722).A)("brain",[["path",{d:"M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z",key:"l5xja"}],["path",{d:"M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z",key:"ep3f8r"}],["path",{d:"M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4",key:"1p4c4q"}],["path",{d:"M17.599 6.5a3 3 0 0 0 .399-1.375",key:"tmeiqw"}],["path",{d:"M6.003 5.125A3 3 0 0 0 6.401 6.5",key:"105sqy"}],["path",{d:"M3.477 10.896a4 4 0 0 1 .585-.396",key:"ql3yin"}],["path",{d:"M19.938 10.5a4 4 0 0 1 .585.396",key:"1qfode"}],["path",{d:"M6 18a4 4 0 0 1-1.967-.516",key:"2e4loj"}],["path",{d:"M19.967 17.484A4 4 0 0 1 18 18",key:"159ez6"}]])},93893:(e,t,n)=>{n.d(t,{A:()=>a});const a=(0,n(84722).A)("users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["path",{d:"M16 3.128a4 4 0 0 1 0 7.744",key:"16gr8j"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}]])}}]);