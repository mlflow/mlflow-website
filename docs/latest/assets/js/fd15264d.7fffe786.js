"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8779],{12946:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>m,contentTitle:()=>s,default:()=>f,frontMatter:()=>a,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"deep-learning/pytorch/guide/index","title":"PyTorch within MLflow","description":"In this guide we will walk you through how to use PyTorch within MLflow. We will demonstrate","source":"@site/docs/deep-learning/pytorch/guide/index.mdx","sourceDirName":"deep-learning/pytorch/guide","slug":"/deep-learning/pytorch/guide/","permalink":"/docs/latest/deep-learning/pytorch/guide/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Pytorch","permalink":"/docs/latest/deep-learning/pytorch/"},"next":{"title":"Quickstart with MLflow PyTorch Flavor","permalink":"/docs/latest/deep-learning/pytorch/quickstart/pytorch_quickstart"}}');var t=o(74848),r=o(28453),i=o(67756);const a={},s="PyTorch within MLflow",m={},c=[{value:"Logging PyTorch Experiments to MLflow",id:"logging-pytorch-experiments-to-mlflow",level:2},{value:"Autologging PyTorch Experiments",id:"autologging-pytorch-experiments",level:3},{value:"Manually Logging PyTorch Experiments",id:"manually-logging-pytorch-experiments",level:3},{value:"Best Practice of Logging PyTorch to MLflow",id:"best-practice-of-logging-pytorch-to-mlflow",level:4},{value:"Saving Your PyTorch Model to MLflow",id:"saving-your-pytorch-model-to-mlflow",level:2},{value:"Model Signature",id:"model-signature",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"pytorch-within-mlflow",children:"PyTorch within MLflow"})}),"\n",(0,t.jsx)(n.p,{children:"In this guide we will walk you through how to use PyTorch within MLflow. We will demonstrate\nhow to track your PyTorch experiments and log your PyTorch models to MLflow."}),"\n",(0,t.jsx)(n.h2,{id:"logging-pytorch-experiments-to-mlflow",children:"Logging PyTorch Experiments to MLflow"}),"\n",(0,t.jsx)(n.h3,{id:"autologging-pytorch-experiments",children:"Autologging PyTorch Experiments"}),"\n",(0,t.jsxs)(n.p,{children:["Unlike other deep learning flavors, MLflow does not have an autologging integration with PyTorch because\nnative PyTorch requires writing custom training loops. If you want to use autologging with PyTorch, please\nuse ",(0,t.jsx)(n.a,{href:"https://lightning.ai",children:"Lightning"})," to train your models. When Lightning is being used, you can turned\non autologging by calling ",(0,t.jsx)(i.B,{fn:"mlflow.pytorch.autolog"})," or ",(0,t.jsx)(i.B,{fn:"mlflow.autolog"}),". For more\ndetails, please refer to the MLflow Lightning Developer Guide."]}),"\n",(0,t.jsx)(n.h3,{id:"manually-logging-pytorch-experiments",children:"Manually Logging PyTorch Experiments"}),"\n",(0,t.jsx)(n.p,{children:"To log your PyTorch experiments, you can insert MLflow logging into your PyTorch training loop, which relies\non the following APIs:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(i.B,{fn:"mlflow.log_metric"})," / ",(0,t.jsx)(i.B,{fn:"mlflow.log_metrics"}),": log metrics such as accuracy and loss during training."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(i.B,{fn:"mlflow.log_param"})," / ",(0,t.jsx)(i.B,{fn:"mlflow.log_params"}),": log parameters such as learning rate and batch size during training."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(i.B,{fn:"mlflow.pytorch.log_model"}),": save your PyTorch model to MLflow, which is usually called at the end of training."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(i.B,{fn:"mlflow.log_artifact"}),": log artifacts such as model checkpoints and plots during training."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"best-practice-of-logging-pytorch-to-mlflow",children:"Best Practice of Logging PyTorch to MLflow"}),"\n",(0,t.jsx)(n.p,{children:"While logging PyTorch experiments is identical to other kinds of manual logging, there are some best\npractices that we recommend you to follow:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Log your model and training parameters via ",(0,t.jsx)(i.B,{fn:"mlflow.log_params"})," at the beginning of training\nloop, such as learning rate, batch size, etc. ",(0,t.jsx)(i.B,{fn:"mlflow.log_params"})," is the\nbatched logging version of ",(0,t.jsx)(i.B,{fn:"mlflow.log_param"}),", which is more efficient than the latter."]}),"\n",(0,t.jsxs)(n.li,{children:["Log your model architecture at the beginning of training via ",(0,t.jsx)(i.B,{fn:"mlflow.log_artifact"}),". You can use\n",(0,t.jsx)(n.code,{children:"torchinfo"})," package to get the model summary."]}),"\n",(0,t.jsxs)(n.li,{children:["Log training and validation metrics via ",(0,t.jsx)(i.B,{fn:"mlflow.log_metric"})," inside your training loop, such as\nloss and accuracy for classification tasks. If you have multiple metrics per logging step, you can\nuse ",(0,t.jsx)(i.B,{fn:"mlflow.log_metrics"})," to log them together."]}),"\n",(0,t.jsxs)(n.li,{children:["Log your trained/finetuned model to MLflow via ",(0,t.jsx)(n.code,{children:"mlflow.pytorch.log_model()"})," at the end of your training."]}),"\n",(0,t.jsxs)(n.li,{children:["[Optional] You can also log your model checkpoints to MLflow via ",(0,t.jsx)(i.B,{fn:"mlflow.log_artifact"})," during\ntraining if you wish to keep middle training status."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The following is an end-to-end example of how to log your PyTorch experiments to MLflow:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport torch\n\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchmetrics import Accuracy\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root="data",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=64)\n\n# Get cpu or gpu for training.\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\n\n# Define the model.\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\ndef train(dataloader, model, loss_fn, metrics_fn, optimizer):\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        accuracy = metrics_fn(pred, y)\n\n        # Backpropagation.\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch\n            mlflow.log_metric("loss", f"{loss:3f}", step=(batch // 100))\n            mlflow.log_metric("accuracy", f"{accuracy:3f}", step=(batch // 100))\n            print(\n                f"loss: {loss:3f} accuracy: {accuracy:3f} [{current} / {len(dataloader)}]"\n            )\n\n\nepochs = 3\nloss_fn = nn.CrossEntropyLoss()\nmetric_fn = Accuracy(task="multiclass", num_classes=10).to(device)\nmodel = NeuralNetwork().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nwith mlflow.start_run():\n    params = {\n        "epochs": epochs,\n        "learning_rate": 1e-3,\n        "batch_size": 64,\n        "loss_function": loss_fn.__class__.__name__,\n        "metric_function": metric_fn.__class__.__name__,\n        "optimizer": "SGD",\n    }\n    # Log training parameters.\n    mlflow.log_params(params)\n\n    # Log model summary.\n    with open("model_summary.txt", "w") as f:\n        f.write(str(summary(model)))\n    mlflow.log_artifact("model_summary.txt")\n\n    for t in range(epochs):\n        print(f"Epoch {t+1}\\n-------------------------------")\n        train(train_dataloader, model, loss_fn, metric_fn, optimizer)\n\n    # Save the trained model to MLflow.\n    mlflow.pytorch.log_model(model, name="model")\n'})}),"\n",(0,t.jsxs)(n.p,{children:["If you run the above code and log to your local MLflow server (for how to use local MLflow server, please read the\n",(0,t.jsx)(n.a,{href:"/getting-started/tracking-server-overview#method-1-start-your-own-mlflow-server",children:"tracking server overview"}),"),\nyou will see results on MLflow UI similar to the screenshot below:"]}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"90%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Basic Example of PyTorch with MLflow",src:o(14433).A+"",width:"3418",height:"1780"})})}),"\n",(0,t.jsx)(n.h2,{id:"saving-your-pytorch-model-to-mlflow",children:"Saving Your PyTorch Model to MLflow"}),"\n",(0,t.jsxs)(n.p,{children:["As we mentioned in the previous section, you can save your PyTorch model to MLflow via ",(0,t.jsx)(i.B,{fn:"mlflow.pytorch.log_model"}),".\nBy default MLflow saves your model with ",(0,t.jsx)(n.code,{children:".pth"})," suffix. A sample code of saving and loading your PyTorch model is as below:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\n\nfrom torch import nn\n\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nmodel = NeuralNetwork()\n\nwith mlflow.start_run() as run:\n    mlflow.pytorch.log_model(model, name="model")\n\nlogged_model = f"runs:/{run.info.run_id}/model"\nloaded_model = mlflow.pyfunc.load_model(logged_model)\nloaded_model.predict(np.random.uniform(size=[1, 28, 28]).astype(np.float32))\n'})}),"\n",(0,t.jsx)(n.p,{children:"You can view the saved file on MLflow UI, which will be similar to below:"}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"90%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Basic PyTorch Saving",src:o(35706).A+"",width:"3430",height:"1298"})})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"mlflow.pytorch.log_model()"})," is compatible with ",(0,t.jsx)(n.code,{children:"torch.jit.script()"}),", if you have a jit-compiled model,\nMLflow will save the compiled graph."]}),"\n",(0,t.jsx)(n.h3,{id:"model-signature",children:"Model Signature"}),"\n",(0,t.jsxs)(n.p,{children:["A model signature is a description of a model's input and output. A model signature is not necessary for loading\na model, you can still load the model and perform inferenece if you know the input format. However, it's a good\npractice to include the signature for better model understanding. To add a model signature to PyTorch model, you\ncan either use the ",(0,t.jsx)(i.B,{fn:"mlflow.models.infer_signature"})," API or manually set the signature."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(i.B,{fn:"mlflow.models.infer_signature"})," takes your input data and model outputs to automatically infer the model signature:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"input = np.random.uniform(size=[1, 28, 28])\nsignature = mlflow.models.infer_signature(\n    input,\n    model(input).detach().numpy(),\n)\n"})}),"\n",(0,t.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,t.jsxs)(n.p,{children:["As of MLflow 2.9.1, there is a caveat that the input and output to ",(0,t.jsx)(n.code,{children:"mlflow.models.infer_signature()"})," cannot be\na ",(0,t.jsx)(n.code,{children:"torch.Tensor"}),", please convert them to ",(0,t.jsx)(n.code,{children:"numpy.ndarray"})," before passing to ",(0,t.jsx)(n.code,{children:"mlflow.models.infer_signature()"}),"."]})}),"\n",(0,t.jsx)(n.p,{children:"You can also manually set the signature:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom mlflow.types import Schema, TensorSpec\n\ninput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 28, 28))])\noutput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After setting the signature, you can include it when calling ",(0,t.jsx)(i.B,{fn:"mlflow.pytorch.log_model"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport numpy as np\n\nfrom torch import nn\nfrom mlflow.types import Schema, TensorSpec\nfrom mlflow.models import ModelSignature\n\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nmodel = NeuralNetwork()\ninput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 28, 28))])\noutput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\nwith mlflow.start_run() as run:\n    mlflow.pytorch.log_model(model, name="model", signature=signature)\n\nlogged_model = f"runs:/{run.info.run_id}/model"\nloaded_model = mlflow.pyfunc.load_model(logged_model)\nloaded_model.predict(np.random.uniform(size=[1, 28, 28]).astype(np.float32))\n'})}),"\n",(0,t.jsx)(n.p,{children:"In your MLflow UI you should be able to see the signature of your model as the screenshot below:"}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"90%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"PyTorch Model Signature",src:o(62945).A+"",width:"3376",height:"1712"})})})]})}function f(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},14433:(e,n,o)=>{o.d(n,{A:()=>l});const l=o.p+"assets/images/pytorch-guide-basic-example-ui-cf99aaf7644ae4e390f28f6be1ba594c.png"},28453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>a});var l=o(96540);const t={},r=l.createContext(t);function i(e){const n=l.useContext(r);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),l.createElement(r.Provider,{value:n},e.children)}},35706:(e,n,o)=>{o.d(n,{A:()=>l});const l=o.p+"assets/images/pytorch-guide-basic-saving-af066edf360c780042fd3085792ec1d2.png"},62945:(e,n,o)=>{o.d(n,{A:()=>l});const l=o.p+"assets/images/pytorch-guide-model-signature-f8e63d83e73f9283d985016616ae1cab.png"},67756:(e,n,o)=>{o.d(n,{B:()=>s});o(96540);const l=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var t=o(29030),r=o(56289),i=o(74848);const a=e=>{const n=e.split(".");for(let o=n.length;o>0;o--){const e=n.slice(0,o).join(".");if(l[e])return e}return null};function s(e){let{fn:n,children:o}=e;const s=a(n);if(!s)return(0,i.jsx)(i.Fragment,{children:o});const m=(0,t.Ay)(`/${l[s]}#${n}`);return(0,i.jsx)(r.A,{to:m,target:"_blank",children:o??(0,i.jsxs)("code",{children:[n,"()"]})})}}}]);