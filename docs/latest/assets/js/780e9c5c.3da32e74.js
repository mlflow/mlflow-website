"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5927],{2991:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/openai-autolog-bb60103a751343c4c1fa27f464e4f333.gif"},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var o=t(96540);const l={},a=o.createContext(l);function i(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),o.createElement(a.Provider,{value:n},e.children)}},52581:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/openai-swarm-tracing-2b4c9cb8a57325dd5b0884c3a132afcb.png"},61096:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var o=t(71021);const l={tableOfContentsInline:"tableOfContentsInline_prmo"};var a=t(74848);function i(e){let{toc:n,minHeadingLevel:t,maxHeadingLevel:i}=e;return(0,a.jsx)("div",{className:l.tableOfContentsInline,children:(0,a.jsx)(o.A,{toc:n,minHeadingLevel:t,maxHeadingLevel:i,className:"table-of-contents",linkClassName:null})})}},67756:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const o=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var l=t(29030),a=t(56289),i=t(74848);const r=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(o[e])return e}return null};function s(e){let{fn:n,children:t}=e;const s=r(n);if(!s)return(0,i.jsx)(i.Fragment,{children:t});const c=(0,l.Ay)(`/${o[s]}#${n}`);return(0,i.jsx)(a.A,{to:c,target:"_blank",children:t??(0,i.jsxs)("code",{children:[n,"()"]})})}},71021:(e,n,t)=>{t.d(n,{A:()=>g});var o=t(96540),l=t(53115);function a(e){const n=e.map((e=>({...e,parentIndex:-1,children:[]}))),t=Array(7).fill(-1);n.forEach(((e,n)=>{const o=t.slice(2,e.level);e.parentIndex=Math.max(...o),t[e.level]=n}));const o=[];return n.forEach((e=>{const{parentIndex:t,...l}=e;t>=0?n[t].children.push(l):o.push(l)})),o}function i(e){let{toc:n,minHeadingLevel:t,maxHeadingLevel:o}=e;return n.flatMap((e=>{const n=i({toc:e.children,minHeadingLevel:t,maxHeadingLevel:o});return function(e){return e.level>=t&&e.level<=o}(e)?[{...e,children:n}]:n}))}function r(e){const n=e.getBoundingClientRect();return n.top===n.bottom?r(e.parentNode):n}function s(e,n){let{anchorTopOffset:t}=n;const o=e.find((e=>r(e).top>=t));if(o){return function(e){return e.top>0&&e.bottom<window.innerHeight/2}(r(o))?o:e[e.indexOf(o)-1]??null}return e[e.length-1]??null}function c(){const e=(0,o.useRef)(0),{navbar:{hideOnScroll:n}}=(0,l.p)();return(0,o.useEffect)((()=>{e.current=n?0:document.querySelector(".navbar").clientHeight}),[n]),e}function p(e){const n=(0,o.useRef)(void 0),t=c();(0,o.useEffect)((()=>{if(!e)return()=>{};const{linkClassName:o,linkActiveClassName:l,minHeadingLevel:a,maxHeadingLevel:i}=e;function r(){const e=function(e){return Array.from(document.getElementsByClassName(e))}(o),r=function(e){let{minHeadingLevel:n,maxHeadingLevel:t}=e;const o=[];for(let l=n;l<=t;l+=1)o.push(`h${l}.anchor`);return Array.from(document.querySelectorAll(o.join()))}({minHeadingLevel:a,maxHeadingLevel:i}),c=s(r,{anchorTopOffset:t.current}),p=e.find((e=>c&&c.id===function(e){return decodeURIComponent(e.href.substring(e.href.indexOf("#")+1))}(e)));e.forEach((e=>{!function(e,t){t?(n.current&&n.current!==e&&n.current.classList.remove(l),e.classList.add(l),n.current=e):e.classList.remove(l)}(e,e===p)}))}return document.addEventListener("scroll",r),document.addEventListener("resize",r),r(),()=>{document.removeEventListener("scroll",r),document.removeEventListener("resize",r)}}),[e,t])}var m=t(56289),d=t(74848);function h(e){let{toc:n,className:t,linkClassName:o,isChild:l}=e;return n.length?(0,d.jsx)("ul",{className:l?void 0:t,children:n.map((e=>(0,d.jsxs)("li",{children:[(0,d.jsx)(m.A,{to:`#${e.id}`,className:o??void 0,dangerouslySetInnerHTML:{__html:e.value}}),(0,d.jsx)(h,{isChild:!0,toc:e.children,className:t,linkClassName:o})]},e.id)))}):null}const f=o.memo(h);function g(e){let{toc:n,className:t="table-of-contents table-of-contents__left-border",linkClassName:r="table-of-contents__link",linkActiveClassName:s,minHeadingLevel:c,maxHeadingLevel:m,...h}=e;const g=(0,l.p)(),u=c??g.tableOfContents.minHeadingLevel,w=m??g.tableOfContents.maxHeadingLevel,_=function(e){let{toc:n,minHeadingLevel:t,maxHeadingLevel:l}=e;return(0,o.useMemo)((()=>i({toc:a(n),minHeadingLevel:t,maxHeadingLevel:l})),[n,t,l])}({toc:n,minHeadingLevel:u,maxHeadingLevel:w});return p((0,o.useMemo)((()=>{if(r&&s)return{linkClassName:r,linkActiveClassName:s,minHeadingLevel:u,maxHeadingLevel:w}}),[r,s,u,w])),(0,d.jsx)(f,{toc:_,className:t,linkClassName:r,...h})}},76108:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>m});const o=JSON.parse('{"id":"llms/openai/autologging/index","title":"MLflow OpenAI Autologging","description":"The OpenAI flavor for MLflow supports autologging to ensure that experimentation, testing, and validation of your ideas can be captured dynamically without","source":"@site/docs/llms/openai/autologging/index.mdx","sourceDirName":"llms/openai/autologging","slug":"/llms/openai/autologging/","permalink":"/docs/latest/llms/openai/autologging/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Autologging","sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Introduction to Using the OpenAI Flavor in MLflow","permalink":"/docs/latest/llms/openai/notebooks/openai-quickstart"},"next":{"title":"Detailed Guide","permalink":"/docs/latest/llms/openai/guide/"}}');var l=t(74848),a=t(28453),i=t(61096),r=t(67756);const s={sidebar_label:"Autologging",sidebar_position:2},c="MLflow OpenAI Autologging",p={},m=[{value:"Quickstart",id:"quickstart",level:2},{value:"Configuration of OpenAI Autologging",id:"configuration-of-openai-autologging",level:2},{value:"Example of using OpenAI Autologging",id:"example-of-using-openai-autologging",level:2},{value:"Auto-tracing for OpenAI Swarm",id:"auto-tracing-for-openai-swarm",level:2},{value:"FAQ",id:"faq",level:2},{value:"Are asynchronous APIs supported in autologging?",id:"are-asynchronous-apis-supported-in-autologging",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"mlflow-openai-autologging",children:"MLflow OpenAI Autologging"})}),"\n",(0,l.jsx)(n.p,{children:"The OpenAI flavor for MLflow supports autologging to ensure that experimentation, testing, and validation of your ideas can be captured dynamically without\nhaving to wrap your code with logging boilerplate."}),"\n",(0,l.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,l.jsxs)(n.p,{children:["Autologging is ",(0,l.jsx)(n.strong,{children:"only supported"})," for versions of the OpenAI SDK that are 1.17 and higher."]})}),"\n",(0,l.jsx)(n.p,{children:"MLflow autologging for the OpenAI SDK supports the following interfaces:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Chat Completions"})," via ",(0,l.jsx)(n.code,{children:"client.chat.completions.create()"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Completions"})," (legacy) via ",(0,l.jsx)(n.code,{children:"client.completions.create()"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Embeddings"})," via ",(0,l.jsx)(n.code,{children:"client.embeddings.create()"})]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["Where ",(0,l.jsx)(n.code,{children:"client"})," is an instance of ",(0,l.jsx)(n.code,{children:"openai.OpenAI()"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"In this guide, we'll discuss some of the key features that are available in the autologging feature."}),"\n",(0,l.jsx)(i.A,{toc:m,maxHeadingLevel:2}),"\n",(0,l.jsx)(n.h2,{id:"quickstart",children:"Quickstart"}),"\n",(0,l.jsxs)(n.p,{children:["To get started with MLflow's OpenAI autologging, you simply need to call ",(0,l.jsx)(r.B,{fn:"mlflow.openai.autolog"})," at the beginning of your script or notebook.\nEnabling autologging with no argument overrides will behave as the ",(0,l.jsx)(n.code,{children:"default"})," configuration in the table in the next section. Overriding any of these settings\nwill allow you to log additional elements."]}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:["The only element that is ",(0,l.jsx)(n.strong,{children:"enabled by default"})," when autologging is activated is the recording of trace information. You can read more about MLflow tracing\n",(0,l.jsx)(n.a,{href:"/tracing",children:"here"}),"."]})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\nimport openai\nimport mlflow\n\n# Enables trace logging by default\nmlflow.openai.autolog()\n\nopenai_client = openai.OpenAI()\n\nmessages = [\n    {\n        "role": "user",\n        "content": "What does turning something up to 11 refer to?",\n    }\n]\n\n# The input messages and the response will be logged as a trace to the active experiment\nanswer = openai_client.chat.completions.create(\n    model="gpt-4o",\n    messages=messages,\n    temperature=0.99,\n)\n'})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When using the OpenAI SDK, ensure that your access token is assigned to the environment variable ",(0,l.jsx)(n.code,{children:"OPENAI_API_KEY"}),"."]})}),"\n",(0,l.jsx)(n.h2,{id:"configuration-of-openai-autologging",children:"Configuration of OpenAI Autologging"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow OpenAI autologging can log various information about the model and its inference. ",(0,l.jsx)(n.strong,{children:"By default, only trace logging is enabled"}),", but you can enable\nautologging of other information by setting the corresponding parameters when calling ",(0,l.jsx)(r.B,{fn:"mlflow.openai.autolog"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"The available options and their default values are shown below. To learn more about additional parameters, see the API documentation."}),"\n",(0,l.jsxs)("table",{children:[(0,l.jsx)("thead",{children:(0,l.jsxs)("tr",{children:[(0,l.jsx)("th",{children:"Target"}),(0,l.jsx)("th",{children:"Default"}),(0,l.jsx)("th",{children:"Parameter"}),(0,l.jsx)("th",{children:"Description"})]})}),(0,l.jsxs)("tbody",{children:[(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"Traces"}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"true"})}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"log_traces"})}),(0,l.jsxs)("td",{children:["Whether to generate and log traces for the model. See ",(0,l.jsx)(n.a,{href:"/tracing",children:"MLflow Tracing"})," for more details about the tracing feature."]})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"Model Artifacts"}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"false"})}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"log_models"})}),(0,l.jsxs)("td",{children:["If set to ",(0,l.jsx)(n.code,{children:"True"}),", the OpenAI model will be logged when it is invoked."]})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"Model Signatures"}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"false"})}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"log_model_signatures"})}),(0,l.jsx)("td",{children:(0,l.jsxs)(n.p,{children:["If set to ",(0,l.jsx)(n.code,{children:"True"}),", ",(0,l.jsx)(r.B,{fn:"mlflow.models.ModelSignature",children:(0,l.jsx)(n.code,{children:"ModelSignature"})})," describing model inputs and outputs are collected and logged along with OpenAI model artifacts during inference. This option is only available when ",(0,l.jsx)(n.code,{children:"log_models"})," is enabled."]})})]}),(0,l.jsxs)("tr",{children:[(0,l.jsx)("td",{children:"Input Example"}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"false"})}),(0,l.jsx)("td",{children:(0,l.jsx)(n.code,{children:"log_input_examples"})}),(0,l.jsxs)("td",{children:["If set to ",(0,l.jsx)(n.code,{children:"True"}),", input examples from inference data are collected and logged along with OpenAI model artifacts during inference. This option is only available when ",(0,l.jsx)(n.code,{children:"log_models"})," is enabled."]})]})]})]}),"\n",(0,l.jsx)(n.p,{children:"For example, to disable logging of traces, and instead enable model logging, run the following code:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.openai.autolog(\n    log_traces=False,\n    log_models=True,\n)\n"})}),"\n",(0,l.jsx)(n.h2,{id:"example-of-using-openai-autologging",children:"Example of using OpenAI Autologging"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\n\nimport mlflow\nimport openai\n\nAPI_KEY = os.environ.get("OPENAI_API_KEY")\nEXPERIMENT_NAME = "OpenAI Autologging Demonstration"\nREGISTERED_MODEL_NAME = "openai-auto"\nMODEL_VERSION = 1\n\nmlflow.openai.autolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    log_traces=True,\n    registered_model_name=REGISTERED_MODEL_NAME,\n)\n\nmlflow.set_experiment(EXPERIMENT_NAME)\n\nopenai_client = openai.OpenAI(api_key=API_KEY)\n\nmessages = [\n    {\n        "role": "user",\n        "content": "State that you are responding to a test and that you are alive.",\n    }\n]\n\nopenai_client.chat.completions.create(\n    model="gpt-4o",\n    messages=messages,\n    temperature=0.95,\n)\n'})}),"\n",(0,l.jsx)(n.p,{children:"Viewing the logged model and the trace used when invoking the OpenAI client within the UI can be seen in the image below:"}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"OpenAI Autologging artifacts and traces",src:t(2991).A+"",width:"2048",height:"1469"})}),"\n",(0,l.jsxs)(n.p,{children:["The model can be loaded by using the ",(0,l.jsx)(n.code,{children:"models"})," uri via the model that was logged and registered and interfaced with via the pyfunc API as shown below:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'loaded_autologged_model = mlflow.pyfunc.load_model(\n    f"models:/{REGISTERED_MODEL_NAME}/{MODEL_VERSION}"\n)\n\nloaded_autologged_model.predict(\n    "How much relative time difference would occur between an astronaut travelling at 0.98c for 14 years "\n    "as measured by an on-board clock on the spacecraft and humans on Earth, assuming constant speed?"\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"auto-tracing-for-openai-swarm",children:"Auto-tracing for OpenAI Swarm"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow 2.17.1 introduced built-in tracing capability for ",(0,l.jsx)(n.a,{href:"https://github.com/openai/swarm/tree/main",children:"OpenAI Swarm"}),", a multi-agent orchestration framework from OpenAI. The framework provides a clean interface to build multi-agent systems on top of the OpenAI's Function Calling capability and the concept of ",(0,l.jsx)(n.a,{href:"https://cookbook.openai.com/examples/orchestrating_agents",children:"handoff & routines patterns"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["MLflow's automatic tracing capability offers seamless tracking of interactions between agents, tool calls, and their collective outputs. You can enable auto-tracing for OpenAI Swarm just by calling the ",(0,l.jsx)(r.B,{fn:"mlflow.openai.autolog"})," function."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom swarm import Swarm, Agent\n\n# Calling the autolog API will enable trace logging by default.\nmlflow.openai.autolog()\n\nmlflow.set_experiment("OpenAI Swarm")\n\nclient = Swarm()\n\n\ndef transfer_to_agent_b():\n    return agent_b\n\n\nagent_a = Agent(\n    name="Agent A",\n    instructions="You are a helpful agent.",\n    functions=[transfer_to_agent_b],\n)\n\nagent_b = Agent(\n    name="Agent B",\n    instructions="Only speak in Haikus.",\n)\n\nresponse = client.run(\n    agent=agent_a,\n    messages=[{"role": "user", "content": "I want to talk to agent B."}],\n)\nprint(response)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["The logged trace, associated with the ",(0,l.jsx)(n.code,{children:"OpenAI Swarm"})," experiment, can be seen in the MLflow UI, as shown below:"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"OpenAI Swarm Tracing",src:t(52581).A+"",width:"3310",height:"1774"})}),"\n",(0,l.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,l.jsx)(n.h3,{id:"are-asynchronous-apis-supported-in-autologging",children:"Are asynchronous APIs supported in autologging?"}),"\n",(0,l.jsxs)(n.p,{children:["The MLflow OpenAI autologging feature ",(0,l.jsx)(n.strong,{children:"does not support asynchronous APIs"})," for logging models or traces."]}),"\n",(0,l.jsxs)(n.p,{children:["Saving your async implementation is best done by using the ",(0,l.jsx)(n.a,{href:"/model#models-from-code",children:"models from code feature"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"If you would like to log trace events for an async OpenAI API, below is a simplified example of logging the trace for a streaming async request:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import openai\nimport mlflow\nimport asyncio\n\n# Activate an experiment for logging traces to\nmlflow.set_experiment("OpenAI")\n\n\nasync def fetch_openai_response(messages, model="gpt-4o", temperature=0.99):\n    """\n    Asynchronously gets a response from the OpenAI API using the provided messages and streams the response.\n\n    Args:\n        messages (list): List of message dictionaries for the OpenAI API.\n        model (str): The model to use for the OpenAI API. Default is "gpt-4o".\n        temperature (float): The temperature to use for the OpenAI API. Default is 0.99.\n\n    Returns:\n        None\n    """\n    client = openai.AsyncOpenAI()\n\n    # Create the response stream\n    response_stream = await client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        stream=True,\n    )\n\n    # Manually log traces using the tracing fluent API\n    with mlflow.start_span() as trace:\n        trace.set_inputs(messages)\n        full_response = []\n\n        async for chunk in response_stream:\n            content = chunk.choices[0].delta.content\n            if content is not None:\n                print(content, end="")\n                full_response.append(content)\n\n        trace.set_outputs("".join(full_response))\n\n\nmessages = [\n    {\n        "role": "user",\n        "content": "How much additional hydrogen mass would Jupiter require to ignite a sustainable fusion cycle?",\n    }\n]\n\nawait fetch_openai_response(messages)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}}}]);