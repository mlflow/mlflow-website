"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6993],{10949:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llama_index_workflow_artifacts-5d43cb612c34e8291b3da6f261aa487d.png"},14259:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llama_index_workflow_trace-9056bcd2314ab61a6d7b89a35587a961.png"},16734:(e,n,t)=>{t.d(n,{d:()=>a});var o=t(58069);const l="codeBlock_oJcR";var i=t(74848);const a=e=>{let{children:n,executionCount:t}=e;return(0,i.jsx)("div",{style:{flexGrow:1,minWidth:0,marginTop:"var(--padding-md)",width:"100%"},children:(0,i.jsx)(o.A,{className:l,language:"python",children:n})})}},20723:(e,n,t)=>{t.d(n,{O:()=>i});var o=t(96540),l=t(74848);function i(e){let{children:n,href:t}=e;const i=(0,o.useCallback)((async e=>{if(e.preventDefault(),window.gtag)try{window.gtag("event","notebook-download",{href:t})}catch{}const n=await fetch(t),o=await n.blob(),l=window.URL.createObjectURL(o),i=document.createElement("a");i.style.display="none",i.href=l;const a=t.split("/").pop();i.download=a,document.body.appendChild(i),i.click(),window.URL.revokeObjectURL(l),document.body.removeChild(i)}),[t]);return(0,l.jsx)("a",{className:"button button--primary",style:{marginBottom:"1rem",display:"block",width:"min-content"},href:t,download:!0,onClick:i,children:n})}},38064:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>m,frontMatter:()=>d,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"llms/llama-index/notebooks/llama_index_workflow_tutorial-ipynb","title":"Building a Tool-calling Agent with LlamaIndex Workflow and MLflow","description":"Download this notebook","source":"@site/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial-ipynb.mdx","sourceDirName":"llms/llama-index/notebooks","slug":"/llms/llama-index/notebooks/llama_index_workflow_tutorial","permalink":"/docs/latest/llms/llama-index/notebooks/llama_index_workflow_tutorial","draft":false,"unlisted":false,"editUrl":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb","tags":[],"version":"current","frontMatter":{"custom_edit_url":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb","slug":"llama_index_workflow_tutorial"}}');var l=t(74848),i=t(28453),a=t(16734),r=t(61536),s=(t(86563),t(20723));const d={custom_edit_url:"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb",slug:"llama_index_workflow_tutorial"},c="Building a Tool-calling Agent with LlamaIndex Workflow and MLflow",h={},p=[{value:"What you will learn",id:"what-you-will-learn",level:2},{value:"Installation",id:"installation",level:2},{value:"Choose your favorite LLM",id:"choose-your-favorite-llm",level:2},{value:"Option 1: OpenAI (default)",id:"option-1-openai-default",level:4},{value:"Option 2: Other Hosted LLMs",id:"option-2-other-hosted-llms",level:4},{value:"Option 3: Local LLM",id:"option-3-local-llm",level:4},{value:"Create an MLflow Experiemnt",id:"create-an-mlflow-experiemnt",level:2},{value:"Define tools",id:"define-tools",level:2},{value:"Define Workflow",id:"define-workflow",level:2},{value:"Workflow Primer",id:"workflow-primer",level:4},{value:"Define a ReAct Agent as a Workflow",id:"define-a-react-agent-as-a-workflow",level:4},{value:"Check the Workflow Visually",id:"check-the-workflow-visually",level:4},{value:"Run the Workflow (with Trace)",id:"run-the-workflow-with-trace",level:2},{value:"Review the Trace",id:"review-the-trace",level:2},{value:"Log the Workflow to an MLflow Experiment",id:"log-the-workflow-to-an-mlflow-experiment",level:2},{value:"Prepare a Model script",id:"prepare-a-model-script",level:4},{value:"Logging the Model",id:"logging-the-model",level:4},{value:"Explore the MLflow UI",id:"explore-the-mlflow-ui",level:2},{value:"Load the Model Back for Inference",id:"load-the-model-back-for-inference",level:2},{value:"Learning More",id:"learning-more",level:2}];function u(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"building-a-tool-calling-agent-with-llamaindex-workflow-and-mlflow",children:"Building a Tool-calling Agent with LlamaIndex Workflow and MLflow"})}),"\n",(0,l.jsx)(s.O,{href:"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb",children:"Download this notebook"}),"\n",(0,l.jsxs)(n.p,{children:["Welcome to this interactive tutorial designed to introduce you to LlamaIndex Workflow and its integration with MLflow. This tutorial is structured as a notebook to provide a hands-on, practical learning experience with ",(0,l.jsx)(n.strong,{children:"Workflow"}),", LlamaIndex's novel approach to design LLM applications, and managing the development process with MLflow."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"LlamaIndex Workflow Graph",src:t(64240).A+"",width:"1023",height:"531"})}),"\n",(0,l.jsx)(n.h2,{id:"what-you-will-learn",children:"What you will learn"}),"\n",(0,l.jsx)(n.p,{children:"By the end of this tutorial you will have:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Created an MVP agentic application with tool calling functionality in a LlamaIndex Workflow."}),"\n",(0,l.jsx)(n.li,{children:"Observed the agent actions with MLflow Tracing."}),"\n",(0,l.jsx)(n.li,{children:"Logged that workflow to the MLflow Experiment."}),"\n",(0,l.jsx)(n.li,{children:"Loaded the model back and performed inference."}),"\n",(0,l.jsx)(n.li,{children:"Explored the MLflow UI to learn about logged artifacts."}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,l.jsx)(n.p,{children:"MLflow's integration with LlamaIndex's Workflow API is available in MLflow >= 2.17.0 and LlamaIndex (core) >= 0.11.16. After installing the packages, you may need to restart the Python kernel to correctly load modules."}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:"%pip install mlflow>=2.17.0 llama-index>=0.11.16 -qqqU\n# Workflow util is required for rendering Workflow as HTML\n%pip install llama-index-utils-workflow -qqqU"}),"\n",(0,l.jsx)(n.h2,{id:"choose-your-favorite-llm",children:"Choose your favorite LLM"}),"\n",(0,l.jsxs)(n.p,{children:["By default, LlamaIndex uses OpenAI as the source for LLms and embedding models. If you are signing up with different LLM providers or using a local model, configure them for use by using the ",(0,l.jsx)(n.code,{children:"Settings"})," object."]}),"\n",(0,l.jsx)(n.h4,{id:"option-1-openai-default",children:"Option 1: OpenAI (default)"}),"\n",(0,l.jsx)(n.p,{children:"LlamaIndex by default uses OpenAI APIs for LLMs and embeddings models. To proceed with this setting, you just need to set the API key in the environment variable."}),"\n",(0,l.jsx)(a.d,{executionCount:1,children:'import os\n\nos.environ["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"'}),"\n",(0,l.jsx)(n.h4,{id:"option-2-other-hosted-llms",children:"Option 2: Other Hosted LLMs"}),"\n",(0,l.jsx)(n.p,{children:"If you want to use other hosted LLMs,"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Download the integration package for the model provider of your choice."}),"\n",(0,l.jsx)(n.li,{children:"Set up required environment variables as specified in the integration documentation."}),"\n",(0,l.jsxs)(n.li,{children:["Instantiate the LLM instance and set it to the global ",(0,l.jsx)(n.code,{children:"Settings"})," object."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"The following cells show an example for using Databricks hosted LLMs (Llama3.1 70B instruct)."}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:"%pip install llama-index-llms-databricks"}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:'import os\n\nos.environ["DATABRICKS_TOKEN"] = "<YOUR_DATABRICKS_API_TOKEN>"\nos.environ["DATABRICKS_SERVING_ENDPOINT"] = "https://YOUR_DATABRICKS_HOST/serving-endpoints/"'}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:'from llama_index.core import Settings\nfrom llama_index.llms.databricks import Databricks\n\nllm = Databricks(model="databricks-meta-llama-3-1-70b-instruct")\nSettings.llm = llm'}),"\n",(0,l.jsx)(n.h4,{id:"option-3-local-llm",children:"Option 3: Local LLM"}),"\n",(0,l.jsxs)(n.p,{children:["LlamaIndex also support locally hosted LLMs. Please refer to the ",(0,l.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/",children:"Starter Tutorial (Local Models)"})," for how to set them up."]}),"\n",(0,l.jsx)(n.h2,{id:"create-an-mlflow-experiemnt",children:"Create an MLflow Experiemnt"}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.em,{children:"Skip this step if you are running this tutorial on a Databricks Notebook. An MLflow experiment is automatically set up when you created any notebook."})}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:'import mlflow\n\nmlflow.set_experiment("MLflow LlamaIndex Workflow Tutorial")'}),"\n",(0,l.jsx)(n.h2,{id:"define-tools",children:"Define tools"}),"\n",(0,l.jsxs)(n.p,{children:["The agents access with various functions and resources via ",(0,l.jsx)(n.code,{children:"tool"})," objects. In this example, we define the simplest possible math tools ",(0,l.jsx)(n.code,{children:"add"})," and ",(0,l.jsx)(n.code,{children:"multiply"})," based on Python functions. For a real-world application, you can create arbitrary tools such as vector search retrieval, web search, or even calling another agent as a tool. Please refer to the ",(0,l.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/",children:"Tools documentation"})," for more details."]}),"\n",(0,l.jsxs)(n.p,{children:["Please ignore the ",(0,l.jsx)(n.code,{children:"### [USE IN MODEL]"})," comment at the beginning of some cells like below. This will be used in later steps in this tutorial!"]}),"\n",(0,l.jsx)(a.d,{executionCount:3,children:'# [USE IN MODEL]\nfrom llama_index.core.tools import FunctionTool\n\n\ndef add(x: int, y: int) -> int:\n  """Useful function to add two numbers."""\n  return x + y\n\n\ndef multiply(x: int, y: int) -> int:\n  """Useful function to multiply two numbers."""\n  return x * y\n\n\ntools = [\n  FunctionTool.from_defaults(add),\n  FunctionTool.from_defaults(multiply),\n]'}),"\n",(0,l.jsx)(n.h2,{id:"define-workflow",children:"Define Workflow"}),"\n",(0,l.jsx)(n.h4,{id:"workflow-primer",children:"Workflow Primer"}),"\n",(0,l.jsxs)(n.p,{children:["LlamaIndex Workflow is an event-driven orchestration framework. At its core, a workflow consists of two fundamental components: ",(0,l.jsx)(n.strong,{children:"Steps"})," and ",(0,l.jsx)(n.strong,{children:"Events"}),"."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Steps"}),": Units of execution within the workflow. Steps are defined as methods marked with the ",(0,l.jsx)(n.code,{children:"@step"})," decorator in a class that implements the ",(0,l.jsx)(n.code,{children:"Workflow"})," base class."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Events"}),": Custom objects that trigger steps. Two special events, ",(0,l.jsx)(n.code,{children:"StartEvent"})," and ",(0,l.jsx)(n.code,{children:"EndEvent"}),", are reserved for dispatch at the beginning and end of the workflow."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Each step specifies its input and output events through its function signature."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"@step\nasync def my_step(self, event: StartEvent) -> FooEvent:\n    # This method triggers when a StartEvent is emitted at the workflow's start,\n    # and then dispatches a FooEvent.\n"})}),"\n",(0,l.jsx)(n.p,{children:"Based on each step\u2019s signature and defined events, LlamaIndex automatically constructs the workflow\u2019s execution flow."}),"\n",(0,l.jsxs)(n.p,{children:["You may notice that the ",(0,l.jsx)(n.code,{children:"my_step"})," function is defined as an async function. LlamaIndex Workflow makes asynchronous operations a first-class feature, enabling easy parallel execution and scalable workflows."]}),"\n",(0,l.jsxs)(n.p,{children:["Another essential component of the workflow is the ",(0,l.jsx)(n.strong,{children:"Context"})," object. This global registry, accessible from any step, allows shared information to be defined without the need to pass it through multiple events."]}),"\n",(0,l.jsx)(n.h4,{id:"define-a-react-agent-as-a-workflow",children:"Define a ReAct Agent as a Workflow"}),"\n",(0,l.jsx)(n.p,{children:"The Workflow definition below models a ReAct Agent that utilizes the simple math tools we defined."}),"\n",(0,l.jsx)(a.d,{executionCount:4,children:'# [USE IN MODEL]\n\n# Event definitions\nfrom llama_index.core.llms import ChatMessage, ChatResponse\nfrom llama_index.core.tools import ToolOutput, ToolSelection\nfrom llama_index.core.workflow import Event\n\n\nclass PrepEvent(Event):\n  """An event to handle new messages and prepare the chat history"""\n\n\nclass LLMInputEvent(Event):\n  """An event to prmopt the LLM with the react prompt (chat history)"""\n\n  input: list[ChatMessage]\n\n\nclass LLMOutputEvent(Event):\n  """An event represents LLM generation"""\n\n  response: ChatResponse\n\n\nclass ToolCallEvent(Event):\n  """An event to trigger tool calls, if any"""\n\n  tool_calls: list[ToolSelection]\n\n\nclass ToolOutputEvent(Event):\n  """An event to handle the results of tool calls, if any"""\n\n  output: ToolOutput'}),"\n",(0,l.jsx)(a.d,{executionCount:15,children:'# [USE IN MODEL]\n\n# Workflow definition\nfrom llama_index.core import Settings\nfrom llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser\nfrom llama_index.core.agent.react.types import ActionReasoningStep, ObservationReasoningStep\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.workflow import (\n  Context,\n  StartEvent,\n  StopEvent,\n  Workflow,\n  step,\n)\n\n\nclass ReActAgent(Workflow):\n  def __init__(self, *args, **kwargs):\n      super().__init__(*args, **kwargs)\n      self.tools = tools\n      # Store the chat history in memory so the agent can handle multiple interactions with users.\n      self.memory = ChatMemoryBuffer.from_defaults(llm=Settings.llm)\n\n  @step\n  async def new_user_msg(self, ctx: Context, ev: StartEvent) -> PrepEvent:\n      """Start workflow with the new user messsage"""\n      # StartEvent carries whatever keys passed to the workflow\'s run() method as attributes.\n      user_input = ev.input\n      user_msg = ChatMessage(role="user", content=user_input)\n      self.memory.put(user_msg)\n\n      # We store the executed reasoning steps in the context. Clear it at the start.\n      await ctx.set("steps", [])\n\n      return PrepEvent()\n\n  @step\n  async def prepare_llm_prompt(self, ctx: Context, ev: PrepEvent) -> LLMInputEvent:\n      """Prepares the react prompt, using the chat history, tools, and current reasoning (if any)"""\n      steps = await ctx.get("steps", default=[])\n      chat_history = self.memory.get()\n\n      # Construct an LLM from the chat history, tools, and current reasoning, using the\n      # built-in prompt template.\n      llm_input = ReActChatFormatter().format(self.tools, chat_history, current_reasoning=steps)\n      return LLMInputEvent(input=llm_input)\n\n  @step\n  async def invoke_llm(self, ev: LLMInputEvent) -> LLMOutputEvent:\n      """Call the LLM with the react prompt"""\n      response = await Settings.llm.achat(ev.input)\n      return LLMOutputEvent(response=response)\n\n  @step\n  async def handle_llm_response(\n      self, ctx: Context, ev: LLMOutputEvent\n  ) -> ToolCallEvent | PrepEvent | StopEvent:\n      """\n      Parse the LLM response to extract any tool calls requested.\n      If theere is no tool call, we can stop and emit a StopEvent. Otherwise, we emit a ToolCallEvent to handle tool calls.\n      """\n      try:\n          step = ReActOutputParser().parse(ev.response.message.content)\n          (await ctx.get("steps", default=[])).append(step)\n\n          if step.is_done:\n              # No additional tool call is required. Ending the workflow by emitting StopEvent.\n              return StopEvent(result=step.response)\n          elif isinstance(step, ActionReasoningStep):\n              # Tool calls are returned from LLM, trigger the tool call event.\n              return ToolCallEvent(\n                  tool_calls=[\n                      ToolSelection(\n                          tool_id="fake",\n                          tool_name=step.action,\n                          tool_kwargs=step.action_input,\n                      )\n                  ]\n              )\n      except Exception as e:\n          error_step = ObservationReasoningStep(\n              observation=f"There was an error in parsing my reasoning: {e}"\n          )\n          (await ctx.get("steps", default=[])).append(error_step)\n\n      # if no tool calls or final response, iterate again\n      return PrepEvent()\n\n  @step\n  async def handle_tool_calls(self, ctx: Context, ev: ToolCallEvent) -> PrepEvent:\n      """\n      Safely calls tools with error handling, adding the tool outputs to the current reasoning. Then, by emitting a PrepEvent, we loop around for another round of ReAct prompting and parsing.\n      """\n      tool_calls = ev.tool_calls\n      tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n\n      # call tools -- safely!\n      for tool_call in tool_calls:\n          if tool := tools_by_name.get(tool_call.tool_name):\n              try:\n                  tool_output = tool(**tool_call.tool_kwargs)\n                  step = ObservationReasoningStep(observation=tool_output.content)\n              except Exception as e:\n                  step = ObservationReasoningStep(\n                      observation=f"Error calling tool {tool.metadata.get_name()}: {e}"\n                  )\n          else:\n              step = ObservationReasoningStep(\n                  observation=f"Tool {tool_call.tool_name} does not exist"\n              )\n          (await ctx.get("steps", default=[])).append(step)\n\n      # prep the next iteration\n      return PrepEvent()'}),"\n",(0,l.jsx)(n.h4,{id:"check-the-workflow-visually",children:"Check the Workflow Visually"}),"\n",(0,l.jsx)(n.p,{children:"Before instantiating the agent object, let's pause and validate if the workflow is constructed as we expect."}),"\n",(0,l.jsxs)(n.p,{children:["To check that, we can render the graphical representation of the workflow by using the ",(0,l.jsx)(n.code,{children:"draw_all_possible_flows"})," utility function."]}),"\n",(0,l.jsxs)(n.p,{children:["(Note: If the rendered HTML is blank, it might be due to the safety feature in Jupyter. In that case, you can trust the notebook by ",(0,l.jsx)(n.code,{children:"!jupyter trust llama_index_workflow_tutorial.ipynb"}),". See ",(0,l.jsx)(n.a,{href:"https://jupyter-notebook.readthedocs.io/en/latest/notebook.html#signing-notebooks",children:"Jupyter documentation"})," for more details.)"]}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:'from IPython.display import HTML\nfrom llama_index.utils.workflow import draw_all_possible_flows\n\ndraw_all_possible_flows(ReActAgent, filename="workflow.html")\n\nwith open("workflow.html") as file:\n  html_content = file.read()\nHTML(html_content)'}),"\n",(0,l.jsx)(a.d,{executionCount:17,children:"# [USE IN MODEL]\nagent = ReActAgent(timeout=180)"}),"\n",(0,l.jsx)(n.h2,{id:"run-the-workflow-with-trace",children:"Run the Workflow (with Trace)"}),"\n",(0,l.jsxs)(n.p,{children:["Now your workflow is all set! But before running that, let's not forget to turn on ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/llms/tracing/index.html",children:"MLflow Tracing"}),", so you get observability into each step during the agent run, and record it for the review later."]}),"\n",(0,l.jsxs)(n.p,{children:["Mlflow supports automatic tracing for LlamaIndex Workflow. To enable it, you just need to call the ",(0,l.jsx)(n.code,{children:"mlflow.llama_index.autolog()"})," function."]}),"\n",(0,l.jsx)(a.d,{executionCount:12,children:"import mlflow\n\nmlflow.llama_index.autolog()"}),"\n",(0,l.jsx)(a.d,{executionCount:18,children:'# Run the workflow\nawait agent.run(input="What is (123 + 456) * 789?")'}),"\n",(0,l.jsx)(r.p,{children:"'The result of (123 + 456) * 789 is 579,027.'"}),"\n",(0,l.jsx)(n.h2,{id:"review-the-trace",children:"Review the Trace"}),"\n",(0,l.jsx)(n.p,{children:"The generated traces are automatically recorded to your MLflow Experiment."}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Open a terminal, run ",(0,l.jsx)(n.code,{children:"mlflow ui --port 5000"})," within the current directory (and keep it running)."]}),"\n",(0,l.jsxs)(n.li,{children:["Navigate to ",(0,l.jsx)(n.code,{children:"http://127.0.0.1:5000"})," in your browser."]}),"\n",(0,l.jsx)(n.li,{children:'Open the experiment "MLflow LlamaIndex Workflow Tutorial".'}),"\n",(0,l.jsx)(n.li,{children:'Navigate to the "Trace" tab below the experiment name header.'}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"LlamaIndex Workflow Trace",src:t(14259).A+"",width:"1495",height:"718"})}),"\n",(0,l.jsx)(n.p,{children:"The Trace records the individual steps inside the workflow execution with its inputs, outputs, and additional metadata such as latency. Let's do a quick exercise to find the following information on the Trace UI."}),"\n",(0,l.jsxs)("html",{children:[(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:"1. Token count used for the first LLM invocation"}),(0,l.jsxs)("p",{children:["You can find token counts for LLm call in the ",(0,l.jsx)("strong",{children:"Attribtues"})," section of the LLM call span, inside the ",(0,l.jsx)("code",{children:"usage"})," field."]})]}),(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:'2. Input numbers for the "add" tool call.'}),(0,l.jsxs)("p",{children:["You can find input numbers ",(0,l.jsx)("code",{children:"x=123"})," and ",(0,l.jsx)("code",{children:"y=456"})," in the ",(0,l.jsx)("code",{children:"Inputs"})," field of the span named ",(0,l.jsx)("code",{children:"FunctionTool.call"}),". That span is located under the ",(0,l.jsx)("code",{children:"ReActAgent.handle_tool_calls"})," step span."]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"log-the-workflow-to-an-mlflow-experiment",children:"Log the Workflow to an MLflow Experiment"}),"\n",(0,l.jsxs)(n.p,{children:["Now that you've built your first ReAct Agent using LlamaIndex Workflow, it\u2019s essential to iteratively refine and optimize for better performance. An ",(0,l.jsx)(n.strong,{children:"MLflow Experiment"})," is the ideal place to record and manage these improvements"]}),"\n",(0,l.jsx)(n.h4,{id:"prepare-a-model-script",children:"Prepare a Model script"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow supports logging LlamaIndex workflows using the ",(0,l.jsx)(n.strong,{children:"Models from Code"})," method, allowing models to be defined and logged directly from a standalone Python script. This approach bypasses the need for risky and brittle serialization methods like ",(0,l.jsx)(n.code,{children:"pickle"}),", using code as the single source of truth for the model definition. Combined with MLflow\u2019s environment-freezing capability, this provides a reliable way to persist the model."]}),"\n",(0,l.jsxs)(n.p,{children:["For more details, see the ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/models.html#models-from-code",children:"MLflow documentation"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"You could manually create a separate Python file by copying the code from this notebook. However, for convenience, we define a utility function to generate a model script automatically from this notebook's content in one step. Running the cell below will create this script in the current directory, ready for MLflow logging."}),"\n",(0,l.jsx)(a.d,{executionCount:22,children:'def generate_model_script(output_path, notebook_path="llama_index_workflow_tutorial.ipynb"):\n  """\n  A utility function to generate a ready-to-log .py script that\n  contains necessary library imports and model definitions.\n\n  Args:\n     output_path: The path to write the .py file to.\n     notebook_path: The path to the tutorial notebook.\n  """\n  import nbformat\n\n  with open(notebook_path, encoding="utf-8") as f:\n      notebook = nbformat.read(f, as_version=4)\n\n  # Filter cells that are code cells and contain the specified marker\n  merged_code = (\n      "\n\n".join(\n          [\n              cell.source\n              for cell in notebook.cells\n              if cell.cell_type == "code" and cell.source.startswith("# [USE IN MODEL]")\n          ]\n      )\n      + "\n\nimport mlflow\n\nmlflow.models.set_model(agent)"\n  )\n\n  # Write to the output .py file\n  with open(output_path, "w", encoding="utf-8") as f:\n      f.write(merged_code)\n\n  print(f"Model code saved to {output_path}")\n\n\n# Pass `notebook_path` argument if you changed the notebook name\ngenerate_model_script(output_path="react_agent.py")'}),"\n",(0,l.jsx)(r.p,{children:"Model code saved to react_agent.py"}),"\n",(0,l.jsx)(n.h4,{id:"logging-the-model",children:"Logging the Model"}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:'import mlflow\n\nwith mlflow.start_run(run_name="react-agent-workflow"):\n  model_info = mlflow.llama_index.log_model(\n      "react_agent.py",\n      name="model",\n      # Logging with an input example help MLflow to record dependency and signature information accurately.\n      input_example={"input": "What is (123 + 456) * 789?"},\n  )'}),"\n",(0,l.jsx)(n.h2,{id:"explore-the-mlflow-ui",children:"Explore the MLflow UI"}),"\n",(0,l.jsx)(n.p,{children:"Let's open the MLflow UI again to see which information is being tracked in the experiment."}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Access the MLflow UI like we did for reviewing traces."}),"\n",(0,l.jsx)(n.li,{children:'Open the experiment "MLflow LlamaIndex Workflow Tutorial".'}),"\n",(0,l.jsxs)(n.li,{children:["The ",(0,l.jsx)(n.code,{children:"Runs"}),' tab in the experiment should contain a run named "react-agent-workflow". Open it.']}),"\n",(0,l.jsxs)(n.li,{children:["On the run page, navigate to the ",(0,l.jsx)(n.code,{children:'"Artifacts"'})," tab."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"The artifacts tab shows various files saved by MLflow in the Run. See the below image and open the annotated files to check which information is stored in each file."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"LlamaIndex Workflow Artifacts",src:t(10949).A+"",width:"1396",height:"867"})}),"\n",(0,l.jsx)(n.h2,{id:"load-the-model-back-for-inference",children:"Load the Model Back for Inference"}),"\n",(0,l.jsx)(n.p,{children:"With all necessary metadata logged to MLflow, you can load the model in a different notebook or deploy it for inference without concerns about environment inconsistencies. Let\u2019s do a quick exercise to demonstrate how this helps in reproducing experiment results."}),"\n",(0,l.jsxs)(n.p,{children:["To simulate a different environment, we\u2019ll remove the ",(0,l.jsx)(n.code,{children:"llm"})," configuration from the global ",(0,l.jsx)(n.code,{children:"Settings"})," object."]}),"\n",(0,l.jsx)(a.d,{executionCount:24,children:'from llama_index.core.llms import MockLLM\n\nSettings.llm = MockLLM(max_tokens=1)\n\nawait agent.run(input="What is (123 + 456) * 789?")'}),"\n",(0,l.jsx)(r.p,{children:"'text'"}),"\n",(0,l.jsx)(n.p,{children:'Since the dummy LLM is configured, the workflow could not generate the correct output but just returns "text".'}),"\n",(0,l.jsxs)(n.p,{children:["Now try loading the model back from the MLflow Experiment by calling ",(0,l.jsx)(n.code,{children:"mlflow.llama_index.load_model()"})," API and run the workflow again."]}),"\n",(0,l.jsx)(a.d,{executionCount:" ",children:'loaded_model = mlflow.llama_index.load_model("runs:/f8e0a0d2dd5546d5ac93ce126358c444/model")\nawait loaded_model.run(input="What is (123 + 456) * 789?")'}),"\n",(0,l.jsx)(r.p,{children:"Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"}),"\n",(0,l.jsx)(r.p,{children:"'(123 + 456) * 789 = 456831'"}),"\n",(0,l.jsx)(n.p,{children:"This time, the output is computed correctly, because MLflow automatically restores the original LLM setting at the time of logging."}),"\n",(0,l.jsx)(n.h2,{id:"learning-more",children:"Learning More"}),"\n",(0,l.jsx)(n.p,{children:"Congratulations! \ud83c\udf89 You\u2019ve successfully learned how to build a tool-calling agent using LlamaIndex Workflow and MLflow."}),"\n",(0,l.jsx)(n.p,{children:"Continue your journey with these advanced resources:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Improve Workflow Quality"}),": Evaluate your workflow to enhance performance with ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/llms/llm-evaluate/index.html",children:"MLflow LLM Evaluation"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Deploy Your Model"}),": Deploy your MLflow model to a serving endpoint with ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/deployment/index.html",children:"MLflow Deployment"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Explore More Examples"}),": Discover additional examples of LlamaIndex Workflow in the ",(0,l.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/workflow/#examples",children:"official documentation"}),"."]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(u,{...e})}):u(e)}},61536:(e,n,t)=>{t.d(n,{p:()=>l});var o=t(74848);const l=e=>{let{children:n,isStderr:t}=e;return(0,o.jsx)("pre",{style:{margin:0,borderRadius:0,background:"none",fontSize:"0.85rem",flexGrow:1,padding:"var(--padding-sm)"},children:n})}},64240:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llama_index_workflow_graph-b212ec728fbcb5e758bd72a3fc374d2d.png"},86563:(e,n,t)=>{t.d(n,{Q:()=>l});var o=t(74848);const l=e=>{let{children:n}=e;return(0,o.jsx)("div",{style:{flexGrow:1,minWidth:0,fontSize:"0.8rem",width:"100%"},children:n})}}}]);