"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["1259"],{19574(e,n,t){t.r(n),t.d(n,{metadata:()=>r,default:()=>w,frontMatter:()=>g,contentTitle:()=>f,toc:()=>v,assets:()=>j});var r=JSON.parse('{"id":"eval-monitor/scorers/llm-judge/tool-call/correctness","title":"ToolCallCorrectness Judge","description":"The ToolCallCorrectness judge evaluates whether the tools called by an agent and the arguments they are called with are correct given the user request.","source":"@site/docs/genai/eval-monitor/scorers/llm-judge/tool-call/correctness.mdx","sourceDirName":"eval-monitor/scorers/llm-judge/tool-call","slug":"/eval-monitor/scorers/llm-judge/tool-call/correctness","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/tool-call/correctness","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Tool Call Evaluation with Built-in Judges","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/tool-call/"},"next":{"title":"ToolCallEfficiency","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/tool-call/efficiency"}}'),l=t(74848),a=t(28453),o=t(77541),s=t(10440),i=t(46858),c=t(42640),d=t(47792),h=t(78010),u=t(57250),p=t(95986),m=t(83884),x=t(55935);let g={},f="ToolCallCorrectness Judge",j={},v=[...m.RM,{value:"Evaluation modes",id:"evaluation-modes",level:2},{value:"Usage examples",id:"usage-examples",level:2},{value:"Using expectations for comparison",id:"using-expectations-for-comparison",level:2},{value:"Parameters",id:"parameters",level:3},{value:"Fuzzy matching (default)",id:"fuzzy-matching-default",level:3},{value:"Exact matching",id:"exact-matching",level:3},{value:"Partial expectations (names only)",id:"partial-expectations-names-only",level:3},{value:"Considering tool call ordering",id:"considering-tool-call-ordering",level:2},...x.RM,{value:"Interpret results",id:"interpret-results",level:2},{value:"Next steps",id:"next-steps",level:2}];function y(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"toolcallcorrectness-judge",children:"ToolCallCorrectness Judge"})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"ToolCallCorrectness"})," judge evaluates whether the tools called by an agent and the arguments they are called with are correct given the user request."]}),"\n",(0,l.jsx)(n.p,{children:"This built-in LLM judge is designed for evaluating AI agents and tool-calling applications where you need to ensure the agent selects appropriate tools and provides correct arguments to fulfill the user's request."}),"\n",(0,l.jsx)(m.Ay,{}),"\n",(0,l.jsx)(n.h2,{id:"evaluation-modes",children:"Evaluation modes"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"ToolCallCorrectness"})," judge supports three modes of evaluation:"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Ground-truth free (default)"}),": When no expectations are provided, uses an LLM to judge whether tool calls are reasonable given the user request and available tools."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"With expectations (fuzzy match)"}),": When expectations are provided and ",(0,l.jsx)(n.code,{children:"should_exact_match=False"}),", uses an LLM to semantically compare actual tool calls against expected tool calls."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"With expectations (exact match)"}),": When expectations are provided and ",(0,l.jsx)(n.code,{children:"should_exact_match=True"}),", performs direct comparison of tool names and arguments."]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"usage-examples",children:"Usage examples"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"ToolCallCorrectness"})," judge can be invoked directly for single trace assessment or used with MLflow's evaluation framework for batch evaluation."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Trace requirements"}),": - The MLflow Trace must contain at least one span with ",(0,l.jsx)(n.code,{children:"span_type"})," set to ",(0,l.jsx)(n.code,{children:"TOOL"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Ground-truth labels"}),": Optional - can provide ",(0,l.jsx)(n.code,{children:"expected_tool_calls"})," in the expectations dictionary for comparison"]}),"\n"]}),"\n",(0,l.jsx)(p.A,{children:(0,l.jsxs)(h.A,{groupId:"invocation-method",children:[(0,l.jsx)(u.A,{value:"direct",label:"Invoke directly",default:!0,children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import ToolCallCorrectness\nimport mlflow\n\n# Get a trace from a previous run\ntrace = mlflow.get_trace("<your-trace-id>")\n\n# Assess if tool calls are correct (ground-truth free mode)\nfeedback = ToolCallCorrectness(name="my_tool_call_correctness")(trace=trace)\nprint(feedback)\n'})})}),(0,l.jsx)(u.A,{value:"evaluate",label:"Invoke with evaluate()",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\nfrom mlflow.genai.scorers import ToolCallCorrectness\n\n# Evaluate traces from previous runs\nresults = mlflow.genai.evaluate(\n    data=traces,  # DataFrame or list containing trace data\n    scorers=[ToolCallCorrectness()],\n)\n"})})})]})}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:["For a complete agent example with this judge, see the ",(0,l.jsx)("ins",{children:(0,l.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/tool-call/",children:"Tool Call Evaluation guide"})}),"."]})}),"\n",(0,l.jsx)(n.h2,{id:"using-expectations-for-comparison",children:"Using expectations for comparison"}),"\n",(0,l.jsx)(n.p,{children:"You can provide expected tool calls to compare against the actual tool calls made by the agent."}),"\n",(0,l.jsx)(n.h3,{id:"parameters",children:"Parameters"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Parameter"}),(0,l.jsx)(n.th,{children:"Type"}),(0,l.jsx)(n.th,{children:"Default"}),(0,l.jsx)(n.th,{children:"Description"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"should_exact_match"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"bool"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"False"})}),(0,l.jsxs)(n.td,{children:["Controls comparison mode when expectations are provided. If ",(0,l.jsx)(n.code,{children:"False"}),", uses LLM for semantic comparison of tool calls. If ",(0,l.jsx)(n.code,{children:"True"}),", performs direct string comparison of tool names and arguments."]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"should_consider_ordering"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"bool"})}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"False"})}),(0,l.jsxs)(n.td,{children:["Whether to enforce the order of tool calls when comparing against expectations. If ",(0,l.jsx)(n.code,{children:"True"}),", tool calls must match the expected order. If ",(0,l.jsx)(n.code,{children:"False"}),", order is ignored."]})]})]})]}),"\n",(0,l.jsx)(n.h3,{id:"fuzzy-matching-default",children:"Fuzzy matching (default)"}),"\n",(0,l.jsx)(n.p,{children:"With fuzzy matching, the LLM semantically compares actual tool calls against expected ones:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import ToolCallCorrectness\n\n# Define expected tool calls\neval_dataset = [\n    {\n        "inputs": {"query": "What\'s the weather in San Francisco?"},\n        "expectations": {\n            "expected_tool_calls": [\n                {"name": "get_weather", "arguments": {"location": "San Francisco, CA"}},\n            ]\n        },\n    },\n    {\n        "inputs": {"query": "What\'s the weather in Tokyo?"},\n        "expectations": {\n            "expected_tool_calls": [\n                {"name": "get_weather", "arguments": {"location": "Tokyo, Japan"}},\n            ]\n        },\n    },\n]\n\n# Evaluate with fuzzy matching (default)\neval_results = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=weather_agent,\n    scorers=[ToolCallCorrectness()],  # should_exact_match=False by default\n)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"exact-matching",children:"Exact matching"}),"\n",(0,l.jsx)(n.p,{children:"With exact matching, tool names and arguments are compared directly:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import ToolCallCorrectness\n\n# Define expected tool calls\neval_dataset = [\n    {\n        "inputs": {"query": "What\'s the weather in San Francisco?"},\n        "expectations": {\n            "expected_tool_calls": [\n                {"name": "get_weather", "arguments": {"location": "San Francisco, CA"}},\n            ]\n        },\n    },\n]\n\n# Use exact matching for stricter comparison\nscorer = ToolCallCorrectness(should_exact_match=True)\n\neval_results = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=weather_agent,\n    scorers=[scorer],\n)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"partial-expectations-names-only",children:"Partial expectations (names only)"}),"\n",(0,l.jsx)(n.p,{children:"You can provide only tool names without arguments to check that the correct tools are called:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import ToolCallCorrectness\n\neval_dataset = [\n    {\n        "inputs": {"query": "What\'s the weather in Tokyo?"},\n        "expectations": {\n            "expected_tool_calls": [\n                {"name": "get_weather"},  # Only check tool name\n            ]\n        },\n    },\n]\n\nscorer = ToolCallCorrectness(should_exact_match=True)\neval_results = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=weather_agent,\n    scorers=[scorer],\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"considering-tool-call-ordering",children:"Considering tool call ordering"}),"\n",(0,l.jsx)(n.p,{children:"By default, the judge ignores the order of tool calls. To enforce ordering:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import ToolCallCorrectness\n\n# Enforce that tools are called in the expected order\nscorer = ToolCallCorrectness(\n    should_exact_match=True,\n    should_consider_ordering=True,\n)\n\n# Example with multiple expected tool calls\neval_dataset = [\n    {\n        "inputs": {"query": "Get weather for Paris and then for London"},\n        "expectations": {\n            "expected_tool_calls": [\n                {"name": "get_weather", "arguments": {"location": "Paris"}},\n                {"name": "get_weather", "arguments": {"location": "London"}},\n            ]\n        },\n    },\n]\n\neval_results = mlflow.genai.evaluate(\n    data=eval_dataset,\n    predict_fn=weather_agent,\n    scorers=[scorer],\n)\n'})}),"\n",(0,l.jsx)(x.Ay,{}),"\n",(0,l.jsx)(n.h2,{id:"interpret-results",children:"Interpret results"}),"\n",(0,l.jsx)(n.p,{children:"The judge returns a Feedback object with:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"value"}),': "yes" if tool calls are correct, "no" if incorrect']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"rationale"}),": Detailed explanation identifying:","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Which tool calls are correct or problematic"}),"\n",(0,l.jsx)(n.li,{children:"Whether arguments match expectations or are reasonable"}),"\n",(0,l.jsx)(n.li,{children:"Why certain tool choices were appropriate or inappropriate"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,l.jsxs)(s.A,{children:[(0,l.jsx)(o.A,{icon:i.A,title:"Evaluate tool call efficiency",description:"Check if tool calls are efficient without redundancy",href:"/genai/eval-monitor/scorers/llm-judge/tool-call/efficiency"}),(0,l.jsx)(o.A,{icon:c.A,title:"Evaluate agents",description:"Learn comprehensive agent evaluation techniques",href:"/genai/eval-monitor/running-evaluation/agents"}),(0,l.jsx)(o.A,{icon:d.A,title:"Build evaluation datasets",description:"Create test cases with expected tool calls for testing",href:"/genai/datasets/"})]})]})}function w(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(y,{...e})}):y(e)}},83884(e,n,t){t.d(n,{Ay:()=>s,RM:()=>a});var r=t(74848),l=t(28453);let a=[{value:"Prerequisites for running the examples",id:"prerequisites-for-running-the-examples",level:2}];function o(e){let n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"prerequisites-for-running-the-examples",children:"Prerequisites for running the examples"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Install MLflow and required packages"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install --upgrade mlflow\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Create an MLflow experiment by following the ",(0,r.jsx)(n.a,{href:"/genai/getting-started/connect-environment/",children:"setup your environment quickstart"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["(Optional, if using OpenAI models) Use the native OpenAI SDK to connect to OpenAI-hosted models. Select a model from the ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/models",children:"available OpenAI models"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\nimport openai\n\n# Ensure your OPENAI_API_KEY is set in your environment\n# os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>" # Uncomment and set if not globally configured\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Create an OpenAI client\nclient = openai.OpenAI()\n\n# Select an LLM\nmodel_name = "gpt-4o-mini"\n'})}),"\n"]}),"\n"]})]})}function s(e={}){let{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}},55935(e,n,t){t.d(n,{Ay:()=>s,RM:()=>a});var r=t(74848),l=t(28453);let a=[{value:"Select the LLM that powers the judge",id:"select-the-llm-that-powers-the-judge",level:2}];function o(e){let n={a:"a",code:"code",h2:"h2",p:"p",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"select-the-llm-that-powers-the-judge",children:"Select the LLM that powers the judge"}),"\n",(0,r.jsxs)(n.p,{children:["You can change the judge model by using the ",(0,r.jsx)(n.code,{children:"model"})," argument in the judge definition. The model must be specified in the format ",(0,r.jsx)(n.code,{children:"<provider>:/<model-name>"}),", where ",(0,r.jsx)(n.code,{children:"<provider>"})," is a LiteLLM-compatible model provider."]}),"\n",(0,r.jsxs)(n.p,{children:["For a list of supported models, see ",(0,r.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/custom-judges/#selecting-judge-models",children:"selecting judge models"}),"."]})]})}function s(e={}){let{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}},75689(e,n,t){t.d(n,{A:()=>i});var r=t(96540);let l=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,t)=>t?t.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},a=(...e)=>e.filter((e,n,t)=>!!e&&""!==e.trim()&&t.indexOf(e)===n).join(" ").trim();var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let s=(0,r.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:t=2,absoluteStrokeWidth:l,className:s="",children:i,iconNode:c,...d},h)=>(0,r.createElement)("svg",{ref:h,...o,width:n,height:n,stroke:e,strokeWidth:l?24*Number(t)/Number(n):t,className:a("lucide",s),...!i&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,n])=>(0,r.createElement)(e,n)),...Array.isArray(i)?i:[i]])),i=(e,n)=>{let t=(0,r.forwardRef)(({className:t,...o},i)=>(0,r.createElement)(s,{ref:i,iconNode:n,className:a(`lucide-${l(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,t),...o}));return t.displayName=l(e),t}},42640(e,n,t){t.d(n,{A:()=>r});let r=(0,t(75689).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},47792(e,n,t){t.d(n,{A:()=>r});let r=(0,t(75689).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},46858(e,n,t){t.d(n,{A:()=>r});let r=(0,t(75689).A)("zap",[["path",{d:"M4 14a1 1 0 0 1-.78-1.63l9.9-10.2a.5.5 0 0 1 .86.46l-1.92 6.02A1 1 0 0 0 13 10h7a1 1 0 0 1 .78 1.63l-9.9 10.2a.5.5 0 0 1-.86-.46l1.92-6.02A1 1 0 0 0 11 14z",key:"1xq2db"}]])},57250(e,n,t){t.d(n,{A:()=>a});var r=t(74848);t(96540);var l=t(34164);function a({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,l.A)("tabItem_Ymn6",t),hidden:n,children:e})}},78010(e,n,t){t.d(n,{A:()=>v});var r=t(74848),l=t(96540),a=t(34164),o=t(88287),s=t(28584),i=t(56347),c=t(99989),d=t(96629),h=t(80618),u=t(41367);function p(e){return l.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}var x=t(19863);function g({className:e,block:n,selectedValue:t,selectValue:l,tabValues:o}){let i=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.a_)(),d=e=>{let n=e.currentTarget,r=o[i.indexOf(n)].value;r!==t&&(c(n),l(r))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{let t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{let t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1]}}n?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:l})=>(0,r.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:h,onClick:d,...l,className:(0,a.A)("tabs__item","tabItem_LNqP",l?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function f({lazy:e,children:n,selectedValue:t}){let o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=o.find(e=>e.props.value===t);return e?(0,l.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,l.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function j(e){let n=function(e){let n,{defaultValue:t,queryString:r=!1,groupId:a}=e,o=function(e){let{values:n,children:t}=e;return(0,l.useMemo)(()=>{let e=n??p(t).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r})),r=(0,h.XI)(e,(e,n)=>e.value===n.value);if(r.length>0)throw Error(`Docusaurus error: Duplicate values "${r.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,t])}(e),[s,x]=(0,l.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let t=n.find(e=>e.default)??n[0];if(!t)throw Error("Unexpected error: 0 tabValues");return t.value})({defaultValue:t,tabValues:o})),[g,f]=function({queryString:e=!1,groupId:n}){let t=(0,i.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(r),(0,l.useCallback)(e=>{if(!r)return;let n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}({queryString:r,groupId:a}),[j,v]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[t,r]=(0,u.Dv)(n);return[t,(0,l.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:a}),y=m({value:n=g??j,tabValues:o})?n:null;return(0,c.A)(()=>{y&&x(y)},[y]),{selectedValue:s,selectValue:(0,l.useCallback)(e=>{if(!m({value:e,tabValues:o}))throw Error(`Can't select invalid tab value=${e}`);x(e),f(e),v(e)},[f,v,o]),tabValues:o}}(e);return(0,r.jsxs)("div",{className:(0,a.A)(o.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,r.jsx)(g,{...n,...e}),(0,r.jsx)(f,{...n,...e})]})}function v(e){let n=(0,x.A)();return(0,r.jsx)(j,{...e,children:p(e.children)},String(n))}},95986(e,n,t){t.d(n,{A:()=>l});var r=t(74848);t(96540);function l({children:e}){return(0,r.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,n,t){t.d(n,{A:()=>c});var r=t(74848);t(96540);var l=t(95310),a=t(34164);let o="tileImage_O4So";var s=t(66497),i=t(92802);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:d,iconSize:h=32,containerHeight:u,title:p,description:m,href:x,linkText:g="Learn more \u2192",className:f}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let j=u?{height:`${u}px`}:{},v={};return c&&(v.width=`${c}px`),d&&(v.height=`${d}px`),(0,r.jsxs)(l.A,{href:x,className:(0,a.A)("tileCard_NHsj",f),children:[(0,r.jsx)("div",{className:"tileIcon_pyoR",style:j,children:e?(0,r.jsx)(e,{size:h}):t?(0,r.jsx)(i.A,{sources:{light:(0,s.default)(n),dark:(0,s.default)(t)},alt:p,className:o,style:v}):(0,r.jsx)("img",{src:(0,s.default)(n),alt:p,className:o,style:v})}),(0,r.jsx)("h3",{children:p}),(0,r.jsx)("p",{children:m}),(0,r.jsx)("div",{className:"tileLink_iUbu",children:g})]})}},10440(e,n,t){t.d(n,{A:()=>a});var r=t(74848);t(96540);var l=t(34164);function a({children:e,className:n}){return(0,r.jsx)("div",{className:(0,l.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,t){t.d(n,{R:()=>o,x:()=>s});var r=t(96540);let l={},a=r.createContext(l);function o(e){let n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);