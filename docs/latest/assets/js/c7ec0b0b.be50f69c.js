"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["9590"],{46172(e,n,t){t.r(n),t.d(n,{metadata:()=>r,default:()=>v,frontMatter:()=>f,contentTitle:()=>y,toc:()=>j,assets:()=>L});var r=JSON.parse('{"id":"tracing/integrations/listing/litellm-proxy","title":"Tracing LiteLLM Proxy","description":"LiteLLM Proxy is a self-hosted LLM gateway that provides a unified OpenAI-compatible API to access 100+ LLM providers. It offers features like load balancing, spend tracking, and rate limiting across multiple providers.","source":"@site/docs/genai/tracing/integrations/listing/litellm-proxy.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/litellm-proxy","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/litellm-proxy","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":102,"frontMatter":{"sidebar_position":102,"sidebar_label":"LiteLLM Proxy"},"sidebar":"genAISidebar","previous":{"title":"Kong AI Gateway","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/kong"},"next":{"title":"OpenRouter","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/openrouter"}}'),i=t(74848),a=t(28453),l=t(78010),s=t(57250),o=t(95986),c=t(46077),d=t(89001),p=t(8060),h=t(10440),u=t(77541),m=t(93893),g=t(60665),x=t(43975);let f={sidebar_position:102,sidebar_label:"LiteLLM Proxy"},y="Tracing LiteLLM Proxy",L={},j=[{value:"Integration Options",id:"integration-options",level:2},{value:"Option 1: Server-side Callback (Recommended)",id:"option-1-server-side-callback-recommended",level:2},...p.RM,{value:"Option 2: Client-side Tracing",id:"option-2-client-side-tracing",level:2},...p.RM,{value:"Combining with Manual Tracing",id:"combining-with-manual-tracing",level:2},{value:"Next Steps",id:"next-steps",level:2}];function b(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"tracing-litellm-proxy",children:"Tracing LiteLLM Proxy"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.litellm.ai/docs/proxy/quick_start",children:"LiteLLM Proxy"})," is a self-hosted LLM gateway that provides a unified OpenAI-compatible API to access 100+ LLM providers. It offers features like load balancing, spend tracking, and rate limiting across multiple providers."]}),"\n",(0,i.jsx)(c.A,{src:"/images/llms/litellm-proxy/litellm-proxy-tracing.png",alt:"LiteLLM Proxy Tracing"}),"\n",(0,i.jsx)(n.admonition,{title:"Looking for LiteLLM SDK?",type:"tip",children:(0,i.jsxs)(n.p,{children:["This guide covers the ",(0,i.jsx)(n.strong,{children:"LiteLLM Proxy Server"}),". If you're using the LiteLLM Python SDK directly in your application, see the ",(0,i.jsx)(n.a,{href:"/genai/tracing/integrations/listing/litellm",children:"LiteLLM SDK Integration"})," guide instead."]})}),"\n",(0,i.jsx)(n.h2,{id:"integration-options",children:"Integration Options"}),"\n",(0,i.jsx)(n.p,{children:"There are two ways to trace LLM calls through LiteLLM Proxy with MLflow:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Approach"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Description"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Best For"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.strong,{children:"Server-side Callback"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Configure MLflow as a callback in LiteLLM Proxy config"}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Centralized tracing for all requests through the proxy"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.strong,{children:"Client-side Tracing"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Use OpenAI SDK with MLflow autolog"}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:["Combining LLM traces with your agent or application traces (",(0,i.jsx)(n.a,{href:"#combining-with-manual-tracing",children:"how to \u2192"}),")"]})]})]})]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["With ",(0,i.jsx)(n.strong,{children:"server-side tracing"}),", all requests through the proxy are captured in a single MLflow experiment, regardless of which client or application made them. For application-specific tracing, consider using ",(0,i.jsx)(n.strong,{children:"client-side tracing"})," where each application manages its own traces."]})}),"\n",(0,i.jsx)(n.h2,{id:"option-1-server-side-callback-recommended",children:"Option 1: Server-side Callback (Recommended)"}),"\n",(0,i.jsx)(n.p,{children:"This approach configures LiteLLM Proxy to send traces directly to MLflow, capturing all LLM calls across all clients using the proxy."}),"\n",(0,i.jsx)(d.A,{number:1,title:"Install LiteLLM with MLflow Support"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install 'litellm[mlflow]'\n"})}),"\n",(0,i.jsx)(d.A,{number:2,title:"Start MLflow Server"}),"\n",(0,i.jsx)(p.Ay,{}),"\n",(0,i.jsx)(d.A,{number:3,title:"Configure LiteLLM Proxy"}),"\n",(0,i.jsx)(n.p,{children:"Add MLflow as a callback in your LiteLLM Proxy configuration file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:'title="litellm_config.yaml"',children:'model_list:\n  - model_name: gpt-4o-mini\n    litellm_params:\n      model: openai/gpt-4o-mini\n      api_key: os.environ/OPENAI_API_KEY\n\nlitellm_settings:\n  success_callback: ["mlflow"]\n  failure_callback: ["mlflow"]\n'})}),"\n",(0,i.jsx)(d.A,{number:4,title:"Set Environment Variables"}),"\n",(0,i.jsx)(n.p,{children:"Configure the MLflow tracking URI before starting the proxy:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Required: Point to your MLflow server\nexport MLFLOW_TRACKING_URI="http://localhost:5000"\n\n# Optional: Set the experiment name\nexport MLFLOW_EXPERIMENT_NAME="LiteLLM Proxy"\n'})}),"\n",(0,i.jsx)(d.A,{number:5,title:"Start LiteLLM Proxy"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"litellm --config litellm_config.yaml\n"})}),"\n",(0,i.jsx)(d.A,{number:6,title:"Make API Calls"}),"\n",(0,i.jsx)(n.p,{children:"Make requests to the proxy using any OpenAI-compatible client:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:4000/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gpt-4o-mini",\n    "messages": [\n      {"role": "user", "content": "Hello, how are you?"}\n    ]\n  }\'\n'})}),"\n",(0,i.jsx)(n.p,{children:"Or use the OpenAI Python SDK:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\n# Point to your LiteLLM Proxy\nclient = OpenAI(\n    base_url="http://localhost:4000/v1", api_key="sk-1234"  # Your LiteLLM Proxy API key\n)\n\nresponse = client.chat.completions.create(\n    model="gpt-4o-mini", messages=[{"role": "user", "content": "Hello, how are you?"}]\n)\nprint(response.choices[0].message.content)\n'})}),"\n",(0,i.jsx)(d.A,{number:7,title:"View Traces in MLflow UI"}),"\n",(0,i.jsxs)(n.p,{children:["Open the MLflow UI at ",(0,i.jsx)(n.a,{href:"http://localhost:5000",children:"http://localhost:5000"})," to see the traces from your LiteLLM Proxy calls."]}),"\n",(0,i.jsx)(n.h2,{id:"option-2-client-side-tracing",children:"Option 2: Client-side Tracing"}),"\n",(0,i.jsx)(n.p,{children:"If you don't have access to configure the LiteLLM Proxy server, you can trace calls on the client side using the OpenAI SDK with MLflow autolog. Since LiteLLM Proxy exposes an OpenAI-compatible API, this works seamlessly."}),"\n",(0,i.jsx)(d.A,{number:1,title:"Install Dependencies"}),"\n",(0,i.jsx)(o.A,{children:(0,i.jsxs)(l.A,{groupId:"programming-language",children:[(0,i.jsx)(s.A,{value:"python",label:"Python",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install 'mlflow[genai]' openai\n"})})}),(0,i.jsx)(s.A,{value:"typescript",label:"TypeScript",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"npm install mlflow-openai openai\n"})})})]})}),"\n",(0,i.jsx)(d.A,{number:2,title:"Start MLflow Server"}),"\n",(0,i.jsx)(p.Ay,{}),"\n",(0,i.jsx)(d.A,{number:3,title:"Enable Tracing and Make API Calls"}),"\n",(0,i.jsx)(o.A,{children:(0,i.jsxs)(l.A,{groupId:"programming-language",children:[(0,i.jsxs)(s.A,{value:"python",label:"Python",default:!0,children:[(0,i.jsxs)(n.p,{children:["Enable tracing with ",(0,i.jsx)(n.code,{children:"mlflow.openai.autolog()"})," and configure the OpenAI client to use LiteLLM Proxy's base URL."]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom openai import OpenAI\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Set tracking URI and experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("LiteLLM Proxy")\n\n# Point OpenAI client to LiteLLM Proxy\nclient = OpenAI(\n    base_url="http://localhost:4000/v1",  # LiteLLM Proxy URL\n    api_key="sk-1234",  # Your LiteLLM Proxy API key\n)\n\n# Make API calls as usual - traces will be captured automatically\nresponse = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is the capital of France?"},\n    ],\n)\nprint(response.choices[0].message.content)\n'})})]}),(0,i.jsxs)(s.A,{value:"typescript",label:"TypeScript",children:[(0,i.jsxs)(n.p,{children:["Initialize MLflow tracing with ",(0,i.jsx)(n.code,{children:"init()"})," and wrap the OpenAI client with the ",(0,i.jsx)(n.code,{children:"tracedOpenAI"})," function."]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import { init } from "mlflow-tracing";\nimport { tracedOpenAI } from "mlflow-openai";\nimport { OpenAI } from "openai";\n\n// Initialize MLflow tracing\ninit({\n  trackingUri: "http://localhost:5000",\n  experimentId: "<experiment-id>",\n});\n\n// Wrap the OpenAI client pointing to LiteLLM Proxy\nconst client = tracedOpenAI(\n  new OpenAI({\n    baseURL: "http://localhost:4000/v1", // LiteLLM Proxy URL\n    apiKey: "sk-1234", // Your LiteLLM Proxy API key\n  })\n);\n\n// Make API calls - traces will be captured automatically\nconst response = await client.chat.completions.create({\n  model: "gpt-4o-mini",\n  messages: [\n    { role: "system", content: "You are a helpful assistant." },\n    { role: "user", content: "What is the capital of France?" },\n  ],\n});\nconsole.log(response.choices[0].message.content);\n'})})]})]})}),"\n",(0,i.jsx)(d.A,{number:4,title:"View Traces in MLflow UI"}),"\n",(0,i.jsxs)(n.p,{children:["Open the MLflow UI at ",(0,i.jsx)(n.a,{href:"http://localhost:5000",children:"http://localhost:5000"})," to see the traces."]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"With client-side tracing, you see traces from your application's perspective. Server-side callback tracing provides a complete view of all proxy activity including requests from other clients."})}),"\n",(0,i.jsx)(n.h2,{id:"combining-with-manual-tracing",children:"Combining with Manual Tracing"}),"\n",(0,i.jsx)(n.p,{children:"You can combine auto-tracing with MLflow's manual tracing to create comprehensive traces that include your application logic:"}),"\n",(0,i.jsx)(o.A,{children:(0,i.jsxs)(l.A,{groupId:"programming-language",children:[(0,i.jsx)(s.A,{value:"python",label:"Python",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import SpanType\nfrom openai import OpenAI\n\nmlflow.openai.autolog()\n\nclient = OpenAI(base_url="http://localhost:4000/v1", api_key="sk-1234")\n\n\n@mlflow.trace(span_type=SpanType.CHAIN)\ndef ask_question(question: str) -> str:\n    """A traced function that calls the LLM through LiteLLM Proxy."""\n    response = client.chat.completions.create(\n        model="gpt-5", messages=[{"role": "user", "content": question}]\n    )\n    return response.choices[0].message.content\n\n\n# The entire function call and nested LLM call will be traced\nanswer = ask_question("What is machine learning?")\nprint(answer)\n'})})}),(0,i.jsx)(s.A,{value:"typescript",label:"TypeScript",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'import { init, trace, SpanType } from "mlflow-tracing";\nimport { tracedOpenAI } from "mlflow-openai";\nimport { OpenAI } from "openai";\n\ninit({\n  trackingUri: "http://localhost:5000",\n  experimentId: "<experiment-id>",\n});\n\nconst client = tracedOpenAI(\n  new OpenAI({\n    baseURL: "http://localhost:4000/v1",\n    apiKey: "sk-1234",\n  })\n);\n\n// Wrap your function with trace() to create a span\nconst askQuestion = trace(\n  { name: "askQuestion", spanType: SpanType.CHAIN },\n  async (question: string): Promise<string> => {\n    const response = await client.chat.completions.create({\n      model: "gpt-5",\n      messages: [{ role: "user", content: question }],\n    });\n    return response.choices[0].message.content ?? "";\n  }\n);\n\n// The entire function call and nested LLM call will be traced\nconst answer = await askQuestion("What is machine learning?");\nconsole.log(answer);\n'})})})]})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(h.A,{children:[(0,i.jsx)(u.A,{icon:m.A,iconSize:48,title:"Track User Feedback",description:"Record user feedback on traces for tracking user satisfaction.",href:"/genai/tracing/collect-user-feedback",linkText:"Learn about feedback \u2192",containerHeight:64}),(0,i.jsx)(u.A,{icon:g.A,iconSize:48,title:"Manage Prompts",description:"Learn how to manage prompts with MLflow's prompt registry.",href:"/genai/prompt-registry",linkText:"Manage prompts \u2192",containerHeight:64}),(0,i.jsx)(u.A,{icon:x.A,iconSize:48,title:"Evaluate Traces",description:"Evaluate traces with LLM judges to understand and improve your AI application's behavior.",href:"/genai/eval-monitor/running-evaluation/traces",linkText:"Evaluate traces \u2192",containerHeight:64})]})]})}function v(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(b,{...e})}):b(e)}},8060(e,n,t){t.d(n,{Ay:()=>d,RM:()=>o});var r=t(74848),i=t(28453),a=t(78010),l=t(57250),s=t(95986);let o=[];function c(e){let n={a:"a",code:"code",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,r.jsx)(s.A,{children:(0,r.jsxs)(a.A,{children:[(0,r.jsxs)(l.A,{value:"local",label:"Local (pip)",default:!0,children:[(0,r.jsxs)(n.p,{children:["If you have a local Python environment >= 3.10, you can start the MLflow server locally using the ",(0,r.jsx)(n.code,{children:"mlflow"})," CLI command."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mlflow server\n"})})]}),(0,r.jsxs)(l.A,{value:"docker",label:"Local (docker)",children:[(0,r.jsx)(n.p,{children:"MLflow also provides a Docker Compose file to start a local MLflow server with a postgres database and a minio server."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"git clone --depth 1 --filter=blob:none --sparse https://github.com/mlflow/mlflow.git\ncd mlflow\ngit sparse-checkout set docker-compose\ncd docker-compose\ncp .env.dev.example .env\ndocker compose up -d\n"})}),(0,r.jsxs)(n.p,{children:["Refer to the ",(0,r.jsx)(n.a,{href:"https://github.com/mlflow/mlflow/tree/master/docker-compose/README.md",children:"instruction"})," for more details, e.g., overriding the default environment variables."]})]})]})})}function d(e={}){let{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},75689(e,n,t){t.d(n,{A:()=>o});var r=t(96540);let i=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,t)=>t?t.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},a=(...e)=>e.filter((e,n,t)=>!!e&&""!==e.trim()&&t.indexOf(e)===n).join(" ").trim();var l={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let s=(0,r.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:t=2,absoluteStrokeWidth:i,className:s="",children:o,iconNode:c,...d},p)=>(0,r.createElement)("svg",{ref:p,...l,width:n,height:n,stroke:e,strokeWidth:i?24*Number(t)/Number(n):t,className:a("lucide",s),...!o&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,n])=>(0,r.createElement)(e,n)),...Array.isArray(o)?o:[o]])),o=(e,n)=>{let t=(0,r.forwardRef)(({className:t,...l},o)=>(0,r.createElement)(s,{ref:o,iconNode:n,className:a(`lucide-${i(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,t),...l}));return t.displayName=i(e),t}},60665(e,n,t){t.d(n,{A:()=>r});let r=(0,t(75689).A)("book-open",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]])},43975(e,n,t){t.d(n,{A:()=>r});let r=(0,t(75689).A)("scale",[["path",{d:"m16 16 3-8 3 8c-.87.65-1.92 1-3 1s-2.13-.35-3-1Z",key:"7g6ntu"}],["path",{d:"m2 16 3-8 3 8c-.87.65-1.92 1-3 1s-2.13-.35-3-1Z",key:"ijws7r"}],["path",{d:"M7 21h10",key:"1b0cd5"}],["path",{d:"M12 3v18",key:"108xh3"}],["path",{d:"M3 7h2c2 0 5-1 7-2 2 1 5 2 7 2h2",key:"3gwbw2"}]])},93893(e,n,t){t.d(n,{A:()=>r});let r=(0,t(75689).A)("users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["path",{d:"M16 3.128a4 4 0 0 1 0 7.744",key:"16gr8j"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}]])},57250(e,n,t){t.d(n,{A:()=>a});var r=t(74848);t(96540);var i=t(34164);function a({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,i.A)("tabItem_Ymn6",t),hidden:n,children:e})}},78010(e,n,t){t.d(n,{A:()=>L});var r=t(74848),i=t(96540),a=t(34164),l=t(88287),s=t(28584),o=t(56347),c=t(99989),d=t(96629),p=t(80618),h=t(41367);function u(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}var g=t(19863);function x({className:e,block:n,selectedValue:t,selectValue:i,tabValues:l}){let o=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.a_)(),d=e=>{let n=e.currentTarget,r=l[o.indexOf(n)].value;r!==t&&(c(n),i(r))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{let t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{let t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1]}}n?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:l.map(({value:e,label:n,attributes:i})=>(0,r.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:p,onClick:d,...i,className:(0,a.A)("tabs__item","tabItem_LNqP",i?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function f({lazy:e,children:n,selectedValue:t}){let l=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=l.find(e=>e.props.value===t);return e?(0,i.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:l.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function y(e){let n=function(e){let n,{defaultValue:t,queryString:r=!1,groupId:a}=e,l=function(e){let{values:n,children:t}=e;return(0,i.useMemo)(()=>{let e=n??u(t).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r})),r=(0,p.XI)(e,(e,n)=>e.value===n.value);if(r.length>0)throw Error(`Docusaurus error: Duplicate values "${r.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,t])}(e),[s,g]=(0,i.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let t=n.find(e=>e.default)??n[0];if(!t)throw Error("Unexpected error: 0 tabValues");return t.value})({defaultValue:t,tabValues:l})),[x,f]=function({queryString:e=!1,groupId:n}){let t=(0,o.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(r),(0,i.useCallback)(e=>{if(!r)return;let n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}({queryString:r,groupId:a}),[y,L]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[t,r]=(0,h.Dv)(n);return[t,(0,i.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:a}),j=m({value:n=x??y,tabValues:l})?n:null;return(0,c.A)(()=>{j&&g(j)},[j]),{selectedValue:s,selectValue:(0,i.useCallback)(e=>{if(!m({value:e,tabValues:l}))throw Error(`Can't select invalid tab value=${e}`);g(e),f(e),L(e)},[f,L,l]),tabValues:l}}(e);return(0,r.jsxs)("div",{className:(0,a.A)(l.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,r.jsx)(x,{...n,...e}),(0,r.jsx)(f,{...n,...e})]})}function L(e){let n=(0,g.A)();return(0,r.jsx)(y,{...e,children:u(e.children)},String(n))}},46077(e,n,t){t.d(n,{A:()=>a});var r=t(74848);t(96540);var i=t(66497);function a({src:e,alt:n,width:t,caption:a,className:l}){return(0,r.jsxs)("div",{className:`container_JwLF ${l||""}`,children:[(0,r.jsx)("div",{className:"imageWrapper_RfGN",style:t?{width:t}:{},children:(0,r.jsx)("img",{src:(0,i.default)(e),alt:n,className:"image_bwOA"})}),a&&(0,r.jsx)("p",{className:"caption_jo2G",children:a})]})}},89001(e,n,t){t.d(n,{A:()=>i});var r=t(74848);t(96540);let i=({number:e,title:n})=>(0,r.jsxs)("div",{className:"stepHeader_RqmM",children:[(0,r.jsx)("div",{className:"stepNumber_exmH",children:e}),(0,r.jsx)("h3",{className:"stepTitle_SzBx",children:n})]})},95986(e,n,t){t.d(n,{A:()=>i});var r=t(74848);t(96540);function i({children:e}){return(0,r.jsx)("div",{className:"wrapper_sf5q",children:e})}},77541(e,n,t){t.d(n,{A:()=>c});var r=t(74848);t(96540);var i=t(95310),a=t(34164);let l="tileImage_O4So";var s=t(66497),o=t(92802);function c({icon:e,image:n,imageDark:t,imageWidth:c,imageHeight:d,iconSize:p=32,containerHeight:h,title:u,description:m,href:g,linkText:x="Learn more \u2192",className:f}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let y=h?{height:`${h}px`}:{},L={};return c&&(L.width=`${c}px`),d&&(L.height=`${d}px`),(0,r.jsxs)(i.A,{href:g,className:(0,a.A)("tileCard_NHsj",f),children:[(0,r.jsx)("div",{className:"tileIcon_pyoR",style:y,children:e?(0,r.jsx)(e,{size:p}):t?(0,r.jsx)(o.A,{sources:{light:(0,s.default)(n),dark:(0,s.default)(t)},alt:u,className:l,style:L}):(0,r.jsx)("img",{src:(0,s.default)(n),alt:u,className:l,style:L})}),(0,r.jsx)("h3",{children:u}),(0,r.jsx)("p",{children:m}),(0,r.jsx)("div",{className:"tileLink_iUbu",children:x})]})}},10440(e,n,t){t.d(n,{A:()=>a});var r=t(74848);t(96540);var i=t(34164);function a({children:e,className:n}){return(0,r.jsx)("div",{className:(0,i.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,t){t.d(n,{R:()=>l,x:()=>s});var r=t(96540);let i={},a=r.createContext(i);function l(e){let n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);