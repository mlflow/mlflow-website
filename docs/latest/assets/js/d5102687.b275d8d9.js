"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4298],{10550:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/trace-decorator-8ae22208121b562582947549f8b9a46e.png"},18186:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/tracing-multi-thread-acfb6f382ce45e030c95b7c0536749bf.png"},21049:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/trace-exception-d23766aa1c06b25b252c98d5d98cfef5.gif"},38829:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/tracing-auto-manual-mix-683000e63e68335e6dbc94a234193b49.png"},67756:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const a=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var o=t(29030),r=t(56289),l=t(74848);const i=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(a[e])return e}return null};function s(e){let{fn:n,children:t}=e;const s=i(n);if(!s)return(0,l.jsx)(l.Fragment,{children:t});const c=(0,o.Ay)(`/${a[s]}#${n}`);return(0,l.jsx)(r.A,{to:c,target:"_blank",children:t??(0,l.jsxs)("code",{children:[n,"()"]})})}},78293:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"tracing/api/manual-instrumentation","title":"Manual Tracing","description":"In addition to the Auto Tracing integrations, you can instrument your Python code using the MLflow Tracing SDK. This is especially useful when you need to instrument your custom Python code.","source":"@site/docs/tracing/api/manual-instrumentation.mdx","sourceDirName":"tracing/api","slug":"/tracing/api/manual-instrumentation","permalink":"/docs/latest/tracing/api/manual-instrumentation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"Manual Tracing"},"sidebar":"docsSidebar","previous":{"title":"APIs","permalink":"/docs/latest/tracing/api/"},"next":{"title":"Query Traces","permalink":"/docs/latest/tracing/api/search"}}');var o=t(74848),r=t(28453),l=t(67756);t(86294),t(61096),t(65537),t(79329);const i={sidebar_position:1,sidebar_label:"Manual Tracing"},s="Manual Tracing",c={},d=[{value:"Decorator",id:"decorator",level:2},{value:"Example",id:"example",level:3},{value:"Customizing Spans",id:"customizing-spans",level:3},{value:"Adding Trace Tags",id:"adding-trace-tags",level:3},{value:"Automatic Exception Handling",id:"automatic-exception-handling",level:3},{value:"Combining with Auto-Tracing",id:"combining-with-auto-tracing",level:3},{value:"Streaming",id:"streaming",level:3},{value:"Function Wrapping",id:"function-wrapping",level:3},{value:"Context Manager",id:"context-manager",level:2},{value:"Multi Threading",id:"multi-threading",level:2},{value:"(Advanced) Low-level Client APIs",id:"advanced-low-level-client-apis",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"manual-tracing",children:"Manual Tracing"})}),"\n",(0,o.jsxs)(n.p,{children:["In addition to the ",(0,o.jsx)(n.a,{href:"/tracing/#automatic-tracing",children:"Auto Tracing"})," integrations, you can instrument your Python code using the MLflow Tracing SDK. This is especially useful when you need to instrument your custom Python code."]}),"\n",(0,o.jsx)(n.h2,{id:"decorator",children:"Decorator"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(l.B,{fn:"mlflow.trace",children:(0,o.jsx)(n.code,{children:"@mlflow.trace"})})," decorator allows you to create a span for any function. This approach provides a simple yet effective way to add tracing to your code with minimal effort:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["MLflow detects the ",(0,o.jsx)(n.strong,{children:"parent-child relationships"})," between functions, making it compatible with auto-tracing integrations."]}),"\n",(0,o.jsxs)(n.li,{children:["Captures ",(0,o.jsx)(n.strong,{children:"exceptions"})," during function execution and records them as span events."]}),"\n",(0,o.jsxs)(n.li,{children:["Automatically logs the function\u2019s ",(0,o.jsx)(n.strong,{children:"name, inputs, outputs, and execution time"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["Can be used alongside ",(0,o.jsx)(n.strong,{children:"auto-tracing"})," features, such as ",(0,o.jsx)(n.code,{children:"mlflow.openai.autolog"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The @mlflow.trace decorator currently support the following types of functions:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Function Type"}),(0,o.jsx)(n.th,{children:"Supported"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Sync"}),(0,o.jsx)(n.td,{children:"\u2705"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Async"}),(0,o.jsx)(n.td,{children:"\u2705  (>= 2.16.0)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Generator"}),(0,o.jsx)(n.td,{children:"\u2705  (>= 2.20.2)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Async Generator"}),(0,o.jsx)(n.td,{children:"\u2705 (>= 2.20.2)"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,o.jsx)(n.p,{children:"The following code is a minimum example of using the decorator for tracing Python functions."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n\n@mlflow.trace(span_type="func", attributes={"key": "value"})\ndef add_1(x):\n    return x + 1\n\n\n@mlflow.trace(span_type="func", attributes={"key1": "value1"})\ndef minus_1(x):\n    return x - 1\n\n\n@mlflow.trace(name="Trace Test")\ndef trace_test(x):\n    step1 = add_1(x)\n    return minus_1(step1)\n\n\ntrace_test(4)\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Tracing Decorator",src:t(10550).A+"",width:"1354",height:"417"})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["When a trace contains multiple spans with same name, MLflow appends an auto-incrementing suffix to them, such as ",(0,o.jsx)(n.code,{children:"_1"}),", ",(0,o.jsx)(n.code,{children:"_2"}),"."]})}),"\n",(0,o.jsx)(n.h3,{id:"customizing-spans",children:"Customizing Spans"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(l.B,{fn:"mlflow.trace",children:(0,o.jsx)(n.code,{children:"@mlflow.trace"})})," decorator accepts following arguments to customize the span to be created:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"name"})," parameter to override the span name from the default (the name of decorated function)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"span_type"})," parameter to set the type of span. Set either one of built-in ",(0,o.jsx)(n.a,{href:"/tracing/tracing-schema#span-types",children:"Span Types"})," or a string."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"attributes"})," parameter to add custom attributes to the span."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'@mlflow.trace(\n    name="call-local-llm", span_type=SpanType.LLM, attributes={"model": "gpt-4o-mini"}\n)\ndef invoke(prompt: str):\n    return client.invoke(\n        messages=[{"role": "user", "content": prompt}], model="gpt-4o-mini"\n    )\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Alternatively, you can update the span dynamically inside the function by using ",(0,o.jsx)(l.B,{fn:"mlflow.get_current_active_span",children:(0,o.jsx)(n.code,{children:"mlflow.get_current_active_span"})})," API."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'@mlflow.trace(span_type=SpanType.LLM)\ndef invoke(prompt: str):\n    model_id = "gpt-4o-mini"\n    # Get the current span (created by the @mlflow.trace decorator)\n    span = mlflow.get_current_active_span()\n    # Set the attribute to the span\n    span.set_attributes({"model": model_id})\n    return client.invoke(messages=[{"role": "user", "content": prompt}], model=model_id)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"adding-trace-tags",children:"Adding Trace Tags"}),"\n",(0,o.jsxs)(n.p,{children:["Tags can be added to traces to provide additional metadata at the trace level. There are a few different ways to set tags on a trace. Please refer to the ",(0,o.jsx)(n.a,{href:"/tracing/api/how-to/#setting-trace-tags",children:"how-to guide"})," for the other methods."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'@mlflow.trace\ndef my_func(x):\n    mlflow.update_current_trace(tags={"fruit": "apple"})\n    return x + 1\n'})}),"\n",(0,o.jsx)(n.h3,{id:"automatic-exception-handling",children:"Automatic Exception Handling"}),"\n",(0,o.jsxs)(n.p,{children:["If an ",(0,o.jsx)(n.code,{children:"Exception"})," is raised during processing of a trace-instrumented operation, an indication will be shown within the UI that the invocation was not\nsuccessful and a partial capture of data will be available to aid in debugging. Additionally, details about the Exception that was raised will be included\nwithin ",(0,o.jsx)(n.code,{children:"Events"})," of the partially completed span, further aiding the identification of where issues are occurring within your code."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Trace Error",src:t(21049).A+"",width:"1192",height:"720"})}),"\n",(0,o.jsx)(n.h3,{id:"combining-with-auto-tracing",children:"Combining with Auto-Tracing"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"@mlflow.trace"})," decorator can be used in conjunction with auto tracing. For example, the following code combines OpenAI auto-tracing with manually defined spans in a single cohesive and integrated trace."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\n\nmlflow.openai.autolog()\n\n\n@mlflow.trace(span_type=SpanType.CHAIN)\ndef run(question):\n    messages = build_messages()\n    # MLflow automatically generates a span for OpenAI invocation\n    response = openai.OpenAI().chat.completions.create(\n        model="gpt-4o-mini",\n        max_tokens=100,\n        messages=messages,\n    )\n    return parse_response(response)\n\n\n@mlflow.trace\ndef build_messages(question):\n    return [\n        {"role": "system", "content": "You are a helpful chatbot."},\n        {"role": "user", "content": question},\n    ]\n\n\n@mlflow.trace\ndef parse_response(response):\n    return response.choices[0].message.content\n\n\nrun("What is MLflow?")\n'})}),"\n",(0,o.jsx)(n.p,{children:"Running this code generates the following single trace:"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:t(38829).A+"",width:"1338",height:"580"})}),"\n",(0,o.jsx)(n.h3,{id:"streaming",children:"Streaming"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"@mlflow.trace"})," decorator can be used to trace functions that return a generator or an iterator, since MLflow 2.20.2."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"@mlflow.trace\ndef stream_data():\n    for i in range(5):\n        yield i\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The above example will generate a trace with a single span for the ",(0,o.jsx)(n.code,{children:"stream_data"})," function. By default, MLflow will capture the all elements yielded by the generator as a list in the span's output. In the example above, the output of the span will be ",(0,o.jsx)(n.code,{children:"[0, 1, 2, 3, 4]"}),"."]}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["A span for a stream function will start when the returned iterator starts to be ",(0,o.jsx)(n.strong,{children:"consumed"}),", and will end when the iterator is exhausted, or an exception is raised during the iteration."]})}),"\n",(0,o.jsxs)(n.p,{children:["If you want to aggregate the elements to be a single span output, you can use the ",(0,o.jsx)(n.code,{children:"output_reducer"})," parameter to specify a custom function to aggregate the elements. The custom function should take a list of yielded elements as inputs."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'@mlflow.trace(output_reducer=lambda x: ",".join(x))\ndef stream_data():\n    for c in "hello":\n        yield c\n'})}),"\n",(0,o.jsxs)(n.p,{children:["In the example above, the output of the span will be ",(0,o.jsx)(n.code,{children:'"h,e,l,l,o"'}),". The raw chunks can still be found in the ",(0,o.jsx)(n.code,{children:"Events"})," tab of the span."]}),"\n",(0,o.jsxs)(n.p,{children:["The following is an advanced example that uses the ",(0,o.jsx)(n.code,{children:"output_reducer"})," to consolidate ChatCompletionChunk output from an OpenAI LLM into a single message object."]}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["Of course, we recommend using the ",(0,o.jsx)(n.a,{href:"../integrations/openai",children:"auto-tracing for OpenAI"})," for examples like this, which does the same job but with one-liner code. The example below is for demonstration purposes."]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport openai\nfrom openai.types.chat import *\nfrom typing import Optional\n\n\ndef aggregate_chunks(outputs: list[ChatCompletionChunk]) -> Optional[ChatCompletion]:\n    """Consolidate ChatCompletionChunks to a single ChatCompletion"""\n    if not outputs:\n        return None\n\n    first_chunk = outputs[0]\n    delta = first_chunk.choices[0].delta\n    message = ChatCompletionMessage(\n        role=delta.role, content=delta.content, tool_calls=delta.tool_calls or []\n    )\n    finish_reason = first_chunk.choices[0].finish_reason\n    for chunk in outputs[1:]:\n        delta = chunk.choices[0].delta\n        message.content += delta.content or ""\n        message.tool_calls += delta.tool_calls or []\n        finish_reason = finish_reason or chunk.choices[0].finish_reason\n\n    base = ChatCompletion(\n        id=first_chunk.id,\n        choices=[Choice(index=0, message=message, finish_reason=finish_reason)],\n        created=first_chunk.created,\n        model=first_chunk.model,\n        object="chat.completion",\n    )\n    return base\n\n\n@mlflow.trace(output_reducer=aggregate_chunks)\ndef predict(messages: list[dict]):\n    stream = openai.OpenAI().chat.completions.create(\n        model="gpt-4o-mini",\n        messages=messages,\n        stream=True,\n    )\n    for chunk in stream:\n        yield chunk\n\n\nfor chunk in predict([{"role": "user", "content": "Hello"}]):\n    print(chunk)\n'})}),"\n",(0,o.jsxs)(n.p,{children:["In the example above, the generated ",(0,o.jsx)(n.code,{children:"predict"})," span will have a single chat completion message as the output, which is aggregated by the custom reducer function."]}),"\n",(0,o.jsx)(n.h3,{id:"function-wrapping",children:"Function Wrapping"}),"\n",(0,o.jsxs)(n.p,{children:["Function wrapping provides a flexible way to add tracing to existing functions without modifying their definitions. This is particularly useful when you want to add tracing to third-party functions or functions defined outside of your control. By wrapping an external function with ",(0,o.jsx)(l.B,{fn:"mlflow.trace",children:(0,o.jsx)(n.code,{children:"@mlflow.trace"})}),", you can capture its inputs, outputs, and execution context."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import math\nimport mlflow\n\n\ndef invocation(x, y, exp=2):\n    # Wrap an external function from the math library\n    traced_pow = mlflow.trace(math.pow)\n    raised = traced_pow(x, exp)\n\n    traced_factorial = mlflow.trace(math.factorial)\n    factorial = traced_factorial(int(raised))\n    return response\n\n\ninvocation(4)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"context-manager",children:"Context Manager"}),"\n",(0,o.jsxs)(n.p,{children:["In addition to the decorator, MLflow allows for creating a span that can then be accessed within any encapsulated arbitrary code block using the ",(0,o.jsx)(l.B,{fn:"mlflow.start_span"})," context manager. It can be useful for capturing complex interactions within your code in finer detail than what is possible by capturing the boundaries of a single function."]}),"\n",(0,o.jsxs)(n.p,{children:["Similarly to the decorator, the context manager automatically captures parent-child relationship, exceptions, execution time, and works with auto-tracing. However, the name, inputs, and outputs of the span must be provided manually. You can set them via the ",(0,o.jsx)(l.B,{fn:"mlflow.entities.Span",children:"Span"})," object that is returned from the context manager."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'with mlflow.start_span(name="my_span") as span:\n    span.set_inputs({"x": 1, "y": 2})\n    z = x + y\n    span.set_outputs(z)\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Below is a slightly more complex example that uses the ",(0,o.jsx)(l.B,{fn:"mlflow.start_span"})," context manager in conjunction with both the decorator and auto-tracing for OpenAI."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import SpanType\n\n\n@mlflow.trace(span_type=SpanType.CHAIN)\ndef start_session():\n    messages = [{"role": "system", "content": "You are a friendly chat bot"}]\n    while True:\n        with mlflow.start_span(name="User") as span:\n            span.set_inputs(messages)\n            user_input = input(">> ")\n            span.set_outputs(user_input)\n\n        if user_input == "BYE":\n            break\n\n        messages.append({"role": "user", "content": user_input})\n\n        response = openai.OpenAI().chat.completions.create(\n            model="gpt-4o-mini",\n            max_tokens=100,\n            messages=messages,\n        )\n        answer = response.choices[0].message.content\n        print(f"\ud83e\udd16: {answer}")\n\n        messages.append({"role": "assistant", "content": answer})\n\n\nmlflow.openai.autolog()\nstart_session()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multi-threading",children:"Multi Threading"}),"\n",(0,o.jsx)(n.p,{children:"MLflow Tracing is thread-safe, traces are isolated by default per thread. But you can also create a trace that spans multiple threads with a few additional steps."}),"\n",(0,o.jsxs)(n.p,{children:["MLflow uses Python's built-in ",(0,o.jsx)(n.a,{href:"https://docs.python.org/3/library/contextvars.html",children:"ContextVar"})," mechanism to ensure thread safety, which is not propagated across threads by default.\nTherefore, you need to manually copy the context from the main thread to the worker thread, as shown in the example below."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import contextvars\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport mlflow\nfrom mlflow.entities import SpanType\nimport openai\n\nclient = openai.OpenAI()\n\n# Enable MLflow Tracing for OpenAI\nmlflow.openai.autolog()\n\n\n@mlflow.trace\ndef worker(question: str) -> str:\n    messages = [\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": question},\n    ]\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=messages,\n        temperature=0.1,\n        max_tokens=100,\n    )\n    return response.choices[0].message.content\n\n\n@mlflow.trace\ndef main(questions: list[str]) -> list[str]:\n    results = []\n    # Almost same as how you would use ThreadPoolExecutor, but two additional steps\n    #  1. Copy the context in the main thread using copy_context()\n    #  2. Use ctx.run() to run the worker in the copied context\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        futures = []\n        for question in questions:\n            ctx = contextvars.copy_context()\n            futures.append(executor.submit(ctx.run, worker, question))\n        for future in as_completed(futures):\n            results.append(future.result())\n    return results\n\n\nquestions = [\n    "What is the capital of France?",\n    "What is the capital of Germany?",\n]\n\nmain(questions)\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:t(18186).A+"",width:"1140",height:"426"})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["In contrast, ",(0,o.jsx)(n.code,{children:"ContextVar"})," is copied to ",(0,o.jsx)(n.strong,{children:"async"})," tasks by default. Therefore, you don't need to manually copy the context when using ",(0,o.jsx)(n.code,{children:"asyncio"}),",\nwhich might be an easier way to handle concurrent I/O-bound tasks in Python with MLflow Tracing."]})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-low-level-client-apis",children:"(Advanced) Low-level Client APIs"}),"\n",(0,o.jsxs)(n.p,{children:["When the decorator or context manager does not meet your requirements, you can use the low-level client APIs. For example, you might need to start and end a span from different functions. The client API is designed as a thin wrapper over the MLflow REST APIs, giving you more control over the trace lifecycle. For more details, please refer to the ",(0,o.jsx)(n.a,{href:"/tracing/api/client",children:"MLflow Tracing Client APIs"})," guide."]}),"\n",(0,o.jsxs)(n.admonition,{type:"warning",children:[(0,o.jsx)(n.p,{children:"When using client APIs, please be aware of the following limitations:"}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The parent-child relationship is NOT captured automatically. You need to manually pass the ID of the parent span."}),"\n",(0,o.jsx)(n.li,{children:"Spans created using the client API do NOT combine with auto-tracing spans."}),"\n",(0,o.jsx)(n.li,{children:"Low-level APIs marked as experimental are subject to change based on backend implementation updates."}),"\n"]})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},86294:(e,n,t)=>{t.d(n,{Zp:()=>s,AC:()=>i,WO:()=>d,tf:()=>h,_C:()=>c,$3:()=>p,jK:()=>m});var a=t(34164);const o={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var r=t(56289),l=t(74848);const i=e=>{let{children:n,isSmall:t,cols:r}=e;return(0,l.jsx)("div",{className:(0,a.A)(o.CardGroup,t?o.AutofillColumns:r?o[`Cols${r}`]:o.MaxThreeColumns),children:n})},s=e=>{let{children:n,link:t=""}=e;return t?(0,l.jsx)(r.A,{className:(0,a.A)(o.Link,o.Card,o.CardBordered),to:t,children:n}):(0,l.jsx)("div",{className:(0,a.A)(o.Card,o.CardBordered),children:n})},c=e=>{let{headerText:n,link:t,text:r}=e;return(0,l.jsx)(s,{link:t,children:(0,l.jsxs)("span",{children:[(0,l.jsx)("div",{className:(0,a.A)(o.CardTitle,o.BoxRoot,o.PaddingBottom4),style:{pointerEvents:"none"},children:(0,l.jsx)("div",{className:(0,a.A)(o.BoxRoot,o.FlexFlex,o.FlexAlignItemsCenter,o.FlexDirectionRow,o.FlexJustifyContentFlexStart,o.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,l.jsx)("div",{className:(0,a.A)(o.BoxRoot,o.BoxHideIfEmpty,o.MarginTop4,o.MarginLeft4),style:{pointerEvents:"auto"},children:(0,l.jsx)("span",{className:"",children:n})})})}),(0,l.jsx)("span",{className:(0,a.A)(o.TextColor,o.CardBody),children:(0,l.jsx)("p",{children:r})})]})})},d=e=>{let{description:n,children:t,link:a}=e;return(0,l.jsx)(s,{link:a,children:(0,l.jsxs)("div",{className:o.LogoCardContent,children:[(0,l.jsx)("div",{className:o.LogoCardImage,children:t}),(0,l.jsx)("p",{className:o.TextColor,children:n})]})})},p=e=>{let{children:n,link:t}=e;return(0,l.jsx)(s,{link:t,children:(0,l.jsx)("div",{className:o.SmallLogoCardContent,children:(0,l.jsx)("div",{className:(0,a.A)("max-height-img-container",o.SmallLogoCardImage),children:n})})})},h=e=>{let{children:n,description:t,name:a,releaseVersion:i,learnMoreLink:c=""}=e;return(0,l.jsx)(s,{children:(0,l.jsxs)("div",{className:o.NewFeatureCardWrapper,children:[(0,l.jsxs)("div",{className:o.NewFeatureCardContent,children:[(0,l.jsxs)("div",{className:o.NewFeatureCardHeading,children:[a,(0,l.jsx)("br",{}),(0,l.jsx)("hr",{className:o.NewFeatureCardHeadingSeparator})]}),(0,l.jsx)("div",{className:o.LogoCardImage,children:n}),(0,l.jsx)("br",{}),(0,l.jsx)("p",{children:t}),(0,l.jsx)("br",{})]}),(0,l.jsxs)("div",{className:o.NewFeatureCardTags,children:[(0,l.jsx)("div",{children:c&&(0,l.jsx)(r.A,{className:"button button--outline button--sm button--primary",to:c,children:"Learn more"})}),(0,l.jsxs)(r.A,{className:"button button--outline button--sm button--primary",to:`https://github.com/mlflow/mlflow/releases/tag/v${i}`,children:["released in ",i]})]})]})})},m=e=>{let{title:n,description:t,link:r=""}=e;return(0,l.jsx)(s,{link:r,children:(0,l.jsxs)("div",{className:o.TitleCardContent,children:[(0,l.jsx)("div",{className:(0,a.A)(o.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:n}),(0,l.jsx)("hr",{className:(0,a.A)(o.TitleCardSeparator),style:{margin:"12px 0"}}),(0,l.jsx)("p",{className:(0,a.A)(o.TextColor),children:t})]})})}}}]);