"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9162],{15210:(e,n,l)=>{l.d(n,{A:()=>o});const o=l.p+"assets/images/llama-index-gateway-936f2cc158a2ca4809ac2d8134fd6904.png"},27645:(e,n,l)=>{l.d(n,{A:()=>o});const o=l.p+"assets/images/llama-index-artifacts-7ee58594f8de2b683d6c9138019a11f7.png"},28453:(e,n,l)=>{l.d(n,{R:()=>i,x:()=>r});var o=l(96540);const t={},a=o.createContext(t);function i(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),o.createElement(a.Provider,{value:n},e.children)}},67476:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>h});const o=JSON.parse('{"id":"llms/llama-index/index","title":"MLflow LlamaIndex Flavor","description":"The llama_index flavor is under active development and is marked as Experimental. Public APIs are","source":"@site/docs/llms/llama-index/index.mdx","sourceDirName":"llms/llama-index","slug":"/llms/llama-index/","permalink":"/docs/latest/llms/llama-index/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"DSPy Optimizer Autologging","permalink":"/docs/latest/llms/dspy/optimizer"},"next":{"title":"Overview","permalink":"/docs/latest/llms/transformers/"}}');var t=l(74848),a=l(28453),i=l(67756),r=l(86294);const s={},d="MLflow LlamaIndex Flavor",c={},h=[{value:"Introduction",id:"introduction",level:2},{value:"Why use LlamaIndex with MLflow?",id:"why-use-llamaindex-with-mlflow",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Concepts",id:"concepts",level:2},{value:"<code>Workflow</code> \ud83c\udd95",id:"workflow-",level:3},{value:"<code>Index</code>",id:"index",level:3},{value:"<code>Engine</code>",id:"engine",level:3},{value:"<code>Settings</code>",id:"settings",level:3},{value:"Usage",id:"usage",level:2},{value:"Saving and Loading Index in MLflow Experiment",id:"saving-and-loading-index-in-mlflow-experiment",level:3},{value:"Creating an Index",id:"creating-an-index",level:4},{value:"Logging the Index to MLflow",id:"logging-the-index-to-mlflow",level:4},{value:"Loading the Index Back for inference",id:"loading-the-index-back-for-inference",level:4},{value:"Enable Tracing",id:"enable-tracing",level:3},{value:"FAQ",id:"faq",level:2},{value:"How to log and load an index with external vector stores?",id:"how-to-log-and-load-an-index-with-external-vector-stores",level:3},{value:"How to log and load a LlamaIndex Workflow?",id:"how-to-log-and-load-a-llamaindex-workflow",level:3},{value:"I have an index logged with <code>query</code> engine type. Can I load it back a <code>chat</code> engine?",id:"i-have-an-index-logged-with-query-engine-type-can-i-load-it-back-a-chat-engine",level:3},{value:"How to use different LLMs for inference with the loaded engine?",id:"how-to-use-different-llms-for-inference-with-the-loaded-engine",level:3}];function m(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"mlflow-llamaindex-flavor",children:"MLflow LlamaIndex Flavor"})}),"\n",(0,t.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"llama_index"})," flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change and new features may be added as the flavor evolves."]})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LlamaIndex"})," \ud83e\udd99 is a powerful data-centric framework designed to seamlessly connect custom data sources to large language models (LLMs).\nIt offers a comprehensive suite of data structures and tools that simplify the process of ingesting, structuring, and\naccessing private or domain-specific data for use with LLMs. LlamaIndex excels in enabling context-aware AI applications\nby providing efficient indexing and retrieval mechanisms, making it easier to build advanced QA systems, chatbots,\nand other AI-driven applications that require integration of external knowledge."]}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"70%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Overview of LlamaIndex and MLflow integration",src:l(15210).A+"",width:"2208",height:"1595"})})}),"\n",(0,t.jsx)(n.h2,{id:"why-use-llamaindex-with-mlflow",children:"Why use LlamaIndex with MLflow?"}),"\n",(0,t.jsx)(n.p,{children:"The integration of the LlamaIndex library with MLflow provides a seamless experience for managing and deploying LlamaIndex engines. The following are some of the key benefits of using LlamaIndex with MLflow:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/tracking",children:"MLflow Tracking"})," allows you to track your indices within MLflow and manage the many moving parts that comprise your LlamaIndex project, such as prompts, LLMs, workflows, tools, global configurations, and more."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/model",children:"MLflow Model"})," packages your LlamaIndex index/engine/workflows with all its dependency versions, input and output interfaces, and other essential metadata. This allows you to deploy your LlamaIndex models for inference with ease, knowing that the environment is consistent across different stages of the ML lifecycle."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/llms/llm-evaluate",children:"MLflow Evaluate"})," provides native capabilities within MLflow to evaluate GenAI applications. This capability facilitates the efficient assessment of inference results from your LlamaIndex models, ensuring robust performance analytics and facilitating quick iterations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/tracing",children:"MLflow Tracing"})," is a powerful observability tool for monitoring and debugging what happens inside the LlamaIndex models, helping you identify potential bottlenecks or issues quickly. With its powerful automatic logging capability, you can instrument your LlamaIndex application without needing to add any code apart from running a single command."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsx)(n.p,{children:"In these introductory tutorials, you will learn the most fundamental components of LlamaIndex and how to leverage the integration with MLflow to bring better maintainability and observability to your LlamaIndex applications."}),"\n",(0,t.jsxs)(r.AC,{children:[(0,t.jsx)(r._C,{headerText:"LlamaIndex Workflows with MLflow",link:"/llms/llama-index/notebooks/llama_index_workflow_tutorial/",text:"Get started with MLflow and LLamaIndex by building a simple agentic Workflow. Learn how to log and load the Workflow for inference, as well as enable tracing for observability."}),(0,t.jsx)(r._C,{headerText:"Building Index with MLflow",link:"/llms/llama-index/notebooks/llama_index_quickstart/",text:"Get started with MLflow and LlamaIndex by exploring the simplest possible index configuration of a VectorStoreIndex."})]}),"\n",(0,t.jsx)(n.h2,{id:"concepts",children:"Concepts"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"Workflow integration is only available in LlamaIndex >= 0.11.0 and MLflow >= 2.17.0."})}),"\n",(0,t.jsxs)(n.h3,{id:"workflow-",children:[(0,t.jsx)(n.code,{children:"Workflow"})," \ud83c\udd95"]}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"Workflow"})," is LlamaIndex's event-driven orchestration framework. It is designed\nas a flexible and interpretable framework for building arbitrary LLM applications such as an agent, a RAG flow, a data extraction pipeline, etc.\nMLflow supports tracking, evaluating, and tracing the ",(0,t.jsx)(n.code,{children:"Workflow"})," objects, which makes them more observable and maintainable."]}),"\n",(0,t.jsx)(n.h3,{id:"index",children:(0,t.jsx)(n.code,{children:"Index"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"Index"})," object is a collection of documents that are indexed for fast information retrieval, providing capabilities for applications such as Retrieval-Augmented Generation (RAG) and Agents. The ",(0,t.jsx)(n.code,{children:"Index"})," object can be logged directly to an MLflow run and loaded back for use as an inference engine."]}),"\n",(0,t.jsx)(n.h3,{id:"engine",children:(0,t.jsx)(n.code,{children:"Engine"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"Engine"})," is a generic interface built on top of the ",(0,t.jsx)(n.code,{children:"Index"})," object, which provides a set of APIs to interact with the index. LlamaIndex provides two types of engines: ",(0,t.jsx)(n.code,{children:"QueryEngine"})," and ",(0,t.jsx)(n.code,{children:"ChatEngine"}),". The ",(0,t.jsx)(n.code,{children:"QueryEngine"})," simply takes a single\nquery and returns a response based on the index. The ",(0,t.jsx)(n.code,{children:"ChatEngine"})," is designed for conversational agents, which keeps track of the conversation history as well."]}),"\n",(0,t.jsx)(n.h3,{id:"settings",children:(0,t.jsx)(n.code,{children:"Settings"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"Settings"})," object is a global service context that bundles commonly used resources throughout the\nLlamaIndex application. It includes settings such as the LLM model, embedding model, callbacks, and more. When logging a LlamaIndex index/engine/workflow, MLflow tracks\nthe state of the ",(0,t.jsx)(n.code,{children:"Settings"})," object so that you can easily reproduce the same result when loading the model back for inference (note that some objects like API keys, non-serializable objects, etc., are not tracked)."]}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,t.jsx)(n.h3,{id:"saving-and-loading-index-in-mlflow-experiment",children:"Saving and Loading Index in MLflow Experiment"}),"\n",(0,t.jsx)(n.h4,{id:"creating-an-index",children:"Creating an Index"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"index"})," object is the centerpiece of the LlamaIndex and MLflow integration. With LlamaIndex, you can create an index from a collection of documents or external vector stores. The following code creates a sample index from Paul Graham's essay data available within the LlamaIndex repository."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"mkdir -p data\ncurl -L https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -o ./data/paul_graham_essay.txt\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader("data").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"logging-the-index-to-mlflow",children:"Logging the Index to MLflow"}),"\n",(0,t.jsxs)(n.p,{children:["You can log the ",(0,t.jsx)(n.code,{children:"index"})," object to the MLflow experiment using the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.log_model"})," function."]}),"\n",(0,t.jsxs)(n.p,{children:["One key step here is to specify the ",(0,t.jsx)(n.code,{children:"engine_type"})," parameter. The choice of engine type does not affect the index itself,\nbut dictates the interface of how you query the index when you load it back for inference."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["QueryEngine (",(0,t.jsx)(n.code,{children:'engine_type="query"'}),") is designed for a simple query-response system that takes a single query string and returns a response."]}),"\n",(0,t.jsxs)(n.li,{children:["ChatEngine (",(0,t.jsx)(n.code,{children:'engine_type="chat"'}),") is designed for a conversational agent that keeps track of the conversation history and responds to a user message."]}),"\n",(0,t.jsxs)(n.li,{children:["Retriever (",(0,t.jsx)(n.code,{children:'engine_type="retriever"'}),") is a lower-level component that returns the top-k relevant documents matching the query."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The following code is an example of logging an index to MLflow with the ",(0,t.jsx)(n.code,{children:"chat"})," engine type."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.set_experiment("llama-index-demo")\n\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index,\n        name="index",\n        engine_type="chat",\n        input_example="What did the author do growing up?",\n    )\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["The above code snippet passes the index object directly to the ",(0,t.jsx)(n.code,{children:"log_model"})," function.\nThis method only works with the default ",(0,t.jsx)(n.code,{children:"SimpleVectorStore"})," vector store, which\nsimply keeps the embedded documents in memory. If your index uses ",(0,t.jsx)(n.strong,{children:"external vector stores"})," such as ",(0,t.jsx)(n.code,{children:"QdrantVectorStore"})," or ",(0,t.jsx)(n.code,{children:"DatabricksVectorSearch"}),", you can use the Model-from-Code\nlogging method. See the ",(0,t.jsx)(n.a,{href:"#how-to-log-and-load-an-index-with-external-vector-stores",children:"How to log an index with external vector stores"})," for more details."]})}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLflow artifacts for the LlamaIndex index",src:l(27645).A+"",width:"2205",height:"1084"})})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Under the hood, MLflow calls ",(0,t.jsx)(n.code,{children:"as_query_engine()"})," / ",(0,t.jsx)(n.code,{children:"as_chat_engine()"})," / ",(0,t.jsx)(n.code,{children:"as_retriever()"})," method on the index object to convert it to the respective engine instance."]})}),"\n",(0,t.jsx)(n.h4,{id:"loading-the-index-back-for-inference",children:"Loading the Index Back for inference"}),"\n",(0,t.jsxs)(n.p,{children:["The saved index can be loaded back for inference using the ",(0,t.jsx)(i.B,{fn:"mlflow.pyfunc.load_model"})," function. This function\ngives an MLflow Python Model backed by the LlamaIndex engine, with the engine type specified during logging."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n\nresponse = model.predict("What was the first program the author wrote?")\nprint(response)\n# >> The first program the author wrote was on the IBM 1401 ...\n\n# The chat engine keeps track of the conversation history\nresponse = model.predict("How did the author feel about it?")\nprint(response)\n# >> The author felt puzzled by the first program ...\n'})}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsxs)(n.p,{children:["To load the index itself back instead of the engine, use the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.load_model"})," function."]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'index = mlflow.llama_index.load_model("runs:/<run_id>/index")\n'})})]}),"\n",(0,t.jsx)(n.h3,{id:"enable-tracing",children:"Enable Tracing"}),"\n",(0,t.jsxs)(n.p,{children:["You can enable tracing for your LlamaIndex code by calling the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.autolog"})," function. MLflow automatically logs the input and output of the LlamaIndex execution to the active MLflow experiment, providing you with a detailed view of the model's behavior."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nmlflow.llama_index.autolog()\n\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat("What was the first program the author wrote?")\n'})}),"\n",(0,t.jsx)(n.p,{children:'Then you can navigate to the MLflow UI, select the experiment, and open the "Traces" tab to find the logged trace for the prediction made by the engine. It is impressive to see how the chat engine coordinates and executes a number of tasks to answer your question!'}),"\n",(0,t.jsx)("div",{className:"center-div",style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Trace view in MLflow UI",src:l(90427).A+"",width:"1608",height:"851"})})}),"\n",(0,t.jsxs)(n.p,{children:["You can disable tracing by running the same function with the ",(0,t.jsx)(n.code,{children:"disable"})," parameter set to ",(0,t.jsx)(n.code,{children:"True"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"mlflow.llama_index.autolog(disable=True)\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["The tracing supports async prediction and streaming response, however, it does not\nsupport the combination of async and streaming, such as the ",(0,t.jsx)(n.code,{children:"astream_chat"})," method."]})}),"\n",(0,t.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,t.jsx)(n.h3,{id:"how-to-log-and-load-an-index-with-external-vector-stores",children:"How to log and load an index with external vector stores?"}),"\n",(0,t.jsxs)(n.p,{children:["If your index uses the default ",(0,t.jsx)(n.code,{children:"SimpleVectorStore"}),", you can log the index directly to MLflow using the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.log_model"})," function. MLflow persists the in-memory index data (embedded documents) to MLflow artifact store, which allows loading the index back with the same data without re-indexing the documents."]}),"\n",(0,t.jsxs)(n.p,{children:["However, when the index uses external vector stores like ",(0,t.jsx)(n.code,{children:"DatabricksVectorSearch"})," and ",(0,t.jsx)(n.code,{children:"QdrantVectorStore"}),", the index data is stored remotely and they do not support local serialization. Thereby, you cannot log the index with these stores directly. For such cases, you can use the ",(0,t.jsx)(n.a,{href:"/model#models-from-code",children:"Model-from-Code"})," logging that provides more control over the index saving process and allow you to use the external vector store."]}),"\n",(0,t.jsxs)(n.p,{children:["To use model-from-code logging, you first need to create a separate Python file that defines the index. If you are on Jupyter notebook, you can use the ",(0,t.jsx)(n.code,{children:"%%writefile"})," magic command to save the cell code to a Python file."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# %%writefile index.py\n\n# Create Qdrant client with your own settings.\nclient = qdrant_client.QdrantClient(\n    host="localhost",\n    port=6333,\n)\n\n# Here we simply load vector store from the existing collection to avoid\n# re-indexing documents, because this Python file is executed every time\n# when the model is loaded. If you don\'t have an existing collection, create\n# a new one by following the official tutorial:\n# https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/\nvector_store = QdrantVectorStore(client=client, collection_name="my_collection")\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n# IMPORTANT: call set_model() method to tell MLflow to log this index\nmlflow.models.set_model(index)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Then you can log the index by passing the Python file path to the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.log_model"})," function. The global ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings",children:"Settings"})," object is saved normally as part of the model."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        "index.py",\n        name="index",\n        engine_type="query",\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The logged index can be loaded back using the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.load_model"})," or ",(0,t.jsx)(i.B,{fn:"mlflow.pyfunc.load_model"})," function, in the same way as with the local index."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'index = mlflow.llama_index.load_model(model_info.model_uri)\nindex.as_query_engine().query("What is MLflow?")\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["The object that is passed to the ",(0,t.jsx)(n.code,{children:"set_model()"})," method must be a LlamaIndex index that is compatible with the engine type specified during logging. More\nobjects support will be added in the future releases."]})}),"\n",(0,t.jsx)(n.h3,{id:"how-to-log-and-load-a-llamaindex-workflow",children:"How to log and load a LlamaIndex Workflow?"}),"\n",(0,t.jsxs)(n.p,{children:["Mlflow supports logging and loading a LlamaIndex Workflow via the ",(0,t.jsx)(n.a,{href:"/model#models-from-code",children:"Model-from-Code"})," feature. For a detailed example of logging and loading a LlamaIndex Workflow, see the ",(0,t.jsx)(n.a,{href:"/llms/llama-index/notebooks/llama_index_workflow_tutorial/",children:"LlamaIndex Workflows with MLflow"})," notebook."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        "/path/to/workflow.py",\n        name="model",\n        input_example={"input": "What is MLflow?"},\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The logged workflow can be loaded back using the ",(0,t.jsx)(i.B,{fn:"mlflow.llama_index.load_model"})," or ",(0,t.jsx)(i.B,{fn:"mlflow.pyfunc.load_model"})," function."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Use mlflow.llama_index.load_model to load the workflow object as is\nworkflow = mlflow.llama_index.load_model(model_info.model_uri)\nawait workflow.run(input="What is MLflow?")\n\n# Use mlflow.pyfunc.load_model to load the workflow as a MLflow Pyfunc Model\n# with standard inference APIs for deployment and evaluation.\npyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\npyfunc.predict({"input": "What is MLflow?"})\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["The MLflow PyFunc Model does not support async inference. When you load the workflow with ",(0,t.jsx)(i.B,{fn:"mlflow.pyfunc.load_model"}),", the ",(0,t.jsx)(n.code,{children:"predict"})," method becomes ",(0,t.jsx)(n.strong,{children:"synchronous"})," and will block until the workflow execution is completed.\nThis also applies when deploying the logged LlamaIndex workflow to a production endpoint using ",(0,t.jsx)(n.a,{href:"/deployment",children:"MLflow Deployment"})," or Databricks ",(0,t.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/index.html",children:"Model Serving"}),"."]})}),"\n",(0,t.jsxs)(n.h3,{id:"i-have-an-index-logged-with-query-engine-type-can-i-load-it-back-a-chat-engine",children:["I have an index logged with ",(0,t.jsx)(n.code,{children:"query"})," engine type. Can I load it back a ",(0,t.jsx)(n.code,{children:"chat"})," engine?"]}),"\n",(0,t.jsxs)(n.p,{children:["While it is not possible to update the engine type of the logged model in-place,\nyou can always load the index back and re-log it with the desired engine type. This process\ndoes ",(0,t.jsx)(n.strong,{children:"not require re-creating the index"}),", so it is an efficient way to switch between\ndifferent engine types."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Log the index with the query engine type first\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index,\n        name="index-query",\n        engine_type="query",\n    )\n\n# Load the index back and re-log it with the chat engine type\nindex = mlflow.llama_index.load_model(model_info.model_uri)\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index,\n        name="index-chat",\n        # Specify the chat engine type this time\n        engine_type="chat",\n    )\n'})}),"\n",(0,t.jsx)(n.p,{children:"Alternatively, you can leverage their standard inference APIs on the loaded LlamaIndex native index object, specifically:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'index.as_chat_engine().chat("hi")'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'index.as_query_engine().query("hi")'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'index.as_retriever().retrieve("hi")'})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-to-use-different-llms-for-inference-with-the-loaded-engine",children:"How to use different LLMs for inference with the loaded engine?"}),"\n",(0,t.jsxs)(n.p,{children:["When saving the index to MLflow, it persists the global ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/",children:"Settings"})," object as a part of the model.\nThis object contains settings such as LLM and embedding models to be used by engines."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\n\nSettings.llm = OpenAI("gpt-4o-mini")\n\n# MLflow saves GPT-4o-Mini as the LLM to use for inference\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(index, name="index", engine_type="chat")\n'})}),"\n",(0,t.jsx)(n.p,{children:"Then later when you load the index back, the persisted settings are also applied globally. This means that the loaded engine will use the same LLM as when it was logged."}),"\n",(0,t.jsxs)(n.p,{children:["However, sometimes you may want to use a different LLM for inference. In such cases, you can update the global ",(0,t.jsx)(n.code,{children:"Settings"})," object directly after loading the index."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Load the index back\nloaded_index = mlflow.llama_index.load_model(model_info.model_uri)\n\nassert Settings.llm.model == "gpt-4o-mini"\n\n\n# Update the settings to use GPT-4 instead\nSettings.llm = OpenAI("gpt-4")\nquery_engine = loaded_index.as_query_engine()\nresponse = query_engine.query("What is the capital of France?")\n'})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},67756:(e,n,l)=>{l.d(n,{B:()=>s});l(96540);const o=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var t=l(29030),a=l(56289),i=l(74848);const r=e=>{const n=e.split(".");for(let l=n.length;l>0;l--){const e=n.slice(0,l).join(".");if(o[e])return e}return null};function s(e){let{fn:n,children:l}=e;const s=r(n);if(!s)return(0,i.jsx)(i.Fragment,{children:l});const d=(0,t.Ay)(`/${o[s]}#${n}`);return(0,i.jsx)(a.A,{to:d,target:"_blank",children:l??(0,i.jsxs)("code",{children:[n,"()"]})})}},86294:(e,n,l)=>{l.d(n,{Zp:()=>s,AC:()=>r,WO:()=>c,tf:()=>m,_C:()=>d,$3:()=>h,jK:()=>p});var o=l(34164);const t={CardGroup:"CardGroup_P84T",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var a=l(56289),i=l(74848);const r=e=>{let{children:n,isSmall:l,cols:a}=e;return(0,i.jsx)("div",{className:(0,o.A)(t.CardGroup,l?t.AutofillColumns:a?t[`Cols${a}`]:t.MaxThreeColumns),children:n})},s=e=>{let{children:n,link:l=""}=e;return l?(0,i.jsx)(a.A,{className:(0,o.A)(t.Link,t.Card,t.CardBordered),to:l,children:n}):(0,i.jsx)("div",{className:(0,o.A)(t.Card,t.CardBordered),children:n})},d=e=>{let{headerText:n,link:l,text:a}=e;return(0,i.jsx)(s,{link:l,children:(0,i.jsxs)("span",{children:[(0,i.jsx)("div",{className:(0,o.A)(t.CardTitle,t.BoxRoot,t.PaddingBottom4),style:{pointerEvents:"none"},children:(0,i.jsx)("div",{className:(0,o.A)(t.BoxRoot,t.FlexFlex,t.FlexAlignItemsCenter,t.FlexDirectionRow,t.FlexJustifyContentFlexStart,t.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,i.jsx)("div",{className:(0,o.A)(t.BoxRoot,t.BoxHideIfEmpty,t.MarginTop4,t.MarginLeft4),style:{pointerEvents:"auto"},children:(0,i.jsx)("span",{className:"",children:n})})})}),(0,i.jsx)("span",{className:(0,o.A)(t.TextColor,t.CardBody),children:(0,i.jsx)("p",{children:a})})]})})},c=e=>{let{description:n,children:l,link:o}=e;return(0,i.jsx)(s,{link:o,children:(0,i.jsxs)("div",{className:t.LogoCardContent,children:[(0,i.jsx)("div",{className:t.LogoCardImage,children:l}),(0,i.jsx)("p",{className:t.TextColor,children:n})]})})},h=e=>{let{children:n,link:l}=e;return(0,i.jsx)(s,{link:l,children:(0,i.jsx)("div",{className:t.SmallLogoCardContent,children:(0,i.jsx)("div",{className:(0,o.A)("max-height-img-container",t.SmallLogoCardImage),children:n})})})},m=e=>{let{children:n,description:l,name:o,releaseVersion:r,learnMoreLink:d=""}=e;return(0,i.jsx)(s,{children:(0,i.jsxs)("div",{className:t.NewFeatureCardWrapper,children:[(0,i.jsxs)("div",{className:t.NewFeatureCardContent,children:[(0,i.jsxs)("div",{className:t.NewFeatureCardHeading,children:[o,(0,i.jsx)("br",{}),(0,i.jsx)("hr",{className:t.NewFeatureCardHeadingSeparator})]}),(0,i.jsx)("div",{className:t.LogoCardImage,children:n}),(0,i.jsx)("br",{}),(0,i.jsx)("p",{children:l}),(0,i.jsx)("br",{})]}),(0,i.jsxs)("div",{className:t.NewFeatureCardTags,children:[(0,i.jsx)("div",{children:d&&(0,i.jsx)(a.A,{className:"button button--outline button--sm button--primary",to:d,children:"Learn more"})}),(0,i.jsxs)(a.A,{className:"button button--outline button--sm button--primary",to:`https://github.com/mlflow/mlflow/releases/tag/v${r}`,children:["released in ",r]})]})]})})},p=e=>{let{title:n,description:l,link:a=""}=e;return(0,i.jsx)(s,{link:a,children:(0,i.jsxs)("div",{className:t.TitleCardContent,children:[(0,i.jsx)("div",{className:(0,o.A)(t.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:n}),(0,i.jsx)("hr",{className:(0,o.A)(t.TitleCardSeparator),style:{margin:"12px 0"}}),(0,i.jsx)("p",{className:(0,o.A)(t.TextColor),children:l})]})})}},90427:(e,n,l)=>{l.d(n,{A:()=>o});const o=l.p+"assets/images/llama-index-trace-91095f2d69e2e6b66a23da37706be082.png"}}]);