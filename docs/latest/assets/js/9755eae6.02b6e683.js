"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[5490],{10493:(e,n,a)=>{a.d(n,{Zp:()=>s,AC:()=>o,WO:()=>h,_C:()=>c,$3:()=>p,jK:()=>d});var t=a(34164);const l={CardGroup:"CardGroup_P84T",NoGap:"NoGap_O9Dj",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardRounded:"SmallLogoCardRounded_X50_",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardHeader:"TitleCardHeader_fUQy",TitleCardHeaderRight:"TitleCardHeaderRight_iBLX",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var i=a(28774),r=a(74848);const o=({children:e,isSmall:n,cols:a,noGap:i})=>(0,r.jsx)("div",{className:(0,t.A)(l.CardGroup,n?l.AutofillColumns:a?l[`Cols${a}`]:l.MaxThreeColumns,i&&l.NoGap),children:e}),s=({children:e,link:n="",style:a})=>n?(0,r.jsx)(i.A,{className:(0,t.A)(l.Link,l.Card,l.CardBordered),style:a,to:n,children:e}):(0,r.jsx)("div",{className:(0,t.A)(l.Card,l.CardBordered),style:a,children:e}),c=({headerText:e,link:n,text:a})=>(0,r.jsx)(s,{link:n,children:(0,r.jsxs)("span",{children:[(0,r.jsx)("div",{className:(0,t.A)(l.CardTitle,l.BoxRoot,l.PaddingBottom4),style:{pointerEvents:"none"},children:(0,r.jsx)("div",{className:(0,t.A)(l.BoxRoot,l.FlexFlex,l.FlexAlignItemsCenter,l.FlexDirectionRow,l.FlexJustifyContentFlexStart,l.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,r.jsx)("div",{className:(0,t.A)(l.BoxRoot,l.BoxHideIfEmpty,l.MarginTop4,l.MarginLeft4),style:{pointerEvents:"auto"},children:(0,r.jsx)("span",{className:"",children:e})})})}),(0,r.jsx)("span",{className:(0,t.A)(l.TextColor,l.CardBody),children:(0,r.jsx)("p",{children:a})})]})}),h=({description:e,children:n,link:a})=>(0,r.jsx)(s,{link:a,children:(0,r.jsxs)("div",{className:l.LogoCardContent,children:[(0,r.jsx)("div",{className:l.LogoCardImage,children:n}),(0,r.jsx)("p",{className:l.TextColor,children:e})]})}),p=({children:e,link:n})=>(0,r.jsx)(i.A,{className:(0,t.A)(l.Card,l.CardBordered,l.SmallLogoCardRounded),to:n,children:(0,r.jsx)("div",{className:l.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,t.A)("max-height-img-container",l.SmallLogoCardImage),children:e})})}),d=({title:e,description:n,link:a="",headerRight:i,children:o})=>(0,r.jsx)(s,{link:a,children:(0,r.jsxs)("div",{className:l.TitleCardContent,children:[(0,r.jsxs)("div",{className:(0,t.A)(l.TitleCardHeader),children:[(0,r.jsx)("div",{className:(0,t.A)(l.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,r.jsx)("div",{className:l.TitleCardHeaderRight,children:i})]}),(0,r.jsx)("hr",{className:(0,t.A)(l.TitleCardSeparator),style:{margin:"12px 0"}}),o?(0,r.jsx)("div",{className:(0,t.A)(l.TextColor),children:o}):(0,r.jsx)("p",{className:(0,t.A)(l.TextColor),dangerouslySetInnerHTML:{__html:n}})]})})},20484:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>p,default:()=>g,frontMatter:()=>h,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"tracing/integrations/listing/langchain","title":"Tracing LangChain\ud83e\udd9c\u26d3\ufe0f","description":"LangChain Tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/langchain.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/langchain","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/langchain","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"LangChain","toc_max_heading_level":2},"sidebar":"genAISidebar","previous":{"title":"Haystack","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/haystack"},"next":{"title":"LangGraph","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/langgraph"}}');var l=a(74848),i=a(28453),r=a(49374),o=(a(10493),a(14252),a(11470)),s=a(19365),c=a(47020);const h={sidebar_position:2,sidebar_label:"LangChain",toc_max_heading_level:2},p="Tracing LangChain\ud83e\udd9c\u26d3\ufe0f",d={},m=[{value:"Getting Started",id:"getting-started",level:2},{value:"1. Start MLflow",id:"1-start-mlflow",level:3},{value:"2. Install dependencies",id:"2-install-dependencies",level:3},{value:"3. Enable tracing",id:"3-enable-tracing",level:3},{value:"4. Define the chain and invoke it",id:"4-define-the-chain-and-invoke-it",level:3},{value:"5. View the trace in the MLflow UI",id:"5-view-the-trace-in-the-mlflow-ui",level:3},{value:"1. Start MLflow",id:"1-start-mlflow-1",level:3},{value:"2. Install the required dependencies:",id:"2-install-the-required-dependencies",level:3},{value:"3. Enable OpenTelemetry",id:"3-enable-opentelemetry",level:3},{value:"4. Define the LangChain agent and invoke it",id:"4-define-the-langchain-agent-and-invoke-it",level:3},{value:"5. View the trace in the MLflow UI",id:"5-view-the-trace-in-the-mlflow-ui-1",level:3},{value:"1. Start MLflow",id:"1-start-mlflow-2",level:3},{value:"2. Install dependencies",id:"2-install-dependencies-1",level:3},{value:"3. Enable OpenTelemetry",id:"3-enable-opentelemetry-1",level:3},{value:"4. Define the LangChain chain and invoke it",id:"4-define-the-langchain-chain-and-invoke-it",level:3},{value:"5. View the trace in the MLflow UI",id:"5-view-the-trace-in-the-mlflow-ui-2",level:3},{value:"Supported APIs",id:"supported-apis",level:2},{value:"Token Usage Tracking",id:"token-usage-tracking",level:2},{value:"Customize Tracing Behavior",id:"customize-tracing-behavior",level:2},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:2}];function f(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"tracing-langchain\ufe0f",children:"Tracing LangChain\ud83e\udd9c\u26d3\ufe0f"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"LangChain Tracing via autolog",src:a(36458).A+"",width:"1876",height:"1080"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"https://www.langchain.com/",children:"LangChain"})," is an open-source framework for building LLM-powered applications."]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"/genai/tracing",children:"MLflow Tracing"})," provides automatic tracing capability for LangChain. You can enable tracing\nfor LangChain by calling the ",(0,l.jsx)(r.B,{fn:"mlflow.langchain.autolog"})," function, and nested traces are automatically logged to the active MLflow Experiment upon invocation of chains. In TypeScript, you can pass the MLflow LangChain callback to the ",(0,l.jsx)(n.code,{children:"callbacks"})," option."]}),"\n",(0,l.jsx)(c.A,{children:(0,l.jsxs)(o.A,{children:[(0,l.jsx)(s.A,{value:"python",label:"Python",default:!0,children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import mlflow\n\nmlflow.langchain.autolog()\n"})})}),(0,l.jsx)(s.A,{value:"typescript",label:"JS / TS",children:(0,l.jsxs)(n.p,{children:["LangChain.js tracing is supported via the OpenTelemetry ingestion. See the ",(0,l.jsx)(n.a,{href:"#getting-started",children:"Getting Started section"})," below for the full setup."]})})]})}),"\n",(0,l.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,l.jsx)(n.p,{children:"MLflow support tracing for LangChain in both Python and TypeScript/JavaScript. Please select the appropriate tab below to get started."}),"\n",(0,l.jsx)(c.A,{children:(0,l.jsxs)(o.A,{children:[(0,l.jsxs)(s.A,{value:"python",label:"Python",default:!0,children:[(0,l.jsx)(n.h3,{id:"1-start-mlflow",children:"1. Start MLflow"}),(0,l.jsxs)(n.p,{children:["Start the MLflow server following the ",(0,l.jsx)(n.a,{href:"/self-hosting",children:"Self-Hosting Guide"}),", if you don't have one already."]}),(0,l.jsx)(n.h3,{id:"2-install-dependencies",children:"2. Install dependencies"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"pip install langchain langchain-openai mlflow\n"})}),(0,l.jsx)(n.h3,{id:"3-enable-tracing",children:"3. Enable tracing"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Calling autolog for LangChain will enable trace logging.\nmlflow.langchain.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_experiment("LangChain")\nmlflow.set_tracking_uri("http://localhost:5000")\n'})}),(0,l.jsx)(n.h3,{id:"4-define-the-chain-and-invoke-it",children:"4. Define the chain and invoke it"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n\nllm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, max_tokens=1000)\n\nprompt_template = PromptTemplate.from_template(\n    "Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. "\n    "Emulate their quirks and mannerisms to the best of your ability, embracing their traits\u2014even if they aren\'t entirely "\n    "constructive or inoffensive. The question is: {question}"\n)\n\nchain = prompt_template | llm | StrOutputParser()\n\n# Let\'s test another call\nchain.invoke(\n    {\n        "person": "Linus Torvalds",\n        "question": "Can I just set everyone\'s access to sudo to make things easier?",\n    }\n)\n'})}),(0,l.jsx)(n.h3,{id:"5-view-the-trace-in-the-mlflow-ui",children:"5. View the trace in the MLflow UI"}),(0,l.jsxs)(n.p,{children:["Visit ",(0,l.jsx)(n.code,{children:"http://localhost:5000"})," (or your custom MLflow tracking server URL) to view the trace in the MLflow UI."]})]}),(0,l.jsxs)(s.A,{value:"typescript-v1",label:"JS / TS (v1)",children:[(0,l.jsx)(n.h3,{id:"1-start-mlflow-1",children:"1. Start MLflow"}),(0,l.jsxs)(n.p,{children:["Start the MLflow server following the ",(0,l.jsx)(n.a,{href:"/self-hosting",children:"Self-Hosting Guide"}),", if you don't have one already."]}),(0,l.jsx)(n.h3,{id:"2-install-the-required-dependencies",children:"2. Install the required dependencies:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"npm i langchain @langchain/core @langchain/openai @arizeai/openinference-instrumentation-langchain\n"})}),(0,l.jsx)(n.h3,{id:"3-enable-opentelemetry",children:"3. Enable OpenTelemetry"}),(0,l.jsx)(n.p,{children:"Enable OpenTelemetry instrumentation for LangChain in your application:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-typescript",children:'import { NodeTracerProvider, SimpleSpanProcessor } from "@opentelemetry/sdk-trace-node";\nimport { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";\nimport { LangChainInstrumentation } from "@arizeai/openinference-instrumentation-langchain";\nimport * as CallbackManagerModule from "@langchain/core/callbacks/manager";\n\n// Set up the OpenTelemetry\nconst provider = new NodeTracerProvider(\n  {\n    spanProcessors: [new SimpleSpanProcessor(new OTLPTraceExporter({\n      // Set MLflow tracking server URL with `/v1/traces` path. You can also use the OTEL_EXPORTER_OTLP_TRACES_ENDPOINT environment variable instead.\n      url: "http://localhost:5000/v1/traces",\n      // Set the experiment ID in the header. You can also use the OTEL_EXPORTER_OTLP_TRACES_HEADERS environment variable instead.\n      headers: {\n        "x-mlflow-experiment-id": "123",\n      },\n    }))],\n  }\n);\nprovider.register();\n\n// Enable LangChain instrumentation\nconst lcInstrumentation = new LangChainInstrumentation();\nlcInstrumentation.manuallyInstrument(CallbackManagerModule);\n'})}),(0,l.jsx)(n.h3,{id:"4-define-the-langchain-agent-and-invoke-it",children:"4. Define the LangChain agent and invoke it"}),(0,l.jsxs)(n.p,{children:["Note that the ",(0,l.jsx)(n.code,{children:"createAgent"})," API is available in LangChain.js v1.0 and later. If you are on LangChain 0.x, see the v0 example instead."]}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-typescript",children:'import { createAgent, tool } from "langchain";\nimport * as z from "zod";\n\nconst getWeather = tool(\n  (input) => `It\'s always sunny in ${input.city}!`,\n  {\n    name: "get_weather",\n    description: "Get the weather for a given city",\n    schema: z.object({\n      city: z.string().describe("The city to get the weather for"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: "gpt-4o-mini",\n  tools: [getWeather],\n});\n\nawait agent.invoke({\n    messages: [{ role: "user", content: "What\'s the weather in Tokyo?" }],\n});\n'})}),(0,l.jsx)(n.h3,{id:"5-view-the-trace-in-the-mlflow-ui-1",children:"5. View the trace in the MLflow UI"}),(0,l.jsxs)(n.p,{children:["Visit ",(0,l.jsx)(n.code,{children:"http://localhost:5000"})," (or your custom MLflow tracking server URL) to view the trace in the MLflow UI."]})]}),(0,l.jsxs)(s.A,{value:"typescript-v0",label:"JS / TS (v0)",children:[(0,l.jsx)(n.h3,{id:"1-start-mlflow-2",children:"1. Start MLflow"}),(0,l.jsxs)(n.p,{children:["Start the MLflow server following the ",(0,l.jsx)(n.a,{href:"https://mlflow.org/docs/latest/self-hosting/index.html",children:"Self-Hosting Guide"}),", if you don't have one already."]}),(0,l.jsx)(n.h3,{id:"2-install-dependencies-1",children:"2. Install dependencies"}),(0,l.jsx)(n.p,{children:"Install the required dependencies:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"npm i langchain @langchain/core @langchain/openai @arizeai/openinference-instrumentation-langchain\n"})}),(0,l.jsx)(n.h3,{id:"3-enable-opentelemetry-1",children:"3. Enable OpenTelemetry"}),(0,l.jsx)(n.p,{children:"Enable OpenTelemetry instrumentation for LangChain in your application:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-typescript",children:'import { NodeTracerProvider, SimpleSpanProcessor } from "@opentelemetry/sdk-trace-node";\nimport { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";\nimport { LangChainInstrumentation } from "@arizeai/openinference-instrumentation-langchain";\nimport * as CallbackManagerModule from "@langchain/core/callbacks/manager";\n\n// Set up the OpenTelemetry\nconst provider = new NodeTracerProvider(\n  {\n    spanProcessors: [new SimpleSpanProcessor(new OTLPTraceExporter({\n      // Set MLflow tracking server URL. You can also use the OTEL_EXPORTER_OTLP_TRACES_ENDPOINT environment variable instead.\n      url: "http://localhost:5000/v1/traces",\n      // Set the experiment ID in the header. You can also use the OTEL_EXPORTER_OTLP_TRACES_HEADERS environment variable instead.\n      headers: {\n        "x-mlflow-experiment-id": "123",\n      },\n    }))],\n  }\n);\nprovider.register();\n\n// Enable LangChain instrumentation\nconst lcInstrumentation = new LangChainInstrumentation();\nlcInstrumentation.manuallyInstrument(CallbackManagerModule);\n'})}),(0,l.jsx)(n.h3,{id:"4-define-the-langchain-chain-and-invoke-it",children:"4. Define the LangChain chain and invoke it"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-typescript",children:'import { OpenAI } from "@langchain/openai";\nimport { PromptTemplate } from "@langchain/core/prompts";\n\nconst model = new OpenAI("gpt-4o-mini");\nconst prompt = PromptTemplate.fromTemplate("What is a good name for a company that makes {product}?");\nconst chain = prompt.pipe({ llm: model });\n\nconst res = await chain.invoke({ product: "colorful socks" });\nconsole.log({ res });\n'})}),(0,l.jsx)(n.h3,{id:"5-view-the-trace-in-the-mlflow-ui-2",children:"5. View the trace in the MLflow UI"}),(0,l.jsxs)(n.p,{children:["Visit ",(0,l.jsx)(n.code,{children:"http://localhost:5000"})," (or your custom MLflow tracking server URL) to view the trace in the MLflow UI."]})]})]})}),"\n",(0,l.jsxs)(n.admonition,{type:"note",children:[(0,l.jsx)(n.p,{children:"This example above has been confirmed working with the following requirement versions:"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:"pip install openai==1.30.5 langchain==0.2.1 langchain-openai==0.1.8 langchain-community==0.2.1 mlflow==2.14.0 tiktoken==0.7.0\n"})})]}),"\n",(0,l.jsx)(n.h2,{id:"supported-apis",children:"Supported APIs"}),"\n",(0,l.jsx)(n.p,{children:"The following APIs are supported by auto tracing for LangChain."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"invoke"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"batch"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"stream"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"ainvoke"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"abatch"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"astream"})}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"get_relevant_documents"})," (for retrievers)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"__call__"})," (for Chains and AgentExecutors)"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"token-usage-tracking",children:"Token Usage Tracking"}),"\n",(0,l.jsxs)(n.p,{children:["MLflow >= 3.1.0 supports token usage tracking for LangChain. The token usage for each LLM call during a chain invocation will be logged in the ",(0,l.jsx)(n.code,{children:"mlflow.chat.tokenUsage"})," span attribute, and the total usage in the entire trace will be logged in the ",(0,l.jsx)(n.code,{children:"mlflow.trace.tokenUsage"})," metadata field."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import json\nimport mlflow\n\nmlflow.langchain.autolog()\n\n# Execute the chain defined in the previous example\nchain.invoke(\n    {\n        "person": "Linus Torvalds",\n        "question": "Can I just set everyone\'s access to sudo to make things easier?",\n    }\n)\n\n# Get the trace object just created\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Token usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 81\n  Output tokens: 257\n  Total tokens: 338\n\n== Token usage for each LLM call: ==\nChatOpenAI:\n  Input tokens: 81\n  Output tokens: 257\n  Total tokens: 338\n"})}),"\n",(0,l.jsx)(n.h2,{id:"customize-tracing-behavior",children:"Customize Tracing Behavior"}),"\n",(0,l.jsxs)(n.p,{children:["Sometimes you may want to customize what information is logged in the traces. You can achieve this by creating a custom callback handler that inherits from ",(0,l.jsx)(r.B,{fn:"mlflow.langchai.langchain_tracer.MlflowLangchainTracer",children:(0,l.jsx)(n.code,{children:"MlflowLangchainTracer"})}),". MlflowLangchainTracer is a callback handler that is injected into the langchain model inference process to log traces automatically. It starts a new span upon a set of actions of the chain such as on_chain_start, on_llm_start, and concludes it when the action is finished. Various metadata such as span type, action name, input, output, latency, are automatically recorded to the span."]}),"\n",(0,l.jsx)(n.p,{children:"The following example demonstrates how to record an additional attribute to the span when a chat model starts running."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from mlflow.langchain.langchain_tracer import MlflowLangchainTracer\n\n\nclass CustomLangchainTracer(MlflowLangchainTracer):\n    # Override the handler functions to customize the behavior. The method signature is defined by LangChain Callbacks.\n    def on_chat_model_start(\n        self,\n        serialized: Dict[str, Any],\n        messages: List[List[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[List[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ):\n        """Run when a chat model starts running."""\n        attributes = {\n            **kwargs,\n            **metadata,\n            # Add additional attribute to the span\n            "version": "1.0.0",\n        }\n\n        # Call the _start_span method at the end of the handler function to start a new span.\n        self._start_span(\n            span_name=name or self._assign_span_name(serialized, "chat model"),\n            parent_run_id=parent_run_id,\n            span_type=SpanType.CHAT_MODEL,\n            run_id=run_id,\n            inputs=messages,\n            attributes=kwargs,\n        )\n'})}),"\n",(0,l.jsx)(n.h2,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,l.jsxs)(n.p,{children:["Auto tracing for LangChain can be disabled globally by calling ",(0,l.jsx)(n.code,{children:"mlflow.langchain.autolog(disable=True)"})," or ",(0,l.jsx)(n.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function g(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(f,{...e})}):f(e)}},36458:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/tracing-top-dcca046565ab33be6afe0447dd328c22.gif"},47020:(e,n,a)=>{a.d(n,{A:()=>i});a(96540);const t={wrapper:"wrapper_sf5q"};var l=a(74848);function i({children:e}){return(0,l.jsx)("div",{className:t.wrapper,children:e})}},49374:(e,n,a)=>{a.d(n,{B:()=>o});a(96540);const t=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var l=a(86025),i=a(74848);const r=e=>{const n=e.split(".");for(let a=n.length;a>0;a--){const e=n.slice(0,a).join(".");if(t[e])return e}return null};function o({fn:e,children:n,hash:a}){const o=r(e);if(!o)return(0,i.jsx)(i.Fragment,{children:n});const s=(0,l.default)(`/${t[o]}#${a??e}`);return(0,i.jsx)("a",{href:s,target:"_blank",children:n??(0,i.jsxs)("code",{children:[e,"()"]})})}}}]);