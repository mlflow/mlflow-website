"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3754],{14252:(e,n,t)=>{t.d(n,{A:()=>a});t(96540);var o=t(65195);const r={tableOfContentsInline:"tableOfContentsInline_prmo"};var l=t(74848);function a({toc:e,minHeadingLevel:n,maxHeadingLevel:t}){return(0,l.jsx)("div",{className:r.tableOfContentsInline,children:(0,l.jsx)(o.A,{toc:e,minHeadingLevel:n,maxHeadingLevel:t,className:"table-of-contents",linkClassName:null})})}},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var o=t(96540);const r={},l=o.createContext(r);function a(e){const n=o.useContext(l);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(l.Provider,{value:n},e.children)}},49374:(e,n,t)=>{t.d(n,{B:()=>s});t(96540);const o=JSON.parse('{"mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html"}');var r=t(86025),l=t(28774),a=t(74848);const i=e=>{const n=e.split(".");for(let t=n.length;t>0;t--){const e=n.slice(0,t).join(".");if(o[e])return e}return null};function s({fn:e,children:n}){const t=i(e);if(!t)return(0,a.jsx)(a.Fragment,{children:n});const s=(0,r.Ay)(`/${o[t]}#${e}`);return(0,a.jsx)(l.A,{to:s,target:"_blank",children:n??(0,a.jsxs)("code",{children:[e,"()"]})})}},65195:(e,n,t)=>{t.d(n,{A:()=>u});var o=t(96540),r=t(6342);function l(e){const n=e.map((e=>({...e,parentIndex:-1,children:[]}))),t=Array(7).fill(-1);n.forEach(((e,n)=>{const o=t.slice(2,e.level);e.parentIndex=Math.max(...o),t[e.level]=n}));const o=[];return n.forEach((e=>{const{parentIndex:t,...r}=e;t>=0?n[t].children.push(r):o.push(r)})),o}function a({toc:e,minHeadingLevel:n,maxHeadingLevel:t}){return e.flatMap((e=>{const o=a({toc:e.children,minHeadingLevel:n,maxHeadingLevel:t});return function(e){return e.level>=n&&e.level<=t}(e)?[{...e,children:o}]:o}))}function i(e){const n=e.getBoundingClientRect();return n.top===n.bottom?i(e.parentNode):n}function s(e,{anchorTopOffset:n}){const t=e.find((e=>i(e).top>=n));if(t){return function(e){return e.top>0&&e.bottom<window.innerHeight/2}(i(t))?t:e[e.indexOf(t)-1]??null}return e[e.length-1]??null}function c(){const e=(0,o.useRef)(0),{navbar:{hideOnScroll:n}}=(0,r.p)();return(0,o.useEffect)((()=>{e.current=n?0:document.querySelector(".navbar").clientHeight}),[n]),e}function d(e){const n=(0,o.useRef)(void 0),t=c();(0,o.useEffect)((()=>{if(!e)return()=>{};const{linkClassName:o,linkActiveClassName:r,minHeadingLevel:l,maxHeadingLevel:a}=e;function i(){const e=function(e){return Array.from(document.getElementsByClassName(e))}(o),i=function({minHeadingLevel:e,maxHeadingLevel:n}){const t=[];for(let o=e;o<=n;o+=1)t.push(`h${o}.anchor`);return Array.from(document.querySelectorAll(t.join()))}({minHeadingLevel:l,maxHeadingLevel:a}),c=s(i,{anchorTopOffset:t.current}),d=e.find((e=>c&&c.id===function(e){return decodeURIComponent(e.href.substring(e.href.indexOf("#")+1))}(e)));e.forEach((e=>{!function(e,t){t?(n.current&&n.current!==e&&n.current.classList.remove(r),e.classList.add(r),n.current=e):e.classList.remove(r)}(e,e===d)}))}return document.addEventListener("scroll",i),document.addEventListener("resize",i),i(),()=>{document.removeEventListener("scroll",i),document.removeEventListener("resize",i)}}),[e,t])}var m=t(28774),p=t(74848);function h({toc:e,className:n,linkClassName:t,isChild:o}){return e.length?(0,p.jsx)("ul",{className:o?void 0:n,children:e.map((e=>(0,p.jsxs)("li",{children:[(0,p.jsx)(m.A,{to:`#${e.id}`,className:t??void 0,dangerouslySetInnerHTML:{__html:e.value}}),(0,p.jsx)(h,{isChild:!0,toc:e.children,className:n,linkClassName:t})]},e.id)))}):null}const f=o.memo(h);function u({toc:e,className:n="table-of-contents table-of-contents__left-border",linkClassName:t="table-of-contents__link",linkActiveClassName:i,minHeadingLevel:s,maxHeadingLevel:c,...m}){const h=(0,r.p)(),u=s??h.tableOfContents.minHeadingLevel,g=c??h.tableOfContents.maxHeadingLevel,w=function({toc:e,minHeadingLevel:n,maxHeadingLevel:t}){return(0,o.useMemo)((()=>a({toc:l(e),minHeadingLevel:n,maxHeadingLevel:t})),[e,n,t])}({toc:e,minHeadingLevel:u,maxHeadingLevel:g});return d((0,o.useMemo)((()=>{if(t&&i)return{linkClassName:t,linkActiveClassName:i,minHeadingLevel:u,maxHeadingLevel:g}}),[t,i,u,g])),(0,p.jsx)(f,{toc:w,className:n,linkClassName:t,...m})}},72839:(e,n,t)=>{t.d(n,{X:()=>r});var o=t(74848);function r({children:e}){return(0,o.jsx)("div",{className:"w-full overflow-x-auto",children:(0,o.jsx)("table",{children:e})})}},74223:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>m,default:()=>u,frontMatter:()=>d,metadata:()=>o,toc:()=>h});const o=JSON.parse('{"id":"deep-learning/transformers/task/index","title":"Tasks in MLflow Transformers Flavor","description":"This page provides an overview of how to use the task parameter in the MLflow Transformers flavor to control","source":"@site/docs/classic-ml/deep-learning/transformers/task/index.mdx","sourceDirName":"deep-learning/transformers/task","slug":"/deep-learning/transformers/task/","permalink":"/docs/latest/ml/deep-learning/transformers/task/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"classicMLSidebar","previous":{"title":"Working with Large Transformers Models","permalink":"/docs/latest/ml/deep-learning/transformers/large-models/"},"next":{"title":"Tutorials","permalink":"/docs/latest/ml/deep-learning/transformers/tutorials/"}}');var r=t(74848),l=t(28453),a=t(14252),i=t(28774),s=t(49374),c=t(72839);const d={},m="Tasks in MLflow Transformers Flavor",p={},h=[{value:"Overview",id:"overview",level:2},{value:"Native Transformers Task Types",id:"native-transformers-task-types",level:2},{value:"Advanced Tasks for OpenAI-Compatible Inference",id:"advanced-tasks-for-openai-compatible-inference",level:2},{value:"Input and Output Formats",id:"input-and-output-formats",level:3},{value:"Code Example of Using <code>llm/v1</code> Tasks",id:"code-example-of-using-llmv1-tasks",level:3},{value:"Provisioned Throughput on Databricks Model Serving",id:"provisioned-throughput-on-databricks-model-serving",level:2},{value:"FAQ",id:"faq",level:2},{value:"How to override the default query parameters for the OpenAI-compatible inference?",id:"how-to-override-the-default-query-parameters-for-the-openai-compatible-inference",level:3}];function f(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"tasks-in-mlflow-transformers-flavor",children:"Tasks in MLflow Transformers Flavor"})}),"\n",(0,r.jsxs)(n.p,{children:["This page provides an overview of how to use the ",(0,r.jsx)(n.code,{children:"task"})," parameter in the MLflow Transformers flavor to control\nthe inference interface of the model."]}),"\n",(0,r.jsx)(a.A,{toc:h}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:["In the MLflow Transformers flavor, ",(0,r.jsx)(n.code,{children:"task"})," plays a crucial role in determining the input and output format of the model.\nThe ",(0,r.jsx)(n.code,{children:"task"})," is a fundamental concept in the Transformers library, which describe the structure of each model's API\n(inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model."]}),"\n",(0,r.jsxs)(n.p,{children:["MLflow utilizes this concept to determine the input and output format of the model, persists the correct\n",(0,r.jsx)(n.a,{href:"/ml/model#model-signatures-and-input-examples",children:"Model Signature"}),", and provides a consistent ",(0,r.jsx)(i.A,{to:"/api_reference/python_api/mlflow.pyfunc.html#inference-api",target:"_blank",children:"Pyfunc Inference API"}),"\nfor serving different types of models. Additionally, on top of the native Transformers task types, MLflow defines a few additional task types to support more complex use cases, such as chat-style applications."]}),"\n",(0,r.jsx)(n.h2,{id:"native-transformers-task-types",children:"Native Transformers Task Types"}),"\n",(0,r.jsxs)(n.p,{children:["For native Transformers tasks, MLflow will automatically infer the task type from the pipeline when you save a pipeline\nwith ",(0,r.jsx)(s.B,{fn:"mlflow.transformers.log_model"}),". You can also specify the task type explicitly by passing the\n",(0,r.jsx)(n.code,{children:"task"})," parameter. The full list of supported task types is available in the ",(0,r.jsx)(n.a,{href:"https://huggingface.co/tasks",children:"Transformers documentation"}),",\nbut note that ",(0,r.jsx)(n.strong,{children:"not all task types are supported in MLflow"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport transformers\n\npipeline = transformers.pipeline("text-generation", model="gpt2")\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.save_model(\n        transformers_model=pipeline,\n        artifact_path="model",\n        save_pretrained=False,\n    )\n\nprint(f"Inferred task: {model_info.flavors[\'transformers\'][\'task\']}")\n# >> Inferred task: text-generation\n'})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-tasks-for-openai-compatible-inference",children:"Advanced Tasks for OpenAI-Compatible Inference"}),"\n",(0,r.jsx)(n.p,{children:"In addition to the native Transformers task types, MLflow defines a few additional task types. Those advanced task types allows you to extend the Transformers pipeline with OpenAI-compatible inference interface, to serve models for specific use cases."}),"\n",(0,r.jsxs)(n.p,{children:["For example, the Transformers ",(0,r.jsx)(n.code,{children:"text-generation"})," pipeline inputs and outputs a single string or a list of strings. However, when serving a model, it is often necessary to have a more structured input and output format. For instance, in a chat-style application, the input may be a list of messages."]}),"\n",(0,r.jsxs)(n.p,{children:["To support these use cases, MLflow defines a set of advanced task types prefixed with ",(0,r.jsx)(n.code,{children:"llm/v1"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"llm/v1/chat"'})," for chat-style applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"llm/v1/completions"'})," for generic completions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"llm/v1/embeddings"'})," for text embeddings generation"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The required step to use these advanced task types is just to specify the ",(0,r.jsx)(n.code,{children:"task"})," parameter as an ",(0,r.jsx)(n.code,{children:"llm/v1"})," task when logging the models."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        name="model",\n        task="llm/v1/chat",  # <= Specify the llm/v1 task type\n        # Optional, recommended for large models to avoid creating a local copy of the model weights\n        save_pretrained=False,\n    )\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["This feature is only available in MLflow 2.11.0 and above. Also, the ",(0,r.jsx)(n.code,{children:"llm/v1/chat"})," task type is only available for models saved with ",(0,r.jsx)(n.code,{children:"transformers >= 4.34.0"}),"."]})}),"\n",(0,r.jsx)(n.h3,{id:"input-and-output-formats",children:"Input and Output Formats"}),"\n",(0,r.jsxs)(c.X,{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Task"}),(0,r.jsx)("th",{children:"Supported pipeline"}),(0,r.jsx)("th",{children:"Input"}),(0,r.jsx)("th",{children:"Output"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"llm/v1/chat"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"text-generation"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.a,{href:"/genai/serving",children:"Chat API spec"})}),(0,r.jsxs)("td",{children:["Returns a ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat/object",children:"Chat Completion"})," object in the json format."]})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"llm/v1/completions"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"text-generation"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.a,{href:"/genai/serving",children:"Completions API spec"})}),(0,r.jsxs)("td",{children:["Returns a ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/text-generation/completions-api",children:"Completion"})," object in the json format."]})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"llm/v1/embeddings"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.code,{children:"feature-extraction"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.a,{href:"/genai/serving",children:"Embeddings API spec"})}),(0,r.jsxs)("td",{children:["Returns a list of ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/embeddings/object",children:"Embedding"})," object. Additionally, the model returns ",(0,r.jsx)(n.code,{children:"usage"})," field, which contains the number of tokens used for the embeddings generation."]})]})]})]}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"The Completion API is considered as legacy, but it is still supported in MLflow for backward compatibility. We recommend using the Chat API for compatibility with the latest APIs from OpenAI and other model providers."})}),"\n",(0,r.jsxs)(n.h3,{id:"code-example-of-using-llmv1-tasks",children:["Code Example of Using ",(0,r.jsx)(n.code,{children:"llm/v1"})," Tasks"]}),"\n",(0,r.jsxs)(n.p,{children:["The following code snippet demonstrates how to log a Transformers pipeline with the ",(0,r.jsx)(n.code,{children:"llm/v1/chat"})," task type, and use the model for chat-style inference. Check out the\n",(0,r.jsx)(n.a,{href:"/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/",children:"notebook tutorial"})," to see more examples in action!"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import mlflow\nimport transformers\n\npipeline = transformers.pipeline(\"text-generation\", \"gpt2\")\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        name=\"model\",\n        task=\"llm/v1/chat\",\n        input_example={\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a bot.\"},\n                {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n            ]\n        },\n        save_pretrained=False,\n    )\n\n# Model metadata logs additional field \"inference_task\"\nprint(model_info.flavors[\"transformers\"][\"inference_task\"])\n# >> llm/v1/chat\n\n# The original native task type is also saved\nprint(model_info.flavors[\"transformers\"][\"task\"])\n# >> text-generation\n\n# Model signature is set to the chat API spec\nprint(model_info.signature)\n# >> inputs:\n# >>   ['messages': Array({content: string (required), name: string (optional), role: string (required)}) (required), 'temperature': double (optional), 'max_tokens': long (optional), 'stop': Array(string) (optional), 'n': long (optional), 'stream': boolean (optional)]\n# >> outputs:\n# >>   ['id': string (required), 'object': string (required), 'created': long (required), 'model': string (required), 'choices': Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), 'usage': {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]\n# >> params:\n# >>     None\n\n# The model can be served with the OpenAI-compatible inference API\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nprediction = pyfunc_model.predict(\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a bot.\"},\n            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n        ],\n        \"temperature\": 0.5,\n        \"max_tokens\": 200,\n    }\n)\nprint(prediction)\n# >> [{'choices': [{'finish_reason': 'stop',\n# >>               'index': 0,\n# >>               'message': {'content': 'I'm doing well, thank you for asking.', 'role': 'assistant'}}],\n# >>   'created': 1719875820,\n# >>   'id': '355c4e9e-040b-46b0-bf22-00e93486100c',\n# >>   'model': 'gpt2',\n# >>   'object': 'chat.completion',\n# >>   'usage': {'completion_tokens': 7, 'prompt_tokens': 13, 'total_tokens': 20}}]\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Note that the input and output modifications only apply when the model is loaded with ",(0,r.jsx)(s.B,{fn:"mlflow.pyfunc.load_model"})," (e.g. when\nserving the model with the ",(0,r.jsx)(n.code,{children:"mlflow models serve"})," CLI tool). If you want to load just the raw pipeline, you can\nuse ",(0,r.jsx)(s.B,{fn:"mlflow.transformers.load_model"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"provisioned-throughput-on-databricks-model-serving",children:"Provisioned Throughput on Databricks Model Serving"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html",children:"Provisioned Throughput"}),"\non Databricks Model Serving is a capability that optimizes inference performance for foundation models with performance guarantees.\nTo serve Transformers models with provisioned throughput, specify ",(0,r.jsx)(n.code,{children:"llm/v1/xxx"})," task type when logging the model. MLflow logs the required metadata\nto enable provisioned throughput on Databricks Model Serving."]}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["When logging large models, you can use ",(0,r.jsx)(n.code,{children:"save_pretrained=False"})," to avoid creating a local copy of the model weights for saving time and disk space.\nPlease refer to the ",(0,r.jsx)(n.a,{href:"/ml/deep-learning/transformers/large-models#transformers-save-pretrained-guide",children:"documentation"})," for more details."]})}),"\n",(0,r.jsx)(n.h2,{id:"faq",children:"FAQ"}),"\n",(0,r.jsx)(n.h3,{id:"how-to-override-the-default-query-parameters-for-the-openai-compatible-inference",children:"How to override the default query parameters for the OpenAI-compatible inference?"}),"\n",(0,r.jsxs)(n.p,{children:["When serving the model saved with the ",(0,r.jsx)(n.code,{children:"llm/v1"})," task type, MLflow uses the same default value as OpenAI APIs for the parameters like ",(0,r.jsx)(n.code,{children:"temperature"})," and ",(0,r.jsx)(n.code,{children:"stop"}),".\nYou can override them by either passing the values at inference time, or by setting different default values when logging the model."]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["At inference time: You can pass the parameters as part of the input dictionary when calling the ",(0,r.jsx)(n.code,{children:"predict()"})," method, just like how you pass the input messages."]}),"\n",(0,r.jsxs)(n.li,{children:["When logging the model: You can override the default values for the parameters by saving a ",(0,r.jsx)(n.code,{children:"model_config"})," parameter when logging the model."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'with mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        name="model",\n        task="llm/v1/chat",\n        model_config={\n            "temperature": 0.5,  # <= Set the default temperature\n            "stop": ["foo", "bar"],  # <= Set the default stop sequence\n        },\n        save_pretrained=False,\n    )\n'})}),"\n",(0,r.jsx)(n.admonition,{title:"attention",type:"warning",children:(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"stop"})," parameter can be used to specify the stop sequence for the ",(0,r.jsx)(n.code,{children:"llm/v1/chat"})," and ",(0,r.jsx)(n.code,{children:"llm/v1/completions"})," tasks.\nWe emulate the behavior of the ",(0,r.jsx)(n.code,{children:"stop"})," parameter in the OpenAI APIs by passing the\n",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate.stopping_criteria",children:"stopping_criteria"}),"\nto the Transformers pipeline, with the token IDs of the given stop sequence. However, the behavior may not be stable because\nthe tokenizer does not always generate the same token IDs for the same sequence in different sentences, especially for ",(0,r.jsx)(n.code,{children:"sentence-piece"})," based tokenizers."]})})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(f,{...e})}):f(e)}}}]);