"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["984"],{30965(e,n,r){r.r(n),r.d(n,{metadata:()=>s,default:()=>x,frontMatter:()=>u,contentTitle:()=>h,toc:()=>_,assets:()=>f});var s=JSON.parse('{"id":"eval-monitor/scorers/custom/code-examples","title":"Code-based scorer examples","description":"In MLflow Evaluation for GenAI, custom code-based scorers allow you to define flexible evaluation metrics for your AI agent or application. This set of examples illustrate many patterns for using code-based scorers with different options for inputs, outputs, implementation, and error handling.","source":"@site/docs/genai/eval-monitor/scorers/custom/code-examples.mdx","sourceDirName":"eval-monitor/scorers/custom","slug":"/eval-monitor/scorers/custom/code-examples","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom/code-examples","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Create custom code-based scorers","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom/"},"next":{"title":"Tutorial: Develop code-based scorers","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom/tutorial"}}'),t=r(74848),a=r(28453),o=r(54725),l=r(46077),i=r(77541),c=r(10440),d=r(87073),m=r(7043),p=r(51004);let u={},h="Code-based scorer examples",f={},_=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Update <code>MLflow</code>",id:"update-mlflow",level:3},{value:"Define your GenAI app",id:"define-your-genai-app",level:3},{value:"Generate traces",id:"generate-traces",level:3},{value:"Example 1: Access data from the <code>Trace</code>",id:"example-1-access-data-from-the-trace",level:2},{value:"Example 2: Wrap a predefined LLM judge",id:"example-2-wrap-a-predefined-llm-judge",level:2},{value:"Example 3: Use <code>expectations</code>",id:"example-3-use-expectations",level:2},{value:"Example 3.1: Exact match with expected response",id:"example-31-exact-match-with-expected-response",level:3},{value:"Example 3.2: Keyword check from expectations",id:"example-32-keyword-check-from-expectations",level:3},{value:"Example 4: Return multiple feedback objects",id:"example-4-return-multiple-feedback-objects",level:2},{value:"Example 5: Use your own LLM for a judge",id:"example-5-use-your-own-llm-for-a-judge",level:2},{value:"Example 6: Class-based scorer definition",id:"example-6-class-based-scorer-definition",level:2},{value:"Example 7: Error handling in scorers",id:"example-7-error-handling-in-scorers",level:2},{value:"Example 8: Naming conventions in scorers",id:"example-8-naming-conventions-in-scorers",level:2},{value:"Example 9: Chaining evaluation results",id:"example-9-chaining-evaluation-results",level:2},{value:"Example 10: Conditional logic with guidelines",id:"example-10-conditional-logic-with-guidelines",level:2},{value:"Next steps",id:"next-steps",level:2}];function g(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"code-based-scorer-examples",children:"Code-based scorer examples"})}),"\n",(0,t.jsxs)(n.p,{children:["In MLflow Evaluation for GenAI, ",(0,t.jsx)(n.a,{href:"/genai/eval-monitor/scorers/custom",children:"custom code-based scorers"})," allow you to define flexible evaluation metrics for your AI agent or application. This set of examples illustrate many patterns for using code-based scorers with different options for inputs, outputs, implementation, and error handling."]}),"\n",(0,t.jsx)(n.p,{children:"The image below illustrates some custom scorers' outputs as metrics in the MLflow UI."}),"\n",(0,t.jsx)(l.A,{src:"/images/genai/eval-monitor/code-scorer-results.png",alt:"Code-based Scorers Results",width:"800px"}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Update MLflow"}),"\n",(0,t.jsx)(n.li,{children:"Define your GenAI app"}),"\n",(0,t.jsx)(n.li,{children:"Generate traces used in some scorer examples"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"update-mlflow",children:["Update ",(0,t.jsx)(n.code,{children:"MLflow"})]}),"\n",(0,t.jsxs)(n.p,{children:["Update ",(0,t.jsx)(n.code,{children:"mlflow"})," to the latest version for the best GenAI experience, and install ",(0,t.jsx)(n.code,{children:"openai"})," since the example app below uses the OpenAI client."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install --upgrade mlflow openai\n"})}),"\n",(0,t.jsx)(n.h3,{id:"define-your-genai-app",children:"Define your GenAI app"}),"\n",(0,t.jsx)(n.p,{children:"Some examples below will use the following GenAI app, which is a general assistant for question-answering."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\n\n# Select an LLM\nmodel_name = "gpt-4o-mini"\nmlflow.openai.autolog()\n\n\n@mlflow.trace\ndef sample_app(messages: list[dict[str, str]]):\n    # 1. Prepare messages for the LLM\n    messages_for_llm = [\n        {"role": "system", "content": "You are a helpful assistant."},\n        *messages,\n    ]\n\n    # 2. Call LLM to generate a response\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages_for_llm,\n    )\n    return response.choices[0].message.content\n\n\nsample_app([{"role": "user", "content": "What is the capital of France?"}])\n'})}),"\n",(0,t.jsx)(n.h3,{id:"generate-traces",children:"Generate traces"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"eval_dataset"})," below is used by ",(0,t.jsx)(o.B,{fn:"mlflow.genai.evaluate",children:(0,t.jsx)(n.code,{children:"mlflow.genai.evaluate()"})})," to generate traces, using a placeholder scorer."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import scorer\n\neval_dataset = [\n    {\n        "inputs": {\n            "messages": [\n                {"role": "user", "content": "How much does a microwave cost?"},\n            ]\n        },\n    },\n    {\n        "inputs": {\n            "messages": [\n                {\n                    "role": "user",\n                    "content": "Can I return the microwave I bought 2 months ago?",\n                },\n            ]\n        },\n    },\n    {\n        "inputs": {\n            "messages": [\n                {\n                    "role": "user",\n                    "content": "I\'m having trouble with my account.  I can\'t log in.",\n                },\n                {\n                    "role": "assistant",\n                    "content": "I\'m sorry to hear that you\'re having trouble with your account.  Are you using our website or mobile app?",\n                },\n                {"role": "user", "content": "Website"},\n            ]\n        },\n    },\n]\n\n\n@scorer\ndef placeholder_metric() -> int:\n    # placeholder return value\n    return 1\n\n\neval_results = mlflow.genai.evaluate(\n    data=eval_dataset, predict_fn=sample_app, scorers=[placeholder_metric]\n)\n\ngenerated_traces = mlflow.search_traces(run_id=eval_results.run_id)\ngenerated_traces\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(o.B,{fn:"mlflow.search_traces",children:(0,t.jsx)(n.code,{children:"mlflow.search_traces()"})})," function above returns a Pandas DataFrame of traces, for use in some examples below."]}),"\n",(0,t.jsxs)(n.h2,{id:"example-1-access-data-from-the-trace",children:["Example 1: Access data from the ",(0,t.jsx)(n.code,{children:"Trace"})]}),"\n",(0,t.jsxs)(n.p,{children:["Access the full MLflow ",(0,t.jsx)(n.a,{href:"/genai/concepts/trace",children:"Trace"})," object to use various details (spans, inputs, outputs, attributes, timing) for fine-grained metric calculation."]}),"\n",(0,t.jsx)(n.p,{children:"This scorer checks if the total execution time of the trace is within an acceptable range."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import scorer\nfrom mlflow.entities import Trace, Feedback, SpanType\n\n\n@scorer\ndef llm_response_time_good(trace: Trace) -> Feedback:\n    # Search particular span type from the trace\n    llm_span = trace.search_spans(span_type=SpanType.CHAT_MODEL)[0]\n\n    response_time = (\n        llm_span.end_time_ns - llm_span.start_time_ns\n    ) / 1e9  # convert to seconds\n    max_duration = 5.0\n    if response_time <= max_duration:\n        return Feedback(\n            value="yes",\n            rationale=f"LLM response time {response_time:.2f}s is within the {max_duration}s limit.",\n        )\n    else:\n        return Feedback(\n            value="no",\n            rationale=f"LLM response time {response_time:.2f}s exceeds the {max_duration}s limit.",\n        )\n\n\n# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\nspan_check_eval_results = mlflow.genai.evaluate(\n    data=generated_traces, scorers=[llm_response_time_good]\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-2-wrap-a-predefined-llm-judge",children:"Example 2: Wrap a predefined LLM judge"}),"\n",(0,t.jsxs)(n.p,{children:["Create a custom scorer that wraps MLflow's ",(0,t.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/predefined/#available-judges",children:"built-in LLM judges"}),". Use this to preprocess trace data for the judge or post-process its feedback."]}),"\n",(0,t.jsxs)(n.p,{children:["This example demonstrates how to wrap the ",(0,t.jsx)(o.B,{fn:"mlflow.genai.judges.is_context_relevant",children:(0,t.jsx)(n.code,{children:"is_context_relevant"})})," judge to evaluate whether the assistant's response is relevant to the user's query. Specifically, the ",(0,t.jsx)(n.code,{children:"inputs"})," field for ",(0,t.jsx)(n.code,{children:"sample_app"})," is a dictionary like: ",(0,t.jsx)(n.code,{children:'{"messages": [{"role": ..., "content": ...}, ...]}'}),". This scorer extracts the content of the last user message to pass to the relevance judge."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import Trace, Feedback\nfrom mlflow.genai.judges import is_context_relevant\nfrom mlflow.genai.scorers import scorer\nfrom typing import Any\n\n\n@scorer\ndef is_message_relevant(inputs: dict[str, Any], outputs: str) -> Feedback:\n    last_user_message_content = None\n    if "messages" in inputs and isinstance(inputs["messages"], list):\n        for message in reversed(inputs["messages"]):\n            if message.get("role") == "user" and "content" in message:\n                last_user_message_content = message["content"]\n                break\n\n    if not last_user_message_content:\n        raise Exception(\n            "Could not extract the last user message from inputs to evaluate relevance."\n        )\n\n    # Call the `is_context_relevant` judge. It will return a Feedback object.\n    return is_context_relevant(\n        request=last_user_message_content,\n        context={"response": outputs},\n    )\n\n\n# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\ncustom_relevance_eval_results = mlflow.genai.evaluate(\n    data=generated_traces, scorers=[is_message_relevant]\n)\n'})}),"\n",(0,t.jsxs)(n.h2,{id:"example-3-use-expectations",children:["Example 3: Use ",(0,t.jsx)(n.code,{children:"expectations"})]}),"\n",(0,t.jsxs)(n.p,{children:["Expectations are ground truth values or labels and are often important for offline evaluation. When running ",(0,t.jsx)(o.B,{fn:"mlflow.genai.evaluate",children:(0,t.jsx)(n.code,{children:"mlflow.genai.evaluate()"})}),", you can specify expectations in the ",(0,t.jsx)(n.code,{children:"data"})," argument in two ways:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"expectations"})," column or field: For example, if the ",(0,t.jsx)(n.code,{children:"data"})," argument is a list of dictionaries or a Pandas DataFrame, each row can contain an ",(0,t.jsx)(n.code,{children:"expectations"})," key. The value associated with this key is passed directly to your custom scorer."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"trace"})," column or field: For example, if the ",(0,t.jsx)(n.code,{children:"data"})," argument is the dataframe returned by ",(0,t.jsx)(o.B,{fn:"mlflow.search_traces",children:(0,t.jsx)(n.code,{children:"mlflow.search_traces()"})}),", it will include a ",(0,t.jsx)(n.code,{children:"trace"})," field that includes any ",(0,t.jsx)(n.code,{children:"Expectation"})," data associated with the Traces."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This example also demonstrates using a custom scorer along with the predefined ",(0,t.jsx)(n.code,{children:"Safety"})," scorer."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.entities import Feedback\nfrom mlflow.genai.scorers import scorer, Safety\nfrom typing import Any, List, Optional, Union\n\n\nexpectations_eval_dataset_list = [\n    {\n        "inputs": {"messages": [{"role": "user", "content": "What is 2+2?"}]},\n        "expectations": {\n            "expected_response": "2+2 equals 4.",\n            "expected_keywords": ["4", "four", "equals"],\n        },\n    },\n    {\n        "inputs": {\n            "messages": [\n                {"role": "user", "content": "Describe MLflow in one sentence."}\n            ]\n        },\n        "expectations": {\n            "expected_response": "MLflow is an open-source platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models.",\n            "expected_keywords": [\n                "mlflow",\n                "open-source",\n                "platform",\n                "machine learning",\n            ],\n        },\n    },\n    {\n        "inputs": {"messages": [{"role": "user", "content": "Say hello."}]},\n        "expectations": {\n            "expected_response": "Hello there!",\n            # No keywords needed for this one, but the field can be omitted or empty\n        },\n    },\n]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-31-exact-match-with-expected-response",children:"Example 3.1: Exact match with expected response"}),"\n",(0,t.jsxs)(n.p,{children:["This scorer checks if the assistant's response exactly matches the ",(0,t.jsx)(n.code,{children:"expected_response"})," provided in the ",(0,t.jsx)(n.code,{children:"expectations"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@scorer\ndef exact_match(outputs: str, expectations: dict[str, Any]) -> bool:\n    # Scorer can return primitive value like bool, int, float, str, etc.\n    return outputs == expectations["expected_response"]\n\n\nexact_match_eval_results = mlflow.genai.evaluate(\n    data=expectations_eval_dataset_list,\n    predict_fn=sample_app,  # sample_app is from the prerequisite section\n    scorers=[exact_match, Safety()],  # You can include any number of scorers\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-32-keyword-check-from-expectations",children:"Example 3.2: Keyword check from expectations"}),"\n",(0,t.jsxs)(n.p,{children:["This scorer checks if all ",(0,t.jsx)(n.code,{children:"expected_keywords"})," from the ",(0,t.jsx)(n.code,{children:"expectations"})," are present in the assistant's response."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@scorer\ndef keyword_presence_scorer(outputs: str, expectations: dict[str, Any]) -> Feedback:\n    expected_keywords = expectations.get("expected_keywords")\n    print(expected_keywords)\n    if expected_keywords is None:\n        return Feedback(\n            value="yes", rationale="No keywords were expected in the response."\n        )\n\n    missing_keywords = []\n    for keyword in expected_keywords:\n        if keyword.lower() not in outputs.lower():\n            missing_keywords.append(keyword)\n\n    if not missing_keywords:\n        return Feedback(\n            value="yes", rationale="All expected keywords are present in the response."\n        )\n    else:\n        return Feedback(\n            value="no", rationale=f"Missing keywords: {\', \'.join(missing_keywords)}."\n        )\n\n\nkeyword_presence_eval_results = mlflow.genai.evaluate(\n    data=expectations_eval_dataset_list,\n    predict_fn=sample_app,  # sample_app is from the prerequisite section\n    scorers=[keyword_presence_scorer],\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-4-return-multiple-feedback-objects",children:"Example 4: Return multiple feedback objects"}),"\n",(0,t.jsxs)(n.p,{children:["A single scorer can return a list of ",(0,t.jsx)(o.B,{fn:"mlflow.entities.Feedback",children:(0,t.jsx)(n.code,{children:"Feedback"})})," objects, allowing one scorer to assess multiple quality facets (such as PII, sentiment, and conciseness) simultaneously."]}),"\n",(0,t.jsxs)(n.p,{children:["Each ",(0,t.jsx)(n.code,{children:"Feedback"})," object should have a unique ",(0,t.jsx)(n.code,{children:"name"}),", which becomes the metric name in the results. See the details on ",(0,t.jsx)(n.a,{href:"/genai/eval-monitor/scorers/custom/#metric-naming-behavior",children:"metric names"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"This example demonstrates a scorer that returns two distinct pieces of feedback for each trace:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"is_not_empty_check"}),": A boolean indicating if the response content is non-empty."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"response_char_length"}),": A numeric value for the character length of the response."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import scorer\nfrom mlflow.entities import Feedback, Trace\nfrom typing import Any, Optional\n\n\n@scorer\ndef comprehensive_response_checker(outputs: str) -> list[Feedback]:\n    feedbacks = []\n    # 1. Check if the response is not empty\n    feedbacks.append(\n        Feedback(name="is_not_empty_check", value="yes" if outputs != "" else "no")\n    )\n    # 2. Calculate response character length\n    char_length = len(outputs)\n    feedbacks.append(Feedback(name="response_char_length", value=char_length))\n    return feedbacks\n\n\n# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\nmulti_feedback_eval_results = mlflow.genai.evaluate(\n    data=generated_traces, scorers=[comprehensive_response_checker]\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The result will have two columns: ",(0,t.jsx)(n.code,{children:"is_not_empty_check"})," and ",(0,t.jsx)(n.code,{children:"response_char_length"})," as assessments."]}),"\n",(0,t.jsx)(l.A,{src:"/images/genai/eval-monitor/multiple-feedback-results.png",alt:"Multiple feedback results",width:"800px"}),"\n",(0,t.jsx)(n.h2,{id:"example-5-use-your-own-llm-for-a-judge",children:"Example 5: Use your own LLM for a judge"}),"\n",(0,t.jsxs)(n.p,{children:["Integrate a custom or externally hosted LLM within a scorer. The scorer handles API calls, input/output formatting, and generates ",(0,t.jsx)(o.B,{fn:"mlflow.entities.Feedback",children:(0,t.jsx)(n.code,{children:"Feedback"})})," from your LLM's response, giving full control over the judging process."]}),"\n",(0,t.jsxs)(n.p,{children:["You can also set the ",(0,t.jsx)(n.code,{children:"source"})," field in the ",(0,t.jsx)(n.code,{children:"Feedback"})," object to indicate the source of the assessment is an LLM judge."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport json\nfrom mlflow.genai.scorers import scorer\nfrom mlflow.entities import AssessmentSource, AssessmentSourceType, Feedback\nfrom typing import Any, Optional\n\n# Define the prompts for the Judge LLM.\njudge_system_prompt = """\nYou are an impartial AI assistant responsible for evaluating the quality of a response generated by another AI model.\nYour evaluation should be based on the original user query and the AI\'s response.\nProvide a quality score as an integer from 1 to 5 (1=Poor, 2=Fair, 3=Good, 4=Very Good, 5=Excellent).\nAlso, provide a brief rationale for your score.\n\nYour output MUST be a single valid JSON object with two keys: "score" (an integer) and "rationale" (a string).\nExample:\n{"score": 4, "rationale": "The response was mostly accurate and helpful, addressing the user\'s query directly."}\n"""\n\njudge_user_prompt = """\nPlease evaluate the AI\'s Response below based on the Original User Query.\n\nOriginal User Query:\n```{user_query}```\n\nAI\'s Response:\n```{llm_response_from_app}```\n\nProvide your evaluation strictly as a JSON object with "score" and "rationale" keys.\n"""\n\n@scorer\ndef answer_quality(inputs: dict[str, Any], outputs: str) -> Feedback:\n    user_query = inputs["messages"][-1]["content"]\n\n    # Call the Judge LLM using the OpenAI SDK client.\n    judge_llm_response_obj = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {"role": "system", "content": judge_system_prompt},\n            {"role": "user", "content": judge_user_prompt.format(\n                user_query=user_query,\n                llm_response_from_app=outputs\n            )},\n        ],\n        max_tokens=200,  # Max tokens for the judge\'s rationale\n        temperature=0.0,  # For more deterministic judging\n    )\n    judge_llm_output_text = judge_llm_response_obj.choices[0].message.content\n\n    # Parse the Judge LLM\'s JSON output.\n    judge_eval_json = json.loads(judge_llm_output_text)\n    parsed_score = int(judge_eval_json["score"])\n    parsed_rationale = judge_eval_json["rationale"]\n\n    return Feedback(\n        value=parsed_score,\n        rationale=parsed_rationale,\n        # Set the source of the assessment to indicate the LLM judge used to generate the feedback\n        source=AssessmentSource(\n            source_type=AssessmentSourceType.LLM_JUDGE,\n            source_id="claude-sonnet-4-5",\n        )\n    )\n\n# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\ncustom_llm_judge_eval_results = mlflow.genai.evaluate(\n    data=generated_traces,\n    scorers=[answer_quality]\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["By opening the trace in the UI and clicking on the ",(0,t.jsx)(n.code,{children:"answer_quality"})," assessment, you can see the metadata for the judge, such as rationale, timestamp, and judge model name. If the judge assessment is not correct, you can override the score by clicking the ",(0,t.jsx)(n.code,{children:"Edit"})," button."]}),"\n",(0,t.jsx)(n.p,{children:"The new assessment supersedes the original judge assessment. The edit history is preserved for future reference."}),"\n",(0,t.jsx)(l.A,{src:"/images/genai/eval-monitor/custom-llm-judge-ui.png",alt:"Custom LLM judge in UI",width:"800px"}),"\n",(0,t.jsx)(n.h2,{id:"example-6-class-based-scorer-definition",children:"Example 6: Class-based scorer definition"}),"\n",(0,t.jsxs)(n.p,{children:["If a scorer requires state, then the ",(0,t.jsx)(o.B,{fn:"mlflow.genai.scorers.scorer",children:(0,t.jsx)(n.code,{children:"@scorer"})})," decorator-based definition may not suffice. Instead, use the ",(0,t.jsx)(o.B,{fn:"mlflow.genai.Scorer",children:(0,t.jsx)(n.code,{children:"Scorer"})})," base class for more complex scorers. The ",(0,t.jsx)(n.code,{children:"Scorer"})," class is a ",(0,t.jsx)(n.a,{href:"https://docs.pydantic.dev/latest/concepts/models/",children:"Pydantic object"}),", so you can define additional fields and use them in the ",(0,t.jsx)(n.code,{children:"__call__"})," method."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import Scorer\nfrom mlflow.entities import Feedback\nfrom typing import Optional\n\n\n# Scorer class is a Pydantic object\nclass ResponseQualityScorer(Scorer):\n    # The `name` field is mandatory\n    name: str = "response_quality"\n\n    # Define additional fields\n    min_length: int = 50\n    required_sections: Optional[list[str]] = None\n\n    # Override the __call__ method to implement the scorer logic\n    def __call__(self, outputs: str) -> Feedback:\n        issues = []\n\n        # Check length\n        if len(outputs.split()) < self.min_length:\n            issues.append(f"Too short (minimum {self.min_length} words)")\n\n        # Check required sections\n        missing = [s for s in self.required_sections if s not in outputs]\n        if missing:\n            issues.append(f"Missing sections: {\', \'.join(missing)}")\n\n        if issues:\n            return Feedback(value=False, rationale="; ".join(issues))\n\n        return Feedback(value=True, rationale="Response meets all quality criteria")\n\n\nresponse_quality_scorer = ResponseQualityScorer(\n    required_sections=["# Summary", "# Sources"]\n)\n\n# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\nclass_based_scorer_results = mlflow.genai.evaluate(\n    data=generated_traces, scorers=[response_quality_scorer]\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-7-error-handling-in-scorers",children:"Example 7: Error handling in scorers"}),"\n",(0,t.jsx)(n.p,{children:"The example below demonstrates handling errors in scorers, using two methods:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Handle errors explicitly: You can explicitly identify bad inputs or catch other exceptions and return an ",(0,t.jsx)(n.code,{children:"AssessmentError"})," as ",(0,t.jsx)(n.code,{children:"Feedback"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Let exceptions propagate (recommended): For most errors, it can be best to let MLflow catch the exception. MLflow will create a ",(0,t.jsx)(n.code,{children:"Feedback"})," object with the error details and will continue executing."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mlflow\nfrom mlflow.genai.scorers import scorer\nfrom mlflow.entities import Feedback, AssessmentError\n\n\n@scorer\ndef resilient_scorer(outputs, trace=None):\n    try:\n        response = outputs.get("response")\n        if not response:\n            return Feedback(\n                value=None,\n                error=AssessmentError(\n                    error_code="MISSING_RESPONSE",\n                    error_message="No response field in outputs",\n                ),\n            )\n        # Your evaluation logic\n        return Feedback(value=True, rationale="Valid response")\n    except Exception as e:\n        # Let MLflow handle the error gracefully\n        raise\n\n\n# Evaluation continues even if some scorers fail.\nresults = mlflow.genai.evaluate(data=generated_traces, scorers=[resilient_scorer])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-8-naming-conventions-in-scorers",children:"Example 8: Naming conventions in scorers"}),"\n",(0,t.jsxs)(n.p,{children:["The following examples illustrate the ",(0,t.jsx)(n.a,{href:"/genai/eval-monitor/scorers/custom/#metric-naming-behavior",children:"naming behavior for code-based scorers"}),". The behavior can be summarized as:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["If the scorer returns one or more ",(0,t.jsx)(n.code,{children:"Feedback"})," objects, then ",(0,t.jsx)(n.code,{children:"Feedback.name"})," fields take precedence, if specified."]}),"\n",(0,t.jsxs)(n.li,{children:["For a primitive return value or unnamed ",(0,t.jsx)(n.code,{children:"Feedback"}),", the function name (for the ",(0,t.jsx)(n.code,{children:"@scorer"})," decorator) or the ",(0,t.jsx)(n.code,{children:"Scorer.name"})," field (for the ",(0,t.jsx)(n.code,{children:"Scorer"})," class) is used."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import Scorer, scorer\nfrom mlflow.entities import Feedback\nfrom typing import Optional, Any, List\n\n\n# Primitive value or single `Feedback` without a name: The scorer function name becomes the metric name.\n@scorer\ndef decorator_primitive(outputs: str) -> int:\n    # metric name = "decorator_primitive"\n    return 1\n\n\n@scorer\ndef decorator_unnamed_feedback(outputs: Any) -> Feedback:\n    # metric name = "decorator_unnamed_feedback"\n    return Feedback(value=True, rationale="Good quality")\n\n\n# Single `Feedback` with an explicit name: The name specified in the `Feedback` object is used as the metric name.\n@scorer\ndef decorator_feedback_named(outputs: Any) -> Feedback:\n    # metric name = "decorator_named_feedback"\n    return Feedback(\n        name="decorator_named_feedback",\n        value=True,\n        rationale="Factual accuracy is high",\n    )\n\n\n# Multiple `Feedback` objects: Names specified in each `Feedback` object are preserved.\n# You must specify a unique name for each `Feedback`.\n@scorer\ndef decorator_named_feedbacks(outputs) -> list[Feedback]:\n    return [\n        Feedback(name="decorator_named_feedback_1", value=True, rationale="No errors"),\n        Feedback(name="decorator_named_feedback_2", value=0.9, rationale="Very clear"),\n    ]\n\n\n# Class returning primitive value\nclass ScorerPrimitive(Scorer):\n    # metric name = "scorer_primitive"\n    name: str = "scorer_primitive"\n\n    def __call__(self, outputs: str) -> int:\n        return 1\n\n\nscorer_primitive = ScorerPrimitive()\n\n\n# Class returning a Feedback object without a name\nclass ScorerFeedbackUnnamed(Scorer):\n    # metric name = "scorer_named_feedback"\n    name: str = "scorer_named_feedback"\n\n    def __call__(self, outputs: str) -> Feedback:\n        return Feedback(value=True, rationale="Good")\n\n\nscorer_feedback_unnamed = ScorerFeedbackUnnamed()\n\n\n# Class returning a Feedback object with a name\nclass ScorerFeedbackNamed(Scorer):\n    # metric name = "scorer_named_feedback"\n    name: str = "scorer_feedback_named"\n\n    def __call__(self, outputs: str) -> Feedback:\n        return Feedback(name="scorer_named_feedback", value=True, rationale="Good")\n\n\nscorer_feedback_named = ScorerFeedbackNamed()\n\n\n# Class returning multiple Feedback objects with names\nclass ScorerNamedFeedbacks(Scorer):\n    # metric names = ["scorer_named_feedback_1", "scorer_named_feedback_2"]\n    name: str = "scorer_named_feedbacks"  # Not used\n\n    def __call__(self, outputs: str) -> List[Feedback]:\n        return [\n            Feedback(name="scorer_named_feedback_1", value=True, rationale="Good"),\n            Feedback(name="scorer_named_feedback_2", value=1, rationale="ok"),\n        ]\n\n\nscorer_named_feedbacks = ScorerNamedFeedbacks()\n\nmlflow.genai.evaluate(\n    data=generated_traces,\n    scorers=[\n        decorator_primitive,\n        decorator_unnamed_feedback,\n        decorator_feedback_named,\n        decorator_named_feedbacks,\n        scorer_primitive,\n        scorer_feedback_unnamed,\n        scorer_feedback_named,\n        scorer_named_feedbacks,\n    ],\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-9-chaining-evaluation-results",children:"Example 9: Chaining evaluation results"}),"\n",(0,t.jsxs)(n.p,{children:["If one scorer indicates problems with a subset of traces, you can collect that subset of traces for further iteration using ",(0,t.jsx)(o.B,{fn:"mlflow.search_traces",children:(0,t.jsx)(n.code,{children:"mlflow.search_traces()"})}),'. The example below finds general "Safety" failures and then analyzes the failed subset of traces using a more customized scorer (a toy example of assessment using a content policy document). Alternatively, you might use the subset of problematic traces to iterate on your AI app itself and improve its performance on the challenging inputs.']}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import Safety, Guidelines\n\n# Run initial evaluation\nresults1 = mlflow.genai.evaluate(data=generated_traces, scorers=[Safety()])\n\n# Use results to create refined dataset\ntraces = mlflow.search_traces(run_id=results1.run_id)\n\n# Filter to problematic traces\nsafety_failures = traces[\n    traces["assessments"].apply(\n        lambda x: any(\n            a["assessment_name"] == "Safety" and a["feedback"]["value"] == "no"\n            for a in x\n        )\n    )\n]\n\n# Updated app (not actually updated in this toy example)\nupdated_app = sample_app\n\n# Re-evaluate with different scorers or updated app\nif len(safety_failures) > 0:\n    results2 = mlflow.genai.evaluate(\n        data=safety_failures,\n        predict_fn=updated_app,\n        scorers=[\n            Guidelines(\n                name="content_policy",\n                guidelines="Response must follow our content policy",\n            )\n        ],\n    )\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-10-conditional-logic-with-guidelines",children:"Example 10: Conditional logic with guidelines"}),"\n",(0,t.jsxs)(n.p,{children:["You can wrap ",(0,t.jsx)(n.a,{href:"/genai/eval-monitor/scorers/llm-judge/guidelines",children:"Guidelines judges"})," in custom code-based scorers to apply different guidelines based on user attributes or other context."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from mlflow.genai.scorers import scorer, Guidelines\n\n\n@scorer\ndef premium_service_validator(inputs, outputs, trace=None):\n    """Custom scorer that applies different guidelines based on user tier"""\n\n    # Extract user tier from inputs (could also come from trace)\n    user_tier = inputs.get("user_tier", "standard")\n\n    # Apply different guidelines based on user attributes\n    if user_tier == "premium":\n        # Premium users get more personalized, detailed responses\n        premium_judge = Guidelines(\n            name="premium_experience",\n            guidelines=[\n                "The response must acknowledge the user\'s premium status",\n                "The response must provide detailed explanations with at least 3 specific examples",\n                "The response must offer priority support options (e.g., \'direct line\' or \'dedicated agent\')",\n                "The response must not include any upselling or promotional content",\n            ],\n        )\n        return premium_judge(inputs=inputs, outputs=outputs)\n    else:\n        # Standard users get clear but concise responses\n        standard_judge = Guidelines(\n            name="standard_experience",\n            guidelines=[\n                "The response must be helpful and professional",\n                "The response must be concise (under 100 words)",\n                "The response may mention premium features as upgrade options",\n            ],\n        )\n        return standard_judge(inputs=inputs, outputs=outputs)\n\n\n# Example evaluation data\neval_data = [\n    {\n        "inputs": {"question": "How do I export my data?", "user_tier": "premium"},\n        "outputs": {\n            "response": "As a premium member, you have access to advanced export options. You can export in 5 formats: CSV, Excel, JSON, XML, and PDF. Here\'s how: 1) Go to Settings > Export, 2) Choose your format and date range, 3) Click \'Export Now\'. For immediate assistance, call your dedicated support line at 1-800-PREMIUM."\n        },\n    },\n    {\n        "inputs": {"question": "How do I export my data?", "user_tier": "standard"},\n        "outputs": {\n            "response": "You can export your data as CSV from Settings > Export. Premium users can access additional formats like Excel and PDF."\n        },\n    },\n]\n\n# Run evaluation with the custom scorer\nresults = mlflow.genai.evaluate(data=eval_data, scorers=[premium_service_validator])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,t.jsxs)(c.A,{children:[(0,t.jsx)(i.A,{icon:d.A,title:"Custom LLM judges",description:"Learn about semantic evaluation using LLM judges",href:"/genai/eval-monitor/scorers/llm-judge/custom-judges"}),(0,t.jsx)(i.A,{icon:m.A,title:"Develop code-based scorers",description:"Step through the development workflow for custom scorers",href:"/genai/eval-monitor/scorers/custom/tutorial"}),(0,t.jsx)(i.A,{icon:p.A,title:"Build evaluation datasets",description:"Create test data for your scorers",href:"/genai/datasets/"})]})]})}function x(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(g,{...e})}):g(e)}},75689(e,n,r){r.d(n,{A:()=>i});var s=r(96540);let t=e=>{let n=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,r)=>r?r.toUpperCase():n.toLowerCase());return n.charAt(0).toUpperCase()+n.slice(1)},a=(...e)=>e.filter((e,n,r)=>!!e&&""!==e.trim()&&r.indexOf(e)===n).join(" ").trim();var o={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let l=(0,s.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:r=2,absoluteStrokeWidth:t,className:l="",children:i,iconNode:c,...d},m)=>(0,s.createElement)("svg",{ref:m,...o,width:n,height:n,stroke:e,strokeWidth:t?24*Number(r)/Number(n):r,className:a("lucide",l),...!i&&!(e=>{for(let n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0})(d)&&{"aria-hidden":"true"},...d},[...c.map(([e,n])=>(0,s.createElement)(e,n)),...Array.isArray(i)?i:[i]])),i=(e,n)=>{let r=(0,s.forwardRef)(({className:r,...o},i)=>(0,s.createElement)(l,{ref:i,iconNode:n,className:a(`lucide-${t(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,r),...o}));return r.displayName=t(e),r}},87073(e,n,r){r.d(n,{A:()=>s});let s=(0,r(75689).A)("brain",[["path",{d:"M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z",key:"l5xja"}],["path",{d:"M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z",key:"ep3f8r"}],["path",{d:"M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4",key:"1p4c4q"}],["path",{d:"M17.599 6.5a3 3 0 0 0 .399-1.375",key:"tmeiqw"}],["path",{d:"M6.003 5.125A3 3 0 0 0 6.401 6.5",key:"105sqy"}],["path",{d:"M3.477 10.896a4 4 0 0 1 .585-.396",key:"ql3yin"}],["path",{d:"M19.938 10.5a4 4 0 0 1 .585.396",key:"1qfode"}],["path",{d:"M6 18a4 4 0 0 1-1.967-.516",key:"2e4loj"}],["path",{d:"M19.967 17.484A4 4 0 0 1 18 18",key:"159ez6"}]])},51004(e,n,r){r.d(n,{A:()=>s});let s=(0,r(75689).A)("database",[["ellipse",{cx:"12",cy:"5",rx:"9",ry:"3",key:"msslwz"}],["path",{d:"M3 5V19A9 3 0 0 0 21 19V5",key:"1wlel7"}],["path",{d:"M3 12A9 3 0 0 0 21 12",key:"mv7ke4"}]])},7043(e,n,r){r.d(n,{A:()=>s});let s=(0,r(75689).A)("hammer",[["path",{d:"m15 12-8.373 8.373a1 1 0 1 1-3-3L12 9",key:"eefl8a"}],["path",{d:"m18 15 4-4",key:"16gjal"}],["path",{d:"m21.5 11.5-1.914-1.914A2 2 0 0 1 19 8.172V7l-2.26-2.26a6 6 0 0 0-4.202-1.756L9 2.96l.92.82A6.18 6.18 0 0 1 12 8.4V10l2 2h1.172a2 2 0 0 1 1.414.586L18.5 14.5",key:"b7pghm"}]])},54725(e,n,r){r.d(n,{B:()=>o});var s=r(74848);r(96540);var t=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.agno":"api_reference/python_api/mlflow.agno.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.webhooks":"api_reference/python_api/mlflow.webhooks.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.typescript":"api_reference/typescript_api/index.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}'),a=r(66497);function o({fn:e,children:n,hash:r}){let o=(e=>{let n=e.split(".");for(let e=n.length;e>0;e--){let r=n.slice(0,e).join(".");if(t[r])return r}return null})(e);if(!o)return(0,s.jsx)(s.Fragment,{children:n});let l=(0,a.default)(`/${t[o]}#${r??e}`);return(0,s.jsx)("a",{href:l,target:"_blank",children:n??(0,s.jsxs)("code",{children:[e,"()"]})})}},46077(e,n,r){r.d(n,{A:()=>a});var s=r(74848);r(96540);var t=r(66497);function a({src:e,alt:n,width:r,caption:a,className:o}){return(0,s.jsxs)("div",{className:`container_JwLF ${o||""}`,children:[(0,s.jsx)("div",{className:"imageWrapper_RfGN",style:r?{width:r}:{},children:(0,s.jsx)("img",{src:(0,t.default)(e),alt:n,className:"image_bwOA"})}),a&&(0,s.jsx)("p",{className:"caption_jo2G",children:a})]})}},77541(e,n,r){r.d(n,{A:()=>c});var s=r(74848);r(96540);var t=r(95310),a=r(34164);let o="tileImage_O4So";var l=r(66497),i=r(92802);function c({icon:e,image:n,imageDark:r,imageWidth:c,imageHeight:d,iconSize:m=32,containerHeight:p,title:u,description:h,href:f,linkText:_="Learn more \u2192",className:g}){if(!e&&!n)throw Error("TileCard requires either an icon or image prop");let x=p?{height:`${p}px`}:{},w={};return c&&(w.width=`${c}px`),d&&(w.height=`${d}px`),(0,s.jsxs)(t.A,{href:f,className:(0,a.A)("tileCard_NHsj",g),children:[(0,s.jsx)("div",{className:"tileIcon_pyoR",style:x,children:e?(0,s.jsx)(e,{size:m}):r?(0,s.jsx)(i.A,{sources:{light:(0,l.default)(n),dark:(0,l.default)(r)},alt:u,className:o,style:w}):(0,s.jsx)("img",{src:(0,l.default)(n),alt:u,className:o,style:w})}),(0,s.jsx)("h3",{children:u}),(0,s.jsx)("p",{children:h}),(0,s.jsx)("div",{className:"tileLink_iUbu",children:_})]})}},10440(e,n,r){r.d(n,{A:()=>a});var s=r(74848);r(96540);var t=r(34164);function a({children:e,className:n}){return(0,s.jsx)("div",{className:(0,t.A)("tilesGrid_hB9N",n),children:e})}},28453(e,n,r){r.d(n,{R:()=>o,x:()=>l});var s=r(96540);let t={},a=s.createContext(t);function o(e){let n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);