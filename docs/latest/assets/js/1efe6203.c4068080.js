"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9129],{16734:(e,t,n)=>{n.d(t,{d:()=>r});var a=n(58069);const i="codeBlock_oJcR";var o=n(74848);const r=e=>{let{children:t,executionCount:n}=e;return(0,o.jsx)("div",{style:{flexGrow:1,minWidth:0,marginTop:"var(--padding-md)",width:"100%"},children:(0,o.jsx)(a.A,{className:i,language:"python",children:t})})}},20723:(e,t,n)=>{n.d(t,{O:()=>o});var a=n(96540),i=n(74848);function o(e){let{children:t,href:n}=e;const o=(0,a.useCallback)((async e=>{if(e.preventDefault(),window.gtag)try{window.gtag("event","notebook-download",{href:n})}catch{}const t=await fetch(n),a=await t.blob(),i=window.URL.createObjectURL(a),o=document.createElement("a");o.style.display="none",o.href=i;const r=n.split("/").pop();o.download=r,document.body.appendChild(o),o.click(),window.URL.revokeObjectURL(i),document.body.removeChild(o)}),[n]);return(0,i.jsx)("a",{className:"button button--primary",style:{marginBottom:"1rem",display:"block",width:"min-content"},href:n,download:!0,onClick:o,children:t})}},29940:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>h,default:()=>f,frontMatter:()=>c,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"llms/llm-evaluate/notebooks/huggingface-evaluation-ipynb","title":"Evaluate a Hugging Face LLM with mlflow.evaluate()","description":"Download this notebook","source":"@site/docs/llms/llm-evaluate/notebooks/huggingface-evaluation-ipynb.mdx","sourceDirName":"llms/llm-evaluate/notebooks","slug":"/llms/llm-evaluate/notebooks/huggingface-evaluation","permalink":"/docs/latest/llms/llm-evaluate/notebooks/huggingface-evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb","tags":[],"version":"current","frontMatter":{"custom_edit_url":"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb","slug":"huggingface-evaluation"},"sidebar":"docsSidebar","previous":{"title":"LLM Evaluation Examples","permalink":"/docs/latest/llms/llm-evaluate/notebooks/"},"next":{"title":"LLM Evaluation with MLflow Example Notebook","permalink":"/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation"}}');var i=n(74848),o=n(28453),r=n(16734),l=n(61536),s=n(86563),d=n(20723);const c={custom_edit_url:"https://github.com/mlflow/mlflow/edit/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb",slug:"huggingface-evaluation"},h="Evaluate a Hugging Face LLM with mlflow.evaluate()",u={},m=[{value:"Start MLflow Server",id:"start-mlflow-server",level:3},{value:"Install necessary dependencies",id:"install-necessary-dependencies",level:3},{value:"Load a pretrained Hugging Face pipeline",id:"load-a-pretrained-hugging-face-pipeline",level:3},{value:"Log the model using MLflow",id:"log-the-model-using-mlflow",level:3},{value:"Load Evaluation Data",id:"load-evaluation-data",level:3},{value:"Define Metrics",id:"define-metrics",level:3},{value:"What is an Evaluation Metric?",id:"what-is-an-evaluation-metric",level:4},{value:"Evaluate",id:"evaluate",level:3},{value:"View results",id:"view-results",level:3},{value:"View results in UI",id:"view-results-in-ui",level:4}];function p(e){const t={a:"a",code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsxs)(t.h1,{id:"evaluate-a-hugging-face-llm-with-mlflowevaluate",children:["Evaluate a Hugging Face LLM with ",(0,i.jsx)(t.code,{children:"mlflow.evaluate()"})]})}),"\n",(0,i.jsx)(d.O,{href:"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb",children:"Download this notebook"}),"\n",(0,i.jsxs)(t.p,{children:["This guide will show how to load a pre-trained Hugging Face pipeline, log it to MLflow, and use ",(0,i.jsx)(t.code,{children:"mlflow.evaluate()"})," to evaluate builtin metrics as well as custom LLM-judged metrics for the model."]}),"\n",(0,i.jsxs)(t.p,{children:["For detailed information, please read the documentation on ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/llms/llm-evaluate/index.html",children:"using MLflow evaluate"}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"start-mlflow-server",children:"Start MLflow Server"}),"\n",(0,i.jsx)(t.p,{children:"You can either:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Start a local tracking server by running ",(0,i.jsx)(t.code,{children:"mlflow ui"})," within the same directory that your notebook is in."]}),"\n",(0,i.jsxs)(t.li,{children:["Use a tracking server, as described in ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html",children:"this overview"}),"."]}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"install-necessary-dependencies",children:"Install necessary dependencies"}),"\n",(0,i.jsx)(r.d,{executionCount:" ",children:"%pip install -q mlflow transformers torch torchvision evaluate datasets openai tiktoken fastapi rouge_score textstat"}),"\n",(0,i.jsx)(r.d,{executionCount:2,children:"# Necessary imports\nimport warnings\n\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import pipeline\n\nimport mlflow\nfrom mlflow.metrics.genai import EvaluationExample, answer_correctness, make_genai_metric"}),"\n",(0,i.jsx)(r.d,{executionCount:3,children:'# Disable FutureWarnings\nwarnings.filterwarnings("ignore", category=FutureWarning)'}),"\n",(0,i.jsx)(t.h3,{id:"load-a-pretrained-hugging-face-pipeline",children:"Load a pretrained Hugging Face pipeline"}),"\n",(0,i.jsx)(t.p,{children:"Here we are loading a text generation pipeline, but you can also use either a text summarization or question answering pipeline."}),"\n",(0,i.jsx)(r.d,{executionCount:4,children:'mpt_pipeline = pipeline("text-generation", model="mosaicml/mpt-7b-chat")'}),"\n",(0,i.jsx)(l.p,{children:"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(t.h3,{id:"log-the-model-using-mlflow",children:"Log the model using MLflow"}),"\n",(0,i.jsx)(t.p,{children:'We log our pipeline as an MLflow Model, which follows a standard format that lets you save a model in different "flavors" that can be understood by different downstream tools. In this case, the model is of the transformers "flavor".'}),"\n",(0,i.jsx)(r.d,{executionCount:5,children:'mlflow.set_experiment("Evaluate Hugging Face Text Pipeline")\n\n# Define the signature\nsignature = mlflow.models.infer_signature(\n  model_input="What are the three primary colors?",\n  model_output="The three primary colors are red, yellow, and blue.",\n)\n\n# Log the model using mlflow\nwith mlflow.start_run():\n  model_info = mlflow.transformers.log_model(\n      transformers_model=mpt_pipeline,\n      name="mpt-7b",\n      signature=signature,\n      registered_model_name="mpt-7b-chat",\n  )'}),"\n",(0,i.jsx)(l.p,{isStderr:!0,children:"Successfully registered model 'mpt-7b-chat'.\nCreated version '1' of model 'mpt-7b-chat'."}),"\n",(0,i.jsx)(t.h3,{id:"load-evaluation-data",children:"Load Evaluation Data"}),"\n",(0,i.jsx)(t.p,{children:"Load in a dataset from Hugging Face Hub to use for evaluation."}),"\n",(0,i.jsx)(t.p,{children:"The data fields in the dataset below represent:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"instruction"}),": Describes the task that the model should perform. Each row within the dataset is a unique instruction (task) to be performed."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"input"}),": Optional contextual information that relates to the task defined in the ",(0,i.jsx)(t.code,{children:"instruction"}),' field. For example, for the instruction "Identify the odd one out", the ',(0,i.jsx)(t.code,{children:"input"}),' contextual guidance is given as the list of items to select an outlier from, "Twitter, Instagram, Telegram".']}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"output"}),": The answer to the instruction (with the optional ",(0,i.jsx)(t.code,{children:"input"})," context provided) as generated by the original evaluation model (",(0,i.jsx)(t.code,{children:"text-davinci-003"})," from OpenAI)"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"text"}),": The final total text as a result of applying the ",(0,i.jsx)(t.code,{children:"instruction"}),", ",(0,i.jsx)(t.code,{children:"input"}),", and ",(0,i.jsx)(t.code,{children:"output"})," to the prompt template used, which is sent to the model for fine tuning purposes."]}),"\n"]}),"\n",(0,i.jsx)(r.d,{executionCount:7,children:'dataset = load_dataset("tatsu-lab/alpaca")\neval_df = pd.DataFrame(dataset["train"])\neval_df.head(10)'}),"\n",(0,i.jsx)(s.Q,{children:(0,i.jsx)("div",{dangerouslySetInnerHTML:{__html:'<div>\n<style scoped>\n  .dataframe tbody tr th:only-of-type {\n      vertical-align: middle;\n  }\n\n  .dataframe tbody tr th {\n      vertical-align: top;\n  }\n\n  .dataframe thead th {\n      text-align: right;\n  }\n</style>\n<table border="1" class="dataframe">\n<thead>\n  <tr style="text-align: right;">\n    <th></th>\n    <th>instruction</th>\n    <th>input</th>\n    <th>output</th>\n    <th>text</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <th>0</th>\n    <td>Give three tips for staying healthy.</td>\n    <td></td>\n    <td>1.Eat a balanced diet and make sure to include...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>1</th>\n    <td>What are the three primary colors?</td>\n    <td></td>\n    <td>The three primary colors are red, blue, and ye...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>2</th>\n    <td>Describe the structure of an atom.</td>\n    <td></td>\n    <td>An atom is made up of a nucleus, which contain...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>3</th>\n    <td>How can we reduce air pollution?</td>\n    <td></td>\n    <td>There are a number of ways to reduce air pollu...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>4</th>\n    <td>Describe a time when you had to make a difficu...</td>\n    <td></td>\n    <td>I had to make a difficult decision when I was ...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>5</th>\n    <td>Identify the odd one out.</td>\n    <td>Twitter, Instagram, Telegram</td>\n    <td>Telegram</td>\n    <td>Below is an instruction that describes a task,...</td>\n  </tr>\n  <tr>\n    <th>6</th>\n    <td>Explain why the following fraction is equivale...</td>\n    <td>4/16</td>\n    <td>The fraction 4/16 is equivalent to 1/4 because...</td>\n    <td>Below is an instruction that describes a task,...</td>\n  </tr>\n  <tr>\n    <th>7</th>\n    <td>Write a short story in third person narration ...</td>\n    <td></td>\n    <td>John was at a crossroads in his life. He had j...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>8</th>\n    <td>Render a 3D model of a house</td>\n    <td></td>\n    <td>&lt;nooutput&gt; This type of instruction cannot be ...</td>\n    <td>Below is an instruction that describes a task....</td>\n  </tr>\n  <tr>\n    <th>9</th>\n    <td>Evaluate this sentence for spelling and gramma...</td>\n    <td>He finnished his meal and left the resturant</td>\n    <td>He finished his meal and left the restaurant.</td>\n    <td>Below is an instruction that describes a task,...</td>\n  </tr>\n</tbody>\n</table>\n</div>'}})}),"\n",(0,i.jsx)(t.h3,{id:"define-metrics",children:"Define Metrics"}),"\n",(0,i.jsxs)(t.p,{children:["Since we are evaluating how well our model can provide an answer to a given instruction, we may want to choose some metrics to help measure this on top of any builtin metrics that ",(0,i.jsx)(t.code,{children:"mlflow.evaluate()"})," gives us."]}),"\n",(0,i.jsx)(t.p,{children:"Let's measure how well our model is doing on the following two metrics:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Is the answer correct?"})," Let's use the predefined metric ",(0,i.jsx)(t.code,{children:"answer_correctness"})," here."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Is the answer fluent, clear, and concise?"})," We will define a custom metric ",(0,i.jsx)(t.code,{children:"answer_quality"})," to measure this."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["We will need to pass both of these into the ",(0,i.jsx)(t.code,{children:"extra_metrics"})," argument for ",(0,i.jsx)(t.code,{children:"mlflow.evaluate()"})," in order to assess the quality of our model."]}),"\n",(0,i.jsx)(t.h4,{id:"what-is-an-evaluation-metric",children:"What is an Evaluation Metric?"}),"\n",(0,i.jsxs)(t.p,{children:["An evaluation metric encapsulates any quantitative or qualitative measure you want to calculate for your model. For each model type, ",(0,i.jsx)(t.code,{children:"mlflow.evaluate()"})," will automatically calculate some set of builtin metrics. Refer ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate",children:"here"})," for which builtin metrics will be calculated for each model type. You can also pass in any other metrics you want to calculate as extra metrics. MLflow provides a set of predefined metrics that you can find ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.metrics.html",children:"here"}),", or you can define your own custom metrics. In the example here, we will use the combination of predefined metrics ",(0,i.jsx)(t.code,{children:"mlflow.metrics.genai.answer_correctness"})," and a custom metric for the quality evaluation."]}),"\n",(0,i.jsxs)(t.p,{children:["Let's load our predefined metrics - in this case we are using ",(0,i.jsx)(t.code,{children:"answer_correctness"})," with GPT-4."]}),"\n",(0,i.jsx)(r.d,{executionCount:9,children:'answer_correctness_metric = answer_correctness(model="openai:/gpt-4")'}),"\n",(0,i.jsxs)(t.p,{children:["Now we want to create a custom LLM-judged metric named ",(0,i.jsx)(t.code,{children:"answer_quality"})," using ",(0,i.jsx)(t.code,{children:"make_genai_metric()"}),". We need to define a metric definition and grading rubric, as well as some examples for the LLM judge to use."]}),"\n",(0,i.jsx)(r.d,{executionCount:8,children:'# The definition explains what "answer quality" entails\nanswer_quality_definition = """Please evaluate answer quality for the provided output on the following criteria:\nfluency, clarity, and conciseness. Each of the criteria is defined as follows:\n- Fluency measures how naturally and smooth the output reads.\n- Clarity measures how understandable the output is.\n- Conciseness measures the brevity and efficiency of the output without compromising meaning.\nThe more fluent, clear, and concise a text, the higher the score it deserves.\n"""\n\n# The grading prompt explains what each possible score means\nanswer_quality_grading_prompt = """Answer quality: Below are the details for different scores:\n- Score 1: The output is entirely incomprehensible and cannot be read.\n- Score 2: The output conveys some meaning, but needs lots of improvement in to improve fluency, clarity, and conciseness.\n- Score 3: The output is understandable but still needs improvement.\n- Score 4: The output performs well on two of fluency, clarity, and conciseness, but could be improved on one of these criteria.\n- Score 5: The output reads smoothly, is easy to understand, and clear. There is no clear way to improve the output on these criteria.\n"""\n\n# We provide an example of a "bad" output\nexample1 = EvaluationExample(\n  input="What is MLflow?",\n  output="MLflow is an open-source platform. For managing machine learning workflows, it "\n  "including experiment tracking model packaging versioning and deployment as well as a platform "\n  "simplifying for on the ML lifecycle.",\n  score=2,\n  justification="The output is difficult to understand and demonstrates extremely low clarity. "\n  "However, it still conveys some meaning so this output deserves a score of 2.",\n)\n\n# We also provide an example of a "good" output\nexample2 = EvaluationExample(\n  input="What is MLflow?",\n  output="MLflow is an open-source platform for managing machine learning workflows, including "\n  "experiment tracking, model packaging, versioning, and deployment.",\n  score=5,\n  justification="The output is easily understandable, clear, and concise. It deserves a score of 5.",\n)\n\nanswer_quality_metric = make_genai_metric(\n  name="answer_quality",\n  definition=answer_quality_definition,\n  grading_prompt=answer_quality_grading_prompt,\n  version="v1",\n  examples=[example1, example2],\n  model="openai:/gpt-4",\n  greater_is_better=True,\n)'}),"\n",(0,i.jsx)(t.h3,{id:"evaluate",children:"Evaluate"}),"\n",(0,i.jsx)(t.p,{children:"We need to set our OpenAI API key, since we are using GPT-4 for our LLM-judged metrics."}),"\n",(0,i.jsx)(t.p,{children:"In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.code,{children:"OPENAI_API_KEY=<your openai API key>"})}),"\n",(0,i.jsxs)(t.p,{children:["Now, we can call ",(0,i.jsx)(t.code,{children:"mlflow.evaluate()"}),". Just to test it out, let's use the first 10 rows of the data. Using the ",(0,i.jsx)(t.code,{children:'"text"'})," model type, toxicity and readability metrics are calculated as builtin metrics. We also pass in the two metrics we defined above into the ",(0,i.jsx)(t.code,{children:"extra_metrics"})," parameter to be evaluated."]}),"\n",(0,i.jsx)(r.d,{executionCount:14,children:'with mlflow.start_run():\n  results = mlflow.evaluate(\n      model_info.model_uri,\n      eval_df.head(10),\n      evaluators="default",\n      model_type="text",\n      targets="output",\n      extra_metrics=[answer_correctness_metric, answer_quality_metric],\n      evaluator_config={"col_mapping": {"inputs": "instruction"}},\n  )'}),"\n",(0,i.jsx)(l.p,{children:"Downloading artifacts:   0%|          | 0/79 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(l.p,{isStderr:!0,children:"2023/12/28 11:57:30 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false"}),"\n",(0,i.jsx)(l.p,{children:"Loading checkpoint shards:   0%|          | 0/66 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(l.p,{isStderr:!0,children:"2023/12/28 12:00:25 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/12/28 12:00:25 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/12/28 12:02:23 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\nUsing default facebook/roberta-hate-speech-dynabench-r4-target checkpoint"}),"\n",(0,i.jsx)(l.p,{children:"  0%|          | 0/1 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(l.p,{children:"  0%|          | 0/1 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(l.p,{isStderr:!0,children:"2023/12/28 12:02:43 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/12/28 12:02:43 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/12/28 12:02:44 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/12/28 12:02:44 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/12/28 12:02:44 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_correctness"}),"\n",(0,i.jsx)(l.p,{children:"  0%|          | 0/10 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(l.p,{isStderr:!0,children:"2023/12/28 12:02:53 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_quality"}),"\n",(0,i.jsx)(l.p,{children:"  0%|          | 0/10 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(t.h3,{id:"view-results",children:"View results"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.code,{children:"results.metrics"})," is a dictionary with the aggregate values for all the metrics calculated. Refer ",(0,i.jsx)(t.a,{href:"https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate",children:"here"})," for details on the builtin metrics for each model type."]}),"\n",(0,i.jsx)(r.d,{executionCount:15,children:"results.metrics"}),"\n",(0,i.jsx)(l.p,{children:"{'toxicity/v1/mean': 0.00809656630299287,\n'toxicity/v1/variance': 0.0004603014839856817,\n'toxicity/v1/p90': 0.010559113975614286,\n'toxicity/v1/ratio': 0.0,\n'flesch_kincaid_grade_level/v1/mean': 4.9,\n'flesch_kincaid_grade_level/v1/variance': 6.3500000000000005,\n'flesch_kincaid_grade_level/v1/p90': 6.829999999999998,\n'ari_grade_level/v1/mean': 4.1899999999999995,\n'ari_grade_level/v1/variance': 16.6329,\n'ari_grade_level/v1/p90': 7.949999999999998,\n'answer_correctness/v1/mean': 1.5,\n'answer_correctness/v1/variance': 1.45,\n'answer_correctness/v1/p90': 2.299999999999999,\n'answer_quality/v1/mean': 2.4,\n'answer_quality/v1/variance': 1.44,\n'answer_quality/v1/p90': 4.1}"}),"\n",(0,i.jsxs)(t.p,{children:["We can also view the ",(0,i.jsx)(t.code,{children:"eval_results_table"}),", which shows us the metrics for each row of data."]}),"\n",(0,i.jsx)(r.d,{executionCount:16,children:'results.tables["eval_results_table"]'}),"\n",(0,i.jsx)(l.p,{children:"Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"}),"\n",(0,i.jsx)(s.Q,{children:(0,i.jsx)("div",{dangerouslySetInnerHTML:{__html:'<div>\n<style scoped>\n  .dataframe tbody tr th:only-of-type {\n      vertical-align: middle;\n  }\n\n  .dataframe tbody tr th {\n      vertical-align: top;\n  }\n\n  .dataframe thead th {\n      text-align: right;\n  }\n</style>\n<table border="1" class="dataframe">\n<thead>\n  <tr style="text-align: right;">\n    <th></th>\n    <th>instruction</th>\n    <th>input</th>\n    <th>text</th>\n    <th>output</th>\n    <th>outputs</th>\n    <th>token_count</th>\n    <th>toxicity/v1/score</th>\n    <th>flesch_kincaid_grade_level/v1/score</th>\n    <th>ari_grade_level/v1/score</th>\n    <th>answer_correctness/v1/score</th>\n    <th>answer_correctness/v1/justification</th>\n    <th>answer_quality/v1/score</th>\n    <th>answer_quality/v1/justification</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <th>0</th>\n    <td>Give three tips for staying healthy.</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>1.Eat a balanced diet and make sure to include...</td>\n    <td>Give three tips for staying healthy.\n1. Eat a...</td>\n    <td>19</td>\n    <td>0.000446</td>\n    <td>4.1</td>\n    <td>4.0</td>\n    <td>2</td>\n    <td>The output provided by the model only includes...</td>\n    <td>3</td>\n    <td>The output is understandable and fluent but it...</td>\n  </tr>\n  <tr>\n    <th>1</th>\n    <td>What are the three primary colors?</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>The three primary colors are red, blue, and ye...</td>\n    <td>What are the three primary colors?\nThe three ...</td>\n    <td>19</td>\n    <td>0.000217</td>\n    <td>5.0</td>\n    <td>4.9</td>\n    <td>5</td>\n    <td>The output provided by the model is completely...</td>\n    <td>5</td>\n    <td>The model\'s output is fluent, clear, and conci...</td>\n  </tr>\n  <tr>\n    <th>2</th>\n    <td>Describe the structure of an atom.</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>An atom is made up of a nucleus, which contain...</td>\n    <td>Describe the structure of an atom.\nAn atom is...</td>\n    <td>18</td>\n    <td>0.000139</td>\n    <td>3.1</td>\n    <td>2.2</td>\n    <td>1</td>\n    <td>The output provided by the model is incomplete...</td>\n    <td>2</td>\n    <td>The output is incomplete and lacks clarity, ma...</td>\n  </tr>\n  <tr>\n    <th>3</th>\n    <td>How can we reduce air pollution?</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>There are a number of ways to reduce air pollu...</td>\n    <td>How can we reduce air pollution?\nThere are ma...</td>\n    <td>18</td>\n    <td>0.000140</td>\n    <td>5.0</td>\n    <td>5.5</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>1</td>\n    <td>The output is entirely incomprehensible and ca...</td>\n  </tr>\n  <tr>\n    <th>4</th>\n    <td>Describe a time when you had to make a difficu...</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>I had to make a difficult decision when I was ...</td>\n    <td>Describe a time when you had to make a difficu...</td>\n    <td>18</td>\n    <td>0.000159</td>\n    <td>5.2</td>\n    <td>2.9</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>2</td>\n    <td>The output is incomplete and lacks clarity, ma...</td>\n  </tr>\n  <tr>\n    <th>5</th>\n    <td>Identify the odd one out.</td>\n    <td>Twitter, Instagram, Telegram</td>\n    <td>Below is an instruction that describes a task,...</td>\n    <td>Telegram</td>\n    <td>Identify the odd one out.\n\n1. A car\n2. A tr...</td>\n    <td>18</td>\n    <td>0.072345</td>\n    <td>0.1</td>\n    <td>-5.4</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>2</td>\n    <td>The output is not clear and lacks fluency. The...</td>\n  </tr>\n  <tr>\n    <th>6</th>\n    <td>Explain why the following fraction is equivale...</td>\n    <td>4/16</td>\n    <td>Below is an instruction that describes a task,...</td>\n    <td>The fraction 4/16 is equivalent to 1/4 because...</td>\n    <td>Explain why the following fraction is equivale...</td>\n    <td>23</td>\n    <td>0.000320</td>\n    <td>6.4</td>\n    <td>7.6</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>2</td>\n    <td>The output is not clear and does not answer th...</td>\n  </tr>\n  <tr>\n    <th>7</th>\n    <td>Write a short story in third person narration ...</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>John was at a crossroads in his life. He had j...</td>\n    <td>Write a short story in third person narration ...</td>\n    <td>20</td>\n    <td>0.000247</td>\n    <td>10.7</td>\n    <td>11.1</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>1</td>\n    <td>The output is exactly the same as the input, a...</td>\n  </tr>\n  <tr>\n    <th>8</th>\n    <td>Render a 3D model of a house</td>\n    <td></td>\n    <td>Below is an instruction that describes a task....</td>\n    <td>&lt;nooutput&gt; This type of instruction cannot be ...</td>\n    <td>Render a 3D model of a house in Blender - Blen...</td>\n    <td>19</td>\n    <td>0.003694</td>\n    <td>5.2</td>\n    <td>2.7</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>2</td>\n    <td>The output is partially understandable but lac...</td>\n  </tr>\n  <tr>\n    <th>9</th>\n    <td>Evaluate this sentence for spelling and gramma...</td>\n    <td>He finnished his meal and left the resturant</td>\n    <td>Below is an instruction that describes a task,...</td>\n    <td>He finished his meal and left the restaurant.</td>\n    <td>Evaluate this sentence for spelling and gramma...</td>\n    <td>18</td>\n    <td>0.003260</td>\n    <td>4.2</td>\n    <td>6.4</td>\n    <td>1</td>\n    <td>The output provided by the model is completely...</td>\n    <td>4</td>\n    <td>The output is fluent and clear, but it is not ...</td>\n  </tr>\n</tbody>\n</table>\n</div>'}})}),"\n",(0,i.jsx)(t.h4,{id:"view-results-in-ui",children:"View results in UI"}),"\n",(0,i.jsx)(t.p,{children:'Finally, we can view our evaluation results in the MLflow UI. We can select our experiment on the left sidebar, which will bring us to the following page. We can see that one run logged our model "mpt-7b-chat", and the other run has the dataset we evaluated.'}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://i.imgur.com/alymcBq.png",alt:"Evaluation Main"})}),"\n",(0,i.jsx)(t.p,{children:"We click on the Evaluation tab and hide any irrelevant runs."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://i.imgur.com/sr7R9TL.png",alt:"Evaluation Filtering"})}),"\n",(0,i.jsx)(t.p,{children:"We can now choose what columns we want to group by, as well as which column we want to compare. In the following example, we are looking at the score for answer correctness for each input-output pair, but we could choose any other metric to compare."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://i.imgur.com/AeoYMEt.png",alt:"Evaluation Selection"})}),"\n",(0,i.jsx)(t.p,{children:"Finally, we get to the following view, where we can see the justification and score for answer correctness for each row."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://i.imgur.com/axsHZxP.png",alt:"Evaluation Comparison"})})]})}function f(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},61536:(e,t,n)=>{n.d(t,{p:()=>i});var a=n(74848);const i=e=>{let{children:t,isStderr:n}=e;return(0,a.jsx)("pre",{style:{margin:0,borderRadius:0,background:"none",fontSize:"0.85rem",flexGrow:1,padding:"var(--padding-sm)"},children:t})}},86563:(e,t,n)=>{n.d(t,{Q:()=>i});var a=n(74848);const i=e=>{let{children:t}=e;return(0,a.jsx)("div",{style:{flexGrow:1,minWidth:0,fontSize:"0.8rem",width:"100%"},children:t})}}}]);