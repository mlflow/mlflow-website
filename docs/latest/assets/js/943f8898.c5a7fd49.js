"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9612],{9318:(e,l,a)=>{a.d(l,{A:()=>n});const n=a.p+"assets/images/llamaindex-tracing-67ed751e565ef74209381a497c70cf18.gif"},10493:(e,l,a)=>{a.d(l,{Zp:()=>s,AC:()=>i,WO:()=>p,_C:()=>m,$3:()=>d,jK:()=>c});var n=a(34164);const t={CardGroup:"CardGroup_P84T",NoGap:"NoGap_O9Dj",MaxThreeColumns:"MaxThreeColumns_FO1r",AutofillColumns:"AutofillColumns_fKhQ",Card:"Card_aSCR",CardBordered:"CardBordered_glGF",CardBody:"CardBody_BhRs",TextColor:"TextColor_a8Tp",BoxRoot:"BoxRoot_Etgr",FlexWrapNowrap:"FlexWrapNowrap_f60k",FlexJustifyContentFlexStart:"FlexJustifyContentFlexStart_ZYv5",FlexDirectionRow:"FlexDirectionRow_T2qL",FlexAlignItemsCenter:"FlexAlignItemsCenter_EHVM",FlexFlex:"FlexFlex__JTE",Link:"Link_fVkl",MarginLeft4:"MarginLeft4_YQSJ",MarginTop4:"MarginTop4_jXKN",PaddingBottom4:"PaddingBottom4_O9gt",LogoCardContent:"LogoCardContent_kCQm",LogoCardImage:"LogoCardImage_JdcX",SmallLogoCardContent:"SmallLogoCardContent_LxhV",SmallLogoCardRounded:"SmallLogoCardRounded_X50_",SmallLogoCardImage:"SmallLogoCardImage_tPZl",NewFeatureCardContent:"NewFeatureCardContent_Rq3d",NewFeatureCardHeading:"NewFeatureCardHeading_f6q3",NewFeatureCardHeadingSeparator:"NewFeatureCardHeadingSeparator_pSx8",NewFeatureCardTags:"NewFeatureCardTags_IFHO",NewFeatureCardWrapper:"NewFeatureCardWrapper_NQ0k",TitleCardContent:"TitleCardContent_l9MQ",TitleCardTitle:"TitleCardTitle__K8J",TitleCardSeparator:"TitleCardSeparator_IN2E",Cols1:"Cols1_Gr2U",Cols2:"Cols2_sRvc",Cols3:"Cols3_KjUS",Cols4:"Cols4_dKOj",Cols5:"Cols5_jDmj",Cols6:"Cols6_Q0OR"};var o=a(28774),r=a(74848);const i=({children:e,isSmall:l,cols:a,noGap:o})=>(0,r.jsx)("div",{className:(0,n.A)(t.CardGroup,l?t.AutofillColumns:a?t[`Cols${a}`]:t.MaxThreeColumns,o&&t.NoGap),children:e}),s=({children:e,link:l=""})=>l?(0,r.jsx)(o.A,{className:(0,n.A)(t.Link,t.Card,t.CardBordered),to:l,children:e}):(0,r.jsx)("div",{className:(0,n.A)(t.Card,t.CardBordered),children:e}),m=({headerText:e,link:l,text:a})=>(0,r.jsx)(s,{link:l,children:(0,r.jsxs)("span",{children:[(0,r.jsx)("div",{className:(0,n.A)(t.CardTitle,t.BoxRoot,t.PaddingBottom4),style:{pointerEvents:"none"},children:(0,r.jsx)("div",{className:(0,n.A)(t.BoxRoot,t.FlexFlex,t.FlexAlignItemsCenter,t.FlexDirectionRow,t.FlexJustifyContentFlexStart,t.FlexWrapNowrap),style:{marginLeft:"-4px",marginTop:"-4px"},children:(0,r.jsx)("div",{className:(0,n.A)(t.BoxRoot,t.BoxHideIfEmpty,t.MarginTop4,t.MarginLeft4),style:{pointerEvents:"auto"},children:(0,r.jsx)("span",{className:"",children:e})})})}),(0,r.jsx)("span",{className:(0,n.A)(t.TextColor,t.CardBody),children:(0,r.jsx)("p",{children:a})})]})}),p=({description:e,children:l,link:a})=>(0,r.jsx)(s,{link:a,children:(0,r.jsxs)("div",{className:t.LogoCardContent,children:[(0,r.jsx)("div",{className:t.LogoCardImage,children:l}),(0,r.jsx)("p",{className:t.TextColor,children:e})]})}),d=({children:e,link:l})=>(0,r.jsx)("div",{className:(0,n.A)(t.Card,t.CardBordered,t.SmallLogoCardRounded),children:l?(0,r.jsx)(o.A,{className:(0,n.A)(t.Link),to:l,children:(0,r.jsx)("div",{className:t.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,n.A)("max-height-img-container",t.SmallLogoCardImage),children:e})})}):(0,r.jsx)("div",{className:t.SmallLogoCardContent,children:(0,r.jsx)("div",{className:(0,n.A)("max-height-img-container",t.SmallLogoCardImage),children:e})})}),c=({title:e,description:l,link:a=""})=>(0,r.jsx)(s,{link:a,children:(0,r.jsxs)("div",{className:t.TitleCardContent,children:[(0,r.jsx)("div",{className:(0,n.A)(t.TitleCardTitle),style:{textAlign:"left",fontWeight:"bold"},children:e}),(0,r.jsx)("hr",{className:(0,n.A)(t.TitleCardSeparator),style:{margin:"12px 0"}}),(0,r.jsx)("p",{className:(0,n.A)(t.TextColor),children:l})]})})},25584:(e,l,a)=>{a.r(l),a.d(l,{assets:()=>m,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"tracing/integrations/listing/llama_index","title":"Tracing LlamaIndex\ud83e\udd99","description":"LlamaIndex Tracing via autolog","source":"@site/docs/genai/tracing/integrations/listing/llama_index.mdx","sourceDirName":"tracing/integrations/listing","slug":"/tracing/integrations/listing/llama_index","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/llama_index","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"sidebar_label":"LlamaIndex"},"sidebar":"genAISidebar","previous":{"title":"OpenAI Agents SDK","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/openai-agent"},"next":{"title":"Bedrock","permalink":"/mlflow-website/docs/latest/genai/tracing/integrations/listing/bedrock"}}');var t=a(74848),o=a(28453),r=a(49374);a(10493),a(14252),a(11470),a(19365);const i={sidebar_position:4,sidebar_label:"LlamaIndex"},s="Tracing LlamaIndex\ud83e\udd99",m={},p=[{value:"Example Usage",id:"example-usage",level:3},{value:"Token usage",id:"token-usage",level:2},{value:"LlamaIndex workflow",id:"llamaindex-workflow",level:3},{value:"Disable auto-tracing",id:"disable-auto-tracing",level:3}];function d(e){const l={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(l.header,{children:(0,t.jsx)(l.h1,{id:"tracing-llamaindex",children:"Tracing LlamaIndex\ud83e\udd99"})}),"\n",(0,t.jsx)(l.p,{children:(0,t.jsx)(l.img,{alt:"LlamaIndex Tracing via autolog",src:a(9318).A+"",width:"1248",height:"720"})}),"\n",(0,t.jsxs)(l.p,{children:[(0,t.jsx)(l.a,{href:"https://www.llamaindex.ai/",children:"LlamaIndex"})," is an open-source framework for building agentic generative AI applications that allow large language models to work with your data in any format."]}),"\n",(0,t.jsxs)(l.p,{children:[(0,t.jsx)(l.a,{href:"/genai/tracing",children:"MLflow Tracing"})," provides automatic tracing capability for LlamaIndex. You can enable tracing\nfor LlamaIndex by calling the ",(0,t.jsx)(r.B,{fn:"mlflow.llama_index.autolog"})," function, and nested traces are automatically logged to the active MLflow Experiment upon invocation of LlamaIndex engines and workflows."]}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:"import mlflow\n\nmlflow.llama_index.autolog()\n"})}),"\n",(0,t.jsx)(l.admonition,{type:"tip",children:(0,t.jsxs)(l.p,{children:["MLflow LlamaIndex integration is not only about tracing. MLflow offers full tracking experience for LlamaIndex, including model tracking, index management, and evaluation. Please checkout the ",(0,t.jsx)(l.strong,{children:(0,t.jsx)(l.a,{href:"/genai/flavors/llama-index",children:"MLflow LlamaIndex Flavor"})})," to learn more!"]})}),"\n",(0,t.jsx)(l.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,t.jsx)(l.p,{children:"First, let's download a test data to create a toy index:"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{children:"!mkdir -p data\n!curl -L https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -o ./data/paul_graham_essay.txt\n"})}),"\n",(0,t.jsx)(l.p,{children:"Load them into a simple in-memory vector index:"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{children:'from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader("data").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n'})}),"\n",(0,t.jsx)(l.p,{children:"Now you can enable LlamaIndex auto tracing and start querying the index:"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'import mlflow\n\n# Enabling tracing for LlamaIndex\nmlflow.llama_index.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri("http://localhost:5000")\nmlflow.set_experiment("LlamaIndex")\n\n# Query the index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What was the first program the author wrote?")\n'})}),"\n",(0,t.jsx)(l.h2,{id:"token-usage",children:"Token usage"}),"\n",(0,t.jsxs)(l.p,{children:["MLflow >= 3.2.0 supports token usage tracking for LlamaIndex. The token usage for each LLM call will be logged in the ",(0,t.jsx)(l.code,{children:"mlflow.chat.tokenUsage"})," attribute. The total token usage throughout the trace will be\navailable in the ",(0,t.jsx)(l.code,{children:"token_usage"})," field of the trace info object."]}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-python",children:'import json\nimport mlflow\nfrom llama_index.llms.openai import OpenAI\n\nmlflow.llama_index.autolog()\n\n# Use the chat complete method to create new chat.\nllm = OpenAI(model="gpt-3.5-turbo")\nSettings.llm = llm\nresponse = llm.chat(\n    [ChatMessage(role="user", content="What is the capital of France?")]\n)\n\n# Get the trace object just created\nlast_trace_id = mlflow.get_last_active_trace_id()\ntrace = mlflow.get_trace(trace_id=last_trace_id)\n\n# Print the token usage\ntotal_usage = trace.info.token_usage\nprint("== Total token usage: ==")\nprint(f"  Input tokens: {total_usage[\'input_tokens\']}")\nprint(f"  Output tokens: {total_usage[\'output_tokens\']}")\nprint(f"  Total tokens: {total_usage[\'total_tokens\']}")\n\n# Print the token usage for each LLM call\nprint("\\n== Detailed usage for each LLM call: ==")\nfor span in trace.data.spans:\n    if usage := span.get_attribute("mlflow.chat.tokenUsage"):\n        print(f"{span.name}:")\n        print(f"  Input tokens: {usage[\'input_tokens\']}")\n        print(f"  Output tokens: {usage[\'output_tokens\']}")\n        print(f"  Total tokens: {usage[\'total_tokens\']}")\n'})}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-bash",children:"== Total token usage: ==\n  Input tokens: 14\n  Output tokens: 7\n  Total tokens: 21\n\n== Detailed usage for each LLM call: ==\nOpenAI.chat:\n  Input tokens: 14\n  Output tokens: 7\n  Total tokens: 21\n"})}),"\n",(0,t.jsx)(l.h3,{id:"llamaindex-workflow",children:"LlamaIndex workflow"}),"\n",(0,t.jsxs)(l.p,{children:["The ",(0,t.jsx)(l.code,{children:"Workflow"})," is LlamaIndex's next-generation GenAI orchestration framework. It is designed as a flexible and interpretable framework for building arbitrary LLM applications such as an agent, a RAG flow, a data extraction pipeline, etc. MLflow supports tracking, evaluating, and tracing the Workflow objects, which makes them more observable and maintainable."]}),"\n",(0,t.jsxs)(l.p,{children:["Automatic tracing for LlamaIndex workflow works off-the-shelf by calling the same ",(0,t.jsx)(l.code,{children:"mlflow.llama_index.autolog()"}),"."]}),"\n",(0,t.jsx)(l.p,{children:"To learn more about MLflow's integration with LlamaIndex Workflow, continue to the following tutorials:"}),"\n",(0,t.jsxs)(l.ul,{children:["\n",(0,t.jsx)(l.li,{children:(0,t.jsx)(l.a,{href:"https://mlflow.org/blog/mlflow-llama-index-workflow",children:"Building Advanced RAG with MLflow and LlamaIndex Workflow"})}),"\n"]}),"\n",(0,t.jsx)(l.h3,{id:"disable-auto-tracing",children:"Disable auto-tracing"}),"\n",(0,t.jsxs)(l.p,{children:["Auto tracing for LlamaIndex can be disabled globally by calling ",(0,t.jsx)(l.code,{children:"mlflow.llama_index.autolog(disable=True)"})," or ",(0,t.jsx)(l.code,{children:"mlflow.autolog(disable=True)"}),"."]})]})}function c(e={}){const{wrapper:l}={...(0,o.R)(),...e.components};return l?(0,t.jsx)(l,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},49374:(e,l,a)=>{a.d(l,{B:()=>i});a(96540);const n=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var t=a(86025),o=a(74848);const r=e=>{const l=e.split(".");for(let a=l.length;a>0;a--){const e=l.slice(0,a).join(".");if(n[e])return e}return null};function i({fn:e,children:l,hash:a}){const i=r(e);if(!i)return(0,o.jsx)(o.Fragment,{children:l});const s=(0,t.Ay)(`/${n[i]}#${a??e}`);return(0,o.jsx)("a",{href:s,target:"_blank",children:l??(0,o.jsxs)("code",{children:[e,"()"]})})}}}]);