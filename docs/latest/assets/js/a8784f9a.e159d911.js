"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([["1114"],{32704(e,t,n){n.r(t),n.d(t,{metadata:()=>r,default:()=>f,frontMatter:()=>c,contentTitle:()=>u,toc:()=>h,assets:()=>m});var r=JSON.parse('{"id":"prompt-registry/structured-output","title":"Structured Output","description":"Learn how to define structured output schemas for your prompts to ensure consistent and validated responses from language models.","source":"@site/docs/genai/prompt-registry/structured-output.mdx","sourceDirName":"prompt-registry","slug":"/prompt-registry/structured-output","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/structured-output","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Structured Output","description":"Learn how to define structured output schemas for your prompts to ensure consistent and validated responses from language models."},"sidebar":"genAISidebar","previous":{"title":"Log Prompts with Models","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/log-with-model"},"next":{"title":"Optimize Prompts","permalink":"/mlflow-website/docs/latest/genai/prompt-registry/optimize-prompts"}}'),s=n(74848),o=n(28453),a=n(10440),i=n(77541),p=n(25005),l=n(93164),d=n(43414);let c={title:"Structured Output",description:"Learn how to define structured output schemas for your prompts to ensure consistent and validated responses from language models."},u="Structured Output",m={},h=[{value:"Overview",id:"overview",level:2},{value:"Basic Usage",id:"basic-usage",level:2},{value:"Using Pydantic Models",id:"using-pydantic-models",level:3},{value:"Using JSON Schema",id:"using-json-schema",level:3},{value:"Advanced Examples",id:"advanced-examples",level:2},{value:"Complex Response Formats",id:"complex-response-formats",level:3},{value:"Chat Prompts with Structured Output",id:"chat-prompts-with-structured-output",level:3},{value:"Loading and Using Structured Prompts",id:"loading-and-using-structured-prompts",level:2},{value:"Integration with Language Models",id:"integration-with-language-models",level:2},{value:"OpenAI Integration",id:"openai-integration",level:3},{value:"LangChain Integration",id:"langchain-integration",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function g(e){let t={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"structured-output",children:"Structured Output"})}),"\n",(0,s.jsx)(t.p,{children:"MLflow Prompt Registry supports defining structured output schemas for your prompts, ensuring that language model responses follow a consistent format and can be validated. This feature is particularly useful for applications that need to parse and process model outputs programmatically."}),"\n",(0,s.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(t.p,{children:"Structured output allows you to:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Define expected response formats"})," using Pydantic models or JSON schemas"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Validate model responses"})," against your defined schema"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Ensure consistency"})," across different model calls"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Improve integration"})," with downstream applications"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Enable type safety"})," in your GenAI applications"]}),"\n"]}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Important"}),": The ",(0,s.jsx)(t.code,{children:"response_format"})," parameter is used for ",(0,s.jsx)(t.strong,{children:"tracking and documentation purposes"})," rather than direct runtime enforcement. MLflow stores this information as metadata to help you understand the expected output structure of your prompts, but it does not automatically validate or enforce the format during model execution. You are responsible for implementing the actual validation and enforcement in your application code."]})}),"\n",(0,s.jsx)(t.h2,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,s.jsx)(t.h3,{id:"using-pydantic-models",children:"Using Pydantic Models"}),"\n",(0,s.jsx)(t.p,{children:"The most common way to define structured output is using Pydantic models:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass SummaryResponse(BaseModel):\n    summary: str\n    key_points: List[str]\n    word_count: int\n\n\n# Register prompt with structured output\nprompt = mlflow.genai.register_prompt(\n    name="summarization-prompt",\n    template="Summarize the following text in {{ num_sentences }} sentences: {{ text }}",\n    response_format=SummaryResponse,\n    commit_message="Added structured output for summarization",\n    tags={"task": "summarization", "structured": "true"},\n)\n'})}),"\n",(0,s.jsx)(t.h3,{id:"using-json-schema",children:"Using JSON Schema"}),"\n",(0,s.jsx)(t.p,{children:"You can also define response formats using JSON schema dictionaries:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\n\n# Define response format as JSON schema\nresponse_schema = {\n    "type": "object",\n    "properties": {\n        "answer": {"type": "string", "description": "The main answer"},\n        "confidence": {"type": "number", "description": "Confidence score (0-1)"},\n        "sources": {\n            "type": "array",\n            "items": {"type": "string"},\n            "description": "List of source references",\n        },\n    },\n    "required": ["answer", "confidence"],\n}\n\n# Register prompt with JSON schema\nprompt = mlflow.genai.register_prompt(\n    name="qa-prompt",\n    template="Answer the following question: {{ question }}",\n    response_format=response_schema,\n    commit_message="Added structured output for Q&A",\n    tags={"task": "qa", "structured": "true"},\n)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"advanced-examples",children:"Advanced Examples"}),"\n",(0,s.jsx)(t.h3,{id:"complex-response-formats",children:"Complex Response Formats"}),"\n",(0,s.jsx)(t.p,{children:"For more complex applications, you can define nested structures:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom datetime import datetime\n\n\nclass AnalysisResult(BaseModel):\n    sentiment: str\n    confidence: float\n    entities: List[str]\n    summary: str\n\n\nclass DocumentAnalysis(BaseModel):\n    document_id: str\n    analysis: AnalysisResult\n    processed_at: datetime\n    metadata: Optional[dict] = None\n\n\n# Register prompt with complex structured output\nprompt = mlflow.genai.register_prompt(\n    name="document-analyzer",\n    template="Analyze the following document: {{ document_text }}",\n    response_format=DocumentAnalysis,\n    commit_message="Added comprehensive document analysis output",\n    tags={"task": "analysis", "complex": "true"},\n)\n'})}),"\n",(0,s.jsx)(t.h3,{id:"chat-prompts-with-structured-output",children:"Chat Prompts with Structured Output"}),"\n",(0,s.jsx)(t.p,{children:"Chat prompts can also use structured output formats:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import mlflow\nfrom pydantic import BaseModel\n\n\nclass ChatResponse(BaseModel):\n    response: str\n    tone: str\n    suggestions: List[str]\n\n\n# Chat prompt with structured output\nchat_template = [\n    {"role": "system", "content": "You are a helpful {{ style }} assistant."},\n    {"role": "user", "content": "{{ question }}"},\n]\n\nprompt = mlflow.genai.register_prompt(\n    name="assistant-chat",\n    template=chat_template,\n    response_format=ChatResponse,\n    commit_message="Added structured output for chat responses",\n    tags={"type": "chat", "structured": "true"},\n)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"loading-and-using-structured-prompts",children:"Loading and Using Structured Prompts"}),"\n",(0,s.jsx)(t.p,{children:"When you load a prompt with structured output, you can access the response format for tracking and documentation purposes:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'# Load the prompt\nprompt = mlflow.genai.load_prompt("prompts:/summarization-prompt/1")\n\n# Check if it has structured output (for tracking purposes)\nif prompt.response_format:\n    print(f"Response format: {prompt.response_format}")\n\n# Format the prompt\nformatted_text = prompt.format(num_sentences=3, text="Your content here...")\n\n# Use with a language model that supports structured output\n# Note: You need to implement validation against your defined schema\n'})}),"\n",(0,s.jsx)(t.h2,{id:"integration-with-language-models",children:"Integration with Language Models"}),"\n",(0,s.jsx)(t.h3,{id:"openai-integration",children:"OpenAI Integration"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import openai\n\nclient = openai.OpenAI()\n\n# Load prompt with structured output\nprompt = mlflow.genai.load_prompt("prompts:/summarization-prompt/1")\n\n# Use with OpenAI\'s response_format parameter\nresponse = client.chat.completions.create(\n    model="gpt-4.1",\n    messages=[\n        {"role": "user", "content": prompt.format(num_sentences=3, text="Your text")}\n    ],\n    response_format=prompt.response_format,  # OpenAI\'s structured output\n)\n\n# Get structured output\nimport json\n\nresult = json.loads(response.choices[0].message.content)\n'})}),"\n",(0,s.jsx)(t.h3,{id:"langchain-integration",children:"LangChain Integration"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\n\n# Load prompt with structured output\nprompt = mlflow.genai.load_prompt("prompts:/qa-prompt/1")\n\n# Create LangChain prompt template\nlangchain_prompt = PromptTemplate.from_template(prompt.template)\n\n# Use with LangChain\'s structured output\nllm = ChatOpenAI(model="gpt-4")\nchain = langchain_prompt | llm.with_structured_output(prompt.response_format)\n\n# Execute the chain\nresult = chain.invoke({"question": "What is MLflow?"})\n# result will be a validated Pydantic model instance\n'})}),"\n",(0,s.jsx)(t.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Structured output"})," is used for ",(0,s.jsx)(t.strong,{children:"tracking and documentation purposes"})," to define expected response formats"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Pydantic models"})," provide type safety and validation schemas for your response formats"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"JSON schemas"})," offer flexibility for complex nested structures"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Integration"})," with popular frameworks like OpenAI and LangChain is straightforward"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Manual validation"})," is required in your application code - MLflow does not enforce the format at runtime"]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsx)(i.A,{icon:p.A,title:"Create and Edit Prompts",description:"Learn the basics of prompt management in MLflow",href:"/genai/prompt-registry/create-and-edit-prompts",linkText:"Manage prompts \u2192"}),(0,s.jsx)(i.A,{icon:l.A,title:"Use Prompts in Apps",description:"See how to integrate prompts into your applications",href:"/genai/prompt-registry/use-prompts-in-apps",linkText:"Integrate prompts \u2192"}),(0,s.jsx)(i.A,{icon:d.A,title:"Evaluate Prompts",description:"Learn how to assess prompt performance with evaluations",href:"/genai/prompt-registry/evaluate-prompts",linkText:"Evaluate prompts \u2192"})]})]})}function f(e={}){let{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(g,{...e})}):g(e)}},75689(e,t,n){n.d(t,{A:()=>p});var r=n(96540);let s=e=>{let t=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,n)=>n?n.toUpperCase():t.toLowerCase());return t.charAt(0).toUpperCase()+t.slice(1)},o=(...e)=>e.filter((e,t,n)=>!!e&&""!==e.trim()&&n.indexOf(e)===t).join(" ").trim();var a={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let i=(0,r.forwardRef)(({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:s,className:i="",children:p,iconNode:l,...d},c)=>(0,r.createElement)("svg",{ref:c,...a,width:t,height:t,stroke:e,strokeWidth:s?24*Number(n)/Number(t):n,className:o("lucide",i),...!p&&!(e=>{for(let t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0})(d)&&{"aria-hidden":"true"},...d},[...l.map(([e,t])=>(0,r.createElement)(e,t)),...Array.isArray(p)?p:[p]])),p=(e,t)=>{let n=(0,r.forwardRef)(({className:n,...a},p)=>(0,r.createElement)(i,{ref:p,iconNode:t,className:o(`lucide-${s(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,n),...a}));return n.displayName=s(e),n}},93164(e,t,n){n.d(t,{A:()=>r});let r=(0,n(75689).A)("code",[["path",{d:"m16 18 6-6-6-6",key:"eg8j8"}],["path",{d:"m8 6-6 6 6 6",key:"ppft3o"}]])},43414(e,t,n){n.d(t,{A:()=>r});let r=(0,n(75689).A)("flask-conical",[["path",{d:"M14 2v6a2 2 0 0 0 .245.96l5.51 10.08A2 2 0 0 1 18 22H6a2 2 0 0 1-1.755-2.96l5.51-10.08A2 2 0 0 0 10 8V2",key:"18mbvz"}],["path",{d:"M6.453 15h11.094",key:"3shlmq"}],["path",{d:"M8.5 2h7",key:"csnxdl"}]])},25005(e,t,n){n.d(t,{A:()=>r});let r=(0,n(75689).A)("pen-line",[["path",{d:"M12 20h9",key:"t2du7b"}],["path",{d:"M16.376 3.622a1 1 0 0 1 3.002 3.002L7.368 18.635a2 2 0 0 1-.855.506l-2.872.838a.5.5 0 0 1-.62-.62l.838-2.872a2 2 0 0 1 .506-.854z",key:"1ykcvy"}]])},77541(e,t,n){n.d(t,{A:()=>l});var r=n(74848);n(96540);var s=n(95310),o=n(34164);let a="tileImage_O4So";var i=n(66497),p=n(92802);function l({icon:e,image:t,imageDark:n,imageWidth:l,imageHeight:d,iconSize:c=32,containerHeight:u,title:m,description:h,href:g,linkText:f="Learn more \u2192",className:x}){if(!e&&!t)throw Error("TileCard requires either an icon or image prop");let y=u?{height:`${u}px`}:{},w={};return l&&(w.width=`${l}px`),d&&(w.height=`${d}px`),(0,r.jsxs)(s.A,{href:g,className:(0,o.A)("tileCard_NHsj",x),children:[(0,r.jsx)("div",{className:"tileIcon_pyoR",style:y,children:e?(0,r.jsx)(e,{size:c}):n?(0,r.jsx)(p.A,{sources:{light:(0,i.default)(t),dark:(0,i.default)(n)},alt:m,className:a,style:w}):(0,r.jsx)("img",{src:(0,i.default)(t),alt:m,className:a,style:w})}),(0,r.jsx)("h3",{children:m}),(0,r.jsx)("p",{children:h}),(0,r.jsx)("div",{className:"tileLink_iUbu",children:f})]})}},10440(e,t,n){n.d(t,{A:()=>o});var r=n(74848);n(96540);var s=n(34164);function o({children:e,className:t}){return(0,r.jsx)("div",{className:(0,s.A)("tilesGrid_hB9N",t),children:e})}},28453(e,t,n){n.d(t,{R:()=>a,x:()=>i});var r=n(96540);let s={},o=r.createContext(s);function a(e){let t=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(o.Provider,{value:t},e.children)}}}]);