/*! For license information please see a62aed86.9697cd43.js.LICENSE.txt */
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6468],{6789:(e,n,r)=>{r.d(n,{A:()=>c});r(96540);var t=r(28774),a=r(34164);const l={tileCard:"tileCard_NHsj",tileIcon:"tileIcon_pyoR",tileLink:"tileLink_iUbu",tileImage:"tileImage_O4So"};var o=r(86025),s=r(21122),i=r(74848);function c({icon:e,image:n,imageDark:r,imageWidth:c,imageHeight:p,iconSize:d=32,containerHeight:u,title:m,description:h,href:f,linkText:x="Learn more \u2192",className:g}){if(!e&&!n)throw new Error("TileCard requires either an icon or image prop");const y=u?{height:`${u}px`}:{},_={};return c&&(_.width=`${c}px`),p&&(_.height=`${p}px`),(0,i.jsxs)(t.A,{href:f,className:(0,a.A)(l.tileCard,g),children:[(0,i.jsx)("div",{className:l.tileIcon,style:y,children:e?(0,i.jsx)(e,{size:d}):r?(0,i.jsx)(s.A,{sources:{light:(0,o.Ay)(n),dark:(0,o.Ay)(r)},alt:m,className:l.tileImage,style:_}):(0,i.jsx)("img",{src:(0,o.Ay)(n),alt:m,className:l.tileImage,style:_})}),(0,i.jsx)("h3",{children:m}),(0,i.jsx)("p",{children:h}),(0,i.jsx)("div",{className:l.tileLink,children:x})]})}},11470:(e,n,r)=>{r.d(n,{A:()=>b});var t=r(96540),a=r(34164),l=r(23104),o=r(56347),s=r(205),i=r(57485),c=r(31682),p=r(70679);function d(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:r}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:r,default:t}})=>({value:e,label:n,attributes:r,default:t}))}(r);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,r])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const r=(0,o.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(a),(0,t.useCallback)(e=>{if(!a)return;const n=new URLSearchParams(r.location.search);n.set(a,e),r.replace({...r.location,search:n.toString()})},[a,r])]}function f(e){const{defaultValue:n,queryString:r=!1,groupId:a}=e,l=u(e),[o,i]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const r=n.find(e=>e.default)??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:l})),[c,d]=h({queryString:r,groupId:a}),[f,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[r,a]=(0,p.Dv)(n);return[r,(0,t.useCallback)(e=>{n&&a.set(e)},[n,a])]}({groupId:a}),g=(()=>{const e=c??f;return m({value:e,tabValues:l})?e:null})();(0,s.A)(()=>{g&&i(g)},[g]);return{selectedValue:o,selectValue:(0,t.useCallback)(e=>{if(!m({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),d(e),x(e)},[d,x,l]),tabValues:l}}var x=r(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=r(74848);function _({className:e,block:n,selectedValue:r,selectValue:t,tabValues:o}){const s=[],{blockElementScrollPositionUntilNextRender:i}=(0,l.a_)(),c=e=>{const n=e.currentTarget,a=s.indexOf(n),l=o[a].value;l!==r&&(i(n),t(l))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const r=s.indexOf(e.currentTarget)+1;n=s[r]??s[0];break}case"ArrowLeft":{const r=s.indexOf(e.currentTarget)-1;n=s[r]??s[s.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:t})=>(0,y.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{s.push(e)},onKeyDown:p,onClick:c,...t,className:(0,a.A)("tabs__item",g.tabItem,t?.className,{"tabs__item--active":r===e}),children:n??e},e))})}function w({lazy:e,children:n,selectedValue:r}){const l=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=l.find(e=>e.props.value===r);return e?(0,t.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:l.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==r}))})}function v(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,y.jsx)(_,{...n,...e}),(0,y.jsx)(w,{...n,...e})]})}function b(e){const n=(0,x.A)();return(0,y.jsx)(v,{...e,children:d(e.children)},String(n))}},19365:(e,n,r)=>{r.d(n,{A:()=>o});r(96540);var t=r(34164);const a={tabItem:"tabItem_Ymn6"};var l=r(74848);function o({children:e,hidden:n,className:r}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,t.A)(a.tabItem,r),hidden:n,children:e})}},28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var t=r(96540);const a={},l=t.createContext(a);function o(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(l.Provider,{value:n},e.children)}},42640:(e,n,r)=>{r.d(n,{A:()=>t});const t=(0,r(84722).A)("bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]])},47020:(e,n,r)=>{r.d(n,{A:()=>l});r(96540);const t={wrapper:"wrapper_sf5q"};var a=r(74848);function l({children:e}){return(0,a.jsx)("div",{className:t.wrapper,children:e})}},47792:(e,n,r)=>{r.d(n,{A:()=>t});const t=(0,r(84722).A)("target",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["circle",{cx:"12",cy:"12",r:"6",key:"1vlfrh"}],["circle",{cx:"12",cy:"12",r:"2",key:"1c9p78"}]])},49374:(e,n,r)=>{r.d(n,{B:()=>s});r(96540);const t=JSON.parse('{"mlflow.ag2":"api_reference/python_api/mlflow.ag2.html","mlflow.anthropic":"api_reference/python_api/mlflow.anthropic.html","mlflow.artifacts":"api_reference/python_api/mlflow.artifacts.html","mlflow.autogen":"api_reference/python_api/mlflow.autogen.html","mlflow.bedrock":"api_reference/python_api/mlflow.bedrock.html","mlflow.catboost":"api_reference/python_api/mlflow.catboost.html","mlflow.client":"api_reference/python_api/mlflow.client.html","mlflow.config":"api_reference/python_api/mlflow.config.html","mlflow.crewai":"api_reference/python_api/mlflow.crewai.html","mlflow.data":"api_reference/python_api/mlflow.data.html","mlflow.deployments":"api_reference/python_api/mlflow.deployments.html","mlflow.diviner":"api_reference/python_api/mlflow.diviner.html","mlflow.dspy":"api_reference/python_api/mlflow.dspy.html","mlflow.entities":"api_reference/python_api/mlflow.entities.html","mlflow.environment_variables":"api_reference/python_api/mlflow.environment_variables.html","mlflow.gateway":"api_reference/python_api/mlflow.gateway.html","mlflow.gemini":"api_reference/python_api/mlflow.gemini.html","mlflow.genai":"api_reference/python_api/mlflow.genai.html","mlflow.groq":"api_reference/python_api/mlflow.groq.html","mlflow.h2o":"api_reference/python_api/mlflow.h2o.html","mlflow.johnsnowlabs":"api_reference/python_api/mlflow.johnsnowlabs.html","mlflow.keras":"api_reference/python_api/mlflow.keras.html","mlflow.langchain":"api_reference/python_api/mlflow.langchain.html","mlflow.lightgbm":"api_reference/python_api/mlflow.lightgbm.html","mlflow.litellm":"api_reference/python_api/mlflow.litellm.html","mlflow.llama_index":"api_reference/python_api/mlflow.llama_index.html","mlflow.metrics":"api_reference/python_api/mlflow.metrics.html","mlflow.mistral":"api_reference/python_api/mlflow.mistral.html","mlflow.models":"api_reference/python_api/mlflow.models.html","mlflow.onnx":"api_reference/python_api/mlflow.onnx.html","mlflow.openai":"api_reference/python_api/mlflow.openai.html","mlflow.paddle":"api_reference/python_api/mlflow.paddle.html","mlflow.pmdarima":"api_reference/python_api/mlflow.pmdarima.html","mlflow.projects":"api_reference/python_api/mlflow.projects.html","mlflow.promptflow":"api_reference/python_api/mlflow.promptflow.html","mlflow.prophet":"api_reference/python_api/mlflow.prophet.html","mlflow.pydantic_ai":"api_reference/python_api/mlflow.pydantic_ai.html","mlflow.pyfunc":"api_reference/python_api/mlflow.pyfunc.html","mlflow.pyspark.ml":"api_reference/python_api/mlflow.pyspark.ml.html","mlflow.pytorch":"api_reference/python_api/mlflow.pytorch.html","mlflow":"api_reference/python_api/mlflow.html","mlflow.sagemaker":"api_reference/python_api/mlflow.sagemaker.html","mlflow.sentence_transformers":"api_reference/python_api/mlflow.sentence_transformers.html","mlflow.server":"api_reference/python_api/mlflow.server.html","mlflow.shap":"api_reference/python_api/mlflow.shap.html","mlflow.sklearn":"api_reference/python_api/mlflow.sklearn.html","mlflow.smolagents":"api_reference/python_api/mlflow.smolagents.html","mlflow.spacy":"api_reference/python_api/mlflow.spacy.html","mlflow.spark":"api_reference/python_api/mlflow.spark.html","mlflow.statsmodels":"api_reference/python_api/mlflow.statsmodels.html","mlflow.system_metrics":"api_reference/python_api/mlflow.system_metrics.html","mlflow.tensorflow":"api_reference/python_api/mlflow.tensorflow.html","mlflow.tracing":"api_reference/python_api/mlflow.tracing.html","mlflow.transformers":"api_reference/python_api/mlflow.transformers.html","mlflow.types":"api_reference/python_api/mlflow.types.html","mlflow.utils":"api_reference/python_api/mlflow.utils.html","mlflow.xgboost":"api_reference/python_api/mlflow.xgboost.html","mlflow.server.auth":"api_reference/auth/python-api.html","mlflow.server.cli":"api_reference/cli.html","mlflow.r":"api_reference/R-api.html","mlflow.java":"api_reference/java_api/index.html","mlflow.python":"api_reference/python_api/index.html","mlflow.rest":"api_reference/rest-api.html","mlflow.llms.deployments.api":"api_reference/llms/deployments/api.html"}');var a=r(86025),l=r(74848);const o=e=>{const n=e.split(".");for(let r=n.length;r>0;r--){const e=n.slice(0,r).join(".");if(t[e])return e}return null};function s({fn:e,children:n,hash:r}){const s=o(e);if(!s)return(0,l.jsx)(l.Fragment,{children:n});const i=(0,a.Ay)(`/${t[s]}#${r??e}`);return(0,l.jsx)("a",{href:i,target:"_blank",children:n??(0,l.jsxs)("code",{children:[e,"()"]})})}},52659:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>y,contentTitle:()=>g,default:()=>v,frontMatter:()=>x,metadata:()=>t,toc:()=>_});const t=JSON.parse('{"id":"eval-monitor/scorers/custom","title":"Custom Code-based Scorers","description":"Custom scorers offer the ultimate flexibility to define precisely how your GenAI application\'s quality is measured. They provide the flexibility to define evaluation metrics tailored to your specific business use case, whether based on simple heuristics, advanced logic, or programmatic evaluations.","source":"@site/docs/genai/eval-monitor/scorers/custom.mdx","sourceDirName":"eval-monitor/scorers","slug":"/eval-monitor/scorers/custom","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/custom","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"genAISidebar","previous":{"title":"Bring Your Own Prompts","permalink":"/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/prompt"},"next":{"title":"Feedback Collection","permalink":"/mlflow-website/docs/latest/genai/assessments/feedback"}}');var a=r(74848),l=r(28453),o=r(49374),s=r(66927),i=r(11470),c=r(19365),p=r(47020),d=r(6789),u=r(65592),m=r(42640),h=r(61878),f=r(47792);const x={},g="Custom Code-based Scorers",y={},_=[{value:"Example Usage",id:"example-usage",level:2},{value:"Input Format",id:"input-format",level:2},{value:"Return Types",id:"return-types",level:2},{value:"Simple values",id:"simple-values",level:3},{value:"Rich feedback",id:"rich-feedback",level:3},{value:"Parsing Traces for Scoring",id:"parsing-traces-for-scoring",level:2},{value:"Example 1: Evaluating Retrieved Documents Recall",id:"example-1-evaluating-retrieved-documents-recall",level:3},{value:"Example 2: Evaluating Tool Call Trajectory",id:"example-2-evaluating-tool-call-trajectory",level:3},{value:"Example 3: Evaluating Sub-Agents Routing",id:"example-3-evaluating-sub-agents-routing",level:3},{value:"Error handling",id:"error-handling",level:2},{value:"Let exceptions propagate (recommended)",id:"let-exceptions-propagate-recommended",level:3},{value:"Handle exceptions explicitly",id:"handle-exceptions-explicitly",level:3},{value:"Next Steps",id:"next-steps",level:2}];function w(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"custom-code-based-scorers",children:"Custom Code-based Scorers"})}),"\n",(0,a.jsx)(n.p,{children:"Custom scorers offer the ultimate flexibility to define precisely how your GenAI application's quality is measured. They provide the flexibility to define evaluation metrics tailored to your specific business use case, whether based on simple heuristics, advanced logic, or programmatic evaluations."}),"\n",(0,a.jsx)(n.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,a.jsxs)(n.p,{children:["To define a custom scorer, you can define a function that takes in the ",(0,a.jsx)(n.a,{href:"#input-format",children:"input arguments"})," and add the ",(0,a.jsx)(o.B,{fn:"mlflow.genai.scorers.scorer",children:"@scorer"})," decorator to the function."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.genai import scorer\n\n\n@scorer\ndef exact_match(outputs: dict, expectations: dict) -> bool:\n    return outputs == expectations["expected_response"]\n'})}),"\n",(0,a.jsxs)(n.p,{children:["To return richer information beyond primitive values, you can return a ",(0,a.jsx)(o.B,{fn:"mlflow.entities.Feedback",children:"Feedback"})," object."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.entities import Feedback\n\n\n@scorer\ndef is_short(outputs: dict) -> Feedback:\n    score = len(outputs.split()) <= 5\n    rationale = (\n        "The response is short enough."\n        if score\n        else f"The response is not short enough because it has ({len(outputs.split())} words)."\n    )\n    return Feedback(value=score, rationale=rationale)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Then you can pass the functions directly to the ",(0,a.jsx)(o.B,{fn:"mlflow.genai.evaluate",children:"mlflow.genai.evaluate"})," function, just like other predefined or LLM-based scorers."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import mlflow\n\neval_dataset = [\n    {\n        "inputs": {"question": "How many countries are there in the world?"},\n        "outputs": "195",\n        "expectations": {"expected_response": "195"},\n    },\n    {\n        "inputs": {"question": "What is the capital of France?"},\n        "outputs": "The capital of France is Paris.",\n        "expectations": {"expected_response": "Paris"},\n    },\n]\n\nmlflow.genai.evaluate(\n    data=eval_dataset,\n    scorers=[exact_match, is_short],\n)\n'})}),"\n",(0,a.jsx)(s.A,{src:"/images/mlflow-3/eval-monitor/scorers/code-scorers-results.png",alt:"Code-based Scorers"}),"\n",(0,a.jsx)(n.h2,{id:"input-format",children:"Input Format"}),"\n",(0,a.jsx)(n.p,{children:"As input, custom scorers have access to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["The ",(0,a.jsx)(n.code,{children:"inputs"})," dictionary, derived from either the input dataset or MLflow post-processing from your trace."]}),"\n",(0,a.jsxs)(n.li,{children:["The ",(0,a.jsx)(n.code,{children:"outputs"})," value, derived from either the input dataset or trace. If ",(0,a.jsx)(n.code,{children:"predict_fn"})," is provided, the ",(0,a.jsx)(n.code,{children:"outputs"})," value will be the return value of the ",(0,a.jsx)(n.code,{children:"predict_fn"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["The ",(0,a.jsx)(n.code,{children:"expectations"})," dictionary, derived from the ",(0,a.jsx)(n.code,{children:"expectations"})," field in the input dataset, or associated with the trace."]}),"\n",(0,a.jsxs)(n.li,{children:["The complete ",(0,a.jsx)(n.a,{href:"/genai/tracing/concepts/trace",children:"MLflow trace"}),", including spans, attributes, and outputs."]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"@scorer\ndef my_scorer(\n    *,\n    inputs: dict[str, Any],\n    outputs: Any,\n    expectations: dict[str, Any],\n    trace: Trace,\n) -> float | bool | str | Feedback | list[Feedback]:\n    # Your evaluation logic here\n    ...\n"})}),"\n",(0,a.jsxs)(n.p,{children:["All parameters are ",(0,a.jsx)(n.strong,{children:"optional"}),"; declare only what your scorer needs:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"# \u2714\ufe0f All of these signatures are valid for scorers\ndef my_scorer(inputs, outputs, expectations, trace) -> bool:\ndef my_scorer(inputs, outputs) -> str:\ndef my_scorer(outputs, expectations) -> Feedback:\ndef my_scorer(trace) -> list[Feedback]:\n\n# \ud83d\udd34 Additional parameters are not allowed\ndef my_scorer(inputs, outputs, expectations, trace, additional_param) -> float\n"})}),"\n",(0,a.jsx)(n.admonition,{title:"Where do these values come from?",type:"info",children:(0,a.jsxs)(n.p,{children:["When running ",(0,a.jsx)(n.code,{children:"mlflow.genai.evaluate()"}),", the inputs, outputs, and expectations parameters can be specified in the data argument, or parsed from the trace. See ",(0,a.jsx)("ins",{children:(0,a.jsx)(n.a,{href:"/genai/eval-monitor/scorers#how-scorers-work",children:"How Scorers Work"})})," for more details."]})}),"\n",(0,a.jsx)(n.h2,{id:"return-types",children:"Return Types"}),"\n",(0,a.jsx)(n.p,{children:"Scorers can return different types depending on your evaluation needs:"}),"\n",(0,a.jsx)(n.h3,{id:"simple-values",children:"Simple values"}),"\n",(0,a.jsx)(n.p,{children:"Return primitive values for straightforward pass/fail or numeric assessments."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Pass/fail strings: ",(0,a.jsx)(n.code,{children:'"yes"'})," or ",(0,a.jsx)(n.code,{children:'"no"'})," render as ",(0,a.jsx)("div",{className:"inline-flex rounded-sm bg-green-100 px-2 py-1 text-sm text-green-600",children:"Pass"})," or ",(0,a.jsx)("div",{className:"inline-flex rounded-sm bg-red-100 px-2 py-1 text-sm text-red-800",children:"Fail"})," in the UI"]}),"\n",(0,a.jsxs)(n.li,{children:["Boolean values: ",(0,a.jsx)(n.code,{children:"True"})," or ",(0,a.jsx)(n.code,{children:"False"})," for binary evaluations"]}),"\n",(0,a.jsx)(n.li,{children:"Numeric values: Integers or floats for scores, counts, or measurements"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"rich-feedback",children:"Rich feedback"}),"\n",(0,a.jsxs)(n.p,{children:["Return ",(0,a.jsx)(o.B,{fn:"mlflow.entities.Feedback",children:"Feedback"})," objects for detailed assessments with additional metadata such as explanation, source info, and error summary."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.entities import Feedback, AssessmentSource\n\n\n@scorer\ndef content_quality(outputs):\n    return Feedback(\n        value=0.85,  # Can be numeric, boolean, or string\n        rationale="Clear and accurate, minor grammar issues",\n        # Optional: source of the assessment. Several source types are supported,\n        # such as "HUMAN", "CODE", "LLM_JUDGE".\n        source=AssessmentSource(source_type="CODE", source_id="grammar_checker_v1"),\n        # Optional: additional metadata about the assessment.\n        metadata={\n            "annotator": "me@example.com",\n        },\n    )\n'})}),"\n",(0,a.jsx)(n.p,{children:"Multiple feedback objects can be returned as a list. Each feedback object will be displayed as a separate metric in the evaluation results."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'@scorer\ndef comprehensive_check(inputs, outputs):\n    return [\n        Feedback(name="relevance", value=True, rationale="Directly addresses query"),\n        Feedback(name="tone", value="professional", rationale="Appropriate for audience"),\n        Feedback(name="length", value=150, rationale="Word count within limits")\n    ]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"parsing-traces-for-scoring",children:"Parsing Traces for Scoring"}),"\n",(0,a.jsxs)(n.p,{children:["Scorers have access to the complete MLflow traces, including spans, attributes, and outputs, allowing you to evaluate the agent's behavior precisely, not just the final output.\nThe ",(0,a.jsx)(o.B,{fn:"mlflow.entities.Trace.search_spans",children:(0,a.jsx)(n.code,{children:"Trace.search_spans"})})," API is a powerful way to retrieve such intermediate information from the trace."]}),"\n",(0,a.jsx)(n.p,{children:"Open the tabs below to see examples of custom scorers that evaluate the detailed behavior of agents by parsing the trace."}),"\n",(0,a.jsx)(p.A,{children:(0,a.jsxs)(i.A,{children:[(0,a.jsxs)(c.A,{value:"retrieved_document_recall",label:"Retrieved Document Recall",children:[(0,a.jsx)(n.h3,{id:"example-1-evaluating-retrieved-documents-recall",children:"Example 1: Evaluating Retrieved Documents Recall"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.entities import SpanType, Trace\nfrom mlflow.genai import scorer\n\n\n@scorer\ndef retrieved_document_recall(trace: Trace, expectations: dict) -> Feedback:\n    # Search for retriever spans in the trace\n    retriever_spans = trace.search_spans(span_type=SpanType.RETRIEVER)\n\n    # If there are no retriever spans\n    if not retriever_spans:\n        return Feedback(\n            value=0,\n            rationale="No retriever span found in the trace.",\n        )\n\n    # Gather all retrieved document URLs from the retriever spans\n    all_document_urls = []\n    for span in retriever_spans:\n        all_document_urls.extend([document["doc_uri"] for document in span.outputs])\n\n    # Compute the recall\n    true_positives = len(\n        set(all_document_urls) & set(expectations["relevant_document_urls"])\n    )\n    expected_positives = len(expectations["relevant_document_urls"])\n    recall = true_positives / expected_positives\n    return Feedback(\n        value=recall,\n        rationale=f"Retrieved {true_positives} relevant documents out of {expected_positives} expected.",\n    )\n'})})]}),(0,a.jsxs)(c.A,{value:"tool_call_trajectory",label:"Tool Call Trajectory",children:[(0,a.jsx)(n.h3,{id:"example-2-evaluating-tool-call-trajectory",children:"Example 2: Evaluating Tool Call Trajectory"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.entities import SpanType, Trace\nfrom mlflow.genai import scorer\n\n\n@scorer\ndef tool_call_trajectory(trace: Trace, expectations: dict) -> Feedback:\n    # Search for tool call spans in the trace\n    tool_call_spans = trace.search_spans(span_type=SpanType.TOOL)\n\n    # Compare the tool trajectory with expectations\n    actual_trajectory = [span.name for span in tool_call_spans]\n    expected_trajectory = expectations["tool_call_trajectory"]\n\n    if actual_trajectory == expected_trajectory:\n        return Feedback(value=1, rationale="The tool call trajectory is correct.")\n    else:\n        return Feedback(\n            value=0,\n            rationale=(\n                "The tool call trajectory is incorrect.\\n"\n                f"Expected: {expected_trajectory}.\\n"\n                f"Actual: {actual_trajectory}."\n            ),\n        )\n'})})]}),(0,a.jsxs)(c.A,{value:"sub_agents_routing",label:"Sub-Agents Routing",children:[(0,a.jsx)(n.h3,{id:"example-3-evaluating-sub-agents-routing",children:"Example 3: Evaluating Sub-Agents Routing"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mlflow.entities import SpanType, Trace\nfrom mlflow.genai import scorer\n\n\n@scorer\ndef is_routing_correct(trace: Trace, expectations: dict) -> Feedback:\n    # Search for sub-agent spans in the trace\n    sub_agent_spans = trace.search_spans(span_type=SpanType.AGENT)\n\n    invoked_agents = [span.name for span in sub_agent_spans]\n    expected_agents = expectations["expected_agents"]\n\n    if invoked_agents == expected_agents:\n        return Feedback(value=True, rationale="The sub-agents routing is correct.")\n    else:\n        return Feedback(\n            value=False,\n            rationale=(\n                "The sub-agents routing is incorrect.\\n"\n                f"Expected: {expected_agents}.\\n"\n                f"Actual: {invoked_agents}."\n            ),\n        )\n'})})]})]})}),"\n",(0,a.jsx)(n.h2,{id:"error-handling",children:"Error handling"}),"\n",(0,a.jsx)(n.p,{children:"When a scorer encounters an error, MLflow provides two approaches:"}),"\n",(0,a.jsx)(n.h3,{id:"let-exceptions-propagate-recommended",children:"Let exceptions propagate (recommended)"}),"\n",(0,a.jsx)(n.p,{children:"The simplest approach is to let exceptions throw naturally. MLflow automatically captures the exception and creates a Feedback object with the error details:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\nimport mlflow\nfrom mlflow.entities import Feedback\nfrom mlflow.genai.scorers import scorer\n\n\n@scorer\ndef is_valid_response(outputs: str) -> Feedback:\n    # Let json.JSONDecodeError propagate if response isn\'t valid JSON\n    data = json.loads(outputs)\n\n    # Let KeyError propagate if required fields are missing\n    summary = data["summary"]\n    confidence = data["confidence"]\n\n    return Feedback(value=True, rationale=f"Valid JSON with confidence: {confidence}")\n\n\n# Run the scorer on invalid data that triggers exceptions\ninvalid_data = [\n    {\n        # Valid JSON\n        "outputs": \'{"summary": "this is a summary", "confidence": 0.95}\'\n    },\n    {\n        # Invalid JSON\n        "outputs": "invalid json",\n    },\n    {\n        # Missing required fields\n        "outputs": \'{"summary": "this is a summary"}\'\n    },\n]\n\nmlflow.genai.evaluate(\n    data=invalid_data,\n    scorers=[is_valid_response],\n)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["When an exception occurs, MLflow creates a ",(0,a.jsx)(o.B,{fn:"mlflow.entities.Feedback",children:"Feedback"})," with:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"value"}),": None"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"error"}),": The exception details, such as exception object, error message, and stack trace"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The error information will be displayed in the evaluation results. Open the corresponding row to see the error details."}),"\n",(0,a.jsx)(s.A,{src:"/images/mlflow-3/eval-monitor/scorers/scorer-error.png",alt:"Scorer Error"}),"\n",(0,a.jsx)(n.h3,{id:"handle-exceptions-explicitly",children:"Handle exceptions explicitly"}),"\n",(0,a.jsxs)(n.p,{children:["For custom error handling or to provide specific error messages, catch exceptions and return a ",(0,a.jsx)(o.B,{fn:"mlflow.entities.Feedback",children:"Feedback"})," with None value and error details:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\nfrom mlflow.entities import AssessmentError, Feedback\n\n\n@scorer\ndef is_valid_response(outputs):\n    try:\n        data = json.loads(outputs)\n        required_fields = ["summary", "confidence", "sources"]\n        missing = [f for f in required_fields if f not in data]\n\n        if missing:\n            # Specify the AssessmentError object explicitly\n            return Feedback(\n                error=AssessmentError(\n                    error_code="MISSING_REQUIRED_FIELDS",\n                    error_message=f"Missing required fields: {missing}",\n                ),\n            )\n\n        return Feedback(value=True, rationale="Valid JSON with all required fields")\n\n    except json.JSONDecodeError as e:\n        # Can pass exception object directly to the error parameter as well\n        return Feedback(error=e)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(u.A,{children:[(0,a.jsx)(d.A,{icon:m.A,title:"Evaluate Agents",description:"Learn how to evaluate AI agents with specialized techniques and scorers",href:"/genai/eval-monitor/running-evaluation/agents"}),(0,a.jsx)(d.A,{icon:h.A,title:"Evaluate Traces",description:"Evaluate production traces to understand and improve your AI application's behavior",href:"/genai/eval-monitor/running-evaluation/traces"}),(0,a.jsx)(d.A,{icon:f.A,title:"Ground Truth Expectations",description:"Learn how to define and manage ground truth data for accurate evaluations",href:"/genai/assessments/expectations"})]})]})}function v(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(w,{...e})}):w(e)}},61878:(e,n,r)=>{r.d(n,{A:()=>t});const t=(0,r(84722).A)("git-branch",[["line",{x1:"6",x2:"6",y1:"3",y2:"15",key:"17qcm7"}],["circle",{cx:"18",cy:"6",r:"3",key:"1h7g24"}],["circle",{cx:"6",cy:"18",r:"3",key:"fqmcym"}],["path",{d:"M18 9a9 9 0 0 1-9 9",key:"n2h4wq"}]])},65592:(e,n,r)=>{r.d(n,{A:()=>o});r(96540);var t=r(34164);const a={tilesGrid:"tilesGrid_hB9N"};var l=r(74848);function o({children:e,className:n}){return(0,l.jsx)("div",{className:(0,t.A)(a.tilesGrid,n),children:e})}},66927:(e,n,r)=>{r.d(n,{A:()=>o});r(96540);const t={container:"container_JwLF",imageWrapper:"imageWrapper_RfGN",image:"image_bwOA",caption:"caption_jo2G"};var a=r(86025),l=r(74848);function o({src:e,alt:n,width:r,caption:o,className:s}){return(0,l.jsxs)("div",{className:`${t.container} ${s||""}`,children:[(0,l.jsx)("div",{className:t.imageWrapper,style:r?{width:r}:{},children:(0,l.jsx)("img",{src:(0,a.Ay)(e),alt:n,className:t.image})}),o&&(0,l.jsx)("p",{className:t.caption,children:o})]})}},84722:(e,n,r)=>{r.d(n,{A:()=>c});var t=r(96540);const a=e=>{const n=(e=>e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,n,r)=>r?r.toUpperCase():n.toLowerCase()))(e);return n.charAt(0).toUpperCase()+n.slice(1)},l=(...e)=>e.filter((e,n,r)=>Boolean(e)&&""!==e.trim()&&r.indexOf(e)===n).join(" ").trim(),o=e=>{for(const n in e)if(n.startsWith("aria-")||"role"===n||"title"===n)return!0};var s={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const i=(0,t.forwardRef)(({color:e="currentColor",size:n=24,strokeWidth:r=2,absoluteStrokeWidth:a,className:i="",children:c,iconNode:p,...d},u)=>(0,t.createElement)("svg",{ref:u,...s,width:n,height:n,stroke:e,strokeWidth:a?24*Number(r)/Number(n):r,className:l("lucide",i),...!c&&!o(d)&&{"aria-hidden":"true"},...d},[...p.map(([e,n])=>(0,t.createElement)(e,n)),...Array.isArray(c)?c:[c]])),c=(e,n)=>{const r=(0,t.forwardRef)(({className:r,...o},s)=>{return(0,t.createElement)(i,{ref:s,iconNode:n,className:l(`lucide-${c=a(e),c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${e}`,r),...o});var c});return r.displayName=a(e),r}}}]);