"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9105],{26745:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"id":"prompt-version-mgmt/version-tracking/compare-app-versions","title":"Compare App Versions","description":"Compare different LoggedModel versions to track improvements and identify the best performing version.","source":"@site/docs/genai/prompt-version-mgmt/version-tracking/compare-app-versions.mdx","sourceDirName":"prompt-version-mgmt/version-tracking","slug":"/prompt-version-mgmt/version-tracking/compare-app-versions","permalink":"/docs/latest/genai/prompt-version-mgmt/version-tracking/compare-app-versions","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Compare App Versions","description":"Compare different LoggedModel versions to track improvements and identify the best performing version."},"sidebar":"genAISidebar","previous":{"title":"Track Application Versions with MLflow","permalink":"/docs/latest/genai/prompt-version-mgmt/version-tracking/track-application-versions-with-mlflow"},"next":{"title":"MLflow GenAI Packaging Integrations","permalink":"/docs/latest/genai/flavors/"}}');var s=i(74848),o=i(28453);const t={title:"Compare App Versions",description:"Compare different LoggedModel versions to track improvements and identify the best performing version."},a="Compare App Versions",l={},m=[{value:"Creating an Improved Version",id:"creating-an-improved-version",level:2},{value:"Comparing Versions in the MLflow UI",id:"comparing-versions-in-the-mlflow-ui",level:2},{value:"Comparing Versions with the MLflow API",id:"comparing-versions-with-the-mlflow-api",level:2},{value:"Ranking and Searching Versions",id:"ranking-and-searching-versions",level:3},{value:"Side-by-Side Comparison",id:"side-by-side-comparison",level:3},{value:"Automating Deployment Decisions",id:"automating-deployment-decisions",level:3},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"compare-app-versions",children:"Compare App Versions"})}),"\n",(0,s.jsx)(n.p,{children:"After evaluating your customer support agent, let's make an improvement and compare the performance between versions. This guide shows how to track changes and objectively compare different versions of your application."}),"\n",(0,s.jsx)(n.h2,{id:"creating-an-improved-version",children:"Creating an Improved Version"}),"\n",(0,s.jsx)(n.p,{children:"Building on our customer support agent from previous sections, let's improve it by adding a more empathetic tone to the prompt:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport subprocess\nimport openai\n\n# Get current git commit for the new version\ntry:\n    git_commit = (\n        subprocess.check_output(["git", "rev-parse", "HEAD"])\n        .decode("ascii")\n        .strip()[:8]\n    )\n    version_identifier = f"git-{git_commit}"\nexcept subprocess.CalledProcessError:\n    version_identifier = "local-dev"  # Fallback if not in a git repo\n\nimproved_model_name = f"customer_support_agent-v2-{version_identifier}"\n\n# Set the new active model\nimproved_model_info = mlflow.set_active_model(name=improved_model_name)\n\n# Log parameters for the improved version\nimproved_params = {\n    "llm": "gpt-4o-mini",\n    "temperature": 0.7,\n    "retrieval_strategy": "vector_search_v3",\n}\nmlflow.log_model_params(model_id=improved_model_info.model_id, params=improved_params)\n\n\n# Define the improved agent with more empathetic prompting\ndef improved_agent(question: str) -> str:\n    client = openai.OpenAI()\n\n    # Enhanced system prompt with empathy focus\n    system_prompt = """You are a caring and empathetic customer support agent.\n    Always acknowledge the customer\'s feelings and frustrations before providing solutions.\n    Use phrases like \'I understand how frustrating this must be\' and \'I\'m here to help\'.\n    Provide clear, actionable steps while maintaining a warm, supportive tone."""\n\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": question},\n        ],\n        temperature=0.7,\n        max_tokens=150,\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now let's evaluate this improved version using the same dataset and scorers from the previous section. This ensures we can make an apples-to-apples comparison between versions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Evaluate the improved version with the same dataset\nevaluation_results_v2 = mlflow.genai.evaluate(\n    data=eval_data,  # Same evaluation dataset from previous section\n    predict_fn=improved_agent,\n    model_id=improved_model_info.model_id,\n    scorers=[\n        relevance_scorer,\n        support_guidelines,\n    ],  # Same scorers from previous section\n)\n\nprint(f"V2 Metrics: {evaluation_results_v2.metrics}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"comparing-versions-in-the-mlflow-ui",children:"Comparing Versions in the MLflow UI"}),"\n",(0,s.jsx)(n.p,{children:"After creating multiple versions of your application, you need to systematically compare them to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validate improvements"})," - Confirm that your changes actually improved the metrics you care about"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Identify regressions"})," - Ensure new versions don't degrade performance in unexpected ways"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Select the best version"})," - Make data-driven decisions about which version to deploy to production"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Guide iteration"})," - Understand which changes had the most impact to inform future improvements"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The MLflow UI provides visual comparison tools that make this analysis intuitive:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigate to your experiment"})," containing the customer support agent versions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Select multiple runs"})," by checking the boxes next to each version's evaluation run"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:'Click "Compare"'})," to see:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Side-by-side parameter differences"}),"\n",(0,s.jsx)(n.li,{children:"Metric comparisons with charts"}),"\n",(0,s.jsx)(n.li,{children:"Parallel coordinates plot for multi-metric analysis"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"comparing-versions-with-the-mlflow-api",children:"Comparing Versions with the MLflow API"}),"\n",(0,s.jsxs)(n.p,{children:["For automated workflows like CI/CD pipelines, regression testing, or programmatic version selection, MLflow provides powerful APIs to search, rank, and compare your ",(0,s.jsx)(n.code,{children:"LoggedModel"})," versions. These APIs enable you to:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Automatically flag versions that don't meet quality thresholds"}),"\n",(0,s.jsx)(n.li,{children:"Generate comparison reports for code reviews"}),"\n",(0,s.jsx)(n.li,{children:"Select the best version for deployment without manual intervention"}),"\n",(0,s.jsx)(n.li,{children:"Track version improvements over time in your analytics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ranking-and-searching-versions",children:"Ranking and Searching Versions"}),"\n",(0,s.jsxs)(n.p,{children:["Use ",(0,s.jsx)(n.code,{children:"search_logged_models"})," to find all versions of your application and rank them by quality, speed, or other performance characteristics. This helps you identify trends and find the best performing versions:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from mlflow import search_logged_models\n\n# Search for all versions of our customer support agent\n# Order by creation time to see version progression\nall_versions = search_logged_models(\n    filter_string=f"name IN (\'{logged_model_name}\', \'{improved_model_name}\')",\n    order_by=[{"field_name": "creation_time", "ascending": False}],  # Most recent first\n    output_format="list",\n)\n\nprint(f"Found {len(all_versions)} versions of customer support agent\\n")\n\n# Compare key metrics across versions\nfor model in all_versions[:2]:  # Compare latest 2 versions\n    print(f"Version: {model.name}")\n    print(f"  Model ID: {model.model_id}")\n    print(f"  Created: {model.creation_timestamp}")\n\n    # Display evaluation metrics\n    for metric in model.metrics:\n        print(f"  {metric.key}: {metric.value}")\n\n    # Display parameters\n    print(f"  Temperature: {model.params.get(\'temperature\', \'N/A\')}")\n    print(f"  LLM: {model.params.get(\'llm\', \'N/A\')}")\n    print()\n\n# Find the best performing version by a specific metric\nbest_by_guidelines = max(\n    all_versions,\n    key=lambda m: next(\n        (m.value for m in m.metrics if m.key == "support_guidelines/mean"), None\n    ),\n)\nprint(f"Best version for support guidelines: {best_by_guidelines.name}")\nprint(\n    f"  Score: {next((m.value for m in best_by_guidelines.metrics if m.key == \'support_guidelines/mean\'), None)}"\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"side-by-side-comparison",children:"Side-by-Side Comparison"}),"\n",(0,s.jsx)(n.p,{children:"Once you've identified versions to compare, perform a detailed side-by-side analysis to understand exactly what changed and how it impacted performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Get the two specific models we want to compare\nv1 = mlflow.get_logged_model(\n    model_id=active_model_info.model_id\n)  # Original from previous section\nv2 = mlflow.get_logged_model(model_id=improved_model_info.model_id)  # Improved\n\nprint("=== Version Comparison ===")\nprint(f"V1: {v1.name} vs V2: {v2.name}\\n")\n\n# Compare parameters (what changed)\nprint("Parameter Changes:")\nall_params = set(v1.params.keys()) | set(v2.params.keys())\nfor param in sorted(all_params):\n    v1_val = v1.params.get(param, "N/A")\n    v2_val = v2.params.get(param, "N/A")\n    if v1_val != v2_val:\n        print(f"  {param}: \'{v1_val}\' \u2192 \'{v2_val}\' \u2713")\n    else:\n        print(f"  {param}: \'{v1_val}\' (unchanged)")\n\n# Compare metrics (impact of changes)\nprint("\\nMetric Improvements:")\nv1_metrics = {m.key: m.value for m in v1.metrics}\nv2_metrics = {m.key: m.value for m in v2.metrics}\n\nmetric_keys = ["relevance_to_query/mean", "support_guidelines/mean"]\nfor metric in metric_keys:\n    v1_score = v1_metrics.get(metric, 0)\n    v2_score = v2_metrics.get(metric, 0)\n    improvement = ((v2_score - v1_score) / max(v1_score, 0.01)) * 100\n    print(f"  {metric}:")\n    print(f"    V1: {v1_score:.3f}")\n    print(f"    V2: {v2_score:.3f}")\n    print(f"    Change: {improvement:+.1f}%")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"automating-deployment-decisions",children:"Automating Deployment Decisions"}),"\n",(0,s.jsxs)(n.p,{children:["One of the most powerful uses of the ",(0,s.jsx)(n.code,{children:"LoggedModels"})," API is automating deployment decisions. Instead of manually reviewing each version, you can codify your quality criteria and let your CI/CD pipeline automatically determine whether a new version is ready for production."]}),"\n",(0,s.jsx)(n.p,{children:"This approach ensures consistent quality standards and enables rapid, safe deployments:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Decision logic based on your criteria\nmin_relevance_threshold = 0.80\nmin_guidelines_threshold = 0.80\n\nif (\n    v2_metrics.get("relevance_to_query/mean", 0) >= min_relevance_threshold\n    and v2_metrics.get("support_guidelines/mean", 0) >= min_guidelines_threshold\n    and v2_metrics.get("support_guidelines/mean", 0)\n    > v1_metrics.get("support_guidelines/mean", 0)\n):\n    print(f"\u2705 Version 2 ({v2.name}) is ready for deployment!")\n    print("   - Meets all quality thresholds")\n    print("   - Shows improvement in support guidelines")\nelse:\n    print("\u274c Version 2 needs more work before deployment")\n'})}),"\n",(0,s.jsx)(n.p,{children:"You can extend this pattern to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Create quality gates"})," in your CI/CD pipeline that block deployments if metrics drop"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement gradual rollouts"})," based on improvement margins"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Trigger alerts"})," when a version shows significant regression"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"You can now deploy your best performing version and monitor its performance in production by linking traces to deployed versions."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(96540);const s={},o=r.createContext(s);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);