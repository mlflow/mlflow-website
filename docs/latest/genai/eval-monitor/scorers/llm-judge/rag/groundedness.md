# RetrievalGroundedness judge

The `RetrievalGroundedness` judge assesses whether your application's response is factually supported by the provided context (either from a RAG system or generated by a tool call), helping detect hallucinations or statements not backed by that context.

This built-in LLM judge is designed for evaluating RAG applications that need to ensure responses are grounded in retrieved information.

## Prerequisites for running the examples[​](#prerequisites-for-running-the-examples "Direct link to Prerequisites for running the examples")

1. Install MLflow and required packages

   bash

   ```bash
   pip install --upgrade mlflow

   ```

2. Create an MLflow experiment by following the [setup your environment quickstart](/mlflow-website/docs/latest/genai/getting-started/connect-environment.md).

3. (Optional, if using OpenAI models) Use the native OpenAI SDK to connect to OpenAI-hosted models. Select a model from the [available OpenAI models](https://platform.openai.com/docs/models).

   python

   ```python
   import mlflow
   import os
   import openai

   # Ensure your OPENAI_API_KEY is set in your environment
   # os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>" # Uncomment and set if not globally configured

   # Enable auto-tracing for OpenAI
   mlflow.openai.autolog()

   # Create an OpenAI client
   client = openai.OpenAI()

   # Select an LLM
   model_name = "gpt-4o-mini"

   ```

## Usage examples[​](#usage-examples "Direct link to Usage examples")

The RetrievalGroundedness judge can be invoked directly for single trace assessment or used with MLflow's evaluation framework for batch evaluation.

**Requirements:**

Trace requirements:

* The MLflow Trace must contain at least one span with `span_type` set to `RETRIEVER`
* `inputs` and `outputs` must be on the Trace's root span

- Invoke directly
- Invoke with evaluate()

python

```python
from mlflow.genai.scorers import RetrievalGroundedness
import mlflow

# Get a trace from a previous run
trace = mlflow.get_trace("<your-trace-id>")

# Assess if the response is grounded in the retrieved context
feedback = RetrievalGroundedness()(trace=trace)
print(feedback)

```

python

```python
import mlflow
from mlflow.genai.scorers import RetrievalGroundedness

# Evaluate traces from previous runs
results = mlflow.genai.evaluate(
    data=traces,  # DataFrame or list containing trace data
    scorers=[RetrievalGroundedness()],
)

```

tip

For a complete RAG application example with these judges, see the [RAG Evaluation guide](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag.md).

## Interpret results[​](#interpret-results "Direct link to Interpret results")

The RetrievalGroundedness judge evaluates each retriever span separately and returns a separate Feedback object for each retriever span in your trace. Each [`Feedback`](/mlflow-website/docs/latest/api_reference/python_api/mlflow.entities.html#mlflow.entities.Feedback) object contains:

* **value**: "yes" if response is grounded in the retrieved context, "no" if it contains hallucinations

* **rationale**: Detailed explanation identifying:

  <!-- -->

  * Which statements are supported by the context
  * Which statements lack support (hallucinations)
  * Specific quotes from context that support or contradict claims

## Select the LLM that powers the judge[​](#select-the-llm-that-powers-the-judge "Direct link to Select the LLM that powers the judge")

You can change the judge model by using the `model` argument in the judge definition. The model must be specified in the format `<provider>:/<model-name>`, where `<provider>` is a LiteLLM-compatible model provider.

For a list of supported models, see [selecting judge models](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/custom-judges.md#selecting-judge-models).

## Next steps[​](#next-steps "Direct link to Next steps")

### [Evaluate context sufficiency](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency.md)

[Check if your retriever provides adequate information](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency.md)

[Learn more →](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/context-sufficiency.md)

### [Evaluate context relevance](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/relevance.md#retrievalrelevance-judge)

[Ensure retrieved documents are relevant to queries](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/relevance.md#retrievalrelevance-judge)

[Learn more →](/mlflow-website/docs/latest/genai/eval-monitor/scorers/llm-judge/rag/relevance.md#retrievalrelevance-judge)

### [Run comprehensive RAG evaluation](/mlflow-website/docs/latest/genai/eval-monitor/quickstart.md)

[Combine multiple judges for complete RAG assessment](/mlflow-website/docs/latest/genai/eval-monitor/quickstart.md)

[Learn more →](/mlflow-website/docs/latest/genai/eval-monitor/quickstart.md)
