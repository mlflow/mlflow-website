# MLflow Evaluation

Classic ML Evaluation System

This documentation covers MLflow's **classic evaluation system** (`mlflow.evaluate`) which uses `EvaluationMetric` and `make_metric` for custom metrics.

**For GenAI/LLM evaluation**, please use the system at [GenAI Evaluation](/mlflow-website/docs/latest/genai/eval-monitor.md) which uses:

* `mlflow.genai.evaluate()` instead of `mlflow.evaluate()`
* `Scorer` objects instead of `EvaluationMetric`
* Built-in LLM judges and scorers

**Important**: These two systems are **not interoperable**. `EvaluationMetric` objects cannot be used with `mlflow.genai.evaluate()`, and `Scorer` objects cannot be used with `mlflow.evaluate()`.

## Introduction[â€‹](#introduction "Direct link to Introduction")

**Model evaluation** is the cornerstone of reliable machine learning, transforming trained models into trustworthy, production-ready systems. MLflow's comprehensive evaluation framework goes beyond simple accuracy metrics, providing deep insights into model behavior, performance characteristics, and real-world readiness through automated testing, visualization, and validation pipelines.

MLflow's evaluation capabilities democratize advanced model assessment, making sophisticated evaluation techniques accessible to teams of all sizes. From rapid prototyping to enterprise deployment, MLflow evaluation ensures your models meet the highest standards of reliability, fairness, and performance.

Why Comprehensive Model Evaluation Matters

#### Beyond Basic Metrics[â€‹](#beyond-basic-metrics "Direct link to Beyond Basic Metrics")

* ğŸ“Š **Holistic Assessment**: Performance metrics, visualizations, and explanations in one unified framework
* ğŸ¯ **Task-Specific Evaluation**: Specialized evaluators for classification, regression, and LLM tasks
* ğŸ” **Model Interpretability**: SHAP integration for understanding model decisions and feature importance
* âš–ï¸ **Fairness Analysis**: Bias detection and ethical AI validation across demographic groups

#### Production Readiness[â€‹](#production-readiness "Direct link to Production Readiness")

* ğŸš€ **Automated Validation**: Threshold-based model acceptance with customizable criteria
* ğŸ“ˆ **Performance Monitoring**: Track model degradation and drift over time
* ğŸ”„ **A/B Testing Support**: Compare candidate models against production baselines
* ğŸ“‹ **Audit Trails**: Complete evaluation history for regulatory compliance and model governance

## Why MLflow Evaluation?[â€‹](#why-mlflow-evaluation "Direct link to Why MLflow Evaluation?")

MLflow's evaluation framework provides a comprehensive solution for model assessment and validation:

* âš¡ **One-Line Evaluation**: Comprehensive model assessment with `mlflow.evaluate()` - minimal configuration required
* ğŸ›ï¸ **Flexible Evaluation Modes**: Evaluate models, functions, or static datasets with the same unified API
* ğŸ“Š **Rich Visualizations**: Automatic generation of performance plots, confusion matrices, and diagnostic charts
* ğŸ”§ **Custom Metrics**: Define domain-specific evaluation criteria with easy-to-use metric builders
* ğŸ§  **Built-in Explainability**: SHAP integration for model interpretation and feature importance analysis
* ğŸ‘¥ **Team Collaboration**: Share evaluation results and model comparisons through MLflow's tracking interface
* ğŸ­ **Enterprise Integration**: Plugin architecture for specialized evaluation frameworks like Giskard and Trubrics

## Core Evaluation Capabilities[â€‹](#core-evaluation-capabilities "Direct link to Core Evaluation Capabilities")

### Automated Model Assessment[â€‹](#automated-model-assessment "Direct link to Automated Model Assessment")

MLflow evaluation transforms complex model assessment into simple, reproducible workflows:

python

```python
import mlflow
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine

# Load and prepare data
wine = load_wine()
X_train, X_test, y_train, y_test = train_test_split(
    wine.data, wine.target, test_size=0.2, random_state=42
)

# Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Create evaluation dataset
eval_data = X_test
eval_data["target"] = y_test

with mlflow.start_run():
    # Log model
    mlflow.sklearn.log_model(model, name="model")

    # Comprehensive evaluation with one line
    result = mlflow.models.evaluate(
        model="models:/my-model/1",
        data=eval_data,
        targets="target",
        model_type="classifier",
        evaluators=["default"],
    )

```

What Gets Automatically Generated

#### Performance Metrics[â€‹](#performance-metrics "Direct link to Performance Metrics")

* ğŸ“Š **Classification**: Accuracy, precision, recall, F1-score, ROC-AUC, confusion matrices
* ğŸ“ˆ **Regression**: MAE, MSE, RMSE, RÂ², residual analysis, prediction vs actual plots
* ğŸ¯ **Custom Metrics**: Domain-specific measures defined with simple Python functions

#### Visual Diagnostics[â€‹](#visual-diagnostics "Direct link to Visual Diagnostics")

* ğŸ“Š **Performance Plots**: ROC curves, precision-recall curves, calibration plots
* ğŸ“ˆ **Feature Importance**: SHAP values, permutation importance, feature interactions

#### Model Explanations[â€‹](#model-explanations "Direct link to Model Explanations")

* ğŸ§  **Global Explanations**: Overall model behavior and feature contributions (with `shap`)
* ğŸ” **Local Explanations**: Individual prediction explanations and decision paths (with `shap`)

### Flexible Evaluation Modes[â€‹](#flexible-evaluation-modes "Direct link to Flexible Evaluation Modes")

MLflow supports multiple evaluation approaches to fit your workflow:

Comprehensive Evaluation Options

#### Model Evaluation[â€‹](#model-evaluation "Direct link to Model Evaluation")

* ğŸ¤– **Logged Models**: Evaluate models that have been logged to MLflow
* ğŸ”„ **Live Models**: Direct evaluation of in-memory model objects
* ğŸ“¦ **Pipeline Evaluation**: End-to-end assessment of preprocessing and modeling pipelines

#### Function Evaluation[â€‹](#function-evaluation "Direct link to Function Evaluation")

* âš¡ **Lightweight Assessment**: Evaluate Python functions without model logging overhead
* ğŸ”§ **Custom Predictions**: Assess complex prediction logic and business rules
* ğŸ¯ **Rapid Prototyping**: Quick evaluation during model development

#### Dataset Evaluation[â€‹](#dataset-evaluation "Direct link to Dataset Evaluation")

* ğŸ“Š **Static Analysis**: Evaluate pre-computed predictions without re-running models
* ğŸ”„ **Batch Processing**: Assess large-scale inference results efficiently
* ğŸ“ˆ **Historical Analysis**: Evaluate model performance on past predictions

## Specialized Evaluation Areas[â€‹](#specialized-evaluation-areas "Direct link to Specialized Evaluation Areas")

Our comprehensive evaluation framework is organized into specialized areas, each designed for specific aspects of model assessment:

[Model Evaluation](/mlflow-website/docs/latest/ml/evaluation/model-eval.md)

[Core model evaluation workflows for classification and regression tasks with automated metrics, visualizations, and performance assessment.](/mlflow-website/docs/latest/ml/evaluation/model-eval.md)

[Dataset Evaluation](/mlflow-website/docs/latest/ml/evaluation/dataset-eval.md)

[Evaluate static datasets and pre-computed predictions without re-running models, perfect for batch processing and historical analysis.](/mlflow-website/docs/latest/ml/evaluation/dataset-eval.md)

[Function Evaluation](/mlflow-website/docs/latest/ml/evaluation/function-eval.md)

[Lightweight evaluation of Python functions and custom prediction logic without the overhead of model logging and registration.](/mlflow-website/docs/latest/ml/evaluation/function-eval.md)

[Custom Metrics & Visualizations](/mlflow-website/docs/latest/ml/evaluation/metrics-visualizations.md)

[Define domain-specific evaluation criteria, custom metrics, and specialized visualizations tailored to your business requirements.](/mlflow-website/docs/latest/ml/evaluation/metrics-visualizations.md)

[SHAP Integration](/mlflow-website/docs/latest/ml/evaluation/shap.md)

[Deep model interpretation with SHAP values, feature importance analysis, and explainable AI capabilities for transparent ML.](/mlflow-website/docs/latest/ml/evaluation/shap.md)

[Plugin Evaluators](/mlflow-website/docs/latest/ml/evaluation/plugin-evaluators.md)

[Extend evaluation capabilities with specialized plugins like Giskard for vulnerability scanning and Trubrics for advanced validation.](/mlflow-website/docs/latest/ml/evaluation/plugin-evaluators.md)

## Advanced Evaluation Features[â€‹](#advanced-evaluation-features "Direct link to Advanced Evaluation Features")

### Enterprise Integration[â€‹](#enterprise-integration "Direct link to Enterprise Integration")

Production-Grade Evaluation

#### Model Governance[â€‹](#model-governance "Direct link to Model Governance")

* ğŸ“‹ **Audit Trails**: Complete evaluation history for regulatory compliance
* ğŸ”’ **Access Control**: Role-based evaluation permissions and result visibility
* ğŸ“Š **Executive Dashboards**: High-level model performance summaries for stakeholders
* ğŸ”„ **Automated Reporting**: Scheduled evaluation reports and performance alerts

#### MLOps Integration[â€‹](#mlops-integration "Direct link to MLOps Integration")

* ğŸš€ **CI/CD Pipelines**: Automated evaluation gates in deployment workflows
* ğŸ“ˆ **Performance Monitoring**: Continuous evaluation of production models
* ğŸ”„ **A/B Testing**: Statistical comparison of model variants in production
* ğŸ“Š **Drift Detection**: Automated alerts for model performance degradation

## Real-World Applications[â€‹](#real-world-applications "Direct link to Real-World Applications")

MLflow evaluation excels across diverse machine learning applications:

* ğŸ¦ **Financial Services**: Credit scoring model validation, fraud detection performance assessment, and regulatory compliance evaluation
* ğŸ¥ **Healthcare**: Medical AI model validation, diagnostic accuracy assessment, and safety-critical model certification
* ğŸ›’ **E-commerce**: Recommendation system evaluation, search relevance assessment, and personalization effectiveness measurement
* ğŸš— **Autonomous Systems**: Safety-critical model validation, edge case analysis, and robustness testing for self-driving vehicles
* ğŸ¯ **Marketing Technology**: Campaign effectiveness measurement, customer segmentation validation, and attribution model assessment
* ğŸ­ **Manufacturing**: Quality control model validation, predictive maintenance assessment, and process optimization evaluation
* ğŸ“± **Technology Platforms**: Content moderation effectiveness, user behavior prediction accuracy, and system performance optimization

## Getting Started[â€‹](#getting-started "Direct link to Getting Started")

Ready to elevate your model evaluation practices with MLflow? Choose the evaluation approach that best fits your current needs:

Quick Start Recommendations

#### For Data Scientists[â€‹](#for-data-scientists "Direct link to For Data Scientists")

Start with [Model Evaluation](/mlflow-website/docs/latest/ml/evaluation/model-eval.md) to understand comprehensive performance assessment, then explore **Custom Metrics** for domain-specific requirements.

#### For ML Engineers[â€‹](#for-ml-engineers "Direct link to For ML Engineers")

Begin with [Function Evaluation](/mlflow-website/docs/latest/ml/evaluation/function-eval.md) for lightweight testing, then advance to **Model Validation** for production readiness assessment.

#### For ML Researchers[â€‹](#for-ml-researchers "Direct link to For ML Researchers")

Explore [SHAP Integration](/mlflow-website/docs/latest/ml/evaluation/shap.md) for model interpretability, then investigate **Plugin Evaluators** for specialized analysis capabilities.

#### For Enterprise Teams[â€‹](#for-enterprise-teams "Direct link to For Enterprise Teams")

Start with [Model Validation](/mlflow-website/docs/latest/ml/evaluation/metrics-visualizations.md) for governance requirements, then implement [Dataset Evaluation](/mlflow-website/docs/latest/ml/evaluation/dataset-eval.md) for large-scale assessment workflows.

Whether you're validating your first model or implementing enterprise-scale evaluation frameworks, MLflow's comprehensive evaluation suite provides the tools and insights needed to build trustworthy, reliable machine learning systems that deliver real business value with confidence.
