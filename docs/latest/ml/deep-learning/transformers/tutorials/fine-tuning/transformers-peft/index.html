<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-classic-ml docs-version-current docs-doc-page docs-doc-id-deep-learning/transformers/tutorials/fine-tuning/transformers-peft-ipynb" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.2"><title data-rh=true>Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT | MLflow</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/ /><meta data-rh=true property=og:locale content=en /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docusaurus_version content=current /><meta data-rh=true name=docusaurus_tag content=docs-classic-ml-current /><meta data-rh=true name=docsearch:version content=current /><meta data-rh=true name=docsearch:docusaurus_tag content=docs-classic-ml-current /><meta data-rh=true property=og:title content="Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT | MLflow"/><meta data-rh=true name=description content="Download this notebook"/><meta data-rh=true property=og:description content="Download this notebook"/><link data-rh=true rel=icon href=/mlflow-website/docs/latest/images/favicon.ico /><link data-rh=true rel=canonical href=https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/ /><link data-rh=true rel=alternate href=https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/ hreflang=en /><link data-rh=true rel=alternate href=https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/ hreflang=x-default /><link data-rh=true rel=preconnect href=https://XKVLO8P882-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/","name":"Deep Learning","position":1},{"@type":"ListItem","item":"https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/","name":"Transformers","position":2},{"@type":"ListItem","item":"https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/","name":"Tutorials","position":3},{"@type":"ListItem","item":"https://mlflow.org/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft","name":"Leveraging PEFT for Fine Tuning","position":4}]}</script><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=GTM-N6WMTTJ"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","GTM-N6WMTTJ",{anonymize_ip:!0}),gtag("config","AW-16857946923",{anonymize_ip:!0})</script><link rel=preconnect href=https://www.googletagmanager.com><script>window.dataLayer=window.dataLayer||[],function(e,t,a,n,r){e[n]=e[n]||[],e[n].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var d=t.getElementsByTagName(a)[0],g=t.createElement(a);g.async=!0,g.src="https://www.googletagmanager.com/gtm.js?id="+r+("dataLayer"!=n?"&l="+n:""),d.parentNode.insertBefore(g,d)}(window,document,"script","dataLayer","GTM-N6WMTTJ")</script><link rel=search type=application/opensearchdescription+xml title=MLflow href=/mlflow-website/docs/latest/opensearch.xml><script src=/mlflow-website/docs/latest/js/runllm.js defer></script><link rel=stylesheet href=/mlflow-website/docs/latest/assets/css/styles.65681509.css /><script src=/mlflow-website/docs/latest/assets/js/runtime~main.a0db6d66.js defer></script><script src=/mlflow-website/docs/latest/assets/js/main.fe92a7ea.js defer></script></head><body class=navigation-with-keyboard><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript>


<svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/mlflow-website/docs/latest/><div class=navbar__logo><img src=/mlflow-website/docs/latest/images/logo-light.svg alt="MLflow Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"/><img src=/mlflow-website/docs/latest/images/logo-dark.svg alt="MLflow Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"/></div></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class="navbar__link docsDropdown_VGp6" data-active=ml><div style=display:flex;gap:8px;align-items:center><div class=dropdownCircle_n91F style=width:10px;height:10px;background-color:var(--ml-color-primary);border-radius:4px></div>ML Docs</div></a><ul class=dropdown__menu><li><a aria-current=page class="dropdown__link ml-docs-link dropdown__link--active" href=/mlflow-website/docs/latest/ml/><div style=display:flex;gap:8px;align-items:center><div style=width:10px;height:10px;background-color:var(--ml-color-primary);border-radius:4px></div>ML Docs</div></a><li><a class="dropdown__link genai-docs-link" href=/mlflow-website/docs/latest/genai/><div style=display:flex;gap:8px;align-items:center><div style=width:10px;height:10px;background-color:var(--genai-color-primary);border-radius:4px></div>GenAI Docs</div></a></ul></div><a href=https://mlflow.org/docs/latest/api_reference/index.html target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">API Reference</a><a class="navbar__item navbar__link" href=/mlflow-website/docs/latest/self-hosting/>Self-Hosting</a><a class="navbar__item navbar__link" href=/mlflow-website/docs/latest/community/>Community</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/mlflow/mlflow target=_blank rel="noopener noreferrer" class="navbar__item navbar__link github-link">GitHub<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sidebar-top-level-category"><a class=menu__link href=/mlflow-website/docs/latest/ml/><span title=MLflow class=linkLabel_WmDU>MLflow</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" href=/mlflow-website/docs/latest/ml/getting-started/><span title="Getting Started" class=categoryLinkLabel_W154>Getting Started</span></a><button aria-label="Expand sidebar category 'Getting Started'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item sidebar-top-level-category"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role=button aria-expanded=true href=/mlflow-website/docs/latest/ml/traditional-ml/><span title="Machine Learning" class=categoryLinkLabel_W154>Machine Learning</span></a></div><ul class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex=0 href=/mlflow-website/docs/latest/ml/traditional-ml/><span title="Traditional ML" class=categoryLinkLabel_W154>Traditional ML</span></a><button aria-label="Expand sidebar category 'Traditional ML'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/><span title="Deep Learning" class=categoryLinkLabel_W154>Deep Learning</span></a><button aria-label="Collapse sidebar category 'Deep Learning'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/pytorch/><span title=Pytorch class=linkLabel_WmDU>Pytorch</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/tensorflow/><span title=TensorFlow class=linkLabel_WmDU>TensorFlow</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/keras/><span title=Keras class=linkLabel_WmDU>Keras</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/><span title=Transformers class=categoryLinkLabel_W154>Transformers</span></a><button aria-label="Collapse sidebar category 'Transformers'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/guide/><span title="ðŸ¤— Transformers within MLflow" class=linkLabel_WmDU>ðŸ¤— Transformers within MLflow</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/large-models/><span title="Working with Large Transformers Models" class=linkLabel_WmDU>Working with Large Transformers Models</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/task/><span title="Transformers Task Types" class=linkLabel_WmDU>Transformers Task Types</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/><span title=Tutorials class=categoryLinkLabel_W154>Tutorials</span></a><button aria-label="Collapse sidebar category 'Tutorials'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model/><span title="Introduction to Conversational Models" class=linkLabel_WmDU>Introduction to Conversational Models</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/><span title="Custom Conversational Models" class=linkLabel_WmDU>Custom Conversational Models</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-fine-tuning/><span title="Introduction to Fine Tuning" class=linkLabel_WmDU>Introduction to Fine Tuning</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/><span title="Leveraging PEFT for Fine Tuning" class=linkLabel_WmDU>Leveraging PEFT for Fine Tuning</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/audio-transcription/whisper/><span title="Introduction to Audio Transcription" class=linkLabel_WmDU>Introduction to Audio Transcription</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/prompt-templating/prompt-templating/><span title="Introduction to Prompt Templating" class=linkLabel_WmDU>Introduction to Prompt Templating</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/text-generation/text-generation/><span title="Text Generation Models" class=linkLabel_WmDU>Text Generation Models</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/translation/component-translation/><span title="Translation Models" class=linkLabel_WmDU>Translation Models</span></a></ul></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/sentence-transformers/><span title="Sentence Transformers" class=categoryLinkLabel_W154>Sentence Transformers</span></a><button aria-label="Expand sidebar category 'Sentence Transformers'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/deep-learning/spacy/><span title=spaCy class=linkLabel_WmDU>spaCy</span></a></ul></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item sidebar-top-level-category"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=true href=/mlflow-website/docs/latest/ml/tracking/><span title="Build " class=categoryLinkLabel_W154>Build </span></a></div><ul class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex=0 href=/mlflow-website/docs/latest/ml/tracking/><span title="MLflow Tracking" class=categoryLinkLabel_W154>MLflow Tracking</span></a><button aria-label="Expand sidebar category 'MLflow Tracking'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/mlflow-website/docs/latest/ml/model/><span title="MLflow Model" class=categoryLinkLabel_W154>MLflow Model</span></a></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/mlflow-website/docs/latest/ml/dataset/><span title="MLflow Datasets" class=linkLabel_WmDU>MLflow Datasets</span></a></ul><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sidebar-top-level-category"><a class=menu__link href=/mlflow-website/docs/latest/ml/evaluation/><span title=Evaluate class=linkLabel_WmDU>Evaluate</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/mlflow-website/docs/latest/ml/model-registry/><span title=Deploy class=categoryLinkLabel_W154>Deploy</span></a></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/mlflow-website/docs/latest/ml/webhooks/><span title=Webhooks class=linkLabel_WmDU>Webhooks</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/mlflow-website/docs/latest/self-hosting/><span title="Team Collaboration" class=categoryLinkLabel_W154>Team Collaboration</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class=menu__list-item-collapsible><a href=https://mlflow.org/docs/latest/api_reference/python_api/index.html target=_blank rel="noopener noreferrer" class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false><span title="API References" class=categoryLinkLabel_W154>API References</span></a></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sidebar-top-level-category"><a class=menu__link href=/mlflow-website/docs/latest/ml/mlflow-3/><span title="MLflow 3 Migration Guide" class=linkLabel_WmDU>MLflow 3 Migration Guide</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class=menu__list-item-collapsible><a href=https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md target=_blank rel="noopener noreferrer" class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false><span title=More class=categoryLinkLabel_W154>More</span></a></div></ul></nav></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><div class=breadcrumbsWrapper_xlkt><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/mlflow-website/docs/latest/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><span class=breadcrumbs__link>Machine Learning</span><li class=breadcrumbs__item><a class=breadcrumbs__link href=/mlflow-website/docs/latest/ml/deep-learning/><span>Deep Learning</span></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/mlflow-website/docs/latest/ml/deep-learning/transformers/><span>Transformers</span></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/><span>Tutorials</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>Leveraging PEFT for Fine Tuning</span></ul></nav></div><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT</h1></header>
<a class="button button--primary" style=margin-bottom:1rem;display:block;width:min-content href=https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft.ipynb download="">Download this notebook</a>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=overview>Overview<a href=#overview class=hash-link aria-label="Direct link to Overview" title="Direct link to Overview" translate=no>â€‹</a></h2>
<p>Many powerful open-source LLMs have emerged and are easily accessible. However, they are not designed to be deployed to your production environment out-of-the-box; instead, you have to <strong>fine-tune</strong> them for your specific tasks, such as a chatbot, content generation, etc. One challenge, though, is that training LLMs is usually very expensive. Even if your dataset for fine-tuning is small, the backpropagation step needs to compute gradients for billions of parameters. For example, fully fine-tuning the Llama7B model requires 112GB of VRAM, i.e. at least two 80GB A100 GPUs. Fortunately, there are many research efforts on how to reduce the cost of LLM fine-tuning.</p>
<p>In this tutorial, we will demonstrate how to build a powerful <strong>text-to-SQL</strong> generator by fine-tuning the Mistral 7B model with <strong>a single 24GB VRAM GPU</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=what-you-will-learn>What You Will Learn<a href=#what-you-will-learn class=hash-link aria-label="Direct link to What You Will Learn" title="Direct link to What You Will Learn" translate=no>â€‹</a></h3>
<ol>
<li class="">Hands-on learning of the typical LLM fine-tuning process.</li>
<li class="">Understand how to use <strong>QLoRA</strong> and <strong>PEFT</strong> to overcome the GPU memory limitation for fine-tuning.</li>
<li class="">Manage the model training cycle using <strong>MLflow</strong> to log the model artifacts, hyperparameters, metrics, and prompts.</li>
<li class="">How to save prompt template and inference parameters (e.g. max_token_length) in MLflow to simplify prediction interface.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-actors>Key Actors<a href=#key-actors class=hash-link aria-label="Direct link to Key Actors" title="Direct link to Key Actors" translate=no>â€‹</a></h3>
<p>In this tutorial, you will learn about the techniques and methods behind efficient LLM fine-tuning by actually running the code. There are more detailed explanations for each cell below, but let's start with a brief preview of a few main important libraries/methods used in this tutorial.</p>
<ul>
<li class=""><a href=https://huggingface.co/mistralai/Mistral-7B-v0.1 target=_blank rel="noopener noreferrer" class="">Mistral-7B-v0.1</a> model is a pretrained text-generation model with 7 billion parameters, developed by <a href=https://mistral.ai/ target=_blank rel="noopener noreferrer" class="">mistral.ai</a>. The model employs various optimization techniques such as Group-Query Attention, Sliding-Window Attention, Byte-fallback BPE tokenizer, and outperforms the Llama 2 13B on benchmarks with fewer parameters.</li>
<li class=""><a href=https://github.com/artidoro/qlora target=_blank rel="noopener noreferrer" class="">QLoRA</a> is a novel method that allows us to fine-tune large foundational models with limited GPU resources. It reduces the number of trainable parameters by learning pairs of rank-decomposition matrices and also applies 4-bit quantization to the frozen pretrained model to further reduce the memory footprint.</li>
<li class=""><a href=https://huggingface.co/docs/peft/en/index target=_blank rel="noopener noreferrer" class="">PEFT</a> is a library developed by HuggingFaceðŸ¤—, that enables developers to easily integrate various optimization methods with pretrained models available on the HuggingFace Hub. With PEFT, you can apply QLoRA to the pretrained model with a few lines of configurations and run fine-tuning just like the normal Transformers model training.</li>
<li class=""><a href=https://mlflow.org/ target=_blank rel="noopener noreferrer" class="">MLflow</a> manages an exploding number of configurations, assets, and metrics during the LLM training on your behalf. MLflow is natively integrated with Transformers and PEFT, and plays a crucial role in organizing the fine-tuning cycle.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=1-environment-set-up>1. Environment Set up<a href=#1-environment-set-up class=hash-link aria-label="Direct link to 1. Environment Set up" title="Direct link to 1. Environment Set up" translate=no>â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=hardware-requirement>Hardware Requirement<a href=#hardware-requirement class=hash-link aria-label="Direct link to Hardware Requirement" title="Direct link to Hardware Requirement" translate=no>â€‹</a></h3>
<p>Please ensure your GPU has at least 20GB of VRAM available. This notebook has been tested on a single NVIDIA A10G GPU with 24GB of VRAM.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token operator">%</span><span class="token plain">sh nvidia</span><span class="token operator">-</span><span class="token plain">smi</span><br/></span></code></pre></div></div></div>
<pre style=margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)>
Wed Feb 21 07:16:13 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |
|  0%   15C    P8              16W / 300W |      4MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                       
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</pre>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=install-python-libraries>Install Python Libraries<a href=#install-python-libraries class=hash-link aria-label="Direct link to Install Python Libraries" title="Direct link to Install Python Libraries" translate=no>â€‹</a></h3>
<p>This tutorial utilizes the following Python libraries:</p>
<ul>
<li class=""><a href=https://pypi.org/project/mlflow/ target=_blank rel="noopener noreferrer" class="">mlflow</a> - for tracking parameters, metrics, and saving trained models. Version <strong>2.11.0 or later</strong> is required to log PEFT models with MLflow.</li>
<li class=""><a href=https://pypi.org/project/transformers/ target=_blank rel="noopener noreferrer" class="">transformers</a> - for defining the model, tokenizer, and trainer.</li>
<li class=""><a href=https://pypi.org/project/peft/ target=_blank rel="noopener noreferrer" class="">peft</a> - for creating a LoRA adapter on top of the Transformer model.</li>
<li class=""><a href=https://pypi.org/project/bitsandbytes/ target=_blank rel="noopener noreferrer" class="">bitsandbytes</a> - for loading the base model with 4-bit quantization for QLoRA.</li>
<li class=""><a href=https://pypi.org/project/accelerate/ target=_blank rel="noopener noreferrer" class="">accelerate</a> - a dependency required by bitsandbytes.</li>
<li class=""><a href=https://pypi.org/project/datasets/ target=_blank rel="noopener noreferrer" class="">datasets</a> - for loading the training dataset from the HuggingFace hub.</li>
</ul>
<p><strong>Note</strong>: Restarting the Python kernel may be necessary after installing these dependencies.</p>
<p>The notebook has been tested with <code>mlflow==2.11.0</code>, <code>transformers==4.35.2</code>, <code>peft==0.8.2</code>, <code>bitsandbytes==0.42.0</code>, <code>accelerate==0.27.2</code>, and <code>datasets==2.17.1</code>.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token operator">%</span><span class="token plain">pip install mlflow</span><span class="token operator">>=</span><span class="token number">2.11</span><span class="token number">.0</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token operator">%</span><span class="token plain">pip install transformers peft accelerate bitsandbytes datasets </span><span class="token operator">-</span><span class="token plain">q </span><span class="token operator">-</span><span class="token plain">U</span><br/></span></code></pre></div></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=2-dataset-preparation>2. Dataset Preparation<a href=#2-dataset-preparation class=hash-link aria-label="Direct link to 2. Dataset Preparation" title="Direct link to 2. Dataset Preparation" translate=no>â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=load-dataset-from-huggingface-hub>Load Dataset from HuggingFace Hub<a href=#load-dataset-from-huggingface-hub class=hash-link aria-label="Direct link to Load Dataset from HuggingFace Hub" title="Direct link to Load Dataset from HuggingFace Hub" translate=no>â€‹</a></h3>
<p>We will use the <code>b-mc2/sql-create-context</code> dataset from the <a href=https://huggingface.co/datasets/b-mc2/sql-create-context target=_blank rel="noopener noreferrer" class="">Hugging Face Hub</a> for this tutorial. This dataset comprises 78.6k pairs of natural language queries and their corresponding SQL statements, making it ideal for training a text-to-SQL model. The dataset includes three columns:</p>
<ul>
<li class=""><code>question</code>: A natural language question posed regarding the data.</li>
<li class=""><code>context</code>: Additional information about the data, such as the schema for the table being queried.</li>
<li class=""><code>answer</code>: The SQL query that represents the expected output.</li>
</ul>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> pandas </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> pd</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> datasets </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> load_dataset</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> IPython</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">display </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> HTML</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> display</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">dataset_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"b-mc2/sql-create-context"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">dataset </span><span class="token operator">=</span><span class="token plain"> load_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> split</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"train"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">display_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset_or_sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  pd</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">set_option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"display.max_colwidth"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token boolean">None</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  pd</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">set_option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"display.width"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token boolean">None</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  pd</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">set_option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"display.max_rows"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token boolean">None</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">isinstance</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset_or_sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">dict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      df </span><span class="token operator">=</span><span class="token plain"> pd</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">DataFrame</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset_or_sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> index</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">else</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      df </span><span class="token operator">=</span><span class="token plain"> pd</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">DataFrame</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset_or_sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  html </span><span class="token operator">=</span><span class="token plain"> df</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">to_html</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">replace</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"\n"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"&lt;br>"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  styled_html </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f"""&lt;style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} &lt;/style> </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">html</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">"""</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  display</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">HTML</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">styled_html</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">display_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">select</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<div style=flex-grow:1;min-width:0;font-size:0.8rem;width:100%><div><style>.dataframe th,.dataframe tbody td{text-align:left;padding-right:30px}</style> <table border=1 class=dataframe>
<thead>
  <tr style="text-align: right;">
    <th></th>
    <th>question</th>
    <th>context</th>
    <th>answer</th>
  </tr>
</thead>
<tbody>
  <tr>
    <th>0</th>
    <td>How many heads of the departments are older than 56 ?</td>
    <td>CREATE TABLE head (age INTEGER)</td>
    <td>SELECT COUNT(*) FROM head WHERE age > 56</td>
  </tr>
  <tr>
    <th>1</th>
    <td>List the name, born state and age of the heads of departments ordered by age.</td>
    <td>CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)</td>
    <td>SELECT name, born_state, age FROM head ORDER BY age</td>
  </tr>
  <tr>
    <th>2</th>
    <td>List the creation year, name and budget of each department.</td>
    <td>CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)</td>
    <td>SELECT creation, name, budget_in_billions FROM department</td>
  </tr>
</tbody>
</table></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=split-train-and-test-dataset>Split Train and Test Dataset<a href=#split-train-and-test-dataset class=hash-link aria-label="Direct link to Split Train and Test Dataset" title="Direct link to Split Train and Test Dataset" translate=no>â€‹</a></h3>
<p>The <code>b-mc2/sql-create-context</code> dataset consists of a single split, "train". We will separate 20% of this as test samples.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain">split_dataset </span><span class="token operator">=</span><span class="token plain"> dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">train_test_split</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">test_size</span><span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> seed</span><span class="token operator">=</span><span class="token number">42</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">train_dataset </span><span class="token operator">=</span><span class="token plain"> split_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"train"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">test_dataset </span><span class="token operator">=</span><span class="token plain"> split_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"test"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f"Training dataset contains </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation builtin" style="color:rgb(189, 147, 249)">len</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation interpolation">train_dataset</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)"> text-to-SQL pairs"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f"Test dataset contains </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation builtin" style="color:rgb(189, 147, 249)">len</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation interpolation">test_dataset</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)"> text-to-SQL pairs"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<pre style=margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)>
Training dataset contains 62861 text-to-SQL pairs
Test dataset contains 15716 text-to-SQL pairs</pre>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=define-prompt-template>Define Prompt Template<a href=#define-prompt-template class=hash-link aria-label="Direct link to Define Prompt Template" title="Direct link to Define Prompt Template" translate=no>â€‹</a></h3>
<p>The Mistral 7B model is a text comprehension model, so we have to construct a text prompt that incorporates the user's question, context, and our system instructions. The new <code>prompt</code> column in the dataset will contain the text prompt to be fed into the model during training. It is important to note that we also include the expected response within the prompt, allowing the model to be trained in a self-supervised manner.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain">PROMPT_TEMPLATE </span><span class="token operator">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(255, 121, 198)"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">### Table:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">{context}</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(255, 121, 198)"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">### Question:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">{question}</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(255, 121, 198)"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">### Response:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">{output}"""</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">apply_prompt_template</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">row</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  prompt </span><span class="token operator">=</span><span class="token plain"> PROMPT_TEMPLATE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      question</span><span class="token operator">=</span><span class="token plain">row</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"question"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      context</span><span class="token operator">=</span><span class="token plain">row</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"context"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      output</span><span class="token operator">=</span><span class="token plain">row</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"answer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">"prompt"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">train_dataset </span><span class="token operator">=</span><span class="token plain"> train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">map</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">apply_prompt_template</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">display_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">select</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<div style=flex-grow:1;min-width:0;font-size:0.8rem;width:100%><div><style>.dataframe th,.dataframe tbody td{text-align:left;padding-right:30px}</style> <table border=1 class=dataframe>
<thead>
  <tr style="text-align: right;">
    <th></th>
    <th>question</th>
    <th>context</th>
    <th>answer</th>
    <th>prompt</th>
  </tr>
</thead>
<tbody>
  <tr>
    <th>0</th>
    <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td>
    <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td>
    <td>SELECT perth FROM table_name_56 WHERE gold_coast = "yes" AND sydney = "yes" AND melbourne = "yes" AND adelaide = "yes"</td>
    <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)<br><br>### Question:<br>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?<br><br>### Response:<br>SELECT perth FROM table_name_56 WHERE gold_coast = "yes" AND sydney = "yes" AND melbourne = "yes" AND adelaide = "yes"</td>
  </tr>
</tbody>
</table></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=padding-the-training-dataset>Padding the Training Dataset<a href=#padding-the-training-dataset class=hash-link aria-label="Direct link to Padding the Training Dataset" title="Direct link to Padding the Training Dataset" translate=no>â€‹</a></h3>
<p>As a final step of dataset preparation, we need to apply <strong>padding</strong> to the training dataset. Padding ensures that all input sequences in a batch are of the same length.</p>
<p>A crucial point to note is the need to <em>add padding to the left</em>. This approach is adopted because the model generates tokens autoregressively, meaning it continues from the last token. Adding padding to the right would cause the model to generate new tokens from these padding tokens, resulting in the output sequence including padding tokens in the middle.</p>
<ul>
<li class="">Padding to right</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for text code with copy and toggle buttons"><span class=languageLabel_zr_I>text</span></div><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain">Today |  is  |   a    |  cold  |  &lt;pad>  ==generate=>  "Today is a cold &lt;pad> day"</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"> How  |  to  | become |  &lt;pad> |  &lt;pad>  ==generate=>  "How to become a &lt;pad> &lt;pad> great engineer".</span><br/></span></code></pre></div></div>
<ul>
<li class="">Padding to left:</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for text code with copy and toggle buttons"><span class=languageLabel_zr_I>text</span></div><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain">&lt;pad> |  Today  |  is  |  a   |  cold     ==generate=>  "&lt;pad> Today is a cold day"</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">&lt;pad> |  &lt;pad>  |  How |  to  |  become   ==generate=>  "&lt;pad> &lt;pad> How to become a great engineer".</span><br/></span></code></pre></div></div>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> AutoTokenizer</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">base_model_id </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"mistralai/Mistral-7B-v0.1"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># You can use a different max length if your custom dataset has shorter/longer input sequences.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">MAX_LENGTH </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">256</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">tokenizer </span><span class="token operator">=</span><span class="token plain"> AutoTokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  base_model_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  model_max_length</span><span class="token operator">=</span><span class="token plain">MAX_LENGTH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  padding_side</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"left"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  add_eos_token</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pad_token </span><span class="token operator">=</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">eos_token</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">tokenize_and_pad_to_fixed_length</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  result </span><span class="token operator">=</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"prompt"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      truncation</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      max_length</span><span class="token operator">=</span><span class="token plain">MAX_LENGTH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      padding</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"max_length"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  result</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"labels"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> result</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"input_ids"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">copy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> result</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">tokenized_train_dataset </span><span class="token operator">=</span><span class="token plain"> train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">map</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">tokenize_and_pad_to_fixed_length</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">assert</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">all</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token builtin" style="color:rgb(189, 147, 249)">len</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"input_ids"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">==</span><span class="token plain"> MAX_LENGTH </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> x </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> tokenized_train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">display_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">tokenized_train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">select</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=3-load-the-base-model-with-4-bit-quantization>3. Load the Base Model (with 4-bit quantization)<a href=#3-load-the-base-model-with-4-bit-quantization class=hash-link aria-label="Direct link to 3. Load the Base Model (with 4-bit quantization)" title="Direct link to 3. Load the Base Model (with 4-bit quantization)" translate=no>â€‹</a></h2>
<p>Next, we'll load the Mistral 7B model, which will serve as our base model for fine-tuning. This model can be loaded from the HuggingFace Hub repository <a href=https://huggingface.co/mistralai/Mistral-7B-v0.1 target=_blank rel="noopener noreferrer" class="">mistralai/Mistral-7B-v0.1</a> using the Transformers' <code>from_pretrained()</code> API. However, here we are also providing a <code>quantization_config</code> parameter.</p>
<p>This parameter embodies the key technique of <a href=https://github.com/artidoro/qlora target=_blank rel="noopener noreferrer" class="">QLoRA</a> that significantly reduces memory usage during fine-tuning. The following paragraph details the method and the implications of this configuration. However, feel free to skip if it appears complex. After all, we rarely need to modify the <code>quantization_config</code> values ourselves :)</p>
<p><strong>How It Works</strong></p>
<p>In short, QLoRA is a combination of <strong>Q</strong>uantization and <strong>LoRA</strong>. To grasp its functionality, it's simpler to begin with LoRA. <a href=https://github.com/microsoft/LoRA target=_blank rel="noopener noreferrer" class="">LoRA (Low Rank Adaptation)</a> is a preceding method for resource-efficient fine-tuning, by reducing the number of trainable parameters through matrix decomposition. Let <code>W'</code> represent the final weight matrix from fine-tuning. In LoRA, <code>W'</code> is approximated by the sum of the original weight and its update, i.e., <code>W + Î”W</code>, then decomposing the delta part into two low-dimensional matrices, i.e., <code>Î”W â‰ˆ AB</code>. Suppose <code>W</code> is <code>m</code>x<code>m</code>, and we select a smaller <code>r</code> for the rank of <code>A</code> and <code>B</code>, where <code>A</code> is <code>m</code>x<code>r</code> and <code>B</code> is <code>r</code>x<code>m</code>. Now, the original trainable parameters, which are quadratic in size of <code>W</code> (i.e., <code>m^2</code>), after decomposition, become <code>2mr</code>. Empirically, we can choose a much smaller number for <code>r</code>, e.g., 32, 64, compared to the full weight matrix size, therefore this significantly reduces the number of parameters to train.</p>
<p><a href=https://github.com/artidoro/qlora target=_blank rel="noopener noreferrer" class="">QLoRA</a> extends LoRA, employing the same strategy for matrix decomposition. However, it further reduces memory usage by applying 4-bit quantization to the frozen pretrained model <code>W</code>. According to their research, the largest memory usage during LoRA fine-tuning is the backpropagation through the frozen parameters <code>W</code> to compute gradients for the adaptors <code>A</code> and <code>B</code>. Thus, quantizing <code>W</code> to 4-bit significantly reduces the overall memory consumption. This is achieved with the <code>load_in_4bit=True</code> setting shown below.</p>
<p>Moreover, QLoRA introduces additional techniques to optimize resource usage without significantly impacting model performance. For more technical details, please refer to <a href=https://arxiv.org/pdf/2305.14314.pdf target=_blank rel="noopener noreferrer" class="">the paper</a>, but we implement them by setting the following quantization configurations in bitsandbytes:</p>
<ul>
<li class="">The 4-bit NormalFloat type is specified by <code>bnb_4bit_quant_type="nf4"</code>.</li>
<li class="">Double quantization is activated by <code>bnb_4bit_use_double_quant=True</code>.</li>
<li class="">QLoRA re-quantizes the 4-bit weights back to a higher precision when computing the gradients for <code>A</code> and <code>B</code>, to prevent performance degradation. This datatype is specified by <code>bnb_4bit_compute_dtype=torch.bfloat16</code>.</li>
</ul>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> torch</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> AutoModelForCausalLM</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> BitsAndBytesConfig</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">quantization_config </span><span class="token operator">=</span><span class="token plain"> BitsAndBytesConfig</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Load the model with 4-bit quantization</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  load_in_4bit</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use double quantization</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  bnb_4bit_use_double_quant</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use 4-bit Normal Float for storing the base model weights in GPU memory</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  bnb_4bit_quant_type</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"nf4"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># De-quantize the weights to 16-bit (Brain) float before the forward/backward pass</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  bnb_4bit_compute_dtype</span><span class="token operator">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">bfloat16</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> AutoModelForCausalLM</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">base_model_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> quantization_config</span><span class="token operator">=</span><span class="token plain">quantization_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=how-does-the-base-model-perform>How Does the Base Model Perform?<a href=#how-does-the-base-model-perform class=hash-link aria-label="Direct link to How Does the Base Model Perform?" title="Direct link to How Does the Base Model Perform?" translate=no>â€‹</a></h3>
<p>First, let's assess the performance of the vanilla Mistral model on the SQL generation task before any fine-tuning. As expected, the model does not produce correct SQL queries; instead, it generates random answers in natural language. This outcome indicates the necessity of fine-tuning the model for our specific task.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> transformers</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">tokenizer </span><span class="token operator">=</span><span class="token plain"> AutoTokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">base_model_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">pipeline </span><span class="token operator">=</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pipeline</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token operator">=</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenizer</span><span class="token operator">=</span><span class="token plain">tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> task</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"text-generation"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">sample </span><span class="token operator">=</span><span class="token plain"> test_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">prompt </span><span class="token operator">=</span><span class="token plain"> PROMPT_TEMPLATE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  context</span><span class="token operator">=</span><span class="token plain">sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"context"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> question</span><span class="token operator">=</span><span class="token plain">sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"question"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> output</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">""</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Leave the answer part blank</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">no_grad</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  response </span><span class="token operator">=</span><span class="token plain"> pipeline</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> max_new_tokens</span><span class="token operator">=</span><span class="token number">256</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> repetition_penalty</span><span class="token operator">=</span><span class="token number">1.15</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> return_full_text</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">display_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">"prompt"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"generated_query"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> response</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"generated_text"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<div style=flex-grow:1;min-width:0;font-size:0.8rem;width:100%><div><style>.dataframe th,.dataframe tbody td{text-align:left;padding-right:30px}</style> <table border=1 class=dataframe>
<thead>
  <tr style="text-align: right;">
    <th></th>
    <th>prompt</th>
    <th>generated_query</th>
  </tr>
</thead>
<tbody>
  <tr>
    <th>0</th>
    <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)<br><br>### Question:<br>What is the lowest numbered game against Phoenix with a record of 29-17?<br><br>### Response:<br></td>
    <td><br>A: The lowest numbered game against Phoenix was played on 03/04/2018. The score was PHO 115 - DAL 106.<br>What is the highest numbered game against Phoenix?<br>A: The highest numbered game against Phoenix was played on 03/04/2018. The score was PHO 115 - DAL 106.<br>Which players have started at Point Guard for Dallas in a regular season game against Phoenix?</td>
  </tr>
</tbody>
</table></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=4-define-a-peft-model>4. Define a PEFT Model<a href=#4-define-a-peft-model class=hash-link aria-label="Direct link to 4. Define a PEFT Model" title="Direct link to 4. Define a PEFT Model" translate=no>â€‹</a></h2>
<p>As discussed earlier, QLoRA stands for <strong>Quantization</strong> + <strong>LoRA</strong>. Having applied the quantization part, we now proceed with the LoRA aspect. Although the mathematics behind LoRA is intricate, <a href=https://huggingface.co/docs/peft/en/index target=_blank rel="noopener noreferrer" class="">PEFT</a> helps us by simplifying the process of adapting LoRA to the pretrained Transformer model.</p>
<p>In the next cell, we create a <a href=https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py target=_blank rel="noopener noreferrer" class="">LoraConfig</a> with various settings for LoRA. Contrary to the earlier <code>quantization_config</code>, these hyperparameters might need optimization to achieve the best model performance for your specific task. <strong>MLflow</strong> facilitates this process by tracking these hyperparameters, the associated model, and its outcomes.</p>
<p>At the end of the cell, we display the number of trainable parameters during fine-tuning, and their percentage relative to the total model parameters. Here, we are training only 1.16% of the total 7 billion parameters.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> peft </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> LoraConfig</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> get_peft_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> prepare_model_for_kbit_training</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Enabling gradient checkpointing, to make the training further efficient</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">gradient_checkpointing_enable</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Set up the model for quantization-aware training e.g. casting layers, parameter freezing, etc.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> prepare_model_for_kbit_training</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">peft_config </span><span class="token operator">=</span><span class="token plain"> LoraConfig</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  task_type</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"CAUSAL_LM"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  r</span><span class="token operator">=</span><span class="token number">32</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># This is the coefficient for the learned Î”W factor, so the larger number will typically result in a larger behavior change after fine-tuning.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  lora_alpha</span><span class="token operator">=</span><span class="token number">64</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Drop out ratio for the layers in LoRA adaptors A and B.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  lora_dropout</span><span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  target_modules</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"q_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"k_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"v_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"o_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"gate_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"up_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"down_proj"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      </span><span class="token string" style="color:rgb(255, 121, 198)">"lm_head"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Bias parameters to train. 'none' is recommended to keep the original model performing equally when turning off the adapter.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  bias</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"none"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">peft_model </span><span class="token operator">=</span><span class="token plain"> get_peft_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> peft_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">peft_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">print_trainable_parameters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<pre style=margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)>trainable params: 85,041,152 || all params: 7,326,773,248 || trainable%: 1.1606903765339511</pre>
<p><strong>That's it!!!</strong> PEFT has made the LoRA setup super easy.</p>
<p>An additional bonus is that the PEFT model exposes the same interfaces as a Transformers model. This means that everything from here on is quite similar to the standard model training process using Transformers.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=5-kick-off-a-training-job>5. Kick-off a Training Job<a href=#5-kick-off-a-training-job class=hash-link aria-label="Direct link to 5. Kick-off a Training Job" title="Direct link to 5. Kick-off a Training Job" translate=no>â€‹</a></h2>
<p>Similar to conventional Transformers training, we'll first set up a Trainer object to organize the training iterations. There are numerous hyperparameters to configure, but MLflow will manage them on your behalf.</p>
<p>To enable MLflow logging, you can specify <code>report_to="mlflow"</code> and name your training trial with the <code>run_name</code> parameter. This action initiates an <a href=https://mlflow.org/docs/latest/ml/tracking.html#runs target=_blank rel="noopener noreferrer" class="">MLflow run</a> that automatically logs training metrics, hyperparameters, configurations, and the trained model.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> datetime </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> datetime</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> transformers</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> TrainingArguments</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> mlflow</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Comment-out this line if you are running the tutorial on Databricks</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">set_experiment</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"MLflow PEFT Tutorial"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">training_args </span><span class="token operator">=</span><span class="token plain"> TrainingArguments</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Set this to mlflow for logging your training</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  report_to</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"mlflow"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Name the MLflow run</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  run_name</span><span class="token operator">=</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f"Mistral-7B-SQL-QLoRA-</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">datetime</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token string-interpolation interpolation">now</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token string-interpolation interpolation">strftime</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation interpolation string" style="color:rgb(255, 121, 198)">'%Y-%m-%d-%H-%M-%s'</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Replace with your output destination</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  output_dir</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"YOUR_OUTPUT_DIR"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># For the following arguments, refer to https://huggingface.co/docs/transformers/main_classes/trainer</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  per_device_train_batch_size</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  gradient_accumulation_steps</span><span class="token operator">=</span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  gradient_checkpointing</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  optim</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"paged_adamw_8bit"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  bf16</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  learning_rate</span><span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  lr_scheduler_type</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"constant"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  max_steps</span><span class="token operator">=</span><span class="token number">500</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  save_steps</span><span class="token operator">=</span><span class="token number">100</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  logging_steps</span><span class="token operator">=</span><span class="token number">100</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  warmup_steps</span><span class="token operator">=</span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># https://discuss.huggingface.co/t/training-llama-with-lora-on-multiple-gpus-may-exist-bug/47005/3</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  ddp_find_unused_parameters</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">trainer </span><span class="token operator">=</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Trainer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  model</span><span class="token operator">=</span><span class="token plain">peft_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  train_dataset</span><span class="token operator">=</span><span class="token plain">tokenized_train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  data_collator</span><span class="token operator">=</span><span class="token plain">transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">DataCollatorForLanguageModeling</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> mlm</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  args</span><span class="token operator">=</span><span class="token plain">training_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># use_cache=True is incompatible with gradient checkpointing.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">peft_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">use_cache </span><span class="token operator">=</span><span class="token plain"> </span><span class="token boolean">False</span><br/></span></code></pre></div></div></div>
<p>The training duration may span several hours, contingent upon your hardware specifications. Nonetheless, the primary objective of this tutorial is to acquaint you with the process of fine-tuning using PEFT and MLflow, rather than to cultivate a highly performant SQL generator. If you don't care much about the model performance, you may specify a smaller number of steps or interrupt the following cell to proceed with the rest of the notebook.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain">trainer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<div style=flex-grow:1;min-width:0;font-size:0.8rem;width:100%><div>
  <div>

    <progress value=500 max=500 style="width:300px; height:20px; vertical-align: middle;"></progress>
    [500/500 45:41, Epoch 0/1]
  </div>
  <table border=1 class=dataframe>
<thead>
<tr style="text-align: left;">
    <th>Step</th>
    <th>Training Loss</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>100</td>
    <td>0.681700</td>
  </tr>
  <tr>
    <td>200</td>
    <td>0.522400</td>
  </tr>
  <tr>
    <td>300</td>
    <td>0.507300</td>
  </tr>
  <tr>
    <td>400</td>
    <td>0.494800</td>
  </tr>
  <tr>
    <td>500</td>
    <td>0.474600</td>
  </tr>
</tbody>
</table><p></div></div>
<pre style=margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)>TrainOutput(global_step=500, training_loss=0.5361956100463867, metrics={'train_runtime': 2747.9223, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.182, 'total_flos': 4.421038813216768e+16, 'train_loss': 0.5361956100463867, 'epoch': 0.06})</pre>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=6-save-the-peft-model-to-mlflow>6. Save the PEFT Model to MLflow<a href=#6-save-the-peft-model-to-mlflow class=hash-link aria-label="Direct link to 6. Save the PEFT Model to MLflow" title="Direct link to 6. Save the PEFT Model to MLflow" translate=no>â€‹</a></h2>
<p>Hooray! We have successfully fine-tuned the Mistral 7B model into an SQL generator. Before concluding the training, one final step is to save the trained PEFT model to MLflow.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=set-prompt-template-and-default-inference-parameters-optional>Set Prompt Template and Default Inference Parameters (optional)<a href=#set-prompt-template-and-default-inference-parameters-optional class=hash-link aria-label="Direct link to Set Prompt Template and Default Inference Parameters (optional)" title="Direct link to Set Prompt Template and Default Inference Parameters (optional)" translate=no>â€‹</a></h3>
<p>LLMs prediction behavior is not only defined by the model weights, but also largely controlled by the prompt and inference paramters such as <code>max_token_length</code>, <code>repetition_penalty</code>. Therefore, it is highly advisable to save those metadata along with the model, so that you can expect the consistent behavior when loading the model later.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=prompt-template>Prompt Template<a href=#prompt-template class=hash-link aria-label="Direct link to Prompt Template" title="Direct link to Prompt Template" translate=no>â€‹</a></h4>
<p>The user prompt itself is free text, but you can harness the input by applying a 'template'. MLflow Transformer flavor supports saving a prompt template with the model, and apply it automatically before the prediction. This also allows you to hide the system prompt from model clients. To save the prompt template, we have to define a single string that contains <code>{prompt}</code> variable, and pass it to the <code>prompt_template</code> argument of <a href=https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model target=_blank rel="noopener noreferrer" class="">mlflow.transformers.log_model</a> API. Refer to <a href=https://mlflow.org/docs/latest/ml/deep-learning/transformers/guide/index.html#saving-prompt-templates-with-transformer-pipelines target=_blank rel="noopener noreferrer" class="">Saving Prompt Templates with Transformer Pipelines</a> for more detailed usage of this feature.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token comment" style="color:rgb(98, 114, 164)"># Basically the same format as we applied to the dataset. However, the template only accepts {prompt} variable so both table and question need to be fed in there.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">prompt_template </span><span class="token operator">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(255, 121, 198)"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">{prompt}</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(255, 121, 198)"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">### Response:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""</span><br/></span></code></pre></div></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=inference-parameters>Inference Parameters<a href=#inference-parameters class=hash-link aria-label="Direct link to Inference Parameters" title="Direct link to Inference Parameters" translate=no>â€‹</a></h4>
<p>Inference parameters can be saved with MLflow model as a part of <a href=https://mlflow.org/docs/latest/ml/model/signatures.html target=_blank rel="noopener noreferrer" class="">Model Signature</a>. The signature defines model input and output format with additional parameters passed to the model prediction, and you can let MLflow to infer it from some sample input using <a href=https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.infer_signature target=_blank rel="noopener noreferrer" class="">mlflow.models.infer_signature</a> API. If you pass the concrete value for parameters, MLflow treats them as default values and apply them at the inference if they are not provided by users. For more details about the Model Signature, please refer to the <a href=https://mlflow.org/docs/latest/ml/model/signatures.html target=_blank rel="noopener noreferrer" class="">MLflow documentation</a>.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">models </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> infer_signature</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">sample </span><span class="token operator">=</span><span class="token plain"> train_dataset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># MLflow infers schema from the provided sample input/output/params</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">signature </span><span class="token operator">=</span><span class="token plain"> infer_signature</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  model_input</span><span class="token operator">=</span><span class="token plain">sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"prompt"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  model_output</span><span class="token operator">=</span><span class="token plain">sample</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"answer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parameters are saved with default values if specified</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  params</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">"max_new_tokens"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">256</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"repetition_penalty"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1.15</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"return_full_text"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">signature</span><br/></span></code></pre></div></div></div>
<pre style=margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)>
inputs: 
[string (required)]
outputs: 
[string (required)]
params: 
['max_new_tokens': long (default: 256), 'repetition_penalty': double (default: 1.15), 'return_full_text': boolean (default: False)]</pre>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=save-the-peft-model-to-mlflow>Save the PEFT Model to MLflow<a href=#save-the-peft-model-to-mlflow class=hash-link aria-label="Direct link to Save the PEFT Model to MLflow" title="Direct link to Save the PEFT Model to MLflow" translate=no>â€‹</a></h3>
<p>Finally, we will call <a href=https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model target=_blank rel="noopener noreferrer" class="">mlflow.transformers.log_model</a> API to log the model to MLflow. A few critical points to remember when logging a PEFT model to MLflow are:</p>
<ol>
<li class=""><strong>MLflow logs the Transformer model as a <a href=https://huggingface.co/docs/transformers/en/main_classes/pipelines target=_blank rel="noopener noreferrer" class="">Pipeline</a>.</strong> A pipeline bundles a model with its tokenizer (or other components, depending on the task type) and simplifies the prediction steps into an easy-to-use interface, making it an excellent tool for ensuring reproducibility. In the code below, we pass the model and tokenizer as a dictionary, then MLflow automatically deduces the correct pipeline type and saves it.</li>
<li class=""><strong>MLflow does not save the base model weight for the PEFT model</strong>. When executing <code>mlflow.transformers.log_model</code>, MLflow only saves the small number of trained parameters, i.e., the PEFT adapter. For the base model, MLflow instead records a reference to the HuggingFace hub (repository name and commit hash), and downloads the base model weights on the fly when loading the PEFT model. This approach significantly reduces storage usage and logging latency; for instance, the logged artifacts size in this tutorial is less than 1GB, while the full Mistral 7B model is about 20GB.</li>
<li class=""><strong>Save a tokenizer without padding</strong>. During fine-tuning, we applied padding to the dataset to standardize the sequence length in a batch. However, padding is no longer necessary at inference, so we save a different tokenizer without padding. This ensures the loaded model can be used for inference immediately.</li>
</ol>
<p><strong>Note</strong>: Currently, manual logging is required for the PEFT adapter and config, while other information, such as dataset, metrics, Trainer parameters, etc., are automatically logged. However, this process may be automated in future versions of MLflow and Transformers.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> mlflow</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Get the ID of the MLflow Run that was automatically created above</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">last_run_id </span><span class="token operator">=</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">last_active_run</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">info</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">run_id</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Save a tokenizer without padding because it is only needed for training</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">tokenizer_no_pad </span><span class="token operator">=</span><span class="token plain"> AutoTokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">base_model_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> add_bos_token</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># If you interrupt the training, uncomment the following line to stop the MLflow run</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># mlflow.end_run()</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">start_run</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">run_id</span><span class="token operator">=</span><span class="token plain">last_run_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">log_params</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">peft_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">to_dict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">log_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      transformers_model</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">"model"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> trainer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"tokenizer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> tokenizer_no_pad</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      prompt_template</span><span class="token operator">=</span><span class="token plain">prompt_template</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      signature</span><span class="token operator">=</span><span class="token plain">signature</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      name</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"model"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># This is a relative path to save model files within MLflow run</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=whats-logged-to-mlflow>What's Logged to MLflow?<a href=#whats-logged-to-mlflow class=hash-link aria-label="Direct link to What's Logged to MLflow?" title="Direct link to What's Logged to MLflow?" translate=no>â€‹</a></h3>
<p>Let's briefly review what is logged/saved to MLflow as a result of your training. To access the MLflow UI, run <code>mlflow server</code> commands and open <code>https://localhost:PORT</code> (PORT is 5000 by default). Select the experiment "MLflow PEFT Tutorial" (or the notebook name when running on Databricks) on the left side. Then click on the latest MLflow Run named <code>Mistral-7B-SQL-QLoRA-2024-...</code> to view the Run details.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=parameters>Parameters<a href=#parameters class=hash-link aria-label="Direct link to Parameters" title="Direct link to Parameters" translate=no>â€‹</a></h4>
<p>The <code>Parameters</code> section displays hundreds of parameters specified for the Trainer, LoraConfig, and BitsAndBytesConfig, such as <code>learning_rate</code>, <code>r</code>, <code>bnb_4bit_quant_type</code>. It also includes default parameters that were not explicitly specified, which is crucial for ensuring reproducibility, especially if the library's default values change.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=metrics>Metrics<a href=#metrics class=hash-link aria-label="Direct link to Metrics" title="Direct link to Metrics" translate=no>â€‹</a></h4>
<p>The <code>Metrics</code> section presents the model metrics collected during the run, such as <code>train_loss</code>. You can visualize these metrics with various types of graphs in the "Chart" tab.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=artifacts>Artifacts<a href=#artifacts class=hash-link aria-label="Direct link to Artifacts" title="Direct link to Artifacts" translate=no>â€‹</a></h4>
<p>The <code>Artifacts</code> section displays the files/directories saved in MLflow as a result of training. For Transformers PEFT training, you should see the following files/directories:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for text code with copy and toggle buttons"><span class=languageLabel_zr_I>text</span></div><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    model/</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ peft/</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”‚  â”œâ”€ adapter_config.json       # JSON file of the LoraConfig</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”‚  â”œâ”€ adapter_module.safetensor # The weight file of the LoRA adapter</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”‚  â””â”€ README.md                 # Empty README file generated by Transformers</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”‚</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ LICENSE.txt                  # License information about the base model (Mistral-7B-0.1)</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ MLModel                      # Contains various metadata about your model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ conda.yaml                   # Dependencies to create conda environment</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ model_card.md                # Model card text for the base model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ model_card_data.yaml         # Model card data for the base model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â”œâ”€ python_env.yaml              # Dependencies to create Python virtual environment</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">      â””â”€ requirements.txt             # Pip requirements for model inference</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=model-metadata>Model Metadata<a href=#model-metadata class=hash-link aria-label="Direct link to Model Metadata" title="Direct link to Model Metadata" translate=no>â€‹</a></h4>
<p>In the MLModel file, you can see the many detailed metadata are saved about the PEFT and base model.
Here is an excerpt of the MLModel file (some fields are omitted for simplicity)</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for text code with copy and toggle buttons"><span class=languageLabel_zr_I>text</span></div><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token plain">flavors:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  transformers:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    peft_adaptor: peft                                 # Points the location of the saved PEFT model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    pipeline_model_type: MistralForCausalLM            # The base model implementation</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    source_model_name: mistralai/Mistral-7B-v0.1.      # Repository name of the base model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    source_model_revision: xxxxxxx                     # Commit hash in the repository for the base model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    task: text-generation                              # Pipeline type</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    torch_dtype: torch.bfloat16                        # Dtype for loading the model</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    tokenizer_type: LlamaTokenizerFast                 # Tokenizer implementation</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"># Prompt template saved with the model above</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">metadata:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  prompt_template: 'You are a powerful text-to-SQL model. Given the SQL tables and</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    natural language question, your job is to write SQL query that answers the question.</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    {prompt}</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    ### Response:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    '</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"># Defines the input and output format of the model, with additional inference parameters with default values</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">signature:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  inputs: '[{"type": "string", "required": true}]'</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  outputs: '[{"type": "string", "required": true}]'</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">  params: '[{"name": "max_new_tokens", "type": "long", "default": 256, "shape": null},</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    {"name": "repetition_penalty", "type": "double", "default": 1.15, "shape": null},</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">    {"name": "return_full_text", "type": "boolean", "default": false, "shape": null}]'</span><br/></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=7-load-the-saved-peft-model-from-mlflow>7. Load the Saved PEFT Model from MLflow<a href=#7-load-the-saved-peft-model-from-mlflow class=hash-link aria-label="Direct link to 7. Load the Saved PEFT Model from MLflow" title="Direct link to 7. Load the Saved PEFT Model from MLflow" translate=no>â€‹</a></h2>
<p>Finally, let's load the model logged in MLflow and evaluate its performance as a text-to-SQL generator. There are two ways to load a Transformer model in MLflow:</p>
<ol>
<li class="">Use <a href=https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.load_model target=_blank rel="noopener noreferrer" class="">mlflow.transformers.load_model()</a>. This method returns a native Transformers pipeline instance.</li>
<li class="">Use <a href=https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model target=_blank rel="noopener noreferrer" class="">mlflow.pyfunc.load_model()</a>. This method returns an MLflow's PythonModel instance that wraps the Transformers pipeline, offering additional features over the native pipeline, such as (1) a unified <code>predict()</code> API for inference, (2) model signature enforcement, and (3) automatically applying a prompt template and default parameters if saved. Please note that not all the Transformer pipelines are supported for pyfunc loading, refer to the <a href=https://mlflow.org/docs/latest/ml/deep-learning/transformers/guide/index.html#supported-transformers-pipeline-types-for-pyfunc target=_blank rel="noopener noreferrer" class="">MLflow documentation</a> for the full list of supported pipeline types.</li>
</ol>
<p>The first option is preferable if you wish to use the model via the native Transformers interface. The second option offers a simplified and unified interface across different model types and is particularly useful for model testing before production deployment. In the following code, we will use the <a href=https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model target=_blank rel="noopener noreferrer" class="">mlflow.pyfunc.load_model()</a> to show how it applies the prompt template and the default inference parameters defined above.</p>
<p><strong>NOTE</strong>: Invoking <code>load_model()</code> loads a new model instance onto your GPU, which may exceed GPU memory limits and trigger an Out Of Memory (OOM) error, or cause the Transformers library to attempt to offload parts of the model to other devices or disk. This offloading can lead to issues, such as a "ValueError: We need an <code>offload_dir</code> to dispatch this model according to this <code>decide_map</code>." If you encounter this error, consider restarting the Python Kernel and loading the model again.</p>
<p><strong>CAUTION</strong>: Restarting the Python Kernel will erase all intermediate states and variables from the above cells. Ensure that the trained PEFT model is properly logged in MLflow before restarting.</p>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token comment" style="color:rgb(98, 114, 164)"># You can find the ID of run in the Run detail page on MLflow UI</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">mlflow_model </span><span class="token operator">=</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pyfunc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"runs:/YOUR_RUN_ID/model"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<div style=flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#F8F8F2;--prism-background-color:#282A36><div class=codeBlockContent_bxn0><div class=codeBlockHeader_C_1e aria-label="Code block header for python code with copy and toggle buttons"><span class=languageLabel_zr_I>python</span></div><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#F8F8F2;background-color:#282A36><code class=codeBlockLines_e6Vv><span class=token-line style=color:#F8F8F2><span class="token comment" style="color:rgb(98, 114, 164)"># We only input table and question, since system prompt is adeed in the prompt template.</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">test_prompt </span><span class="token operator">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">### Table:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(255, 121, 198)"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">### Question:</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">When Essendon played away; where did they play?</span><br/></span><span class=token-line style=color:#F8F8F2><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Inference parameters like max_tokens_length are set to default values specified in the Model Signature</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">generated_query </span><span class="token operator">=</span><span class="token plain"> mlflow_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">test_prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#F8F8F2><span class="token plain">display_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">"prompt"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> test_prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"generated_query"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> generated_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br/></span></code></pre></div></div></div>
<div style=flex-grow:1;min-width:0;font-size:0.8rem;width:100%><div><style>.dataframe th,.dataframe tbody td{text-align:left;padding-right:30px}</style> <table border=1 class=dataframe>
<thead>
  <tr style="text-align: right;">
    <th></th>
    <th>prompt</th>
    <th>generated_query</th>
  </tr>
</thead>
<tbody>
  <tr>
    <th>0</th>
    <td><br>### Table:<br>CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)<br><br>### Question:<br>When Essendon played away; where did they play?<br></td>
    <td>SELECT venue FROM table_name_50 WHERE away_team = "essendon"</td>
  </tr>
</tbody>
</table></div></div>
<p>Perfect!! The fine-tuned model now generates the SQL query properly. As you can see in the code and result above, the system prompt and default inference parameters are applied automatically, so we don't have to pass it to the loaded model. This is super powerful when you want to deploy multiple models (or update an existing model) with different the system prompt or parameters, because you don't have to edit client's implementation as they are abstracted behind the MLflow model :)</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion" translate=no>â€‹</a></h2>
<p>In this tutorial, you learned how to fine-tune a large language model with QLoRA for text-to-SQL task using PEFT. You also learned the key role of MLflow in the LLM fine-tuning process, which tracks parameters and metrics during the fine-tuning, and manage models and other assets.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=whats-next>What's Next?<a href=#whats-next class=hash-link aria-label="Direct link to What's Next?" title="Direct link to What's Next?" translate=no>â€‹</a></h3>
<ul>
<li class=""><a href=https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/huggingface-evaluation.html target=_blank rel="noopener noreferrer" class="">Evaluate a Hugging Face LLM with MLflow</a> - Model evaluation is a critical steps in the model development. Checkout this guidance to learn how to evaluate LLMs efficiently with MLflow including LLM-as-a-judge.</li>
<li class=""><a href=https://mlflow.org/docs/latest/ml/deployment/index.html target=_blank rel="noopener noreferrer" class="">Deploy MLflow Model to Production</a> - MLflow model stores rich metadata and provides unified interface for prediction, which streamline the easy deployment process. Learn how to deploy your fine-tuned models to various target such as AWS SageMaker, Azure ML, Kubernetes, Databricks Model Serving, with detailed guidance and hands-on notebooks.</li>
<li class=""><a href=https://mlflow.org/docs/latest/ml/deep-learning/transformers/index.html target=_blank rel="noopener noreferrer" class="">MLflow Transformers Flavor Documentation</a> - Learn more about MLflow and Transformers integration and continue on more tutorials.</li>
<li class=""><a href=https://mlflow.org/docs/latest/llms/index.html target=_blank rel="noopener noreferrer" class="">Large Language Models in MLflow</a> - MLflow provides more LLM-related features and integrates to many other libraries such as OpenAI and Langchain.</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href=https://github.com/mlflow/mlflow/edit/master/docs/docs/classic-ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft.ipynb target=_blank rel="noopener noreferrer" class=theme-edit-this-page><svg fill=currentColor height=20 width=20 viewBox="0 0 40 40" class=iconEdit_Z9Sw aria-hidden=true><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"/></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-fine-tuning/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>Introduction to Fine Tuning</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/mlflow-website/docs/latest/ml/deep-learning/transformers/tutorials/audio-transcription/whisper/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Introduction to Audio Transcription</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#overview class="table-of-contents__link toc-highlight">Overview</a><ul><li><a href=#what-you-will-learn class="table-of-contents__link toc-highlight">What You Will Learn</a><li><a href=#key-actors class="table-of-contents__link toc-highlight">Key Actors</a></ul><li><a href=#1-environment-set-up class="table-of-contents__link toc-highlight">1. Environment Set up</a><ul><li><a href=#hardware-requirement class="table-of-contents__link toc-highlight">Hardware Requirement</a><li><a href=#install-python-libraries class="table-of-contents__link toc-highlight">Install Python Libraries</a></ul><li><a href=#2-dataset-preparation class="table-of-contents__link toc-highlight">2. Dataset Preparation</a><ul><li><a href=#load-dataset-from-huggingface-hub class="table-of-contents__link toc-highlight">Load Dataset from HuggingFace Hub</a><li><a href=#split-train-and-test-dataset class="table-of-contents__link toc-highlight">Split Train and Test Dataset</a><li><a href=#define-prompt-template class="table-of-contents__link toc-highlight">Define Prompt Template</a><li><a href=#padding-the-training-dataset class="table-of-contents__link toc-highlight">Padding the Training Dataset</a></ul><li><a href=#3-load-the-base-model-with-4-bit-quantization class="table-of-contents__link toc-highlight">3. Load the Base Model (with 4-bit quantization)</a><ul><li><a href=#how-does-the-base-model-perform class="table-of-contents__link toc-highlight">How Does the Base Model Perform?</a></ul><li><a href=#4-define-a-peft-model class="table-of-contents__link toc-highlight">4. Define a PEFT Model</a><li><a href=#5-kick-off-a-training-job class="table-of-contents__link toc-highlight">5. Kick-off a Training Job</a><li><a href=#6-save-the-peft-model-to-mlflow class="table-of-contents__link toc-highlight">6. Save the PEFT Model to MLflow</a><ul><li><a href=#set-prompt-template-and-default-inference-parameters-optional class="table-of-contents__link toc-highlight">Set Prompt Template and Default Inference Parameters (optional)</a><li><a href=#save-the-peft-model-to-mlflow class="table-of-contents__link toc-highlight">Save the PEFT Model to MLflow</a><li><a href=#whats-logged-to-mlflow class="table-of-contents__link toc-highlight">What's Logged to MLflow?</a></ul><li><a href=#7-load-the-saved-peft-model-from-mlflow class="table-of-contents__link toc-highlight">7. Load the Saved PEFT Model from MLflow</a><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a><ul><li><a href=#whats-next class="table-of-contents__link toc-highlight">What's Next?</a></ul></ul></div></div></div></div></main></div></div></div><footer class="relative pb-30 flex flex-col pt-30 bg-bottom bg-no-repeat bg-cover bg-size-[auto_360px] 2xl:bg-size-[100%_360px] bg-brand-black"><div style="position:absolute;width:100%;bottom:0;pointer-events:none;mask-composite:intersect;height:360px;background-image:repeating-linear-gradient(
        to right,
        rgba(0, 0, 0, 0.05),
        rgba(0, 0, 0, 0.25) 18px,
        transparent 2px,
        transparent 10px
      ),
      radial-gradient(
        circle at bottom center,
        oklch(0.7533 0.11 216.4) 0%,
        transparent 60%
      ),
      linear-gradient(to right, color-mix(in srgb, oklch(0.7533 0.11 216.4), navy 40%), color-mix(in srgb, oklch(0.7533 0.11 216.4), teal 40%));mask-image:radial-gradient(ellipse at center bottom, black 60%, transparent 80%),
      linear-gradient(to top, black 10%, transparent 40%)"></div><div class=z-1><div class="flex flex-row justify-between items-start md:items-center px-6 lg:px-20 gap-10 xs:gap-0 max-w-container"><div class="flex flex-col gap-8"><svg xmlns=http://www.w3.org/2000/svg width=109 height=40 fill=none viewBox="0 0 109 40" class="h-[36px] shrink-0"><path fill=#fff d="M0 31.032v-15.53h3.543v1.968c.893-1.594 2.84-2.425 4.593-2.425 2.042 0 3.828.926 4.658 2.745 1.216-2.043 3.034-2.745 5.043-2.745 2.81 0 5.49 1.787 5.49 5.904v10.083h-3.574v-9.478c0-1.818-.926-3.19-3-3.19-1.947 0-3.224 1.53-3.224 3.445v9.22H9.893v-9.475c0-1.786-.898-3.18-3.003-3.18-1.977 0-3.223 1.467-3.223 3.444v9.221ZM27.855 31.032V7.929h3.703v23.103ZM30.07 39.487c.833.232 1.582.383 3.17.383 2.953 0 6.436-1.665 7.353-6.339l3.786-18.739h5.629l.687-3.117h-5.686l.765-3.725c.586-2.892 2.186-4.358 4.754-4.358.668 0 .48.058 1.076.17L52.427.57c-.793-.237-1.503-.379-3.049-.379a7.32 7.32 0 0 0-4.528 1.484c-1.441 1.114-2.393 2.75-2.827 4.863l-1.066 5.137h-5.034l-.411 3.12h4.823L36.859 32.12c-.383 1.965-1.5 4.315-4.708 4.315-.727 0-.463-.055-1.121-.162ZM53.342 30.905H49.64l5.074-23.309h3.701ZM71.807 16.477A7.962 7.962 0 0 0 60.97 27.904l2.425-1.78a5.005 5.005 0 0 1 3.845-8.145v1.895ZM62.618 29.472a7.962 7.962 0 0 0 10.836-11.428l-2.425 1.78a5.005 5.005 0 0 1-3.845 8.145v-1.894ZM78.092 15.493h4.044l.823 10.612 5.759-10.612 3.839.055 1.508 10.557 5.074-10.612 3.701.055-7.678 15.493H91.46l-1.783-11.106-5.895 11.106h-3.84ZM105.072 15.768h-.766v-.266h1.845v.272h-.765v2.243h-.314ZM106.614 15.502h.383l.482 1.34q.091.259.178.524h.018c.059-.176.113-.352.172-.524l.478-1.34h.383v2.515h-.298V16.63c0-.22.024-.523.04-.747h-.016l-.191.574-.475 1.304h-.208l-.481-1.302-.191-.574h-.015c.017.224.042.527.042.747v1.387h-.291Z"/></svg><div class="text-xs text-left md:text-nowrap md:w-0">Â© 2025 MLflow Project, a Series of LF Projects, LLC.</div></div><div class="flex flex-col flex-wrap justify-end md:text-right md:flex-row gap-x-10 lg:gap-x-20 gap-y-5 w-2/5 md:w-auto md:pt-2 max-w-fit"><div><a href=https://mlflow.org target=_blank rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Components</a></div><div><a href=https://mlflow.org/releases target=_blank rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Releases</a></div><div><a href=https://mlflow.org/blog target=_blank rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Blog</a></div><div><a class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white" href=/mlflow-website/docs/latest/>Docs</a></div><div><a href=https://mlflow.org/ambassadors target=_blank rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Ambassador Program</a></div></div></div></div></footer></div></body>