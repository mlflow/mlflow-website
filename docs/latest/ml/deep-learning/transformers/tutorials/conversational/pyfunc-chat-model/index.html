<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-classic-ml docs-version-current docs-doc-page docs-doc-id-deep-learning/transformers/tutorials/conversational/pyfunc-chat-model-ipynb" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Deploying a Transformer model as an OpenAI-compatible Chatbot | MLflow</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-classic-ml-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-classic-ml-current"><meta data-rh="true" property="og:title" content="Deploying a Transformer model as an OpenAI-compatible Chatbot | MLflow"><meta data-rh="true" name="description" content="Download this notebook"><meta data-rh="true" property="og:description" content="Download this notebook"><link data-rh="true" rel="icon" href="/docs/latest/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/"><link data-rh="true" rel="alternate" href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/" hreflang="en"><link data-rh="true" rel="alternate" href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XKVLO8P882-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Deep Learning","item":"https://mlflow.org/docs/latest/ml/deep-learning/"},{"@type":"ListItem","position":2,"name":"Transformers","item":"https://mlflow.org/docs/latest/ml/deep-learning/transformers/"},{"@type":"ListItem","position":3,"name":"Tutorials","item":"https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/"},{"@type":"ListItem","position":4,"name":"Custom Conversational Models","item":"https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-N6WMTTJ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","GTM-N6WMTTJ",{anonymize_ip:!0}),gtag("config","AW-16857946923",{anonymize_ip:!0})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-N6WMTTJ",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="search" type="application/opensearchdescription+xml" title="MLflow" href="/docs/latest/opensearch.xml">





<script src="/docs/latest/js/runllm.js" defer="defer"></script><link rel="stylesheet" href="/docs/latest/assets/css/styles.4b5aaf2c.css">
<script src="/docs/latest/assets/js/runtime~main.8927a552.js" defer="defer"></script>
<script src="/docs/latest/assets/js/main.24fe4bbc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/latest/"><div class="navbar__logo"><img src="/docs/latest/images/logo-light.svg" alt="MLflow Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/docs/latest/images/logo-dark.svg" alt="MLflow Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link docsDropdown_VGp6" data-active="ml"><div style="display:flex;gap:8px;align-items:center"><div class="dropdownCircle_n91F" style="width:10px;height:10px;background-color:var(--ml-color-primary);border-radius:4px"></div>ML Docs</div></a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link ml-docs-link dropdown__link--active" href="/docs/latest/ml/"><div style="display:flex;gap:8px;align-items:center"><div style="width:10px;height:10px;background-color:var(--ml-color-primary);border-radius:4px"></div>ML Docs</div></a></li><li><a class="dropdown__link genai-docs-link" href="/docs/latest/genai/"><div style="display:flex;gap:8px;align-items:center"><div style="width:10px;height:10px;background-color:var(--genai-color-primary);border-radius:4px"></div>GenAI Docs</div></a></li></ul></div><a href="https://mlflow.org/docs/latest/api_reference/index.html" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API Reference</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/mlflow/mlflow" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link github-link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sidebar-top-level-category"><a class="menu__link" href="/docs/latest/ml/">MLflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a href="https://mlflow.org/docs/latest/genai/mlflow-3" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK">MLflow 3.0<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/latest/ml/getting-started/">Getting Started</a><button aria-label="Expand sidebar category &#x27;Getting Started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/latest/ml/traditional-ml/">Machine Learning</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/traditional-ml/">Traditional ML</a><button aria-label="Expand sidebar category &#x27;Traditional ML&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/latest/ml/deep-learning/">Deep Learning</a><button aria-label="Collapse sidebar category &#x27;Deep Learning&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/keras/">Keras</a><button aria-label="Expand sidebar category &#x27;Keras&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/pytorch/">PyTorch</a><button aria-label="Expand sidebar category &#x27;PyTorch&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/tensorflow/">TensorFlow</a><button aria-label="Expand sidebar category &#x27;TensorFlow&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/">Transformers</a><button aria-label="Collapse sidebar category &#x27;Transformers&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/guide/">🤗 Transformers within MLflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/large-models/">Working with Large Transformers Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/task/">Transformers Task Types</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/">Tutorials</a><button aria-label="Collapse sidebar category &#x27;Tutorials&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model/">Introduction to Conversational Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/">Custom Conversational Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-fine-tuning/">Introduction to Fine Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/">Leveraging PEFT for Fine Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/audio-transcription/whisper/">Introduction to Audio Transcription</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/prompt-templating/prompt-templating/">Introduction to Prompt Templating</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/text-generation/text-generation/">Text Generation Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/translation/component-translation/">Translation Models</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/sentence-transformers/">Sentence Transformers</a><button aria-label="Expand sidebar category &#x27;Sentence Transformers&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/spacy/">spaCy</a><button aria-label="Expand sidebar category &#x27;spaCy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/docs/latest/ml/tracking/">Build </a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/tracking/">MLflow Tracking</a><button aria-label="Expand sidebar category &#x27;MLflow Tracking&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/latest/ml/model/">MLflow Model</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/dataset/">MLflow Datasets</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/latest/ml/evaluation/">Evaluate</a><button aria-label="Expand sidebar category &#x27;Evaluate&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/latest/ml/model-registry/">Deploy</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/latest/ml/tracking/#tracking-setup">Team Collaboration</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a href="https://mlflow.org/docs/latest/api_reference/python_api/index.html" target="_blank" rel="noopener noreferrer" class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false">API References</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer" class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false">More</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs/latest/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Machine Learning</span></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/latest/ml/deep-learning/"><span>Deep Learning</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/latest/ml/deep-learning/transformers/"><span>Transformers</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/latest/ml/deep-learning/transformers/tutorials/"><span>Tutorials</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Custom Conversational Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Deploying a Transformer model as an OpenAI-compatible Chatbot</h1></header>
<a class="button button--primary" style="margin-bottom:1rem;display:block;width:min-content" href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model.ipynb" download="">Download this notebook</a>
<p>Welcome to our tutorial on using Transformers and MLflow to create an OpenAI-compatible chat model. In MLflow 2.11 and up, MLflow&#x27;s Transformers flavors support special task type <code>llm/v1/chat</code>, which turns thousands of <a href="https://huggingface.co/models?pipeline_tag=text-generation" target="_blank" rel="noopener noreferrer">text-generation</a> models on Hugging Face into conversational chat bots that are interoperable with OpenAI models. This enables you to seamlessly swap out your chat app’s backing LLM or to easily evaluate different models without having to edit your client-side code.</p>
<p>If you haven&#x27;t already seen it, you may find it helpful to go through our <a href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model.html" target="_blank" rel="noopener noreferrer">introductory notebook on chat and Transformers</a> before proceeding with this one, as this notebook is slightly higher-level and does not delve too deeply into the inner workings of Transformers or MLflow Tracking.</p>
<p><strong>Note</strong>: This page covers how to deploy a <strong>Transformers</strong> models as a chatbot. If you are using a different framework or a custom python model, use <a href="https://mlflow.org/docs/latest/genai/flavors/chat-model-intro/index.html" target="_blank" rel="noopener noreferrer">ChatModel</a> instead to build an OpenAI-compatible chat bot.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="learning-objectives">Learning objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning objectives" title="Direct link to Learning objectives">​</a></h3>
<p>In this tutorial, you will:</p>
<ul>
<li>Create an OpenAI-compatible chat model using TinyLLama-1.1B-Chat</li>
<li>Log the model to MLflow and load it back for local inference.</li>
<li>Serve the model with MLflow Model Serving</li>
</ul>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">pip install mlflow</span><span class="token operator">&gt;=</span><span class="token number">2.11</span><span class="token number">.0</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain">q </span><span class="token operator">-</span><span class="token plain">U</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># OpenAI-compatible chat model support is available for Transformers 4.34.0 and above</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">pip install transformers</span><span class="token operator">&gt;=</span><span class="token number">4.34</span><span class="token number">.0</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain">q </span><span class="token operator">-</span><span class="token plain">U</span><br></span></code></pre></div></div></div>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Disable tokenizers warnings when constructing pipelines</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">env TOKENIZERS_PARALLELISM</span><span class="token operator">=</span><span class="token plain">false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> warnings</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Disable a few less-than-useful UserWarnings from setuptools and pydantic</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">warnings</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">filterwarnings</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;ignore&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> category</span><span class="token operator">=</span><span class="token plain">UserWarning</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">env: TOKENIZERS_PARALLELISM=false</pre>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="building-a-chat-model">Building a Chat Model<a href="#building-a-chat-model" class="hash-link" aria-label="Direct link to Building a Chat Model" title="Direct link to Building a Chat Model">​</a></h3>
<p>MLflow&#x27;s native Transformers integration allows you to specify the <code>task</code> param when saving or logging your pipelines. Originally, this param accepts any of the <a href="https://huggingface.co/tasks" target="_blank" rel="noopener noreferrer">Transformers pipeline task types</a>, but the <code>mlflow.transformers</code> flavor adds a few more MLflow-specific keys for <code>text-generation</code> pipeline types.</p>
<p>For <code>text-generation</code> pipelines, instead of specifying <code>text-generation</code> as the task type, you can provide one of two string literals conforming to the <a href="https://mlflow.org/docs/latest/genai/governance/ai-gateway/#deployments-configuration-details" target="_blank" rel="noopener noreferrer">MLflow AI Gateway&#x27;s endpoint_type specification</a> (&quot;llm/v1/embeddings&quot; can be specified as a task on models saved with <code>mlflow.sentence_transformers</code>):</p>
<ul>
<li>&quot;llm/v1/chat&quot; for chat-style applications</li>
<li>&quot;llm/v1/completions&quot; for generic completions</li>
</ul>
<p>When one of these keys is specified, MLflow will automatically handle everything required to serve a chat or completions model. This includes:</p>
<ul>
<li>Setting a chat/completions compatible signature on the model</li>
<li>Performing data pre- and post-processing to ensure the inputs and outputs conform to the <a href="https://mlflow.org/docs/latest/genai/serving/responses-agent#openai-api-compatibility" target="_blank" rel="noopener noreferrer">Chat/Completions API spec</a>, which is compatible with OpenAI&#x27;s API spec.</li>
</ul>
<p>Note that these modifications only apply when the model is loaded with <code>mlflow.pyfunc.load_model()</code> (e.g. when serving the model with the <code>mlflow models serve</code> CLI tool). If you want to load just the base pipeline, you can always do so via <code>mlflow.transformers.load_model()</code>.</p>
<p>In the next few cells, we&#x27;ll learn how serve a chat model with a local Transformers pipeline and MLflow, using TinyLlama-1.1B-Chat as an example.</p>
<p>To begin, let&#x27;s go through the original flow of saving a text generation pipeline:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> pipeline</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> mlflow</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">generator </span><span class="token operator">=</span><span class="token plain"> pipeline</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text-generation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  model</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># save the model using the vanilla `text-generation` task type</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">save_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  path</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-text-generation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> transformers_model</span><span class="token operator">=</span><span class="token plain">generator</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> task</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text-generation&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/4268198845.py:11: FutureWarning: The &#x27;transformers&#x27; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
mlflow.transformers.save_model(</pre>
<p>Now, let&#x27;s load the model and use it for inference. Our loaded model is a <code>text-generation</code> pipeline, and let&#x27;s take a look at its signature to see its expected inputs and outputs.</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># load the model for inference</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pyfunc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-text-generation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">signature</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">2024/02/26 21:06:51 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">inputs: 
[string (required)]
outputs: 
[string (required)]
params: 
None</pre>
<p>Unfortunately, it only accepts <code>string</code> as input, which isn&#x27;t directly compatible with a chat interface. When interacting with OpenAI&#x27;s API, for example, we expect to simply be able to input a list of messages. In order to do this with our current model, we&#x27;ll have to write some additional boilerplate:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># first, apply the tokenizer&#x27;s chat template, since the</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># model is tuned to accept prompts in a chat format. this</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># also converts the list of messages to a string.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">messages </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Write me a hello world program in python&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">prompt </span><span class="token operator">=</span><span class="token plain"> generator</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">apply_chat_template</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  messages</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenize</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> add_generation_prompt</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">[&#x27;&lt;|user|&gt;
Write me a hello world program in python&lt;/s&gt;
&lt;|assistant|&gt;
Here&#x27;s a simple hello world program in Python:

```python
print(&quot;Hello, world!&quot;)
```

This program prints the string &quot;Hello, world!&quot; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#x27;]</pre>
<p>Now we&#x27;re getting somewhere, but formatting our messages prior to inference is cumbersome.</p>
<p>Additionally, the output format isn&#x27;t compatible with the OpenAI API spec either--it&#x27;s just a list of strings. If we were looking to evaluate different model backends for our chat app, we&#x27;d have to rewrite some of our client-side code to both format the input, and to parse this new response.</p>
<p>To simplify all this, let&#x27;s just pass in <code>&quot;llm/v1/chat&quot;</code> as the task param when saving the model.</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># save the model using the `&quot;llm/v1/chat&quot;`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># task type instead of `text-generation`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">save_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  path</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-chat&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> transformers_model</span><span class="token operator">=</span><span class="token plain">generator</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> task</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;llm/v1/chat&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/609241782.py:3: FutureWarning: The &#x27;transformers&#x27; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
mlflow.transformers.save_model(</pre>
<p>Once again, let&#x27;s load the model and inspect the signature:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pyfunc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-chat&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">signature</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">2024/02/26 21:10:04 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">inputs: 
[&#x27;messages&#x27;: Array({content: string (required), name: string (optional), role: string (required)}) (required), &#x27;temperature&#x27;: double (optional), &#x27;max_tokens&#x27;: long (optional), &#x27;stop&#x27;: Array(string) (optional), &#x27;n&#x27;: long (optional), &#x27;stream&#x27;: boolean (optional)]
outputs: 
[&#x27;id&#x27;: string (required), &#x27;object&#x27;: string (required), &#x27;created&#x27;: long (required), &#x27;model&#x27;: string (required), &#x27;choices&#x27;: Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), &#x27;usage&#x27;: {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]
params: 
None</pre>
<p>Now when performing inference, we can pass our messages in a dict as we&#x27;d expect to do when interacting with the OpenAI API. Furthermore, the response we receive back from the model also conforms to the spec.</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">messages </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Write me a hello world program in python&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;messages&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> messages</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">[{&#x27;id&#x27;: &#x27;8435a57d-9895-485e-98d3-95b1cbe007c0&#x27;,
&#x27;object&#x27;: &#x27;chat.completion&#x27;,
&#x27;created&#x27;: 1708949437,
&#x27;model&#x27;: &#x27;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#x27;,
&#x27;usage&#x27;: {&#x27;prompt_tokens&#x27;: 24, &#x27;completion_tokens&#x27;: 71, &#x27;total_tokens&#x27;: 95},
&#x27;choices&#x27;: [{&#x27;index&#x27;: 0,
  &#x27;finish_reason&#x27;: &#x27;stop&#x27;,
  &#x27;message&#x27;: {&#x27;role&#x27;: &#x27;assistant&#x27;,
   &#x27;content&#x27;: &#x27;Here&#x27;s a simple hello world program in Python:

```python
print(&quot;Hello, world!&quot;)
```

This program prints the string &quot;Hello, world!&quot; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#x27;}}]}]</pre>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="serving-the-chat-model">Serving the Chat Model<a href="#serving-the-chat-model" class="hash-link" aria-label="Direct link to Serving the Chat Model" title="Direct link to Serving the Chat Model">​</a></h3>
<p>To take this example further, let&#x27;s use MLflow to serve our chat model, so we can interact with it like a web API. To do this, we can use the <code>mlflow models serve</code> CLI tool.</p>
<p>In a terminal shell, run:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ mlflow models serve -m tinyllama-chat</span><br></span></code></pre></div></div>
<p>When the server has finished initializing, you should be able to interact with the model via HTTP requests. The input format is almost identical to the format described in the <a href="https://mlflow.org/docs/latest/ml/deployment/index.html#chat" target="_blank" rel="noopener noreferrer">MLflow Deployments Server docs</a>, with the exception that <code>temperature</code> defaults to <code>1.0</code> instead of <code>0.0</code>.</p>
<p>Here&#x27;s a quick example:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token operator">%</span><span class="token plain">sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl http</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token operator">//</span><span class="token number">127.0</span><span class="token number">.0</span><span class="token number">.1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token number">5000</span><span class="token operator">/</span><span class="token plain">invocations   </span><span class="token operator">-</span><span class="token plain">H </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;Content-Type: application/json&#x27;</span><span class="token plain">   </span><span class="token operator">-</span><span class="token plain">d </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;{ &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write me a hello world program in python&quot;}] }&#x27;</span><span class="token plain">   </span><span class="token operator">|</span><span class="token plain"> jq</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                               Dload  Upload   Total   Spent    Left  Speed
100   706  100   617  100    89     25      3  0:00:29  0:00:23  0:00:06   160</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">[
{
  &quot;id&quot;: &quot;fc3d08c3-d37d-420d-a754-50f77eb32a92&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;created&quot;: 1708949465,
  &quot;model&quot;: &quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;,
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 24,
    &quot;completion_tokens&quot;: 71,
    &quot;total_tokens&quot;: 95
  },
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Here&#x27;s a simple hello world program in Python:

```python
print(&quot;Hello, world!&quot;)
```

This program prints the string &quot;Hello, world!&quot; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&quot;
      }
    }
  ]
}
]</pre>
<p>It&#x27;s that easy!</p>
<p>You can also call the API with a few optional inference params to adjust the model&#x27;s responses. These map to Transformers pipeline params, and are passed in directly at inference time.</p>
<ul>
<li><code>max_tokens</code> (maps to <code>max_new_tokens</code>): The maximum number of new tokens the model should generate.</li>
<li><code>temperature</code> (maps to <code>temperature</code>): Controls the creativity of the model&#x27;s response. Note that this is not guaranteed to be supported by all models, and in order for this param to have an effect, the pipeline must have been created with <code>do_sample=True</code>.</li>
<li><code>stop</code> (maps to <code>stopping_criteria</code>): A list of tokens at which to stop generation.</li>
</ul>
<p>Note: <code>n</code> does not have an equivalent Transformers pipeline param, and is not supported in queries. However, you can implement a model that consumes the <code>n</code> param using Custom Pyfunc (details below).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In this tutorial, you learned how to create an OpenAI-compatible chat model by specifying &quot;llm/v1/chat&quot; as the task when saving Transformers pipelines.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What&#x27;s next?<a href="#whats-next" class="hash-link" aria-label="Direct link to What&#x27;s next?" title="Direct link to What&#x27;s next?">​</a></h3>
<ul>
<li><a href="https://mlflow.org/docs/latest/llms/chat-model-intro/index.html" target="_blank" rel="noopener noreferrer">Learn about custom ChatModel</a>. If you&#x27;re looking for futrher customization or models outside Transformers, the linked page provides a hand-on guidance for how to build a chat bot with MLflow&#x27;s <code>ChatModel</code> class.</li>
<li><a href="https://mlflow.org/docs/latest/ml/deployment/index.html" target="_blank" rel="noopener noreferrer">More on MLflow AI Gateway</a>. In this tutorial, we saw how to deploy a model using a local server, but MLflow provides many other ways to deploy your models to production. Check out this page to learn more about the different options.</li>
<li><a href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/index.html" target="_blank" rel="noopener noreferrer">More on MLflow&#x27;s Transformers Integration</a>. This page provides a comprehensive overview on MLflow&#x27;s Transformers integrations, along with lots of hands-on guides and notebooks. Learn how to fine-tune models, use prompt templates, and more!</li>
<li><a href="https://mlflow.org/docs/latest/genai/index.html" target="_blank" rel="noopener noreferrer">Other LLM Integrations</a>. Aside from Transformers, MLflow has integrations with many other popular LLM libraries, such as Langchain and OpenAI.</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/mlflow/mlflow/edit/master/docs/docs/classic-ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model.ipynb" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction to Conversational Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-fine-tuning/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Introduction to Fine Tuning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning objectives</a></li><li><a href="#building-a-chat-model" class="table-of-contents__link toc-highlight">Building a Chat Model</a></li><li><a href="#serving-the-chat-model" class="table-of-contents__link toc-highlight">Serving the Chat Model</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a><ul><li><a href="#whats-next" class="table-of-contents__link toc-highlight">What&#39;s next?</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="pb-30 flex flex-col pt-30 bg-bottom bg-no-repeat bg-cover bg-size-[auto_360px] 2xl:bg-size-[100%_360px] bg-brand-black" style="background-image:linear-gradient(#0e1414 60%, #0c141400 90% 100%),url(/docs/latest/assets/images/footer-blue-bg-2664116e3788212e9f50cfc6024806f8.png)"><div class="flex flex-row justify-between items-start md:items-center px-6 lg:px-20 gap-10 xs:gap-0 max-w-container"><div class="flex flex-col gap-8"><svg xmlns="http://www.w3.org/2000/svg" width="109" height="40" fill="none" viewBox="0 0 109 40" class="h-[36px] shrink-0"><path fill="#fff" d="M0 31.032v-15.53h3.543v1.968c.893-1.594 2.84-2.425 4.593-2.425 2.042 0 3.828.926 4.658 2.745 1.216-2.043 3.034-2.745 5.043-2.745 2.81 0 5.49 1.787 5.49 5.904v10.083h-3.574v-9.478c0-1.818-.926-3.19-3-3.19-1.947 0-3.224 1.53-3.224 3.445v9.22H9.893v-9.475c0-1.786-.898-3.18-3.003-3.18-1.977 0-3.223 1.467-3.223 3.444v9.221ZM27.855 31.032V7.929h3.703v23.103ZM30.07 39.487c.833.232 1.582.383 3.17.383 2.953 0 6.436-1.665 7.353-6.339l3.786-18.739h5.629l.687-3.117h-5.686l.765-3.725c.586-2.892 2.186-4.358 4.754-4.358.668 0 .48.058 1.076.17L52.427.57c-.793-.237-1.503-.379-3.049-.379a7.32 7.32 0 0 0-4.528 1.484c-1.441 1.114-2.393 2.75-2.827 4.863l-1.066 5.137h-5.034l-.411 3.12h4.823L36.859 32.12c-.383 1.965-1.5 4.315-4.708 4.315-.727 0-.463-.055-1.121-.162ZM53.342 30.905H49.64l5.074-23.309h3.701ZM71.807 16.477A7.962 7.962 0 0 0 60.97 27.904l2.425-1.78a5.005 5.005 0 0 1 3.845-8.145v1.895ZM62.618 29.472a7.962 7.962 0 0 0 10.836-11.428l-2.425 1.78a5.005 5.005 0 0 1-3.845 8.145v-1.894ZM78.092 15.493h4.044l.823 10.612 5.759-10.612 3.839.055 1.508 10.557 5.074-10.612 3.701.055-7.678 15.493H91.46l-1.783-11.106-5.895 11.106h-3.84ZM105.072 15.768h-.766v-.266h1.845v.272h-.765v2.243h-.314ZM106.614 15.502h.383l.482 1.34q.091.259.178.524h.018c.059-.176.113-.352.172-.524l.478-1.34h.383v2.515h-.298V16.63c0-.22.024-.523.04-.747h-.016l-.191.574-.475 1.304h-.208l-.481-1.302-.191-.574h-.015c.017.224.042.527.042.747v1.387h-.291Z"></path></svg><div class="text-xs text-left md:text-nowrap md:w-0">© 2025 MLflow Project, a Series of LF Projects, LLC.</div></div><div class="flex flex-col flex-wrap justify-end md:text-right md:flex-row gap-x-10 lg:gap-x-20 gap-y-5 w-2/5 md:w-auto md:pt-2 max-w-fit"><div><a href="https://mlflow.org" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Components</a></div><div><a href="https://mlflow.org/releases" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Releases</a></div><div><a href="https://mlflow.org/blog" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Blog</a></div><div><a class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white" href="/docs/latest/">Docs</a></div><div><a href="https://mlflow.org/ambassadors" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Ambassador Program</a></div></div></div></footer></div>
</body>
</html>