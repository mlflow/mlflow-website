<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-classic-ml docs-version-current docs-doc-page docs-doc-id-deep-learning/transformers/tutorials/conversational/pyfunc-chat-model-ipynb" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Deploying a Transformer model as an OpenAI-compatible Chatbot | MLflow</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-classic-ml-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-classic-ml-current"><meta data-rh="true" property="og:title" content="Deploying a Transformer model as an OpenAI-compatible Chatbot | MLflow"><meta data-rh="true" name="description" content="Download this notebook"><meta data-rh="true" property="og:description" content="Download this notebook"><link data-rh="true" rel="icon" href="/docs/latest/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/"><link data-rh="true" rel="alternate" href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/" hreflang="en"><link data-rh="true" rel="alternate" href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XKVLO8P882-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Deep Learning","item":"https://mlflow.org/docs/latest/ml/deep-learning/"},{"@type":"ListItem","position":2,"name":"Transformers","item":"https://mlflow.org/docs/latest/ml/deep-learning/transformers/"},{"@type":"ListItem","position":3,"name":"Tutorials","item":"https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/"},{"@type":"ListItem","position":4,"name":"Custom Conversational Models","item":"https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-N6WMTTJ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","GTM-N6WMTTJ",{anonymize_ip:!0}),gtag("config","AW-16857946923",{anonymize_ip:!0})</script>
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-N6WMTTJ",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>



<link rel="search" type="application/opensearchdescription+xml" title="MLflow" href="/docs/latest/opensearch.xml">





<script src="/docs/latest/js/runllm.js" defer="defer"></script><link rel="stylesheet" href="/docs/latest/assets/css/styles.4b5aaf2c.css">
<script src="/docs/latest/assets/js/runtime~main.8927a552.js" defer="defer"></script>
<script src="/docs/latest/assets/js/main.24fe4bbc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/latest/"><div class="navbar__logo"><img src="/docs/latest/images/logo-light.svg" alt="MLflow Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/docs/latest/images/logo-dark.svg" alt="MLflow Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link docsDropdown_VGp6" data-active="ml"><div style="display:flex;gap:8px;align-items:center"><div class="dropdownCircle_n91F" style="width:10px;height:10px;background-color:var(--ml-color-primary);border-radius:4px"></div>ML Docs</div></a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link ml-docs-link dropdown__link--active" href="/docs/latest/ml/"><div style="display:flex;gap:8px;align-items:center"><div style="width:10px;height:10px;background-color:var(--ml-color-primary);border-radius:4px"></div>ML Docs</div></a></li><li><a class="dropdown__link genai-docs-link" href="/docs/latest/genai/"><div style="display:flex;gap:8px;align-items:center"><div style="width:10px;height:10px;background-color:var(--genai-color-primary);border-radius:4px"></div>GenAI Docs</div></a></li></ul></div><a href="https://mlflow.org/docs/latest/api_reference/index.html" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API Reference</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/mlflow/mlflow" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link github-link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sidebar-top-level-category"><a class="menu__link" href="/docs/latest/ml/">MLflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a href="https://mlflow.org/docs/latest/genai/mlflow-3" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK">MLflow 3.0<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/latest/ml/getting-started/">Getting Started</a><button aria-label="Expand sidebar category &#x27;Getting Started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/latest/ml/traditional-ml/">Machine Learning</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/traditional-ml/">Traditional ML</a><button aria-label="Expand sidebar category &#x27;Traditional ML&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/latest/ml/deep-learning/">Deep Learning</a><button aria-label="Collapse sidebar category &#x27;Deep Learning&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/keras/">Keras</a><button aria-label="Expand sidebar category &#x27;Keras&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/pytorch/">PyTorch</a><button aria-label="Expand sidebar category &#x27;PyTorch&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/tensorflow/">TensorFlow</a><button aria-label="Expand sidebar category &#x27;TensorFlow&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/">Transformers</a><button aria-label="Collapse sidebar category &#x27;Transformers&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/guide/">ðŸ¤— Transformers within MLflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/large-models/">Working with Large Transformers Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/task/">Transformers Task Types</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/">Tutorials</a><button aria-label="Collapse sidebar category &#x27;Tutorials&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model/">Introduction to Conversational Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model/">Custom Conversational Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-fine-tuning/">Introduction to Fine Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-peft/">Leveraging PEFT for Fine Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/audio-transcription/whisper/">Introduction to Audio Transcription</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/prompt-templating/prompt-templating/">Introduction to Prompt Templating</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/text-generation/text-generation/">Text Generation Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/deep-learning/transformers/tutorials/translation/component-translation/">Translation Models</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/sentence-transformers/">Sentence Transformers</a><button aria-label="Expand sidebar category &#x27;Sentence Transformers&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/deep-learning/spacy/">spaCy</a><button aria-label="Expand sidebar category &#x27;spaCy&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/docs/latest/ml/tracking/">Build </a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/latest/ml/tracking/">MLflow Tracking</a><button aria-label="Expand sidebar category &#x27;MLflow Tracking&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/latest/ml/model/">MLflow Model</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/latest/ml/dataset/">MLflow Datasets</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/latest/ml/evaluation/">Evaluate</a><button aria-label="Expand sidebar category &#x27;Evaluate&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/latest/ml/model-registry/">Deploy</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/latest/ml/tracking/#tracking-setup">Team Collaboration</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a href="https://mlflow.org/docs/latest/api_reference/python_api/index.html" target="_blank" rel="noopener noreferrer" class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false">API References</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed sidebar-top-level-category"><div class="menu__list-item-collapsible"><a href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer" class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false">More</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs/latest/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Machine Learning</span></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/latest/ml/deep-learning/"><span>Deep Learning</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/latest/ml/deep-learning/transformers/"><span>Transformers</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/latest/ml/deep-learning/transformers/tutorials/"><span>Tutorials</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Custom Conversational Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Deploying a Transformer model as an OpenAI-compatible Chatbot</h1></header>
<a class="button button--primary" style="margin-bottom:1rem;display:block;width:min-content" href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model.ipynb" download="">Download this notebook</a>
<p>Welcome to our tutorial on using Transformers and MLflow to create an OpenAI-compatible chat model. In MLflow 2.11 and up, MLflow&#x27;s Transformers flavors support special task type <code>llm/v1/chat</code>, which turns thousands of <a href="https://huggingface.co/models?pipeline_tag=text-generation" target="_blank" rel="noopener noreferrer">text-generation</a> models on Hugging Face into conversational chat bots that are interoperable with OpenAI models. This enables you to seamlessly swap out your chat appâ€™s backing LLM or to easily evaluate different models without having to edit your client-side code.</p>
<p>If you haven&#x27;t already seen it, you may find it helpful to go through our <a href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model.html" target="_blank" rel="noopener noreferrer">introductory notebook on chat and Transformers</a> before proceeding with this one, as this notebook is slightly higher-level and does not delve too deeply into the inner workings of Transformers or MLflow Tracking.</p>
<p><strong>Note</strong>: This page covers how to deploy a <strong>Transformers</strong> models as a chatbot. If you are using a different framework or a custom python model, use <a href="https://mlflow.org/docs/latest/genai/flavors/chat-model-intro/index.html" target="_blank" rel="noopener noreferrer">ChatModel</a> instead to build an OpenAI-compatible chat bot.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="learning-objectives">Learning objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning objectives" title="Direct link to Learning objectives">â€‹</a></h3>
<p>In this tutorial, you will:</p>
<ul>
<li>Create an OpenAI-compatible chat model using TinyLLama-1.1B-Chat</li>
<li>Log the model to MLflow and load it back for local inference.</li>
<li>Serve the model with MLflow Model Serving</li>
</ul>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">pip install mlflow</span><span class="token operator">&gt;=</span><span class="token number">2.11</span><span class="token number">.0</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain">q </span><span class="token operator">-</span><span class="token plain">U</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># OpenAI-compatible chat model support is available for Transformers 4.34.0 and above</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">pip install transformers</span><span class="token operator">&gt;=</span><span class="token number">4.34</span><span class="token number">.0</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain">q </span><span class="token operator">-</span><span class="token plain">U</span><br></span></code></pre></div></div></div>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Disable tokenizers warnings when constructing pipelines</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">env TOKENIZERS_PARALLELISM</span><span class="token operator">=</span><span class="token plain">false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> warnings</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Disable a few less-than-useful UserWarnings from setuptools and pydantic</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">warnings</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">filterwarnings</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;ignore&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> category</span><span class="token operator">=</span><span class="token plain">UserWarning</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">env: TOKENIZERS_PARALLELISM=false</pre>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="building-a-chat-model">Building a Chat Model<a href="#building-a-chat-model" class="hash-link" aria-label="Direct link to Building a Chat Model" title="Direct link to Building a Chat Model">â€‹</a></h3>
<p>MLflow&#x27;s native Transformers integration allows you to specify the <code>task</code> param when saving or logging your pipelines. Originally, this param accepts any of the <a href="https://huggingface.co/tasks" target="_blank" rel="noopener noreferrer">Transformers pipeline task types</a>, but the <code>mlflow.transformers</code> flavor adds a few more MLflow-specific keys for <code>text-generation</code> pipeline types.</p>
<p>For <code>text-generation</code> pipelines, instead of specifying <code>text-generation</code> as the task type, you can provide one of two string literals conforming to the <a href="https://mlflow.org/docs/latest/genai/governance/ai-gateway/#deployments-configuration-details" target="_blank" rel="noopener noreferrer">MLflow AI Gateway&#x27;s endpoint_type specification</a> (&quot;llm/v1/embeddings&quot; can be specified as a task on models saved with <code>mlflow.sentence_transformers</code>):</p>
<ul>
<li>&quot;llm/v1/chat&quot; for chat-style applications</li>
<li>&quot;llm/v1/completions&quot; for generic completions</li>
</ul>
<p>When one of these keys is specified, MLflow will automatically handle everything required to serve a chat or completions model. This includes:</p>
<ul>
<li>Setting a chat/completions compatible signature on the model</li>
<li>Performing data pre- and post-processing to ensure the inputs and outputs conform to the <a href="https://mlflow.org/docs/latest/genai/serving/responses-agent#openai-api-compatibility" target="_blank" rel="noopener noreferrer">Chat/Completions API spec</a>, which is compatible with OpenAI&#x27;s API spec.</li>
</ul>
<p>Note that these modifications only apply when the model is loaded with <code>mlflow.pyfunc.load_model()</code> (e.g. when serving the model with the <code>mlflow models serve</code> CLI tool). If you want to load just the base pipeline, you can always do so via <code>mlflow.transformers.load_model()</code>.</p>
<p>In the next few cells, we&#x27;ll learn how serve a chat model with a local Transformers pipeline and MLflow, using TinyLlama-1.1B-Chat as an example.</p>
<p>To begin, let&#x27;s go through the original flow of saving a text generation pipeline:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> pipeline</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> mlflow</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">generator </span><span class="token operator">=</span><span class="token plain"> pipeline</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text-generation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  model</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># save the model using the vanilla `text-generation` task type</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">save_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  path</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-text-generation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> transformers_model</span><span class="token operator">=</span><span class="token plain">generator</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> task</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text-generation&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/4268198845.py:11: FutureWarning: The &#x27;transformers&#x27; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
mlflow.transformers.save_model(</pre>
<p>Now, let&#x27;s load the model and use it for inference. Our loaded model is a <code>text-generation</code> pipeline, and let&#x27;s take a look at its signature to see its expected inputs and outputs.</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># load the model for inference</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pyfunc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-text-generation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">signature</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">2024/02/26 21:06:51 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">inputs: 
[string (required)]
outputs: 
[string (required)]
params: 
None</pre>
<p>Unfortunately, it only accepts <code>string</code> as input, which isn&#x27;t directly compatible with a chat interface. When interacting with OpenAI&#x27;s API, for example, we expect to simply be able to input a list of messages. In order to do this with our current model, we&#x27;ll have to write some additional boilerplate:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># first, apply the tokenizer&#x27;s chat template, since the</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># model is tuned to accept prompts in a chat format. this</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># also converts the list of messages to a string.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">messages </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Write me a hello world program in python&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">prompt </span><span class="token operator">=</span><span class="token plain"> generator</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">apply_chat_template</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  messages</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenize</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> add_generation_prompt</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">[&#x27;&lt;|user|&gt;
Write me a hello world program in python&lt;/s&gt;
&lt;|assistant|&gt;
Here&#x27;s a simple hello world program in Python:

```python
print(&quot;Hello, world!&quot;)
```

This program prints the string &quot;Hello, world!&quot; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#x27;]</pre>
<p>Now we&#x27;re getting somewhere, but formatting our messages prior to inference is cumbersome.</p>
<p>Additionally, the output format isn&#x27;t compatible with the OpenAI API spec either--it&#x27;s just a list of strings. If we were looking to evaluate different model backends for our chat app, we&#x27;d have to rewrite some of our client-side code to both format the input, and to parse this new response.</p>
<p>To simplify all this, let&#x27;s just pass in <code>&quot;llm/v1/chat&quot;</code> as the task param when saving the model.</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># save the model using the `&quot;llm/v1/chat&quot;`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># task type instead of `text-generation`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transformers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">save_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  path</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-chat&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> transformers_model</span><span class="token operator">=</span><span class="token plain">generator</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> task</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;llm/v1/chat&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">/var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/609241782.py:3: FutureWarning: The &#x27;transformers&#x27; MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.
mlflow.transformers.save_model(</pre>
<p>Once again, let&#x27;s load the model and inspect the signature:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> mlflow</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pyfunc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;tinyllama-chat&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">signature</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">2024/02/26 21:10:04 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">Loading checkpoint shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">inputs: 
[&#x27;messages&#x27;: Array({content: string (required), name: string (optional), role: string (required)}) (required), &#x27;temperature&#x27;: double (optional), &#x27;max_tokens&#x27;: long (optional), &#x27;stop&#x27;: Array(string) (optional), &#x27;n&#x27;: long (optional), &#x27;stream&#x27;: boolean (optional)]
outputs: 
[&#x27;id&#x27;: string (required), &#x27;object&#x27;: string (required), &#x27;created&#x27;: long (required), &#x27;model&#x27;: string (required), &#x27;choices&#x27;: Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), &#x27;usage&#x27;: {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]
params: 
None</pre>
<p>Now when performing inference, we can pass our messages in a dict as we&#x27;d expect to do when interacting with the OpenAI API. Furthermore, the response we receive back from the model also conforms to the spec.</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">messages </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Write me a hello world program in python&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;messages&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> messages</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">[{&#x27;id&#x27;: &#x27;8435a57d-9895-485e-98d3-95b1cbe007c0&#x27;,
&#x27;object&#x27;: &#x27;chat.completion&#x27;,
&#x27;created&#x27;: 1708949437,
&#x27;model&#x27;: &#x27;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#x27;,
&#x27;usage&#x27;: {&#x27;prompt_tokens&#x27;: 24, &#x27;completion_tokens&#x27;: 71, &#x27;total_tokens&#x27;: 95},
&#x27;choices&#x27;: [{&#x27;index&#x27;: 0,
  &#x27;finish_reason&#x27;: &#x27;stop&#x27;,
  &#x27;message&#x27;: {&#x27;role&#x27;: &#x27;assistant&#x27;,
   &#x27;content&#x27;: &#x27;Here&#x27;s a simple hello world program in Python:

```python
print(&quot;Hello, world!&quot;)
```

This program prints the string &quot;Hello, world!&quot; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&#x27;}}]}]</pre>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="serving-the-chat-model">Serving the Chat Model<a href="#serving-the-chat-model" class="hash-link" aria-label="Direct link to Serving the Chat Model" title="Direct link to Serving the Chat Model">â€‹</a></h3>
<p>To take this example further, let&#x27;s use MLflow to serve our chat model, so we can interact with it like a web API. To do this, we can use the <code>mlflow models serve</code> CLI tool.</p>
<p>In a terminal shell, run:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ mlflow models serve -m tinyllama-chat</span><br></span></code></pre></div></div>
<p>When the server has finished initializing, you should be able to interact with the model via HTTP requests. The input format is almost identical to the format described in the <a href="https://mlflow.org/docs/latest/ml/deployment/index.html#chat" target="_blank" rel="noopener noreferrer">MLflow Deployments Server docs</a>, with the exception that <code>temperature</code> defaults to <code>1.0</code> instead of <code>0.0</code>.</p>
<p>Here&#x27;s a quick example:</p>
<div style="flex-grow:1;min-width:0;margin-top:var(--padding-md);width:100%"><div class="codeBlock_oJcR language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token operator">%</span><span class="token plain">sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl http</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token operator">//</span><span class="token number">127.0</span><span class="token number">.0</span><span class="token number">.1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token number">5000</span><span class="token operator">/</span><span class="token plain">invocations   </span><span class="token operator">-</span><span class="token plain">H </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;Content-Type: application/json&#x27;</span><span class="token plain">   </span><span class="token operator">-</span><span class="token plain">d </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;{ &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write me a hello world program in python&quot;}] }&#x27;</span><span class="token plain">   </span><span class="token operator">|</span><span class="token plain"> jq</span><br></span></code></pre></div></div></div>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                               Dload  Upload   Total   Spent    Left  Speed
100   706  100   617  100    89     25      3  0:00:29  0:00:23  0:00:06   160</pre>
<pre style="margin:0;border-radius:0;background:none;font-size:0.85rem;flex-grow:1;padding:var(--padding-sm)">[
{
  &quot;id&quot;: &quot;fc3d08c3-d37d-420d-a754-50f77eb32a92&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;created&quot;: 1708949465,
  &quot;model&quot;: &quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;,
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 24,
    &quot;completion_tokens&quot;: 71,
    &quot;total_tokens&quot;: 95
  },
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Here&#x27;s a simple hello world program in Python:

```python
print(&quot;Hello, world!&quot;)
```

This program prints the string &quot;Hello, world!&quot; to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.&quot;
      }
    }
  ]
}
]</pre>
<p>It&#x27;s that easy!</p>
<p>You can also call the API with a few optional inference params to adjust the model&#x27;s responses. These map to Transformers pipeline params, and are passed in directly at inference time.</p>
<ul>
<li><code>max_tokens</code> (maps to <code>max_new_tokens</code>): The maximum number of new tokens the model should generate.</li>
<li><code>temperature</code> (maps to <code>temperature</code>): Controls the creativity of the model&#x27;s response. Note that this is not guaranteed to be supported by all models, and in order for this param to have an effect, the pipeline must have been created with <code>do_sample=True</code>.</li>
<li><code>stop</code> (maps to <code>stopping_criteria</code>): A list of tokens at which to stop generation.</li>
</ul>
<p>Note: <code>n</code> does not have an equivalent Transformers pipeline param, and is not supported in queries. However, you can implement a model that consumes the <code>n</code> param using Custom Pyfunc (details below).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">â€‹</a></h2>
<p>In this tutorial, you learned how to create an OpenAI-compatible chat model by specifying &quot;llm/v1/chat&quot; as the task when saving Transformers pipelines.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What&#x27;s next?<a href="#whats-next" class="hash-link" aria-label="Direct link to What&#x27;s next?" title="Direct link to What&#x27;s next?">â€‹</a></h3>
<ul>
<li><a href="https://mlflow.org/docs/latest/llms/chat-model-intro/index.html" target="_blank" rel="noopener noreferrer">Learn about custom ChatModel</a>. If you&#x27;re looking for futrher customization or models outside Transformers, the linked page provides a hand-on guidance for how to build a chat bot with MLflow&#x27;s <code>ChatModel</code> class.</li>
<li><a href="https://mlflow.org/docs/latest/ml/deployment/index.html" target="_blank" rel="noopener noreferrer">More on MLflow AI Gateway</a>. In this tutorial, we saw how to deploy a model using a local server, but MLflow provides many other ways to deploy your models to production. Check out this page to learn more about the different options.</li>
<li><a href="https://mlflow.org/docs/latest/ml/deep-learning/transformers/index.html" target="_blank" rel="noopener noreferrer">More on MLflow&#x27;s Transformers Integration</a>. This page provides a comprehensive overview on MLflow&#x27;s Transformers integrations, along with lots of hands-on guides and notebooks. Learn how to fine-tune models, use prompt templates, and more!</li>
<li><a href="https://mlflow.org/docs/latest/genai/index.html" target="_blank" rel="noopener noreferrer">Other LLM Integrations</a>. Aside from Transformers, MLflow has integrations with many other popular LLM libraries, such as Langchain and OpenAI.</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/mlflow/mlflow/edit/master/docs/docs/classic-ml/deep-learning/transformers/tutorials/conversational/pyfunc-chat-model.ipynb" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/latest/ml/deep-learning/transformers/tutorials/conversational/conversational-model/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction to Conversational Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/latest/ml/deep-learning/transformers/tutorials/fine-tuning/transformers-fine-tuning/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Introduction to Fine Tuning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning objectives</a></li><li><a href="#building-a-chat-model" class="table-of-contents__link toc-highlight">Building a Chat Model</a></li><li><a href="#serving-the-chat-model" class="table-of-contents__link toc-highlight">Serving the Chat Model</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a><ul><li><a href="#whats-next" class="table-of-contents__link toc-highlight">What&#39;s next?</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="pb-30 flex flex-col pt-30 bg-bottom bg-no-repeat bg-cover bg-size-[auto_360px] 2xl:bg-size-[100%_360px] bg-brand-black" style="background-image:linear-gradient(#0e1414 60%, #0c141400 90% 100%),url(/docs/latest/assets/images/footer-blue-bg-2664116e3788212e9f50cfc6024806f8.png)"><div class="flex flex-row justify-between items-start md:items-center px-6 lg:px-20 gap-10 xs:gap-0 max-w-container"><div class="flex flex-col gap-8"><svg xmlns="http://www.w3.org/2000/svg" width="109" height="40" fill="none" viewBox="0 0 109 40" class="h-[36px] shrink-0"><path fill="#fff" d="M0 31.032v-15.53h3.543v1.968c.893-1.594 2.84-2.425 4.593-2.425 2.042 0 3.828.926 4.658 2.745 1.216-2.043 3.034-2.745 5.043-2.745 2.81 0 5.49 1.787 5.49 5.904v10.083h-3.574v-9.478c0-1.818-.926-3.19-3-3.19-1.947 0-3.224 1.53-3.224 3.445v9.22H9.893v-9.475c0-1.786-.898-3.18-3.003-3.18-1.977 0-3.223 1.467-3.223 3.444v9.221ZM27.855 31.032V7.929h3.703v23.103ZM30.07 39.487c.833.232 1.582.383 3.17.383 2.953 0 6.436-1.665 7.353-6.339l3.786-18.739h5.629l.687-3.117h-5.686l.765-3.725c.586-2.892 2.186-4.358 4.754-4.358.668 0 .48.058 1.076.17L52.427.57c-.793-.237-1.503-.379-3.049-.379a7.32 7.32 0 0 0-4.528 1.484c-1.441 1.114-2.393 2.75-2.827 4.863l-1.066 5.137h-5.034l-.411 3.12h4.823L36.859 32.12c-.383 1.965-1.5 4.315-4.708 4.315-.727 0-.463-.055-1.121-.162ZM53.342 30.905H49.64l5.074-23.309h3.701ZM71.807 16.477A7.962 7.962 0 0 0 60.97 27.904l2.425-1.78a5.005 5.005 0 0 1 3.845-8.145v1.895ZM62.618 29.472a7.962 7.962 0 0 0 10.836-11.428l-2.425 1.78a5.005 5.005 0 0 1-3.845 8.145v-1.894ZM78.092 15.493h4.044l.823 10.612 5.759-10.612 3.839.055 1.508 10.557 5.074-10.612 3.701.055-7.678 15.493H91.46l-1.783-11.106-5.895 11.106h-3.84ZM105.072 15.768h-.766v-.266h1.845v.272h-.765v2.243h-.314ZM106.614 15.502h.383l.482 1.34q.091.259.178.524h.018c.059-.176.113-.352.172-.524l.478-1.34h.383v2.515h-.298V16.63c0-.22.024-.523.04-.747h-.016l-.191.574-.475 1.304h-.208l-.481-1.302-.191-.574h-.015c.017.224.042.527.042.747v1.387h-.291Z"></path></svg><div class="text-xs text-left md:text-nowrap md:w-0">Â© 2025 MLflow Project, a Series of LF Projects, LLC.</div></div><div class="flex flex-col flex-wrap justify-end md:text-right md:flex-row gap-x-10 lg:gap-x-20 gap-y-5 w-2/5 md:w-auto md:pt-2 max-w-fit"><div><a href="https://mlflow.org" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Components</a></div><div><a href="https://mlflow.org/releases" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Releases</a></div><div><a href="https://mlflow.org/blog" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Blog</a></div><div><a class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white" href="/docs/latest/">Docs</a></div><div><a href="https://mlflow.org/ambassadors" target="_blank" rel="noopener noreferrer" class="text-[15px] font-medium no-underline hover:no-underline transition-opacity hover:opacity-80 text-white visited:text-white">Ambassador Program</a></div></div></div></footer></div>
</body>
</html>