

<!DOCTYPE html>
<!-- source: docs/source/model/models-from-code.rst -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Models From Code Guide</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/model/models-from-code.html">
  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="MLflow 2.17.3.dev0 documentation" href="../index.html"/>
        <link rel="up" title="MLflow Models" href="../models.html"/>
        <link rel="next" title="MLflow Model Registry" href="/../model-registry.html"/>
        <link rel="prev" title="MLflow Signature Playground Notebook" href="/notebooks/signature_examples.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../_static/jquery.js"></script>
<script type="text/javascript" src="../_static/underscore.js"></script>
<script type="text/javascript" src="../_static/doctools.js"></script>
<script type="text/javascript" src="../_static/tabs.js"></script>
<script type="text/javascript" src="../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../index.html" class="wy-nav-top-logo"
      ><img src="../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.17.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home"><img src="../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../new-features/index.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llms/index.html">LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llms/tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects.html">MLflow Projects</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../models.html">MLflow Models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../models.html#storage-format">Storage Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#managing-model-dependencies">Managing Model Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#model-signatures-and-input-examples">Model Signatures And Input Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#model-api">Model API</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../models.html#models-from-code">Models From Code</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Models From Code Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#differences-with-legacy-serialization">Differences with Legacy serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#core-requirements-for-using-models-from-code">Core requirements for using Models From Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-models-from-code-in-a-jupyter-notebook">Using Models From Code in a Jupyter Notebook</a></li>
<li class="toctree-l4"><a class="reference internal" href="#examples-of-using-models-from-code">Examples of Using Models From Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#faq-for-models-from-code">FAQ for Models from Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#built-in-model-flavors">Built-In Model Flavors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#model-evaluation">Model Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#model-customization">Model Customization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#built-in-deployment-tools">Built-In Deployment Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#export-a-python-function-model-as-an-apache-spark-udf">Export a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model as an Apache Spark UDF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#deployment-to-custom-targets">Deployment to Custom Targets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html#id11">Community Model Flavors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../models.html">MLflow Models</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Models From Code Guide</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/model/models-from-code.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="models-from-code-guide">
<h1>Models From Code Guide<a class="headerlink" href="#models-from-code-guide" title="Permalink to this headline"> </a></h1>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Models from Code is available in MLflow 2.12.2 and above. If you are using a version earlier than what supports this feature,
you are required to use the legacy serialization methods outlined in the <a class="reference external" href="../models.html#custom-python-models">Custom Python Model</a> documentation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Models from code is only available for <a class="reference external" href="../llms/langchain/index.html">LangChain</a>, <cite>LlamaIndex &lt;../llm/llama-index/index.html&gt;</cite>, and custom <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> (PythonModel instances) models. If you are
using other libraries directly, using the provided saving and logging functionality within specific model flavors is recommended.</p>
</div>
<p>The models from code feature is a comprehensive overhaul of the process of defining, storing, and loading both custom models and specific flavor
implementations that do not depend on serialized model weights (such as <a class="reference external" href="../llms/langchain/index.html">LangChain</a> and
<a class="reference external" href="../llms/llama-index/index.html">LlamaIndex</a>).</p>
<p>The key difference between legacy serialization of these models and the Models from Code approach is in how a model is represented during serialization.</p>
<p>In the legacy approach, serialization is done on the model object using either <code class="docutils literal notranslate"><span class="pre">cloudpickle</span></code> (custom pyfunc and LangChain) or a custom serializer that has incomplete coverage
(in the case of LlamaIndex) of all functionality within the underlying package. For custom pyfunc, the usage of <code class="docutils literal notranslate"><span class="pre">cloudpickle</span></code> to serialize object instances creates a binary file that is used to reconstruct the object when loaded.</p>
<p>In models from code, for the model types that are supported, a simple script is saved with the definition of either the custom pyfunc or the flavor’s
interface (i.e., in the case of LangChain, we can define and mark an LCEL chain directly as a model within a script).</p>
<p>The greatest gain associated with using models from code for custom <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> and supported library implementations is in the reduction of repetitive trial-and-error debugging
that can occur when working on an implementation. The workflow shown below illustrates how these two methdologies compare when working on a solution for a custom model:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/models_from_code_journey.png"><img alt="Models from code comparison with legacy serialization" src="../_images/models_from_code_journey.png" style="width: 80%;" /></a>
</div>
<div class="section" id="differences-with-legacy-serialization">
<h2>Differences with Legacy serialization<a class="headerlink" href="#differences-with-legacy-serialization" title="Permalink to this headline"> </a></h2>
<p>In the legacy mode for custom models, an instance of your subclassed <a class="reference internal" href="../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel" title="mlflow.pyfunc.PythonModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlflow.pyfunc.PythonModel</span></code></a> is submitted in the call to <code class="docutils literal notranslate"><span class="pre">log_model</span></code>. When called via an object
reference, MLflow will utilize <code class="docutils literal notranslate"><span class="pre">cloudpickle</span></code> to attempt to serialize your object.</p>
<p>In the native flavor serialization for <code class="docutils literal notranslate"><span class="pre">LangChain</span></code>, <code class="docutils literal notranslate"><span class="pre">cloudpickle</span></code> is used to store object references. However, only a subset of all object types that can be
used within <code class="docutils literal notranslate"><span class="pre">LangChain</span></code> are available for serializing due to external state references or the use of lambda functions within the APIs. <code class="docutils literal notranslate"><span class="pre">LlamaIndex</span></code>, on the
other hand, utilizes a custom serializer in the native implementation of the flavor that does not cover all possible uses of the library due to the need for
excessively complex implementations to support edge case features within the library.</p>
<p>In models from code, instead of passing an object reference to an instance of your custom model, you will simply pass a path reference to a script that
contains your model definition. When this mode is employed, MLflow will simply execute this script (along with any <code class="docutils literal notranslate"><span class="pre">code_paths</span></code> dependencies prior to running
the main script) in the execution environment and instantiating whichever object you define in the call to <a class="reference internal" href="../python_api/mlflow.models.html#mlflow.models.set_model" title="mlflow.models.set_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.set_model()</span></code></a>, assigning that
object as the inference target.</p>
<p>At no point in this process are there dependencies on serialization libraries such as <a class="reference external" href="https://docs.python.org/3/library/pickle.html">pickle</a> or
<a class="reference external" href="https://pypi.org/project/cloudpickle/1.1.1/">cloudpickle</a>, removing the broad limitations that these serialization packages have, such as:</p>
<ul class="simple">
<li><p><strong>Portability and Compatiblility</strong>: Loading a pickle or cloudpickle file in a Python version that was different than the one used to serialize the object does not guarantee compatiblity.</p></li>
<li><p><strong>Complex Object Serialization</strong>: File handles, sockets, external connections, dynamic references, lambda functions and system resources are unavailable for pickling.</p></li>
<li><p><strong>Readability</strong>: Pickle and CloudPickle both store their serialized objects in a binary format that is impossible to read by humans.</p></li>
<li><p><strong>Performance</strong>: Object serialization and dependency inspection can be very slow, particularly for complex implementations with many code reference dependencies.</p></li>
</ul>
</div>
<div class="section" id="core-requirements-for-using-models-from-code">
<h2>Core requirements for using Models From Code<a class="headerlink" href="#core-requirements-for-using-models-from-code" title="Permalink to this headline"> </a></h2>
<p>There are some important concepts to be aware of when using the models from code feature, as there are operations that are performed when logging a model
via a script that may not be immediately apparent.</p>
<ul class="simple">
<li><p><strong>Imports</strong>: Models from code does not capture external references for non-pip installable packages, just as the legacy <code class="docutils literal notranslate"><span class="pre">cloudpickle</span></code> implementation does not. If you have external references (see the examples below), you must define these dependencies via <code class="docutils literal notranslate"><span class="pre">code_paths</span></code> arguments.</p></li>
<li><p><strong>Execution during logging</strong>: In order to validate that the script file that you’re logging is valid, the code will be executed before being written to disk, exactly as other methods of model logging.</p></li>
<li><p><strong>Requirements inference</strong>: Packages that are imported at the top of your defined model script will be inferred as requirements if they are installable from PyPI, regardless of whether you use them in the model execution logic or not.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you define import statements that are never used within your script, these will still be included in the requirements listing. It is recommended to use a linter
that is capable of determining unused import statements while writing your implementation so that you are not including irrelevant package dependencies.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When logging models from code, make sure that your code does not contain any sensitive information, such as API keys, passwords, or other confidential data. The code will be stored in plain text in the MLflow model artifact, and anyone with access to the artifact will be able to view the code.</p>
</div>
</div>
<div class="section" id="using-models-from-code-in-a-jupyter-notebook">
<h2>Using Models From Code in a Jupyter Notebook<a class="headerlink" href="#using-models-from-code-in-a-jupyter-notebook" title="Permalink to this headline"> </a></h2>
<p><a class="reference external" href="https://jupyter.org/">Jupyter</a> (IPython Notebooks) are a very convenient way to work with AI applications and modeling in general. One slight limitation that they
have is in their cell-based execution model. Due to the nature of how they are defined and run, the models from code feature does not directly support defining
a notebook as a model. Rather, this feature requires that models are defined as Python scripts (the file extension <strong>must end in ‘.py’</strong>).</p>
<p>Fortunately, the folks that maintain the core kernel that Jupyter uses (<a class="reference external" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">IPython</a>) have created a
number of magic commands that are usable within notebooks to enhance the usability of notebooks as a development environment for AI practitioners. One of the most
useful magic commands that can be used within any notebook environment that is based upon IPython (<code class="docutils literal notranslate"><span class="pre">Jupyter</span></code>, <code class="docutils literal notranslate"><span class="pre">Databricks</span> <span class="pre">Notebooks</span></code>, etc.) is the <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> command.</p>
<p>The <a class="reference external" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile">%%writefile</a> magic command, when written as the first line of a notebook
cell, will capture the contents of the cell (not the entire notebook, mind you, only the current cell scope) with the exception of the magic command itself and write
those contents to the file that you define.</p>
<p>For example, running the following in a notebook:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%%writefile &quot;./hello.py&quot;

print(&quot;hello!&quot;)
</pre></div>
</div>
<p>Will result in a file being created, located in the same directory as your notebook, that contains:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hello!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is an optional <code class="docutils literal notranslate"><span class="pre">-a</span></code> append command that can be used with the <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> magic command. This option will <strong>append</strong> the cell contents to the file
being targeted for saving the cell contents to. It is <strong>not recommended</strong> to use this option due to the chances of creating difficult-to-debug overrides within
a script that could contain multiple copies of your model definition logic. It is recommended to use the default behavior of <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code>, which is to overwrite
the local file each time that the cell is executed to ensure that the state of your cell’s contents are always reflected in the saved script file.</p>
</div>
</div>
<div class="section" id="examples-of-using-models-from-code">
<h2>Examples of Using Models From Code<a class="headerlink" href="#examples-of-using-models-from-code" title="Permalink to this headline"> </a></h2>
<p>Each of these examples will show usage of the <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> magic command at the top of the script definition cell block in order to simulate defining the model code or other
dependencies from within a single notebook. If you are writing your implementations within an IDE or a text editor, do not place this magic command at the top of your
script.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Simple Example</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Models with Code Paths dependencies</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Models From Code with LangChain</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><h3>Building a simple Models From Code model</h3><div class="line-block">
<div class="line"><br /></div>
</div>
<p>In this example, we will define a very basic  model that, when called via <code class="docutils literal notranslate"><span class="pre">predict()</span></code>, will utilize the input float value as an exponent to the number <code class="docutils literal notranslate"><span class="pre">2</span></code>.
The first code block, repesenting a discrete notebook cell, will create a file named <code class="docutils literal notranslate"><span class="pre">basic.py</span></code> in the same directory as the notebook. The contents of this
file will be the model definition <code class="docutils literal notranslate"><span class="pre">BasicModel</span></code>, as well as the import statements and the MLflow function <code class="docutils literal notranslate"><span class="pre">set_model</span></code> that will instantiate an instance of
this model to be used for inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If running in a Jupyter or Databricks notebook cell, uncomment the following line:</span>
<span class="c1"># %%writefile &quot;./basic.py&quot;</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">mlflow.pyfunc</span> <span class="kn">import</span> <span class="n">PythonModel</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">set_model</span>


<span class="k">class</span> <span class="nc">BasicModel</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numbers</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="o">**</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="n">model_input</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>


<span class="c1"># Specify which definition in this script represents the model instance</span>
<span class="n">set_model</span><span class="p">(</span><span class="n">BasicModel</span><span class="p">())</span>
</pre></div>
</div>
<p>The next section shows another cell that contains the logging logic.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;Basic Model From Code&quot;</span><span class="p">)</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;basic.py&quot;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>  <span class="c1"># Define the model as the path to the script that was just saved</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;arithemtic_model&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="p">[</span><span class="mf">42.0</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Looking at this stored model within the MLflow UI, we can see that the script in the first cell was recorded as an artifact to the run.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/basic_model_from_code_ui.png"><img alt="The MLflow UI showing the stored model code as a serialized python script" src="../_images/basic_model_from_code_ui.png" style="width: 80%;" /></a>
</div>
<p>When we load this model via <code class="docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code>, this script will be executed and an instance of <code class="docutils literal notranslate"><span class="pre">BasicModel</span></code> will be constructed, exposing the <code class="docutils literal notranslate"><span class="pre">predict</span></code>
method as our entry point for inference, just as with the alternative legacy mode of logging a custom model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">])</span>

<span class="c1"># or, with a Pandas DataFrame input</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]))</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><h3>Using Models from Code with code_paths dependencies</h3><div class="line-block">
<div class="line"><br /></div>
</div>
<p>In this example, we will explore a more complex scenario that demonstrates how to work with multiple Python scripts and leverage the <code class="docutils literal notranslate"><span class="pre">code_paths</span></code>
feature in MLflow for model management. Specifically, we will define a simple script that contains a function that performs basic arithmetic
operations, and then use this function within an <code class="docutils literal notranslate"><span class="pre">AddModel</span></code> custom <code class="docutils literal notranslate"><span class="pre">PythonModel</span></code> that we will define in a separate script.
This model will be logged with MLflow, allowing us to perform predictions using the stored model.</p>
<blockquote>
<div><p>To learn more about the <code class="docutils literal notranslate"><span class="pre">code_paths</span></code> feature in MLflow, see the <a class="reference external" href="../model/dependencies.html#caveats-of-code-paths-option">guidelines on usage here</a>.</p>
</div></blockquote>
<p>This tutorial will show you how to:</p>
<ul class="simple">
<li><p>Create multiple Python files from within a Jupyter notebook.</p></li>
<li><p>Log a custom model with MLflow that relies on external code defined in another file.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">code_paths</span></code> feature to include additional scripts when logging the model, ensuring that all dependencies are available when the model is loaded for inference.</p></li>
</ul>
<h4>Defining a dependent code script</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>In the first step, we define our <code class="docutils literal notranslate"><span class="pre">add</span></code> function in a file named <code class="docutils literal notranslate"><span class="pre">calculator.py</span></code>, including the magic <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> command if we’re running in a notebook cell:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If running in a Jupyter or Databricks notebook cell, uncomment the following line:</span>
<span class="c1"># %%writefile &quot;./calculator.py&quot;</span>


<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
<h4>Defining the model as a Python file</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Next, we create a new file, <code class="docutils literal notranslate"><span class="pre">math_model.py</span></code>, which contains the <code class="docutils literal notranslate"><span class="pre">AddModel</span></code> class. This script will be responsible for importing the <code class="docutils literal notranslate"><span class="pre">add</span></code> function from our external script, defining our model,
performing predictions, and validating the input data types. The predict method will leverage the <code class="docutils literal notranslate"><span class="pre">add</span></code> function to perform the addition of two numbers provided as input.</p>
<p>The following code block writes the <code class="docutils literal notranslate"><span class="pre">AddModel</span></code> class definition to <code class="docutils literal notranslate"><span class="pre">math_model.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If running in a Jupyter or Databricks notebook cell, uncomment the following line:</span>
<span class="c1"># %%writefile &quot;./math_model.py&quot;</span>

<span class="kn">from</span> <span class="nn">mlflow.pyfunc</span> <span class="kn">import</span> <span class="n">PythonModel</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">set_model</span>

<span class="kn">from</span> <span class="nn">calculator</span> <span class="kn">import</span> <span class="n">add</span>


<span class="k">class</span> <span class="nc">AddModel</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="n">model_input</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">model_input</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>


<span class="n">set_model</span><span class="p">(</span><span class="n">AddModel</span><span class="p">())</span>
</pre></div>
</div>
<p>This model introduces error handling by checking the existence and types of the inputs, ensuring robustness. It serves as a practical example of
how custom logic can be encapsulated within an MLflow model while leveraging external dependencies.</p>
<h4>Logging the Model from Code</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Once the <code class="docutils literal notranslate"><span class="pre">AddModel</span></code> custom Python model is defined, we can proceed to log it with MLflow. This process involves specifying the path to the <code class="docutils literal notranslate"><span class="pre">math_model.py</span></code>
script and using the <code class="docutils literal notranslate"><span class="pre">code_paths</span></code> parameter to include <code class="docutils literal notranslate"><span class="pre">calculator.py</span></code> as a dependency. This ensures that when the model is loaded in
a different environment or on another machine, all necessary code files are available for proper execution.</p>
<p>The following code block demonstrates how to log the model using MLflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;Arithemtic Model From Code&quot;</span><span class="p">)</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;math_model.py&quot;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>  <span class="c1"># The model is defined as the path to the script containing the model definition</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;arithemtic_model&quot;</span><span class="p">,</span>
        <span class="n">code_paths</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;calculator.py&quot;</span>
        <span class="p">],</span>  <span class="c1"># dependency definition included for the model to successfully import the implementation</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>This step registers the <code class="docutils literal notranslate"><span class="pre">AddModel</span></code> model with MLflow, ensuring that both the primary model script and its dependencies are stored as
artifacts. By including <code class="docutils literal notranslate"><span class="pre">calculator.py</span></code> in the <code class="docutils literal notranslate"><span class="pre">code_paths</span></code> argument, we ensure that the model can be reliably reloaded and used for
predictions, regardless of the environment in which it is deployed.</p>
<h4>Loading and Viewing the model</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>After logging the model, it can be loaded back into the notebook or any other environment that has access to the MLflow tracking server.
When the model is loaded, the <code class="docutils literal notranslate"><span class="pre">calculator.py</span></code> script will be executed along with the <code class="docutils literal notranslate"><span class="pre">math_model.py</span></code> script, ensuring that the
<code class="docutils literal notranslate"><span class="pre">add</span></code> function is available for use by the <code class="docutils literal notranslate"><span class="pre">ArithmeticModel</span></code>’s script’s import statement.</p>
<p>The following code block demonstrates how to load the model and make predictions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_model_from_code</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">my_model_from_code</span><span class="o">.</span><span class="n">predict</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="mi">9001</span><span class="p">})</span>
</pre></div>
</div>
<p>This example showcases the model’s ability to handle different numerical inputs, perform addition, and maintain a history of calculations.
The output of these predictions includes both the result of the arithmetic operation and the history log, which can be useful for auditing and
tracing the computations performed by the model.</p>
<p>Looking at the stored model within the MLflow UI, you can see that both the <code class="docutils literal notranslate"><span class="pre">math_model.py</span></code> and <code class="docutils literal notranslate"><span class="pre">calculator.py</span></code> scripts are recorded as
artifacts in the run. This comprehensive logging allows you to track not just the model’s parameters and metrics but also the code that
defines its behavior, making it visible and debuggable directly from within the UI.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/model_from_code_code_paths.png"><img alt="The MLflow UI showing models from code usage along with dependent code_paths script stored in the model artifacts" src="../_images/model_from_code_code_paths.png" style="width: 80%;" /></a>
</div>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><h3>MLflow's native LangChain Models from Code support</h3><div class="line-block">
<div class="line"><br /></div>
</div>
<p>In this slightly more advanced example, we will explore how to use the <a class="reference external" href="../llms/langchain/index.html">MLflow LangChain integration</a> to define
and manage a chain of operations for an AI model. This chain will help generate landscape design recommendations based on specific regional
and area-based inputs. The example showcases how to define a custom prompt, use a large language model (LLM) for generating responses, and
log the entire setup as a model using MLflow’s tracking features.</p>
<p>This tutorial will guide you through:</p>
<ul class="simple">
<li><p>Writing a script to define a custom LangChain model that processes input data to generate landscape design recommendations.</p></li>
<li><p>Logging the model with MLflow using the langchain integration, ensuring the entire chain of operations is captured.</p></li>
<li><p>Loading and using the logged model for making predictions in different contexts.</p></li>
</ul>
<h4>Defining the Model with LCEL</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>First, we will create a Python script named <code class="docutils literal notranslate"><span class="pre">mfc.py</span></code>, which defines the chain of operations for generating landscape design recommendations.
This script utilizes the LangChain library along with MLflow’s <code class="docutils literal notranslate"><span class="pre">autolog</span></code> feature for enabling the <a class="reference external" href="../llms/tracing/index.html">capture of traces</a>.</p>
<p>In this script:</p>
<ul class="simple">
<li><p><strong>Custom Functions</strong> (get_region and get_area): These functions extract specific pieces of information (region and area) from the input data.</p></li>
<li><p><strong>Prompt Template</strong>: A <code class="docutils literal notranslate"><span class="pre">PromptTemplate</span></code> is defined to structure the input for the language model, specifying the task and context in which the model will operate.</p></li>
<li><p><strong>Model Definition</strong>: We use the <code class="docutils literal notranslate"><span class="pre">ChatOpenAI</span></code> model to generate responses based on the structured prompt.</p></li>
<li><p><strong>Chain Creation</strong>: The chain is created by connecting the input processing, prompt template, model invocation, and output parsing steps.</p></li>
</ul>
<p>The following code block writes this chain definition to the mfc.py file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If running in a Jupyter or Databricks notebook cell, uncomment the following line:</span>
<span class="c1"># %%writefile &quot;./mfc.py&quot;</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="kn">import</span> <span class="nn">mlflow</span>


<span class="k">def</span> <span class="nf">get_region</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
    <span class="n">default</span> <span class="o">=</span> <span class="s2">&quot;Virginia, USA&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;region&quot;</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default</span>


<span class="k">def</span> <span class="nf">get_area</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
    <span class="n">default</span> <span class="o">=</span> <span class="s2">&quot;5000 square feet&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;area&quot;</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default</span>


<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;You are a highly accomplished landscape designer that provides suggestions for landscape design decisions in a particular&quot;</span>
    <span class="s2">&quot; geographic region. Your goal is to suggest low-maintenance hardscape and landscape options that involve the use of materials and&quot;</span>
    <span class="s2">&quot; plants that are native to the region mentioned. As part of the recommendations, a general estimate for the job of creating the&quot;</span>
    <span class="s2">&quot; project should be provided based on the square footage estimate. The region is: </span><span class="si">{region}</span><span class="s2"> and the square footage estimate is:&quot;</span>
    <span class="s2">&quot; </span><span class="si">{area}</span><span class="s2">. Recommendations should be for a moderately sophisticated suburban housing community within the region.&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;region&quot;</span><span class="p">,</span> <span class="s2">&quot;area&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">4096</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;region&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">get_region</span><span class="p">),</span>
        <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">get_area</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">model</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</pre></div>
</div>
<p>This script encapsulates the logic required to construct the full chain using the
<a class="reference external" href="https://python.langchain.com/v0.1/docs/expression_language/">LangChain Expression Language (LCEL)</a>, as well as the custom default logic
that the chain will use for input processing. The defined chain is then specified as the model’s interface object using the <code class="docutils literal notranslate"><span class="pre">set_model</span></code> function.</p>
<h4>Logging the model using Models from Code</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Once the chain is defined in <code class="docutils literal notranslate"><span class="pre">mfc.py</span></code>, we log it using MLflow. This step involves specifying the path to the script that contains the chain
definition and using MLflow’s <code class="docutils literal notranslate"><span class="pre">langchain</span></code> integration to ensure that all aspects of the chain are captured.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">input_example</span></code> provided to the logging function serves as a template to demonstrate how the model should be invoked. This example is
also stored as part of the logged model, making it easier to understand and replicate the model’s use case.</p>
<p>The following code block demonstrates how to log the LangChain model using MLflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;Landscaping&quot;</span><span class="p">)</span>

<span class="n">chain_path</span> <span class="o">=</span> <span class="s2">&quot;./mfc.py&quot;</span>

<span class="n">input_example</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;region&quot;</span><span class="p">:</span> <span class="s2">&quot;Austin, TX, USA&quot;</span><span class="p">,</span>
                <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="s2">&quot;1750 square feet&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">lc_model</span><span class="o">=</span><span class="n">chain_path</span><span class="p">,</span>  <span class="c1"># Defining the model as the script containing the chain definition and the set_model call</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;chain&quot;</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>In this step, the entire chain of operations, from input processing to AI model inference, is logged as a single, cohesive model. Avoiding the
potential complexities associated with object serialization of the defined chain components, using the models from code feature ensures that
the exact code and logic that were used to develop and test a chain is what is executed when deploying the application without the risk of
incomplete or non-existent serialization capabilities.</p>
<h4>Loading and Viewing the Model</h4><div class="line-block">
<div class="line"><br /></div>
</div>
<p>After logging the model, it can be loaded back into your environment for inference. This step demonstrates how to load the chain and
use it to generate landscape design recommendations based on new input data.</p>
<p>The following code block shows how to load the model and run predictions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the model and run inference</span>
<span class="n">landscape_chain</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">question</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;region&quot;</span><span class="p">:</span> <span class="s2">&quot;Raleigh, North Carolina USA&quot;</span><span class="p">,</span>
                <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="s2">&quot;3850 square feet&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">landscape_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
</pre></div>
</div>
<p>This code block demonstrates how to invoke the loaded chain with new data, generating a response that provides landscape design suggestions
tailored to the specified region and area.</p>
<p>Once the model is logged, you can explore its details in the MLflow UI. The interface will show the script <code class="docutils literal notranslate"><span class="pre">mfc.py</span></code> as an artifact of the
logged model, along with the chain definition and associated metadata. This allows you to easily review the model’s components,
input examples, and other key information.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/langchain_model_from_code.png"><img alt="The MLflow UI showing models from code usage and the mfc.py script that defines the LangChain LCEL chain definition" src="../_images/langchain_model_from_code.png" style="width: 80%;" /></a>
</div>
<p>When you load this model using <a class="reference internal" href="../python_api/mlflow.langchain.html#mlflow.langchain.load_model" title="mlflow.langchain.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.load_model()</span></code></a>, the entire chain defined in <code class="docutils literal notranslate"><span class="pre">mfc.py</span></code> is executed, and the model
behaves as expected, generating AI-driven recommendations for landscape design.</p>
</div></div>
</div>
<div class="section" id="faq-for-models-from-code">
<h2>FAQ for Models from Code<a class="headerlink" href="#faq-for-models-from-code" title="Permalink to this headline"> </a></h2>
<p>There are several aspects of using the models from code feature for logging models that you should be aware of. While the behavior is similar to that of
using legacy model serialization, there are a few notable differences that you will need to make to your development workflow and code architecture.</p>
<div class="section" id="dependency-management-and-requirements">
<h3>Dependency Management and Requirements<a class="headerlink" href="#dependency-management-and-requirements" title="Permalink to this headline"> </a></h3>
<p>Proper management of dependencies and requirements is crucial for ensuring that your model can be loaded or deployed in new environments.</p>
<div class="section" id="why-did-i-get-a-nameerror-when-loading-my-model-from-a-saved-script">
<h4>Why did I get a NameError when loading my model from a saved script?<a class="headerlink" href="#why-did-i-get-a-nameerror-when-loading-my-model-from-a-saved-script" title="Permalink to this headline"> </a></h4>
<p>When defining the script (or cell, if developing in a notebook), ensure that all of the required import statements are defined within the script.
Failing to include the import dependencies will not only result in a name resolution error, but the requirement dependencies will not be included
in the model’s <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file.</p>
</div>
<div class="section" id="loading-my-model-is-giving-me-an-importerror">
<h4>Loading my model is giving me an ImportError.<a class="headerlink" href="#loading-my-model-is-giving-me-an-importerror" title="Permalink to this headline"> </a></h4>
<p>If you have external dependencies to your model definition script that are not available on PyPI, you must include these references using the
<code class="docutils literal notranslate"><span class="pre">code_paths</span></code> argument when logging or saving your model. You may need to manually add import dependencies from these external scripts to the
<code class="docutils literal notranslate"><span class="pre">extra_pip_requirements</span></code> argument when logging your model to ensure that all required dependencies are available to your model during loading.</p>
</div>
<div class="section" id="why-is-my-requirements-txt-file-filled-with-packages-that-my-model-isn-t-using">
<h4>Why is my requirements.txt file filled with packages that my model isn’t using?<a class="headerlink" href="#why-is-my-requirements-txt-file-filled-with-packages-that-my-model-isn-t-using" title="Permalink to this headline"> </a></h4>
<p>MLflow will build the list of requirements from a models from code script based on the module-level import statements. There isn’t an inspection
process that runs to validate whether your model’s logic requires everything that is stated as an import. It is highly recommended to prune your
imports within these scripts to only include the minimal required import statements that your model requires to function. Having excessive imports
of large packages will introduce installation delays when loading or deploying your model as well as increased memory pressure in your deployed
inference environment.</p>
</div>
</div>
<div class="section" id="logging-using-models-from-code">
<h3>Logging using Models From Code<a class="headerlink" href="#logging-using-models-from-code" title="Permalink to this headline"> </a></h3>
<p>When logging models from a defined Python file, you will encounter some slight differences between the legacy model serialization process of
supplying an object reference.</p>
<div class="section" id="i-accidentally-included-an-api-key-in-my-script-what-do-i-do">
<h4>I accidentally included an API Key in my script. What do I do?<a class="headerlink" href="#i-accidentally-included-an-api-key-in-my-script-what-do-i-do" title="Permalink to this headline"> </a></h4>
<p>Due to the fact that the models from code feature stores your script definition in plain text, completely visible within the MLflow UI’s artifact viewer,
including sensitive data such as access keys or other authorization-based secrets is a security risk. If you have accidentally left a sensitive
key defined directly in your script when logging your model, it is advisable to:</p>
<ol class="arabic simple">
<li><p>Delete the MLflow run that contains the leaked key. You can do this via the UI or through <a class="reference external" href="../python_api/mlflow.client.html#mlflow.client.MlflowClient.delete_run">the delete_run API</a>.</p></li>
<li><p>Delete the artifacts associated with the run. You can do this via the <a class="reference external" href="../cli.html#mlflow-gc">mlflow gc</a> cli command.</p></li>
<li><p>Rotate your sensitive keys by generating a new key and deleting the leaked secret from the source system administration interface.</p></li>
<li><p>Re-log the model to a new run, making sure to not set sensitive keys in your model definition script.</p></li>
</ol>
</div>
<div class="section" id="why-is-my-model-getting-executed-when-i-log-it">
<h4>Why is my model getting executed when I log it?<a class="headerlink" href="#why-is-my-model-getting-executed-when-i-log-it" title="Permalink to this headline"> </a></h4>
<p>In order to validate that the code is executable within the python file that defines a model, MLflow will instantiate the object that is defined as a model within
the <code class="docutils literal notranslate"><span class="pre">set_model</span></code> API. If you have external calls that are made during the initialization of your model, these will be made to ensure that your code is executable
prior to logging. If such calls require authenticated access to services, please ensure that the environment that you are logging your model from has the
appropriate authentication configured so that your code can run.</p>
</div>
</div>
</div>
<div class="section" id="additional-resources">
<h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline"> </a></h2>
<p>For additional related context topics that can enhance your understanding of MLflow’s “Models From Code” feature, consider exploring the following sections in the MLflow documentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="../models.html#model-api">Model API Documentation</a></p></li>
<li><p><a class="reference external" href="../model/dependencies.html">Managing Dependencies in MLflow Models</a></p></li>
</ul>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="notebooks/signature_examples.html" class="btn btn-neutral" title="MLflow Signature Playground Notebook" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="../model-registry.html" class="btn btn-neutral" title="MLflow Model Registry" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../',
      VERSION:'2.17.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../_static/clippy.svg";</script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>